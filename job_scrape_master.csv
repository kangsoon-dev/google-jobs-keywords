scrape_time,job_title,publisher,time_posted,salary,job_type,desc
29-Apr-2022 T11:46,Data Engineer,ITCONNECTUS PTE. LTD.,16 hours ago,$5K–$13K a month,Full–time,"Immediate joiners

Requirements
• Technical Data Architect who is strong on Data Warehousing, Big Data, Data Analytics & Data Science for solutioning
• Strong Google Cloud Platform Data Components – BigQuery, BigTable, CloudSQL, Dataproc, Data Flow, Data Fusion, Etc
• Demonstrate extensive skills and success in the implementation of technology projects within a professional environment, with a particular focus on data engineering
• Experienced in delivering Medium to Big Enterprise Data Projects
• GCP Experience including and not limited to MPP systems, Database systems, ETL and ELT systems and Data Flow compute Good to have skills
• A high-level understanding of AI and ML technologies and desire to develop your skills in these areas of emerging technology.
• Should be able to advise the best of breed for the client solutions; Skills Needed
• The Data Engineer coaches the junior data engineering personnel position by bringing them up to speed and help them get better understanding of overall Data ecosystem.
• Prior experience developing, building and deploying on GCP
• Working on Solution deck, IP build, client meetings on requirement gathering"
29-Apr-2022 T11:46,Data Engineer,ITCONNECTUS PTE. LTD.,16 hours ago,$5K–$13K a month,Full–time,"Immediate joiners

Requirements
• Technical Data Architect who is strong on Data Warehousing, Big Data, Data Analytics & Data Science for solutioning
• Strong Google Cloud Platform Data Components – BigQuery, BigTable, CloudSQL, Dataproc, Data Flow, Data Fusion, Etc
• Demonstrate extensive skills and success in the implementation of technology projects within a professional environment, with a particular focus on data engineering
• Experienced in delivering Medium to Big Enterprise Data Projects
• GCP Experience including and not limited to MPP systems, Database systems, ETL and ELT systems and Data Flow compute Good to have skills
• A high-level understanding of AI and ML technologies and desire to develop your skills in these areas of emerging technology.
• Should be able to advise the best of breed for the client solutions; Skills Needed
• The Data Engineer coaches the junior data engineering personnel position by bringing them up to speed and help them get better understanding of overall Data ecosystem.
• Prior experience developing, building and deploying on GCP
• Working on Solution deck, IP build, client meetings on requirement gathering"
29-Apr-2022 T11:46,Data Engineer,Asm Technology Singapore Pte Ltd,16 hours ago,,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform. We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets. You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).

Your Responsibilities

Manage and support on-premise and Cloud-based data lake and warehouse systems

Design, build, support and optimize new and existing data structure and ETL processes

Build scalable and efficient data pipelines & services to help analytics teams to process the data

Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results

Liaise with third party tool providers to understand and improve data workflow

Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms

Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification

Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines

At least 2 years' experience in data engineering, automation and integration is preferred

Strong programming and scripting skills in Python and other modern programming languages

Strong data management, schema design and SQL development skills

Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)

Self-motivated and proactive, willing to learn new things

Good communication skills and strong team player

What our preferred candidates have

Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions

Understand and experienced with Cloud platform, eg. Microsoft Azure, AWS, GCP

Business intelligence and reporting tools, eg. Power BI, Tableau, Qlik, etc

Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)

REST/Web API development and management

Knowledge in Statistical software is an advantage

Experience In building machine learning models is a plus"
29-Apr-2022 T11:46,"Principal Data Engineer, Data Engineering",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Principal Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you are responsible for helping to shape the direction of the data engineering domain in GovTech; working with other domain experts, the delivery teams, vendors, and partners to influence technology roadmaps and the adoption of suitable data solutions across government. In addition, you will teach, mentor, and proactively grow fellow data engineers within the organization.

You will be leading multiple teams of data engineers to drive impact at scale, by building whole-of-government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will architect, design and build next-generation central data infrastructure to galvanise digitalisation in the public sector. Taking a more integrated approach to lay the foundation infrastructure and driving pervasive adoption of advanced data technologies throughout the economy and society, you will architect solutions that enable public agencies and sectoral partners to better integrate, exploit and understand their data to improve operations and decision-making, and deliver high-quality digital services to citizens and businesses in Singapore.

Join us on this journey to make a difference for the nation and our future generations, if you are:
• Experienced leading data pipeline building and data wrangling and enjoys optimizing data systems and building them from the ground up as part of multiple cross-functional teams,
• Have strong project management and organizational skills, self-directed and comfortable supporting the data needs across multiple teams, systems and products,
• Excited by the prospect of optimizing or even re-designing central and agencies’ data architecture to support next generation of government products and data initiatives, and
• Recognized expert with proven technical leadership, experienced working in a agile team to build systems from architecture to deployment for cloud and on-prem infrastructure, and
• Have the desire to serve the public good through the use of technology.

What you will be working on:
• Take the driving seat and lead our data engineers in complex technical projects to:
• Expand and optimize our data and data pipeline architecture, as well as optimize data flow systems for cross functional teams in a secured and scalable manner.
• Assemble large, complex data sets that meet functional / non-functional business requirements, transforming data into formats that are easy to consume and analyse.
• Identify, design, and implement internal process improvements, e.g., automating manual processes, optimizing data delivery, re-designing infra for greater scalability.
• Build the infra required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and industry standard Infra-as-Code tools.
• Build analytics tools that utilize the data pipeline to provide actionable insights into key performance metrics and support our next gen of products and data initiatives.
• Work with stakeholders including the Executive, Product, Data and Design teams to support their data and data pipelining use cases, and ensure optimal and secured data delivery architecture is consistent across projects, products and platforms.
• Drive innovative initiatives that uplift data capability across the government sector:
• Develop a strategic sense of key organisations, software and hardware tools, methodologies, and best practices in the data engineering landscape.
• Be the bridge between engineering and product line management and translate market requirements into product definitions and architecture designs.
• Analyse and solve complex data engineering problems, and translate architecture designs into implementations that satisfy the market requirements.
• Educate the team on new technological advances and work alongside them in proof-of-concept research projects and subsequent agile product delivery.
• Provide expert perspective to help shape data-related strategy and initiatives, and be our technical ambassador by writing internal blogs or publishing white papers.

What we are looking for:
• Candidate with 8+ years of experience in a Lead Data Engineer role, who has attained a Bachelor’s degree in Computer Science or other relevant engineering degree.
• In-depth working knowledge across a variety of relational SQL and NoSQL databases, including efficient query authoring, performance optimisation and troubleshooting.
• Experience building and optimizing data, data pipelines, and architectures, with processes to support transformation, data structures, metadata, dependency and workload management.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for further improvement.
• Working knowledge of message queuing, stream processing, and scalable data stores.
• A successful history of processing and extracting value from large datasets from different sources and systems, with strong analytics skills related to working with unstructured data.
• You should also have experience using the following software/tools:
• Big data and stream-processing tools: Hadoop, Spark, Beam, Flink, Kafka, etc.
• Data pipelining and workflow management tools: Luigi, Airflow, etc
• Cloud computing environments and services: AWS, GCP, Azure
• DevSecOps tools: Git, Kubernetes, Docker, Terraform etc
• Programming languages: Python, Java, C++, Scala, etc

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours”"
29-Apr-2022 T11:46,Data Engineer,Thakral One,16 hours ago,$8.5K–$10.5K a month,Full–time,"• Work with Banking team to understand existing SAS code logic written by techno-functional users
• Develop the SAS logic in Big Data environment, using spark, scala and pyspark utilizing client Hadoop ecosystem of the inhouse Data Lake
• Perform unit testing and system integration testing
• Work closely with Business Analysts team to review the test results and obtain sign off
• Deploy the new code in the client Production environment
• Prepare necessary design/operations documentation for future usage
• Perform peers Code quality review and be gatekeeper for quality checks
• 5-8 years of application development experience in Spark, Spark SQL, Scala is a must
• Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc
• Technical proficiency on data mining techniques and performance optimization
• Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL)
• Experience on SAS will be beneficial but not mandatory
• Handling of reporting packages (Tableau, QlikView) is nice to have
• AWS experience is nice to have
• Degree in Computer Science or Engineering is a must
• Good problem diagnosis and creative problem-solving skills
• Passion to learn and master diverse new technologies in the open-source community
• Accuracy and attention to detail
• Team-working, Verbal and Written communication skills

Desired Skills and Experience
Tableau, PySpark, Scala, Big Data, Pipelines, Unit Testing, Hadoop, Application Development, MariaDB, Spark, Data Mining, SQL, Attention to Detail, Python, System Integration Testing, S3"
29-Apr-2022 T11:46,Data Engineer,POWER IT SERVICES,13 hours ago,,Full–time,"Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

· Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

· A successful history of manipulating, processing and extracting value from large datasets.

· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science Statistics, Informatics, Information Systems or another quantitative field.

They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:46,Staff Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,16 hours ago,$8K–$13K a month,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 5 years of experience in Data Engineering role and have good knowledge / working experience in:

• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.

• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.

• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.

• Functional programming languages, e.g. Scala.

• Virtualization and container environment such as Docker and Kubernetes.

•Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:46,Data Engineering Lead,Stemly Pte. Ltd.,16 hours ago,,Full–time,"We are looking for a Senior Data Engineer with a track record of building enterprise grade data platform for cloud products .

This is an exciting opportunity for a technologist to build an innovative SaaS product using state-of-the-art and modern cloud technology stack.

As a Data Engineering Lead, you will be working on all aspects of data, from platform and infra build out to pipeline engineering and writing tooling / services for augmenting and fronting the core platform.

The data platform interfaces with large systems to ingest data (batch, micro-batch) to create statistical / machine learning models in a distributed setting.

You will work closely with data scientists, product managers, legal, compliance and business stakeholders.

You will also evaluate new technologies / frameworks and contribute to various open source big data technologies. You are encouraged to think out of the box and have fun exploring the latest patterns and designs in the fields of software and data engineering.

Your responsibilities
• Architecting and implementing a robust data platform for Stemly's products. Design and develop extremely efficient, reliable and observable data pipelines using technologies such as Airflow, Kubernetes, ElasticSearch, Kafka / KubeMQ, Postgres, ClickHouse etc.
• Design and deliver the next-gen data lifecycle management suite of tools / frameworks , including ingestion and consumption on the top of the data lake to support real-time, API-based and serverless use-cases, along with batch (mini / micro)
• Build and expose metadata catalogue for the Data Lake for easy exploration, profiling as well as lineage requirements
• Work on the development of the data platform iteratively by building quick POCs and converting ideas into real products
• Make data discoverable and easy to use for Data Scientists and Analysts. Enable Data Science teams to test and productize various ML models
• Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.

Support your colleagues by reviewing code and designs
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner

About you
• 8+ years of professional experience in software development in Python and Pandas
• Extensive experience with object-oriented design, coding and testing patterns, including experience with engineering software platforms and data infrastructures
• Must have sound understanding for Big Data components & administration fundamentals
• Hands-on in building a complete data platform using various open source technologies. Must have the knowledge to build metadata, lineage, observability and discoverability for data platform
• Experienced in DevOps best practices like CI / CD, containerization, blue-green deployments, secrets management etc. in the Data ecosystem
• Expert in the principles of distributed computing and data modelling
• Expert in building optimized SQL queries
• Conversant with the latest developments in the areas of Machine Learning models and efficiently supporting the data scientists is a plus
• Experience with Airflow, ClickHouse, Postgres, Kubernetes, GraphDB is a big plus
• Familiarity with Google Cloud Platform (e.g. GCS, Cloud Composer, BigQuery) is a plus
• Outstanding communication and interpersonal skills

About us

We are scientists and engineers with a strong passion for cutting-edge technology and decision science. Our mission is to enable enterprises to reduce time to decision with state-of-the-art forecasting and optimization.

Our algorithms are based on automatic machine learning techniques and allow for the discovery of optimal forecasting models.

We unlock sizable business and financial impact in terms of lower lost demand, decrease in inventory, lower working capital and financing costs.

How to apply

You may already know if you're a fit, but perhaps you're worried about some of the requirements We're looking for smart and passionate Engineers;

if you're the right candidate, we're flexible"
29-Apr-2022 T11:46,Data Engineer,Eastspring Investments (singapore) Limited,16 hours ago,,Full–time,"At Eastspring Investments, we understand that success comes from the talent and commitment of our people. We strive to build a business that you can shape, an inclusive workplace where everyone's ideas are valued and a culture where we can thrive together. Our people stay connected and tuned in to what's happening around us, keeping us ahead of the curve. While focused on the long-term, we look to the future to bring growth, development and benefit to everyone whose lives we touch.

Key Accountabilities

Main responsibilities:

Design and implement our Data and AI central data platform as well as related tools/systems for advanced business analytics and enterprise data governance

Manage data modeling design, writing, and optimizing ETL jobs

Participate in building and enhancing enterprise cloud data warehouse

Deliver and manage in-house and cloud-native data solutions to meet business requirements across firm-wide business units

Assist in creating and monitoring analytics dashboards, for different business functions

Ensure quality, integrity and accuracy of datasets through tracked, secured and auditable controls

Work with stakeholders to assist with data-related technical issues and support their data needs.

Follow and enforce best practices in software development and data engineering

Requirements:

Excellent coding skills with Python and SQL, and solid understanding of object-oriented analysis and design

Working knowledge of common algorithms and data structures, with strong analytical and problem-solving skills

Hands-on experience with Linux and shell scripting

Working experience with containerization (Docker/K8S) and task orchestration tools (Airflow/Luigi, etc.)

Experience with cloud service and tools (AWS/Azure/GCP), as well as cloud data warehouse platforms.

Experience with modern DevOps practices including version control, TDD, CI/CD, etc., for both code and configuration changes.

Basic understanding and experience with ML/AI concepts (e.g. deep learning, deep reinforcement learning, deep bayesian learning), workflows, and toolsets (Jupyter Notebook, etc.), and libraries (Numpy, Pandas, Scikit Learn, PyTorch, etc.) preferably in both cloud-native and desktop deployments.

Experience with traditional RDBMS based systems, including Data Lake, Data Warehouses and Marts, and more modern NoSQL and cloud-native big-data technology stacks such as document-oriented databases, Hadoop, columnar data files (e.g., Parquet), etc.

Familiar with REST APIs, service-oriented architectures (SOA) / microservices, virtualization, and serverless deployment architectures.

Demonstrated ability to understand, work with and deliver robust solutions in more than one programming language, framework, technology stack, runtime environment, etc..

QUALIFICATIONS / EXPERIENCE:

Degree level or higher in Computer Science or another quantitative field

1-5 year(s) technical experience showing increasing sophistication of solutions implemented, and ability to deliver

Fluency in both written and spoken English"
29-Apr-2022 T11:46,Data Engineer - APAC,Tamr,16 hours ago,,Full–time,"Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE
who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being
customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation
This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.
Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion
Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:46,"Data Engineer, Data Engineering",Bank of Singapore,17 hours ago,,Full–time,"At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programmes that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group’s global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today

Bank of Singapore opens doors to new opportunities.
Start your career with Bank of Singapore as a Data Engineer (6 months contract) in our Data Engineering team
Bank of Singapore opens doors to new opportunities. At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programs that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group's global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today
Bank of Singapore has defined an ambitious roadmap for accelerating its data excellence through the implementation of various bank-wide programs. As part of this roadmap, building the foundation of good data governance and management plays a key role in establishing trust in the data that we use.
General Description:
The incumbent will be based in the Data engineering team within the Data Hub team, supporting Bank of Singapore's data sourcing, curation, transformation, and data analytic services to accelerate their data initiatives. He/she will be responsible for expanding and optimizing BOS data pipelines as well as supporting data analytics needs.
Core activities :
• Be responsible for supporting data analytics and dashboarding with respects to the data availability / stability.
• Understanding the data and identifying any potential data issues as well as correcting issues arising from data.
• Define and implement data models suitable for dashboarding
• Power BI dashboarding support

Qualifications
Requirement :
General Knowledge & Experience
• Minimum of 6 years of working experience in data related functions
• Understanding of banking / financial services with exposure to data analytics preferred
Programming & Data
• Strong hands-on skills in SQL & PL/SQL
• Solid background in traditional structured database environments such as Teradata / Oracle
• Knowledge on Power BI dashboard functionality
• Strong skills in Power BI related technologies like power query and DAX
• Exposure to programming languages such as python
Communication & Soft Skills
• Curiosity & a real passion for understanding ""why?""
• Pragmatic ""can do"" approach to finding data based solutions to problems
• Good communication skills with ability to engage across business and technical audiences"
29-Apr-2022 T11:46,Data Engineer,ITCONNECTUS PTE. LTD.,16 hours ago,$5K–$13K a month,Full–time,"Immediate joiners

Requirements
• Technical Data Architect who is strong on Data Warehousing, Big Data, Data Analytics & Data Science for solutioning
• Strong Google Cloud Platform Data Components – BigQuery, BigTable, CloudSQL, Dataproc, Data Flow, Data Fusion, Etc
• Demonstrate extensive skills and success in the implementation of technology projects within a professional environment, with a particular focus on data engineering
• Experienced in delivering Medium to Big Enterprise Data Projects
• GCP Experience including and not limited to MPP systems, Database systems, ETL and ELT systems and Data Flow compute Good to have skills
• A high-level understanding of AI and ML technologies and desire to develop your skills in these areas of emerging technology.
• Should be able to advise the best of breed for the client solutions; Skills Needed
• The Data Engineer coaches the junior data engineering personnel position by bringing them up to speed and help them get better understanding of overall Data ecosystem.
• Prior experience developing, building and deploying on GCP
• Working on Solution deck, IP build, client meetings on requirement gathering"
29-Apr-2022 T11:46,Data Engineer,Asm Technology Singapore Pte Ltd,16 hours ago,,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform. We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets. You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).

Your Responsibilities

Manage and support on-premise and Cloud-based data lake and warehouse systems

Design, build, support and optimize new and existing data structure and ETL processes

Build scalable and efficient data pipelines & services to help analytics teams to process the data

Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results

Liaise with third party tool providers to understand and improve data workflow

Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms

Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification

Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines

At least 2 years' experience in data engineering, automation and integration is preferred

Strong programming and scripting skills in Python and other modern programming languages

Strong data management, schema design and SQL development skills

Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)

Self-motivated and proactive, willing to learn new things

Good communication skills and strong team player

What our preferred candidates have

Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions

Understand and experienced with Cloud platform, eg. Microsoft Azure, AWS, GCP

Business intelligence and reporting tools, eg. Power BI, Tableau, Qlik, etc

Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)

REST/Web API development and management

Knowledge in Statistical software is an advantage

Experience In building machine learning models is a plus"
29-Apr-2022 T11:46,"Principal Data Engineer, Data Engineering",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Principal Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you are responsible for helping to shape the direction of the data engineering domain in GovTech; working with other domain experts, the delivery teams, vendors, and partners to influence technology roadmaps and the adoption of suitable data solutions across government. In addition, you will teach, mentor, and proactively grow fellow data engineers within the organization.

You will be leading multiple teams of data engineers to drive impact at scale, by building whole-of-government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will architect, design and build next-generation central data infrastructure to galvanise digitalisation in the public sector. Taking a more integrated approach to lay the foundation infrastructure and driving pervasive adoption of advanced data technologies throughout the economy and society, you will architect solutions that enable public agencies and sectoral partners to better integrate, exploit and understand their data to improve operations and decision-making, and deliver high-quality digital services to citizens and businesses in Singapore.

Join us on this journey to make a difference for the nation and our future generations, if you are:
• Experienced leading data pipeline building and data wrangling and enjoys optimizing data systems and building them from the ground up as part of multiple cross-functional teams,
• Have strong project management and organizational skills, self-directed and comfortable supporting the data needs across multiple teams, systems and products,
• Excited by the prospect of optimizing or even re-designing central and agencies’ data architecture to support next generation of government products and data initiatives, and
• Recognized expert with proven technical leadership, experienced working in a agile team to build systems from architecture to deployment for cloud and on-prem infrastructure, and
• Have the desire to serve the public good through the use of technology.

What you will be working on:
• Take the driving seat and lead our data engineers in complex technical projects to:
• Expand and optimize our data and data pipeline architecture, as well as optimize data flow systems for cross functional teams in a secured and scalable manner.
• Assemble large, complex data sets that meet functional / non-functional business requirements, transforming data into formats that are easy to consume and analyse.
• Identify, design, and implement internal process improvements, e.g., automating manual processes, optimizing data delivery, re-designing infra for greater scalability.
• Build the infra required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and industry standard Infra-as-Code tools.
• Build analytics tools that utilize the data pipeline to provide actionable insights into key performance metrics and support our next gen of products and data initiatives.
• Work with stakeholders including the Executive, Product, Data and Design teams to support their data and data pipelining use cases, and ensure optimal and secured data delivery architecture is consistent across projects, products and platforms.
• Drive innovative initiatives that uplift data capability across the government sector:
• Develop a strategic sense of key organisations, software and hardware tools, methodologies, and best practices in the data engineering landscape.
• Be the bridge between engineering and product line management and translate market requirements into product definitions and architecture designs.
• Analyse and solve complex data engineering problems, and translate architecture designs into implementations that satisfy the market requirements.
• Educate the team on new technological advances and work alongside them in proof-of-concept research projects and subsequent agile product delivery.
• Provide expert perspective to help shape data-related strategy and initiatives, and be our technical ambassador by writing internal blogs or publishing white papers.

What we are looking for:
• Candidate with 8+ years of experience in a Lead Data Engineer role, who has attained a Bachelor’s degree in Computer Science or other relevant engineering degree.
• In-depth working knowledge across a variety of relational SQL and NoSQL databases, including efficient query authoring, performance optimisation and troubleshooting.
• Experience building and optimizing data, data pipelines, and architectures, with processes to support transformation, data structures, metadata, dependency and workload management.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for further improvement.
• Working knowledge of message queuing, stream processing, and scalable data stores.
• A successful history of processing and extracting value from large datasets from different sources and systems, with strong analytics skills related to working with unstructured data.
• You should also have experience using the following software/tools:
• Big data and stream-processing tools: Hadoop, Spark, Beam, Flink, Kafka, etc.
• Data pipelining and workflow management tools: Luigi, Airflow, etc
• Cloud computing environments and services: AWS, GCP, Azure
• DevSecOps tools: Git, Kubernetes, Docker, Terraform etc
• Programming languages: Python, Java, C++, Scala, etc

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours”"
29-Apr-2022 T11:46,Data Engineer,Thakral One,16 hours ago,$8.5K–$10.5K a month,Full–time,"• Work with Banking team to understand existing SAS code logic written by techno-functional users
• Develop the SAS logic in Big Data environment, using spark, scala and pyspark utilizing client Hadoop ecosystem of the inhouse Data Lake
• Perform unit testing and system integration testing
• Work closely with Business Analysts team to review the test results and obtain sign off
• Deploy the new code in the client Production environment
• Prepare necessary design/operations documentation for future usage
• Perform peers Code quality review and be gatekeeper for quality checks
• 5-8 years of application development experience in Spark, Spark SQL, Scala is a must
• Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc
• Technical proficiency on data mining techniques and performance optimization
• Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL)
• Experience on SAS will be beneficial but not mandatory
• Handling of reporting packages (Tableau, QlikView) is nice to have
• AWS experience is nice to have
• Degree in Computer Science or Engineering is a must
• Good problem diagnosis and creative problem-solving skills
• Passion to learn and master diverse new technologies in the open-source community
• Accuracy and attention to detail
• Team-working, Verbal and Written communication skills

Desired Skills and Experience
Tableau, PySpark, Scala, Big Data, Pipelines, Unit Testing, Hadoop, Application Development, MariaDB, Spark, Data Mining, SQL, Attention to Detail, Python, System Integration Testing, S3"
29-Apr-2022 T11:46,Data Engineer,POWER IT SERVICES,13 hours ago,,Full–time,"Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

· Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

· A successful history of manipulating, processing and extracting value from large datasets.

· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science Statistics, Informatics, Information Systems or another quantitative field.

They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:46,Staff Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,16 hours ago,$8K–$13K a month,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 5 years of experience in Data Engineering role and have good knowledge / working experience in:

• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.

• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.

• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.

• Functional programming languages, e.g. Scala.

• Virtualization and container environment such as Docker and Kubernetes.

•Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:46,Data Engineering Lead,Stemly Pte. Ltd.,16 hours ago,,Full–time,"We are looking for a Senior Data Engineer with a track record of building enterprise grade data platform for cloud products .

This is an exciting opportunity for a technologist to build an innovative SaaS product using state-of-the-art and modern cloud technology stack.

As a Data Engineering Lead, you will be working on all aspects of data, from platform and infra build out to pipeline engineering and writing tooling / services for augmenting and fronting the core platform.

The data platform interfaces with large systems to ingest data (batch, micro-batch) to create statistical / machine learning models in a distributed setting.

You will work closely with data scientists, product managers, legal, compliance and business stakeholders.

You will also evaluate new technologies / frameworks and contribute to various open source big data technologies. You are encouraged to think out of the box and have fun exploring the latest patterns and designs in the fields of software and data engineering.

Your responsibilities
• Architecting and implementing a robust data platform for Stemly's products. Design and develop extremely efficient, reliable and observable data pipelines using technologies such as Airflow, Kubernetes, ElasticSearch, Kafka / KubeMQ, Postgres, ClickHouse etc.
• Design and deliver the next-gen data lifecycle management suite of tools / frameworks , including ingestion and consumption on the top of the data lake to support real-time, API-based and serverless use-cases, along with batch (mini / micro)
• Build and expose metadata catalogue for the Data Lake for easy exploration, profiling as well as lineage requirements
• Work on the development of the data platform iteratively by building quick POCs and converting ideas into real products
• Make data discoverable and easy to use for Data Scientists and Analysts. Enable Data Science teams to test and productize various ML models
• Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.

Support your colleagues by reviewing code and designs
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner

About you
• 8+ years of professional experience in software development in Python and Pandas
• Extensive experience with object-oriented design, coding and testing patterns, including experience with engineering software platforms and data infrastructures
• Must have sound understanding for Big Data components & administration fundamentals
• Hands-on in building a complete data platform using various open source technologies. Must have the knowledge to build metadata, lineage, observability and discoverability for data platform
• Experienced in DevOps best practices like CI / CD, containerization, blue-green deployments, secrets management etc. in the Data ecosystem
• Expert in the principles of distributed computing and data modelling
• Expert in building optimized SQL queries
• Conversant with the latest developments in the areas of Machine Learning models and efficiently supporting the data scientists is a plus
• Experience with Airflow, ClickHouse, Postgres, Kubernetes, GraphDB is a big plus
• Familiarity with Google Cloud Platform (e.g. GCS, Cloud Composer, BigQuery) is a plus
• Outstanding communication and interpersonal skills

About us

We are scientists and engineers with a strong passion for cutting-edge technology and decision science. Our mission is to enable enterprises to reduce time to decision with state-of-the-art forecasting and optimization.

Our algorithms are based on automatic machine learning techniques and allow for the discovery of optimal forecasting models.

We unlock sizable business and financial impact in terms of lower lost demand, decrease in inventory, lower working capital and financing costs.

How to apply

You may already know if you're a fit, but perhaps you're worried about some of the requirements We're looking for smart and passionate Engineers;

if you're the right candidate, we're flexible"
29-Apr-2022 T11:46,Data Engineer,Eastspring Investments (singapore) Limited,16 hours ago,,Full–time,"At Eastspring Investments, we understand that success comes from the talent and commitment of our people. We strive to build a business that you can shape, an inclusive workplace where everyone's ideas are valued and a culture where we can thrive together. Our people stay connected and tuned in to what's happening around us, keeping us ahead of the curve. While focused on the long-term, we look to the future to bring growth, development and benefit to everyone whose lives we touch.

Key Accountabilities

Main responsibilities:

Design and implement our Data and AI central data platform as well as related tools/systems for advanced business analytics and enterprise data governance

Manage data modeling design, writing, and optimizing ETL jobs

Participate in building and enhancing enterprise cloud data warehouse

Deliver and manage in-house and cloud-native data solutions to meet business requirements across firm-wide business units

Assist in creating and monitoring analytics dashboards, for different business functions

Ensure quality, integrity and accuracy of datasets through tracked, secured and auditable controls

Work with stakeholders to assist with data-related technical issues and support their data needs.

Follow and enforce best practices in software development and data engineering

Requirements:

Excellent coding skills with Python and SQL, and solid understanding of object-oriented analysis and design

Working knowledge of common algorithms and data structures, with strong analytical and problem-solving skills

Hands-on experience with Linux and shell scripting

Working experience with containerization (Docker/K8S) and task orchestration tools (Airflow/Luigi, etc.)

Experience with cloud service and tools (AWS/Azure/GCP), as well as cloud data warehouse platforms.

Experience with modern DevOps practices including version control, TDD, CI/CD, etc., for both code and configuration changes.

Basic understanding and experience with ML/AI concepts (e.g. deep learning, deep reinforcement learning, deep bayesian learning), workflows, and toolsets (Jupyter Notebook, etc.), and libraries (Numpy, Pandas, Scikit Learn, PyTorch, etc.) preferably in both cloud-native and desktop deployments.

Experience with traditional RDBMS based systems, including Data Lake, Data Warehouses and Marts, and more modern NoSQL and cloud-native big-data technology stacks such as document-oriented databases, Hadoop, columnar data files (e.g., Parquet), etc.

Familiar with REST APIs, service-oriented architectures (SOA) / microservices, virtualization, and serverless deployment architectures.

Demonstrated ability to understand, work with and deliver robust solutions in more than one programming language, framework, technology stack, runtime environment, etc..

QUALIFICATIONS / EXPERIENCE:

Degree level or higher in Computer Science or another quantitative field

1-5 year(s) technical experience showing increasing sophistication of solutions implemented, and ability to deliver

Fluency in both written and spoken English"
29-Apr-2022 T11:46,Data Engineer - APAC,Tamr,16 hours ago,,Full–time,"Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE
who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being
customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation
This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.
Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion
Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:46,"Data Engineer, Data Engineering",Bank of Singapore,17 hours ago,,Full–time,"At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programmes that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group’s global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today

Bank of Singapore opens doors to new opportunities.
Start your career with Bank of Singapore as a Data Engineer (6 months contract) in our Data Engineering team
Bank of Singapore opens doors to new opportunities. At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programs that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group's global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today
Bank of Singapore has defined an ambitious roadmap for accelerating its data excellence through the implementation of various bank-wide programs. As part of this roadmap, building the foundation of good data governance and management plays a key role in establishing trust in the data that we use.
General Description:
The incumbent will be based in the Data engineering team within the Data Hub team, supporting Bank of Singapore's data sourcing, curation, transformation, and data analytic services to accelerate their data initiatives. He/she will be responsible for expanding and optimizing BOS data pipelines as well as supporting data analytics needs.
Core activities :
• Be responsible for supporting data analytics and dashboarding with respects to the data availability / stability.
• Understanding the data and identifying any potential data issues as well as correcting issues arising from data.
• Define and implement data models suitable for dashboarding
• Power BI dashboarding support

Qualifications
Requirement :
General Knowledge & Experience
• Minimum of 6 years of working experience in data related functions
• Understanding of banking / financial services with exposure to data analytics preferred
Programming & Data
• Strong hands-on skills in SQL & PL/SQL
• Solid background in traditional structured database environments such as Teradata / Oracle
• Knowledge on Power BI dashboard functionality
• Strong skills in Power BI related technologies like power query and DAX
• Exposure to programming languages such as python
Communication & Soft Skills
• Curiosity & a real passion for understanding ""why?""
• Pragmatic ""can do"" approach to finding data based solutions to problems
• Good communication skills with ability to engage across business and technical audiences"
29-Apr-2022 T11:46,Senior Data Engineer,Toptal,4 hours ago,,Full–time,"About The Job

Toptal developers work with speed and efficiency to deliver the highest quality of work. We are looking for someone who is passionate about their client’s business, and ready to work on exciting projects with Fortune 500 companies and Silicon Valley startups, with great rates and zero hassles. If you are looking for a place to advance your career, enhance your skill set, and build connections around the globe, Toptal is right for you.

About Toptal

Toptal is an exclusive network of top freelancers from around the world. Fortune 500 companies and Silicon Valley startups hire Toptal for their most important projects. Toptal is one of the fastest-growing fully remote networks and empowers freelance software developers, designers, finance experts, product managers, and project managers worldwide to grow and excel in their freelance careers.

Toptal clients vary in sizes and industries, from enterprise organizations and big tech companies to Silicon Valley startups and renowned universities. Once you enter the network, our matchers will contact you with project opportunities that fit your expertise and preferences. We have experts in over 120 countries who get to work remotely on projects that meet their career ambitions.

About The Role

As a Data Engineer, your main goal is to be one step ahead of data scientists and analysts. You will support them by providing infrastructure and tools they can use to deliver end-to-end solutions to business problems that can be developed rapidly and maintained easily. This is more than building and maintaining ETL pipelines. We need innovation, creativity, and solutions that will have a significant impact on the client’s velocity.

Requirements
• 3+ years of professional experience in software development
• Working experience with Python and Pandas.
• Familiarity with the basic principles of distributed computing and data modeling.
• Extensive experience with object-oriented design and coding and testing patterns, including experience with engineering software platforms and data infrastructures.
• Working experience with Airflow and Luigi is a big plus.
• Working experience with Scala is a plus.
• Familiarity with Google Cloud Platform (e.g. GCS and BigQuery) is a plus.
• Working experience with Dimensional Modeling and Rails is a plus.
• Outstanding communication and interpersonal skills.
• Full-time availability is a strong advantage

If you’re interested in pursuing an engaging career working on full-time freelance jobs for exclusive clients, take the next step by clicking apply and filling out the short form to get started.

#RemoteJobDataEngineering"
29-Apr-2022 T11:46,Data Engineer,Tech Mahindra Limited (singapore Branch),8 hours ago,,Full–time,"What to expect:
Manage & develop data warehouse/data lake and pipeline plans.
Develop and uphold best practices with respect to documentation & data protocols
Design, build and launch new data models in production.
To also develop new data extraction, transformation and loading processes in production.

Work in a cross-functional team, Interfacing with engineers, product managers and product designers to understand data needs to define and review technical specifications.
Build machine learning/reasoning models at scale.
How To Succeed:
Degree in Computer Science, Information Systems, Computer Engineering, Mathematics, or related disciplines.

At least 3 years of experience in data engineering, with experience in Kotlin/Scala/ Python, and experiences in Amazon Web Services stack, such as ECS, Kinesis, EMR, and DynamoDB.
Experienced in building production-grade models using machine learning frameworks such as Tensorflow, Scikit-learn, PySpark and/or others is required.
Experienced in custom ETL & ELT & Streaming big data design, implementation & maintenance and data warehouse/data lake space.
Experienced/willing to use Infrastructure-as-code for deploying pipelines.
Hands-on and deep experience with schema design and dimensional data modelling.
Ability to write efficient SQL statements.
Ability to analyse data to identify deliverables, gaps and inconsistencies.
Excellent communication skills including the ability to identify and communicate data-driven insights.
Experience in using Hadoop, Spark, HBase, Hive and Pig, is a plus.
Experience in software engineering, with experience in Kotlin or Java, and Spring Boot is a plus"
29-Apr-2022 T11:46,Data Engineer (Cyber security/ Linux/ Big Data/ East),TRUST RECRUIT PTE. LTD.,1 hour ago,,Full–time,"• Leading company specialised in Big Data & Cyber Security industry
• Good learning opportunity & project exposure
• No exp is welcomed
• Learn / Know Programming Language
Responsibilities:
• Study business and application specific requirements, review corporate technology to design and implement the big data architecture and solution to meet business requirements.
• Provide big data analysis consultancy and advisory to customers in the areas of cyber security, application monitoring, networking monitoring, infrastructure monitoring or business analytics.
• Conduct proof of concept/value on big data analytics related solution
Requirements:
• Knowledge in networking, windows servers and Linux servers
• Knowledge in Cybersecurity related solutions
• Knowledge in programming languages

HOW TO APPLY: Interested applicants, kindly send your resume in MS WORD format to ref17@trustrecruit.com.sg or please click on “Apply Now” and provide the below details in your resume.
We regret only shortlisted candidates will be notified.
Important Note: Trust Recruit Pte Ltd is committed to safeguarding your personal data in accordance with the Personal Data Protection Act (PDPA).
Please read our privacy statement on our corporate website www.trustrecruit.com.sg.
Trust Recruit Pte Ltd
EA License No: 19C9950
EA Personnel: VIvian Chung Fhui Tiin
EA Personnel Reg No: R22106436"
29-Apr-2022 T11:46,Data Engineer,Asm Technology Singapore Pte Ltd,16 hours ago,,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform

We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets

You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).
Your Responsibilities
Manage and support on-premise and Cloud-based data lake and warehouse systems
Design, build, support and optimize new and existing data structure and ETL processes
Build scalable and efficient data pipelines & services to help analytics teams to process the data
Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results
Liaise with third party tool providers to understand and improve data workflow
Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms
Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification
Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines
At least 2 years' experience in data engineering, automation and integration is preferred
Strong programming and scripting skills in Python and other modern programming languages
Strong data management, schema design and SQL development skills
Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)
Self-motivated and proactive, willing to learn new things
Good communication skills and strong team player

What our preferred candidates have
Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions
Understand and experienced with Cloud platform, eg

Microsoft Azure, AWS, GCP
Business intelligence and reporting tools, eg

Power BI, Tableau, Qlik, etc
Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)
REST/Web API development and management
Knowledge in Statistical software is an advantage
Experience In building machine learning models is a plus"
29-Apr-2022 T11:46,"Data Engineer (5 days, Orchard, $3500-4000)",MACHSPEED HUMAN RESOURCES PTE. LTD.,9 hours ago,,Full–time,"Leading System Integrator company providing IT solutions for network, IT infrastructure, analytics, cyber security and managed services.

Job Responsibilities
• Design, engineer, configure and administer BI project based on given functional and technical requirements
• Collaborate with pre-sales, project and relevant internal teams to deliver the optimal technical solution to client's business problems
• Develop real-time and batch data ingesting and processing pipelines to be used for analysis, machine learning, dashboards, alerts and visualizations
• Review and monitor ETL tasks and performance
• Support testing and deployment
• Provide recommendations and implementation changes to optimize in the customer environment.
• Write and develop custom scripts as needed

Requirements
• Minimum Degree/Diploma in Computer Science, Engineering or IT with at least 2 years of relevant experience
• Hands-on experience in scripting/programming
• Possess knowledge in Networking and Servers(Windows and Linux)
• Possess CEH/ECSA/CISSP/ECSA/CompTIA Security+ certification will be an added advantage
• Working hours: 9am - 6pm, Mondays to Fridays
• Salary range: $3500 - 4000

We thank you for your interest in this position, applicants with relevant experience please send Microsoft word resume with expected salary to

apply@machspeed.com.sg

for faster processing, alternatively, please visit https://www.facebook.com/MachspeedHR for more jobs available, thank you.

You may also call +6563362530 (Look for BingCheng) to find out more

Thank you very much.

Agency License No. 12C6200

EA Registration No: R1437671"
29-Apr-2022 T11:46,Senior Data Engineer,GRASSHOPPER PTE. LTD.,16 hours ago,,Full–time,"OVERVIEW:

We seek a Senior Data Engineer to support our high-frequency trading and quantitative research. At Grasshopper we recognize that effective research and trading strategies are built upon a solid foundation in data collection, storage and service. Grasshopper trades at a number of exchanges globally, and our success depends on our ability to scale by leveraging the potential of data.

You will, in close collaboration with stakeholders, design and develop highly scalable and fault-tolerant data pipelines in a cloud environment focusing on scalability and performance, as well as the next generation of tools that empower users to generate insights on the data they work with.

RESPONSIBILITIES:

• Work with a team of data engineers across locations, managing project schedules.

• Expand the imagination of data stakeholders on what is possible and educate them in leveraging data they already have.

• Work proactively with traders, researchers and various members outside the data team to meet their needs for timely, accurate and complete data sets.

• Build highly scalable data pipelines to process and analyse billions of messages in real time.

• Set strong technical/architectural/cultural foundations, advocate good design in software development as well as data engineering, and lead by example.

SKILLS & KNOWLEDGE REQUIREMENTS:

• Strong technical leadership qualities, good at working with both people and with code.

• Extensive experience with data modelling and designing/supporting both streaming and batch ETL pipelines.

• Extensive experience in SQL and databases.

• Domain knowledge in finance, especially in buy-side trading, is not required but good to have.

• Proficiency in at least two mainstream programming languages (preferably Python, Java, C++).

• Proficiency in a programming language of a non-OOP paradigm (e.g. functional/logic programming).

• Experience with FP libraries like scalaz/cats/ZIO is a plus.

• Demonstrates good judgment in software design balancing design principles with pragmatics, with an emphasis on simplicity. See Rich Hickey’s Simple Made Easy talk: [ Link removed ]

• Good to have experience in Google BigQuery.

• Experience working in some cloud services platform (e.g. Google Cloud, AWS, Azure).

• Experience with messaging middleware such as Solace or Kafka.

• Experience with machine learning is good to have.

WHO WE ARE LOOKING FOR:

We are looking for team-players who are creative in their approach to problem solving. They take the initiative to explore different ways to resolve an issue, and systematically find the most efficient and effective way to do it. They are adept at expressing their ideas and solutions into clean and maintainable code.

THE ENVIRONMENT WE OFFER:

As a growing firm with a tightly knit team, we respect and listen to all our employees. You will get the chance to make an impact by having your voice heard by everyone, including the management.

Our employees enjoy a high level of autonomy at work. We focus on substance, not form - as long as you can perform, you will be recognized and rewarded. We are also dedicated to supporting our staff and ensuring they develop holistically to maximize their potential in the long-term.

We also provide flexible working arrangement as required and a casual and fun work environment to boot!

Recommended Skills

Amazon Web Services

Apache Kafka

C++ (Programming Language)

Cloud Computing

Constraint Logic Programming

Creativity"
29-Apr-2022 T11:46,Data Engineer,Supernova,16 hours ago,,Full–time,"About Supernova

We are the fastest growing lifestyle & beauty omni-channel ecommerce company. We build iconic beauty and lifestyle brands for a new generation of women worldwide. Independently owned with a global marketing reach, we connect with over 15 million women on social media every week. Headquartered in Singapore, we currently have offices in Berlin, London and LA. Our brands include Sand & Sky and Coco & Eve.

About Supernova's Internal Engine

Together we create Superstars! Our three company pillars are guided by this motto: Our product and brands are of the highest quality and within the top five in their category; our customers are the focus of our actions and their lives improve with our products; our talent and employees grow and develop within Supernova.

This means that investing in hiring, developing and retaining the brightest minds in the world is a top priority. Living our core values of #MakeAnImpact, #TalkAboutIt, #SeekNewOpportunities, #DelightAndSurprise and #MoveForwardTogether creates a unique culture across five different time zones and allows us to realise the enormous potential of this exciting industry. If you're an ambitious, smart, natural collaborator who likes taking risks, influencing, and innovating in a challenging hyper-growth environment, we'd love to talk to you about joining our team.

The role

This role provides tremendous opportunity for the right candidate: to capture the energy and rapid pace of a fast growing start up, while being responsible for a sizeable and growing part of our global footprint.

As Supernova's Data Engineer, you are responsible for the design and implementation of modern, scalable data-centric services and solutions.

You have strong data infrastructure and architecture skills, with a proven track record of handling high data volumes to serve batch as well as streaming needs.

This is a remote position, working as part of a strong and growing Data Team.

Responsibilities:
• Define and build data pipelines from a wide variety of data sources that will enable faster, better, data-informed decision-making within the business.
• Develop custom integrations via REST/SOAP/event-driven architecture.
• Work with stakeholders including the Brand, Product, Ops and Marketing teams to assist with data-related issues and support their data infrastructure needs.
• Build tools for effective maintenance and monitoring of the data infrastructure.
• Contribute to an ongoing effort to improve data reliability, efficiency and quality.
• Performance tuning of data pipelines jobs for optimal end-user experience.

Your Profile:
• Minimum Bachelor's degree in a quantitative field is required e.g. Computer Science, Statistics, Mathematics
• 5+ years of experience in data engineering roles using Python / Java.
• Experience in building the solution architecture, provisioning of infrastructure and delivering data-centric services and applications either in GCP or AWS.
• Strong experience in Event Driven Architecture using Kafka or Pub/Sub etc.
• Extensive knowledge in ELT processing and workflow orchestration using Airflow/ Nifi or similar tools.
• Experience with development ecosystems such as Git, Docker, Jenkins and CI/CD.
• Preferably with a background in Startup / Ecommerce.
• Ability to plan your work and commit to deadlines.
• Ability to work with cross-functional teams to deliver quality results.

This position is available immediately. All applications will be treated confidentially.

Please note that we can only contact successful applicants.

Supernova is an equal opportunity employer. Our values define the working culture and environment we strive to create – fair, diverse, respectful and welcoming of different views. We embrace a culture reflecting a variety of perspectives, insights and backgrounds to drive innovation. We build talented and diverse teams to drive business results and encourage our people to develop to their full potential. We promote harmonious working relationships and encourage a learning and development culture. We welcome applications from talented people coming from all cultures, countries, races, genders, sexual orientations, disabilities, beliefs and generations and are committed to providing a working environment free from harassment, discrimination and retaliation.

See our Applicant Data Protection policy for information on how we collect, use and process your personal data: http://www.supernova.xyz/applicant-data-protection-notice"
29-Apr-2022 T11:46,Data Engineer 3 - AWS & Python (Contractual),The Economist,16 hours ago,,Contractor,"What will you experience

At Economist Intelligence Unit (EIU) we believe having the right work-life balance is super important; striking balance between your personal and professional life is critical to wellbeing and happiness. We offer flexible working and have recently shifted to a 'remote first' working policy with a minimum expectation of coming to the office two days a month, however you can come in more often if you wish to.

How you will contribute:
• Build data pipelines: Architecting, creating and maintaining data pipelines and ETL processes in AWS via Python, Glue and Lambda
• Support and Transition: Support and optimise our current desktop data tool set and Excel analysis pipeline to a transformative Cloud scale Big Data Architecture environment.
• Work in an agile environment: within a collaborative agile product team using Kanban
• Collaborate across departments: Work in close relationship with data science teams and with business (economists/data) analysts in refining their data requirements for various initiatives and data consumption requirements.
• Educate and train: Required to train colleagues such as data scientists, analysts, and stakeholders in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases.
• Participate in ensuring compliance and governance during data use: To ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives.
• Become a data and analytics evangelist: This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals.

To succeed in this role it would be an advantage if you possess:
• Experience with programing in Python, and Lambda functions
• Knowledge of building bespoke ETL solutions, and extracting data using Data APIs
• MS SQL Server (data modelling, T-SQL, and SSIS) for managing business data and reporting
• Prior experience in design and developing microservice architecture
• Ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management.
• A combination of IT skills, data governance skills, analytics skills and economics knowledge
• An advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (postgraduation diploma or related) or a related quantitative field or equivalent work experience.
• Experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms"
29-Apr-2022 T11:46,Senior Spark/Hadoop Engineer,Ridik Pte. Ltd.,16 hours ago,,Full–time,"Requirements
• Must have application development experience in Spark, Spark SQL, Scala is a must
• Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop / HDFS, S3, Colibra, Claudera Workbench, etc.
• Technical proficiency on data mining techniques and performance optimization
• Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL)
• Experience on SAS will be beneficial but not mandatory
• Handling of reporting packages (Tableau, QlikView) is nice to have
• AWS experience is nice to have
• Degree in Computer Science or Engineering is a must
• Good problem diagnosis and creative problem-solving skills
• Passion to learn and master diverse new technologies in the open-source community
• Accuracy and attention to detail
• Team-working, Verbal and Written communication skills"
29-Apr-2022 T11:46,Senior Data Engineer,Kkr Singapore Pte. Ltd.,8 hours ago,,Full–time,"Position Summary

We are looking for a savvy Data Engineer to be based in Hong Kong / Singapore, to join our growing team of data and analytics experts.

The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.
The ideal candidate is an experienced cloud-based services, data architecture, data engineering, data pipeline development (ETL), and analytical tools. We are seeking an engineer who enjoys optimizing data systems and building enterprise scale data services.

The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.
Skill / Experience Required
Bachelor's Degree in Computer Science/Engineering or a related discipline.
10+ years Development experience
Experience in Python and related open source modules
Experience in Python development including Web application frameworks such as Flask / FAST API
Experience working with RESTful API Services
Strong database skills with a thorough understanding of relational database and understanding of Object Oriented databases
Exposure with the AWS Stack / RDS / is preferred
Knowledge of open source solutions and trending technologies
Good communication and written skills
Ability to be self-sufficient and proactive individual contributor
Exposure to Private/Public Markets

Desirable
Understanding of Object Oriented Programming and Design Patterns
Knowledge of web standards, security, accessibility, browser compatibility
Knowledge of JavaScript, HTML5 and awareness of frameworks such as
Experience in a Business Intelligence tool e.g. Tableau and Dremio
Exposure to ML / AI and python libraries (pandas, NumPy, SciPy) and concepts"
29-Apr-2022 T11:46,Data Engineer,Percept Solutions Pte. Ltd.,16 hours ago,,Full–time,"We are looking for an IT professional who has gathered some years of working experience in Business Intelligence, Data Analytics, Report Development, SQL and DWH design.Role Responsibilities:Use SQL and/or programming language and tools to perform tasks such as data discovery, data QA, and data preparation based on requirements provided by the business.Build data assets in AWS and aligning the data architecture based on business requirements and working towards a common data model.Support the Business Analyst in translating business requirements to technical requirementsSupport Data Analytics Projects & Initiatives, like Campaign Automation with AIConstantly searching for optimizations, improvements and innovations (e.g. predictive AI)Skills and Competencies:Data Engineering & Data Analytics Expert, DWH Design, SQL, ETL, AWS, Tableau, QliSense, Power BIIndependent and reliable working style as well as enjoying team collaborationGood understanding about Agile software development methods and IT security is a plusBusiness Knowledge of Captive or Financial Services, Banking is a big plusTo apply please click the Apply button or send us your updated profile to [HIDDEN TEXT]EA Licence No.:18S9405 / EA Reg. No.:R1330864Percept Solutions is undergoing a growth phase and are on the lookout for talent. Applicants are encouraged to follow Percept Solutions on LinkedIn @ https://www.linkedin.com/company/percept-solutions/ to stay up to date on our upcoming roles and events"
29-Apr-2022 T11:46,Data Engineer,Azendian Solutions Pte. Ltd.,17 hours ago,,Full–time,"Responsibilities:
• Responsible for data analysis and generate reports for estate sub-system performance
• Develop data set processes
• Develop analytics programs, machine learning algorithms and statistical method, to make raw data useful for smart estate application
• Identify ways to improve data reliability, efficiency and quality
• Conduct research and explore for innovative solutions
• Monitor compliance to project requirement, performance standards and specifications
• Perform overall quality control of the work and report regularly on project status
• Cooperate and communicate effectively with product development and operation team for implementation

Requirements:
• Relevant Bachelor's degree or equivalent in electrical/electronic engineering or computing
• 3+ years' related field and project planning experience
• Excellent data science knowledge is required
• Programming skill in Python and Java experience is required
• Strong analytic skill is required
• Knowledge in data management is required
• Knowledge in ETL is preferred
• Must have strong written and verbal communication skills
• Able to work comfortably and independently in a fast-paced environment"
29-Apr-2022 T11:46,Data Engineering Lead (Remote Possible),Glints,14 hours ago,,Full–time,"At Glints, we are building the #1 tech-enabled recruitment and career discovery platform in Southeast Asia that helps people and organisations realise their human potential by joining great organisations, learning the right skills and building great teams.

We’re looking for a Data Engineering Lead to join our Data Engineer team, to
improve Data Governance and promote Data Democratization in the company
What You’ll Be Doing

• Identifying organizational needs for Data Engineering and developing a roadmap within the organization
• To promote ownership of data across the organization
• To improve the quality and integrity of Data in Glints
• To improve discoverability of data and encourage usage of data
• To develop security around data usage
• Conveying organization need to Data Engineering Team for implementation of the solution
• Mentor and groom members of the team to be technical leaders; Hire and expand the team if needed

Why You Should Join Us

• Opportunity to determine Data Engineering Roadmap and realise it with a team of enthusiastic engineers

Who We Are Looking For

• Proficient with Python and Scrum Framework
• Experience with leading Data Engineering Team in improving Data Warehouse
• Managerial or Leadership experience in mentoring and grooming Data Engineering Team
• Able to understand the business equation of the companies and identify levers which the Data Engineering Team can pull

Let’s Realise Human Potential.

We have impacted many lives since we were founded in 2013, but there’s still plenty to be done. If you’re ready to grow and make an impact, you’ve come to the right place.

What is Glints?

Glints is an online talent recruitment and career discovery platform with the enduring purpose of contributing to a world where people and organisations can realize their human potential.

Our tech-enabled approach to recruitment and career discovery connects more than 1.5 million candidates to their next dream opportunities, has supported more than 30,000 companies in finding top talents, doubled recruiter efficiency, and has raised more than US$30M+ from best venture investors in Asia to date.

Our agility and firm hold on our core purpose and values have allowed us to remain resilient and thrive through tumultuous times, and we are proud to be recognised by LinkedIn as one of the Top 10 Startups in Singapore in 2020.

Who We Are

At Glints, personal and professional growth are just as important as business growth. That's why we created the Glints Culture Code: #RIIBCOH. It defines our values, guides our decisions and actions, and is what makes us special.

Relentlessly Resourceful: Whatever it takes, just make it happen (ethically)
Integrity: Have courage, be guided by the truth, don’t be afraid
Impact: Missionaries, not mercenaries
Beginners’ Mindset: Stay humble, don’t be attached to ego
Customer Obsessed: Customers First
Ownership: Care intensely about the mission and take responsibility
High Standards: Dream big and deliver epic outcomes fast

Where We Work

Glints operates in multiple locations across Greater Southeast Asia, including Singapore, Indonesia, Vietnam, Taiwan and Malaysia.

Learn more about Glints and our culture at bit.ly/glintsculture,
Or check out our Careers Page at https://glints.com/careers"
29-Apr-2022 T11:46,Staff Data Engineer,Techgig,16 hours ago,,Full–time,"About Job
CTC Undisclosed Job Location Singapore Experience 8 - 11 yrs
Description
Job Description and Responsibilities
We are developing and executing a shared strategic vision for Loyalty platforms and products that enable Visa to be the world-leading data-driven payments company. As a Staff Data Engineer, you will work closely with a world-class team of Development and Test Engineers.
This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide excellent user experiences. You will be an integral part of the Loyalty development team focusing on design and building of software solutions that leverage data to solve business problems.
Strategic thinker and good business acumen to orient data engineering to the business needs of internal clients.
Lead development and oversee programming testing functions to ensure that projects are securely delivered while fulfilling expectations.
Identify opportunities for further enhancements and refinements to standards and processes.
Lead by example: demonstrating accountability, mentoring junior team members, and by contributing to departmental procedures, best practices, and standards.
The candidate will be extensively involved in hands-on activities including POCs, design, documentation, coding, unit testing, implementation, and documentation of solutions for new development, system enhancements, and production support.
Candidate must be flexible and willing to switch tasks based on team's needs.
Development and programming functions to ensure that projects are delivered on time and within budget with good code quality.
Work with architects, systems analysts, project managers, QA, and other developers to successfully implement business requirements while applying the latest available tools and technology.
Responsible for the architecture, design, development, implementation of data-based software applications. This includes working with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations.
Interact with business units to define requirements/modifications and use case to utilize in designing appropriate solutions.
Provide recommendation on scope and scale of effort required to develop solution.
Design, develop, document, and implement new programs and subprograms, as well as enhancements, modifications, and corrections to existing software - Develop testing and debugging routines.
Create documentation and procedures for installation and maintenance.
Build and maintain relationship with global and virtual teams and third parties on software development or support issues.
Identify opportunities for further enhancements and refinements to standards, best practices, and development methodologies.
Work directly with Architects, System Analysts, Dev leads and QA team leads to manage the technical aspects of a development pipeline.
This is a hybrid position. Hybrid employees can alternate time between both home and office. Employees in hybrid roles are expected to work from the office three days a week, Monday (Sunday in some countries where Sunday is the start of the week) and Wednesdays. Wednesdays are designated as in-office collaboration days. The third day in the office will be decided based on team needs and determined in partnership with senior leadership.
Qualifications
Preferred Qualifications
Masters Degree in Computer Science or related field with 6 years of relevant experience or BS Degree with 8 years of experience.
Previous exposure to financial services is a plus, but not required.
Extensive experience in architecting and developing real-time applications that are fault-tolerant, scalable and can handle high volumes.
Experience in best practices for API development and design patterns.
Quick learner: self-starter, detailed and thorough.
Experience in all phases of software development life cycle including project management, functional requirements definition, technical design, development, testing, quality assurance, system certification, systems implementation, and system validation.
Consistently able to assess and evaluate problems in a production environment and manage risk to the service when recommending change.
Strong secure coding practices.
Good Knowledge on Hadoop framework and related Big Data Technologies (HDFS, Map Reduce, Spark, HBase, Kafka).
Strong knowledge in Java or Scala or Python.
Strong knowledge of database concepts, systems architecture, and data structures is a must.
Java/J2EE/Angular, Spring Cloud, Microservices and strong knowledge on API development is desired.
Experience working in an Agile and Test-Driven Development environment.
Process oriented with strong analytical and problem-solving skills.
Work independently and mentor others in the team and with minimal supervision.
Ability to juggle multiple projects and change direction mid-course based on business drivers.
Ability to work independently in a high throughput environment.
Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style"
29-Apr-2022 T11:46,"Facilities Engineer (Data Centre, office hours)",Randstad Singapore,7 hours ago,,Full–time,"about the job

This is a company whose expertise has made them the leader in their field is seeking a high-calibre Facilities Engineer in Singapore to support the data centre operations in Singapore.

The Facilities Engineer will play a crucial role in the company’s continued development and success in Singapore. Your key purpose will be to oversee the execution of the local data operations and manage the engineering operations and core infrastructure. You will be responsible for the incident reporting, vendor management, change management, documentation of operational procedures as well as the management of the day to day operations. This is a rare opportunity for a hands-on individual to be part of a growing organization that provides a stimulation work environment.

skills and experience required.

To be successful in the role, you would have:
• Ideally have at least 5 years experience in relevant Facilities Management within Data Center Operations
• Relevant qualifications in Engineering or Mechanical Engineering or Electrical Engineering are preferred.
• Have an understanding of construction, commissioning and operation of mission critical systems.
• Able to work shift hours is a must
• Great communication skills are necessary this is a client facing role.
• Ability to work independently and make logical decisions

how to apply

To apply online, please click on the ‘apply’ function below.

Please indicate your availability, expected salary, and reason for leaving your current job in your CV.

EA: 94C3609 / R1767516

Applicants must be fully vaccinated or have a valid exemption in accordance with MOM’s regulations to allow them to enter the workplace. Applicants may be required to share verifiable COVID-19 vaccination documents or proof of a valid exemption at the point of offer. Randstad Pte. Limited and/or the Client reserves the right to withdraw an offer if the applicant fails to provide verifiable COVID-19 vaccination and/or proof of exemption documents.

skills

no additional skills required

qualifications

no additional qualifications required

education

Bachelor Degree"
29-Apr-2022 T11:46,Data Engineer,Huxley,16 hours ago,,Full–time,"We are currently looking to fill 2 Data Engineer positions focusing on data modelling and risk related projects:

Data Engineer should have the following:

3+ year of experience developing machine models

Proficient in either Python, Java and/or Scala

Master's Degree with working experience OR Phd

Understanding of deep learning is a plus

Data Engineer will be responsible for:

Building Models reliable on machine learning, graph mining and other data driven technologies. Identify potential threats in KYC, Payments, Credit, Reward etc

Maintain the Models to meet business expectations.

Work on data that analyses customer feedback and satisfaction analysis by leveraging on machine learning techniques.

Collaborate effectively with various stakeholders

Please note that this opportunity is open for Singaporeans only."
29-Apr-2022 T11:46,Principal Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,9 hours ago,,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 10-12 years of experience in Data Engineering role and have good knowledge / working experience in:
• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.
• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.
• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.
• Functional programming languages, e.g. Scala.
• Virtualization and container environment such as Docker and Kubernetes.
• Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:46,Data Engineer,"nSearch Global Pte Ltd, EA Licence No: 10C3636",16 hours ago,,Full–time,"Our client, one of Asia-Pacific’s leading organizations is looking for:
Data Engineer

Responsibilities:

Perform data extraction, cleaning, transformation, and flow. Web scraping may be also a part of the work scope in data extraction Design, build, launch and maintain efficient and reliable large-scale batch and real-time data pipelines with data processing frameworks Collaborate with Project Manager, Visualisation Developer, Data Scientist, UX Designers and Data Analyst to build scalable data-driven products Work in an Agile Environment that practices Continuous Integration and Delivery Support and advise best practices in data management standards, policies and procedures

Requirements:

Interest in being the bridge between engineering and analytics Knowledge about system design, data structure and algorithms Familiar with data modelling, data access, and data storage infrastructure like Data Mart, Data Lake, and Data Warehouse Proficient in general data cleaning and transformation (e.g. SQL, pandas, R, etc) Comfortable in at least one scripting language (eg. Python) Familiar with rest API and web requests/protocols in general Proficient with processing data from open-standard file format rchange format (e.g. XML, JSON) Good in building ETL pipeline (eg.

Talend Data Integration, Informatica Power

Center, IBM DataStage, SAS Data Integration) Good in database design and various databases (e.g. MS SQL Server, S, IBM DB2, Postgres etc) ----------------------------------------------------------------------------------------------------------------- Interested applicants can also email CV at harry (for faster processing, please state the exact job / position title applied “Data Engineer”) Only shortlisted candidates will be notified.

------------------------------------------------------------------------------------------------------------------ EA License Number: 10C3636 EA Personnel Name:
Arora, Hardeep EA Personnel Registration Number: R1111454"
29-Apr-2022 T11:46,Data Engineer,"Huxley APAC, EA Licence No: 09C5506",23 hours ago,,Full–time,"Huxley is currently working with a digital assets company that is rapidly expanding in the region. This is an exciting opportunity for highly motivated individuals keen to work in a busy environment and contribute with ideas.

We are currently looking to fill 2 Data Engineer positions focusing on data modelling and risk related projects:

Data Engineer should have the following:
• 3+ year of experience developing machine models
• Proficient in either Python, Java and/or Scala
• Master's Degree with working experience OR Phd
• Understanding of deep learning is a plus

Data Engineer will be responsible for:
• Building Models reliable on machine learning, graph mining and other data driven technologies. Identify potential threats in KYC, Payments, Credit, Reward etc
• Maintain the Models to meet business expectations.
• Work on data that analyses customer feedback and satisfaction analysis by leveraging on machine learning techniques.
• Collaborate effectively with various stakeholders

Please note that this opportunity is open for Singaporeans only.

If you are interested in the above-mentioned Data Engineer role located in Singapore, do contact me and attach your latest CV. I will be glad to provide additional information and guide you through the next steps where necessary.

Consultant Details:

Anastasija Horoscuka

EA License No.: R

Huxley is a trading division of SThree Pte Limited

(Registration Number: E | SThree Pte Limited Licence Number 16S8216 | Huxley Licence Number J)

As per Ministry of Manpower (MoM) requirements, if you're suitable for any roles that we will be putting you forward for, we will need to request for your identification number. Please be assured that this will not be disclosed with our client or any other parties other than MoM.

Award winner of:

Recruitment Agency of the Year by Asia Recruitment Awards 2019 | Best Client Services by Asia Recruitment Awards 2017 | Best Overseas Operation by Global Recruiters 2017 | Highly Commended for Best Large Recruitment Business 2017 | Commended for Best In-House Training by Global Recruiters 2017"
29-Apr-2022 T11:46,Senior Data Engineer – AI & IoT,Holloberg,9 hours ago,,Full–time,"Senior Data Engineer – AI & IoT Jobs in Singapore at Holloberg

Title: Senior Data Engineer – AI & IoT

Company: Holloberg

Location: Singapore

We have partnered with a growing Technology company to look for a Senior Data Engineer. An opening for someone looking to be involved with cutting edge innovative technologies such as Artificial Intelligence and Internet of Things, and gain exposure in a dynamic environment. It’s a great opportunity for someone to join a multicultural team with great company culture.

Responsibilities:

Create and maintain optimal data pipeline architecture.

Assemble complex data sets to meet specific requirements.

Creating data systems that ingest data from various sources.

Implement flows with distributed systems and cloud architecture.

Write efficient, well documented, and highly readable code.

Schedule/automate data pipelines and monitor their performance.

Write ad-hoc queries in order to perform analysis

Interact with the teams to gather requirements and explain his work

Requirements:

Experience with Data Warehousing and Databases (MySQL, PostgreSQL or MongoDB),

Having experience with Python would be a plus.

If you’d like to hear more, please apply and/or contact us at:www.holloberg.com

Business Registration No: 201805144DEA Registration No: R1653145License No: 18C9522

Job Types: Full-time, Permanent

Salary: $7,000.00 – $9,000.00 per month

Benefits:

Cell phone reimbursement

Dental insurance

Health insurance

Vision insurance

Work from home

Schedule:

Flexible hours

Monday to Friday

Supplemental Pay:

Performance bonus

Yearly bonus"
29-Apr-2022 T11:46,Data Engineer,Achieve Group,20 hours ago,,Full–time,"Job Description
• Up to $8.5k basic
• Experienced in Health Informatics/ data management is advantageous
• Multi-award-winning healthcare IT leader

Our client played a key role in helping all major public healthcare institutions in Asia Pacific, transforming healthcare through smart technology, to achieve international benchmarks for advanced technology used in patient care.

Job Description
• Support translation of data business needs into technical system requirements
• Build data flow channels and processing systems to extract, transform, load and integrate data from various sources
• Develop complex code, scripts and data pipelines to process structured and unstructured data
• Make data science code production ready to ensure it is maintainable, scalable and debuggable.
• Deploy machine learning models to production and build the framework to monitor the performance of the model.
• Test data system configurations
• Support the handling and logging of errors
• Monitor data system performance
• Develop tools to improve data flows between internal/external systems and the data lake/warehouse

Requirements
• Degree/Master in Computer Science, Information Technology, Computer Engineering or equivalent.
• Demonstrate good, in-depth knowledge in relevant Extract-Transform-Load (ETL) hardware/software products, frameworks and methodologies.
• Experience with at least two of the following areas:
• databases (e.g. Oracle, MS SQL, MySQL, Teradata)
• big data (e.g. Hadoop ecosystem)
• ETL development using ETL tools (e.g. Informatica, IBM Datastage, Talend)
• data repository design (e.g. operational data stores, dimensional data stores, data marts)
• data interrogation techniques (e.g. SQL, NoSQL).
• structured and unstructured data analytics.
• batch and real-time data ingestion and processing
• data quality tools and processes.
• data transformation and terminology equivalence mapping.
• Experience in data modelling for analytics (e.g. star schemas, snowflake schemas).
• Experience with data acquisition tools (e.g., ETL, real-time data capture, and change data capture).

How to Apply

Simply submit your application by emailing a detailed copy of your updated Resume in MS Word Format to

or call your friendly Consultant, Esther Quak Soo Chin (Reg. No.: R21101335)(ICS), at 6590 9907 for a confidential discussion.

or Click the ""Apply Now"" button at the bottom of the page

YOUR SUCCESS IS OUR ACHIEVEMENT

Notice:

We regret that only short-listed candidates will be notified. All applications will be treated with the strictest confidence.

By submitting any application or resume to us, you will be deemed to have read & agreed to the terms of our Privacy Policy, and consented to us collecting, using, retaining and disclosing your personal information to prospective employers for their consideration, and for our marketing EDMs which you may opt out by unsubscribing in the mailer. You may refer and access our website at for more information.

Cessation of Collection of full NRIC Numbers:

In compliance with the Personal Data Protection Act and commitment to protect candidates' personal data, Achieve Group will cease to collect, process or use full NRIC numbers during our screening and job application process.

Kindly ensure your resumes provided to us does not contain your full NRIC number and full home address during your job application"
29-Apr-2022 T11:46,"Manager, Data Informatics",Twillio,16 hours ago,,Full–time,"See yourself at Twilio Join the team as our next Manager, Data Informatics. Who we are & why we're hiring Twilio powers real-time business communications and data solutions that help build better applications and customer experiences. Although we're headquartered in San Francisco, we have presence throughout South America, Europe, Asia and Australia. We're on a journey to becoming a globally anti-racist, anti-oppressive, anti-bias company that actively opposes racism and all forms of oppression and bias. At Twilio, we support wherever we do business. We employ thousands of Twilions worldwide, and we're looking for more builders, creators, and visionaries to help fuel our growth momentum. About the job This position is needed tomanage a team of data analysts to optimize processes and improve/sustain systems. Twilio is growing rapidly and seeking a Manager of Data Analytics team to be a key player in the process optimization and data integrity and processes evolution to be more efficient across customers/sales/carrier relations and operations teams. You will work with Data Analysts, Systems Admins, Developers and Business Users to ensure the data integrity and process optimization projects, be accountable and improve efficiencies throughout Twilio. Reporting to the Head of PGE PMO & Process/Systems, this position is critical in supporting Twilio's growth efforts throughout the entire carrier's onboarding lifecycle and super network operations. Responsibilities In this role, you'll: Live and have proven success partnering cross-functionally inside and outside the R&D and GTM orgs to align resources to accomplish multiple projects. They also have: Able to maintain and ensure data integrity across the business. Knows how to create and maintain reports and dashboards using Salesforce, looker and google analytics. Able to improve the customer experience and optimize the internal processes. Develop tools using Salesforce and integrated systems (i.e. DocuSign, Aptus Agreements +) Driven teammate with good communication skills and enthusiasm for collaboration. Manage the PLC of Salesforce from requirements gathering, implementation, testing to enabling users Impeccable attention to detail in a fast-paced work environment. Problem solving skill is a must and willingness to learn. Develop and maintain project plans and end-to-end documentation. Qualifications Not all applicants will have skills that match a job description exactly. Twilio values diverse experiences in other industries, and we encourage everyone who meets the required qualifications to apply. While having desired qualifications make for a strong candidate, we encourage applicants with alternative experiences to also apply. If your career is just starting or hasn't followed a traditional path, don't let that stop you from considering Twilio. We are always looking for people who will bring something new to the table! Required: 3+ years of managerial experience with a bachelor's degree in Business, Mathematics, Statistics, Computer Science, Engineering, Economics or related field. Experience analyzing and tracking performance indicators across projects. Be a professional for data, analytics, and testing to ensure accurate and proper interpretation of core business metrics and consumer behavior. Experience working independently and as part of a team. Will take the initiative to drive projects forward even under ambiguous circumstances. Require a SFDC certification. Desired: Experience with BI analytics Excellent written and verbal communication skills. Ability to influence and build effective working relationships with all levels of the organization. Location This role will be based in our Singaporeoffice. Approximately 10% travel is anticipated. What We Offer There are many benefits to working at Twilio, including, in addition to competitive pay, things like generous time-off, ample parental and wellness leave, healthcare, a retirement savings program, and much more. Offerings vary by location. Twilio thinks big. Do you We like to solve problems, take initiative, pitch in when needed, and are always up for trying new things. That's why we seek out colleagues who embody our values - something we call. Additionally, we empower employees to build by supporting their volunteering and donation efforts. So, if you're ready to unleash your full potential, do your best work, and be the best version of yourself, apply now! If this role isn't what you're looking for, . The successful candidate's starting salary will be determined based on permissible, non-discriminatory factors such as skills, experience, and geographic location within the state. This role is also eligible to participate in Twilio's equity plan and for the following benefits: health care insurance, 401(k) retirement account, paid sick time, paid personal time off, paid parental leave"
29-Apr-2022 T11:46,Data Engineer,Brenntag Asia Pacific Pte. Ltd.,16 hours ago,,Full–time,"At DigiB, we combine cutting-edge start-up environment with resources of a global company to make game – changing decision and drive digital transformation in the chemical industry

We design tools that help our customers, employees and suppliers to do their job in their best possible and efficient way.

We are developing Digital Products for our customers & partners

We will utilise Technology in a smart way to enable improved processes and reduce pains

We will touch on areas in Customer Service, Technology, Data, Supply Chain and Finance to name a few.

An idea of what you will do:

You will be responsible for building, maintaining & managing the cloud infrastructure
You will be responsible to setup & manage services connected to cloud infrastructure – including but not limited to AD, SSL Certificates, VPNs, Mail Servers, API configurations
You will design and deliver the production support management tools & services – Monitoring, Alerting, Capacity & Performance management
You will be responsible for setting up and managing log analysis & security monitoring analysis
You will be responsible for investigating & resolving infrastructure issues and collaborate with technology providers to resolve in a timely manner
You will administer & control access to all infrastructure services & tools
You will maintain application database & system logs backups and perform restores as needed by team
You will collaborate with IT security officers to ensure platform access & data is secured
You will be responsible for infrastructure failover assessments & disaster recovery drills
You will collaborate with technology services providers to proactively implement preventive measures and provide RCA report of infra issues
You will maintain complete inventory of all infrastructure resources
You will publish weekly reports of infrastructure usage, availability, risks and issues
You will support technology manager to assess platform reliability, robustness, security vulnerability and comparisons of cloud services and management tools
You will manage the Application code deployments, including defining and implementing any automated solutions for deployments
You will assist with the design and implementation of the continuous integration, automated testing and continuous deployment initiative
Who you are and your experience:
You have a degree or equivalent in computing/engineering from a reputable institution or relevant work experience
You have working experience building & maintaining modern technology platforms
You have hands on experience in enterprise systems & network architecture, cloud infrastructure i.e

Azure
You are familiar with enterprise systems like ERP, CRM, ECM, BPM & DWH systems
You have experience in collaborating with regional/global technology teams and diverse technology & service providers
You are able to work unsupervised & can make informed decisions
You are a hands-on team player and lead by example
You thrive in an unstructured and extremely agile environment
You have excellent communication and interpersonal skills to establish and build sustainable internal and external relationships
You should have a Growth mindset
Your Technical Skills:
You MUST have deep expertise in one of the Cloud technologies e.g

AWS, Azure
Must have hands on experience managing VMs, Containers using Unix shells (SSH)
Must have experience in setting up networking infrastructure, DNS, DMZ, VPNs, bastion, mail servers & API Gateways
You should have experience in one or more of the following API / integration services - WSO2 API, BPM, Mulesoft, TIBCO ESB
You should have experience using message queues / data streaming using Kafka, RabbitMQ, IBM MQ etc.
You should have experience with monitoring & Log analysis tools such as Grafana, Splunk etc
You have sound knowledge of Java technology stack , Spring Framework and Data Adapters
You have experience using micro-services deployment and Dockers management
You have DBA experience managing SQL & No-SQL databases and data management (PostgreSQL, Mongo, Redis)
You are familiar with API messaging specification JSON, YAML & REST
You are familiar with data engineering and supporting tools e.g

Hadoop, Spark, Kafka, Qlik
Familiarity with SAP data stream integration or similar enterprise solution"
29-Apr-2022 T11:46,Data Engineer,Geniebook,16 hours ago,,Full–time,"Loved by over 150,000 users, Geniebook is Singapore’s largest online learning platform for English, Mathematics and Science (EMS) syllabus. From AI-personalised worksheets to live classes and teacher chats, we are a powerful suite of complementary learning products designed to help students accelerate their academic performance. We’re looking out for amazing global talents to scale Geniebook exponentially across Southeast Asia, and we have many exciting roles opening up across a wide range of functions.

So join us today to inspire a new generation of learners and help even more students learn smarter and do better!

Job Scope:
• Design, build and maintain reliable and efficient pipelines to extract and process data from multiple sources to suit diverse data consumers and business units
• Work closely with the Engineering team to make sure the data handling infrastructure and pipelines are up-to-date, efficient, and in line with industry standards.

Job Requirements:
• Bachelor's degree or equivalent experience in quantitive field (Statistics, Mathematics, Computer Science, Engineering, etc.)
• At least 1 - 2 years of related experience
• Knowledge of database theories and design
• Experience in relational and non-relational database management systems, such as MySQL, NoSQL, or similar
• Strong programming skills, particularly in Python, C++
• Familiarity with distributed processing and managing of big data using Apache Hadoop, MapReduce, Apache Hive, or similar frameworks
• Previous exposure to ETL tools and technologies
• Proven competency in creating and maintaining a Data Warehouse or Data Science Pipeline
• Awareness of data storing, maintenance, and migrations costs.
• Ability to work in a multidisciplinary team"
29-Apr-2022 T11:46,"AVP, Data Engineer - BI Tools",United Overseas Bank,5 hours ago,,Full–time,"AVP, Data Engineer - BI Tools

AVP, Data Engineer - BI Tools

Posting Date: 25-Apr-2022

Location: Alexandra, Singapore, SG

Company: United Overseas Bank Limited

About UOB
United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices.
Our history spans more than 80 years. Over this time, we have been guided by our values - Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.

About the Department
The Technology and Operations function is comprised of five teams of specialists with distinct capabilities: business partnership, technology, operations, risk governance and planning support and services. We work closely together to harness the power of technology to support our physical and digital banking services and operations. This includes developing, centralising and standardising technology systems as well as banking operations in Singapore and overseas branches.

Job Responsibilities
You will be responsible for the end-to-end software development and support for all work related to projects, quarterly change requests, L3 production fixes. This includes software product implementation and administration, application design, development, implementation, testing and support. You will be expected to work on Finance & Risk Analytics.
You will also be responsible for quality assurance of the team's delivery in conformance with the Bank-defined software delivery methodology and tools. You will partner with other technology functions to help deliver required technology solutions.

Other responsibilities include:

• Create frameworks, technical features which helps in faster operationalisation of Data models, Analytical models(including AI/ML) and user generated contents (dashboards, reports etc.)

• Effectively partner with citizen data scientists in enabling faster adoption of AL/ML model based systems

• Independently install, customise and integrate software packages and programs

• Carry out POCs involving new data technologies

• Design and develop application frameworks for data integration

• Create technical documents such as solution design, program specifications for target solutions

• Perform design and development of applications which may not be limited to: Software Applications, Data Integration, User Interfaces, Automation

• Maintain and recommend software improvements to ensure a platform centric management of software applications

• Performance tuning

• Work with production support team members to conduct root cause analysis of issues, review new and existing code and/or perform unit testing

• Perform tasks as part of a cross functional development team using agile or other methodologies and utilising project management software

Job Requirements
Functional skillsets
Data Lake, EDW, Data Mart, Data Integration & Visualization
Hands-on experience in implementing large scale data warehouse & analytics platforms in financial services industry with good functional knowledge of products & services offered in Retail bank / Wholesale / Global Markets covering some of the following analytics domains:

• Setting up and running BI tools oriented platform
• Design and develop QlikSense & Microsoft Power BI applications
• Design and develop Applications in SAS, Microsoft-R, Python
• Integration of BI tools with data stores (EDW, data marts )
• Experience in Data Modeling, Data mapping for Data Warehouse and Data Marts solutions
• Experience in financial domain - Retail , Wholesale, Compliance, Digital
• Expertise in design of role based fine grained access control
• Designing cloud ready data solutions, Virtualization
Technical skillsets

• Expertise in installing and configuring of BI/Analytical tools like Qlik, Power BI, SAS, Microsoft R
• Expertise in administration (QMC, powershell scripting), infrastructure design of such tools
• Integration with other data systems like EDW, Data mart, CRM, Digital
• Expertise in building BI semantic layer (Qlik - QVD, PowerBI - Direct Query, SSAS, In Memory SSAS modelling, SSRS, Excel Add In)
• Creating application framework specific to BI tools to speed up user adoption
• Good working experience in security features of BI tools including integration with Active Directory, solutions to protect data in motion and at rest
• Good working experience in fine tuning on line BI queries, BI models, analysis of DAX queries
• Reports/Dashboard/Application development expertise using QLIK, PowerBI, SAS, Microsoft R, Python
2 to 3 technical certifications from enclosed list:

• Cloudera Hadoop distribution - Hive, Impala, Spark, Kudo, Kafka, Flume
• Teradata - Bteq, Query Grid, GCFR, MDM, Data Mover, BAR
• Informatica Data Integration - PC, IDR, BDM, MM, IDQ, EDC
• Data modelling tools (Erwin)
• QlikSense
• Microsoft Power BI - Direct Query, SSAS, SSRS, Excel Add In
• Microsoft - R
• Data science workbenches - Cloudera Workbench, Jupyter, DataRobot, H2O.AI, IBM DSX
• Data Virtualization tool - Denodo, Dremio
• AS400
• Language - SQL, Java, Python, Scala, Pyspark
• Automation / scripting - CtrlM, Shell Scripting, Groovy
Experience for an added advantage:

• CI/CD software, Testing Tools - Jenkins, SonarQube
• Version Control Tool - Aldon+LMe, CA Endeavor
• Deployment Tool kit -Jenkins
• Service or Incident Management (IcM) Tools - Remedy
• Source Code Repository Tool - Bitbucket
• Scheduling Tool - Control-M
• Defect Management Tool - JIRA
• Application Testing tool - QuerySurge
• Cloud certification
• Platforms provided by FICO, Experian, SAS for credit and portfolio management

Be a part of UOB Family
UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.

Apply now and make a difference"
29-Apr-2022 T11:46,Principal Data Engineer,SingTel,27 minutes ago,,Full–time,"Job Description :

At Singtel, we believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers. We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative.
Join us and experience what it's like to be with an Employer of Choice*. Together, let's create a brighter digital future for all. *Awarded at the HR Fest Awards 2020.
We are seeking a Principal Data Engineer who will be responsible to lead the data engineering solutions, deliveries, and operations. Ensure the efficient and sustainable operation of Singtel Unified Data Platform and Event Streaming Platform, and to build and maintain large-scale, highly available, high-performance distributed systems based on system availability and performance. You will be part of Group IT Data & Platform Management team. In this position you will work with business, IT, and data professionals.
The role covers the following key objectives:
• Lead a team of data engineers and be a mentor to the team on development best practices
• Take ownership of the team deliverables, delegate the development tasks efficiently to the team and ensure timely delivery
• Provide solution oversight to delivery architects and data engineers
• Additional to being the lead of the development team, contribute as a senior individual contributor on complex modules of the platform architecture, design, and development
• Drive, lead and align new capabilities in data engineering and data integration across Singtel Group
• Develop new data solutions and accelerators that help to deploy our data platform and engineering services at scale
• Lead, manage and run data platform operations and application support
• Setup and operate site reliable engineering (SRE) for data engineering to achieve efficient, stable, and sustainable operations
• Plan and manage annual budget for both capital expenditures and operating expenses
• Define standards and guidelines for development and operations
• Contribute technical and thought leadership to Singtel data platform and engineering initiatives, programs, and roadmap
• Build and maintain strong relationship with business leaders, Group IT domains (departments) and IT service providers to deliver value via data
• Present and pitch at relevant senior leadership levels and /or executive steering committees
• Promote an engineering-centric and devops culture through building relationships with development & operations and driving enhancements to the end-to-end release process\
• Build, hire and retain a high-performance data engineering team

Requirements
• Bachelor's degree in Business Management, IT, Computer Science or equivalent.
• At least 12 years of experience in big data and / or data warehouse, including at least 8 years of experience in leading data engineering and operations
• Expert in building and optimizing data pipelines using Scala, Spark
• Experience in managing large scale data warehouse / data lake in both on-prem and cloud with high availability and scalability
• Experience in data management, data architecture and design
• Experience in event streaming with Kafka
• Strong technical knowledge of data integrations including a data engineering framework
• Strong knowledge of SQL, Scala, Spark, Hadoop
• Hands-on experience in handling incident, problem, configuration, capacity, and availability management
• Experience with DevOps tools and environment
• Experience with Cloud environment - AWS, Azure, GCP
• Experience with monitoring and load balancing tools
• Experience with SRE
• Experience in managing and driving outsourced vendors to delivery and operations objectives
• Experience in leading complex and major change initiatives and programs
• Strong background in operational and capital finances, and IT budget development"
29-Apr-2022 T11:46,Senior Data Engineer,Quantexa,21 hours ago,,Full–time,"Founded in 2016 with only a handful of individuals, Quantexa was built with a purpose that through a greater understanding of context, better decisions can be made. 6 years, 10 locations and 500+ employees later we still believe that today. We connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data.

Due to the continuous success and high demand from our customers, we are looking for a Senior Data Engineer with a proven track record to join the Quantexa family. 🚀

What does a Senior Data Engineer role at Quantexa look like?

In order to be a successful data Engineer at Quantexa, you’ll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.

Being Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. You’ll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service. 🥇

We want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala, Data Fusion and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but don’t worry If that’s not your strongest language or if you haven’t used it before, we make sure that every Quantexan goes through our training academy so they’re comfortable and confident with using our platform.

Requirements

• We’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.
• The desire to learn and code in Scala

• Experience in working in an Agile environment
• Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.
• A strong coding background in either Java, Python or Scala
• Experience of building data processing pipelines for use in production “hands off” batch systems, including either traditional ETL pipelines and/or analytics pipelines.
• Passion and drive to grow within one of the UK’s fastest growing scale-ups.
• Consulting or business facing skills and a desire to work with customers.

Benefits

Why join Quantexa?

We know that just having an excellent glass door rating isn’t enough, so we’ve put together a competitive package as a way of saying thank you for all your hard work and dedication.

We offer:

• Competitive salary 💰
• Company bonus
• Private healthcare with Sigma plus life insurance and critical illness
• Free Calm App Subscription #1 app for meditation, relaxation and sleep 🧘‍♀️
• Annual leave, national holidays + birthday off! 🌴
• Ongoing personal development
• Great WeWork Office Space & Company wide socials"
29-Apr-2022 T11:46,Data Engineer,St Engineering Unmanned & Integrated Systems Pte. Ltd.,16 hours ago,,Full–time,"We are looking for an experienced AI Engineer to join our multidisciplinary team in transforming our MRO business, using deep learning and neuro-linguistic programming (NLP) to help us improve various business outcomes and drive innovation.Job Responsibilities:Perform research and development (R&D) and processes to meet the needs of our AI strategyWork with operation teams to identify and prioritize key areas of the business where AI solutions can drive significant business benefitDesign and develop AI Solutions for MRO business Optimization.Work on functional design, process design (including scenario design, flow mapping), prototyping, testing, training, and defining support procedures, working with Group Engineering Centre and Operation Teams.Document and articulate solution architecture and lessons learned for each exploration and accelerated incubationJob Requirements:Degree in Computer science, Electrical Engineering / Mechanical Engineering or equivalent2+ years of experience in applying AI to practical and comprehensive technology solutionsProficient in C, C++ and Python programmingFamiliar with Windows and Linux programming environmentsProven experience with ML, deep learning, Tensorflow, NLPStrong interpersonal and communication skills.Ability to contribute as a team player or independently.Ability to demonstrate a high level of initiative and resourcefulness.Location: Ang Mo KioSingaporean only"
29-Apr-2022 T11:46,Data Engineer,Tata Consultancy Services,16 hours ago,,Full–time,"Required Qualifications3-5 years' experience in ETL workflows on Big Data platform using Informatica Data Engineering Quality (version 10.X)Experience in data profiling, cleansing, parsing, standardization, verification, matching and data quality exception handling.Strong technical skills in SQL/PLSQL, HiveQL, SparkSQLAble to apply best-practice optimization techniques on ETL workflowsExperience and understanding of testing, coding, design, documentation and change management procedures.Strong work ethic with a highly positive, hands-on, can-do attitude and flexible team player.Ability to manage tasks independently, take ownership of responsibilities and work with minimal supervision."
29-Apr-2022 T11:46,"AVP, Software Engineer, Data Technology, Technology and Operations",DBS Bank,16 hours ago,,Full–time,"Business Function

Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation.

In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.

Responsibilities
• Build the enterprise data platform and tooling for the users of the data platform.
• Incident management : troubleshoot business critical incidents. Analyse patterns of production incidents, develop permanent remediation plans, and implement automation to prevent future incidents from occurring through software engineering.
• Programming and scripting together with development teams to automate everything to remove the toil.
• Setting up strategies for observability, automating monitoring and auto-remediation of known issues.
• Work with remote teams.

Requirements
• Bachelor's degree in Computer Science or a related technical field involving software or systems engineering, or equivalent practical experience.
• Experience programming in at least one JVM languages (Java, Scala, Clojure).
• Experience programming in another language C, C++, Python, Javascript or Go.
• Strong experience in managing at least two of the following : Kafka, YARN, Spark, Cassandra, Elasticsearch, Kubernetes, Ansible.
• Strong experience in problem solving and analyzing global scale distributed systems including at least two of the above.
• Strong experience in managing at least one of the following monitoring solutions : Open-Telemetry, Prometheus, Grafana.
• Expertise in Unix / Linux systems, IP networking, performance and application issues.
• Experience in designing, analyzing and building automation and tools for large scale systems.
• Ability to debug, optimize code, and automate routine tasks.
• Knowledge of Linux systems internals.
• Hands-on technical experience.
• Effective communication skills.
• Experience with software product development.
• Experience with Java application servers and JVM configuration.
• Experience writing Kubernetes controllers and operators.
• Ability to work independently and make decisions under minimal supervision.
• Experience in building solutions with AWS, Google, Azures and other cloud services.

Apply Now

We offer a competitive salary and benefits package and the professional advantages of a dynamic environment that supports your development and recognises your achievements"
29-Apr-2022 T11:46,Data Engineer Big Data,ADECCO PERSONNEL PTE LTD,22 hours ago,,,"Job Description
• Development efforts in an agile software development environment, participate in sprint planning, task breakdown, and daily stand ups
• Work with product owners and architect to understand objectives and translate these into a system level design and implementation
• Implement designs that meet quality standards, coding standards, and provide a rich user experience across platforms
• Design and implement new frameworksand software that meets DBS’s standards in performance, reliability, and maintainability
• Create rapid prototypes to vet new design, interaction, and integration concepts.
• Collaborate across cross locations IT teams in delivering software components
• Investigate and resolve SIT and UAT defects
• Troubleshoot and solve production issues related to performance and reliability throughout the software stack
• Proactively identify bottlenecks in the system and work to resolve these issues before they become a problem

Requirements
• 2 – 5 years of Actimize development (WLF, CDD, SAM)
• Relevant AML experience in banking environment
• Able to perform linux / windows scripting.
• Experience in Java development will have an advantage
• Candidate with operational experience, preferably in support of critical application systems, will have an advantage

Saghana Sithara | Registration Number: R1550224"
29-Apr-2022 T11:46,Data Engineer - APAC,Tamr,22 hours ago,,Full–time,"Company Description

Tamr is the enterprise data mastering company trusted by large enterprises like Blackstone, the US Air Force, Toyota, and GSK. The company’s patented software platform uses machine learning supplemented with human feedback to master and prepare data across myriad silos to deliver previously unavailable business-changing insights. With a co-founding team led by Andy Palmer (founding CEO of Vertica) and Mike Stonebraker (Turing Award winner) and backed by top-tier investors such as NEA and GV, Tamr is transforming how companies get value from their data.

Job Description

Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE

who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being

customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation

This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.

Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion

Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:46,Data Engineer,Endowus,6 hours ago,,Full–time,"About us
Endowus is Asia's leading fee-only digital wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping all investors grow their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.
Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones. Our mission is clear: help people invest better so they can live easier today, and better tomorrow.

The people you will work with
The team has deep domain knowledge in finance and technology, bringing together decades of experience at Goldman Sachs, Morgan Stanley, UBS, Credit Suisse, The Blackstone Group, AQR, Grab, Dropbox, Lyft, Redmart, Carousell, Bytedance, Grab, Kakao, Alibaba, and more See our leadership team here . We practise inclusion and treasure our diversity in background and experience. A diverse team is our biggest asset and we look for people who share our belief in Endowus' clear mission.

Investors, recognition, licensing
Endowus is backed by global leading strategic and venture capital investors including UBS, Samsung Ventures, EDBI, Prosus Ventures, ZVC, Singtel Innov8, Lightspeed Venture Partners, and SoftBank Ventures Asia.

Endowus has been recognised by the industry with the following awards: Singapore's Rising Star and Fintech Innovation (Asia Asset Management's Best of the Best Awards 2021), LinkedIn Top Start-ups 2021, WealthTech of the Year (Asia FinTech Awards 2021), and the top 15 Singapore's Best Workplaces Award (Great Place to Work).
Endowus is licensed by the Monetary Authority of Singapore (MAS).

About this role; responsibilities & ownership
• We are looking for a Data Engineer who thrives in a fast-paced environment and enjoys driving innovation through rapid prototyping and iterative development.
• Being part of Endowus's Technology team, you will build end-to-end product features that you are confident of delighting user experiences.
• You will need to leverage the entire technology stack to realise these goals and will be part of a team that is constantly tackling difficult questions of scale, architecture, and interaction. Requirements & qualifications

• Bachelors' or above in Computer Science, a related field, or equivalent professional experience
• 3-6 years of experience in designing and implementing key components for highly scalable, distributed data collection and analysis systems built for handling large volumes of data in cloud
• Advanced working SQL knowledge and experience working with relational databases or BigQuery, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimising data pipelines and data sets in Apache Flink or Apache Spark to answer specific business questions.
• Experience in at least one of the JVM languages such as Java, Scala.
• Experience Build processes supporting data transformation, data structures, metadata, dependency and workload management. (experience in Airflow is Plus)
• Experience with distributed databases, such as Cassandra, and the key issues affecting their performance and reliability.
• Hands-on development with key technologies including Scala, Apache Flink and other relevant distributed computing languages, frameworks, and libraries
• Independence and self-reliance while being a proactive team player with excellent communication skills.
• Self-driven, has a strong sense of ownership and able to work and learn independently
• Comfortable with numbers and motivated by steep learning curves Nice to haves
• Experience with message broker systems, such as Apache Kafka.
• Developed mission-critical products, e.g. trading platforms, medical applications
• Basic knowledge of finance and trading
• Preferably some exposure to CQRS / Event Sourcing patterns
• Familiarity with Docker, Kubernetes and Terraform Remote Okay
• We are open to hiring remotely in Asia time zones. Benefits & perks
• Stock options in a fast growing company *
• Employee discount for investing at Endowus
• Flexible working hours and locations so you can live life and your best work - you are trusted to be responsible
• Competitive Staff Benefits; Annual Leave, Medical insurance, Dental *
• Productivity Equipment; Omnidesk Standing Desk & Ergonomic Chairs, Dell UltraSharp USB Type-C Monitor, high-end computer peripherals
• Choice of Apple or PC equipment
• Additional S$250 /HKD 1450 budget for your favourite hardware to boost productivity * Note: * is only applicable to Full-Time employees"
29-Apr-2022 T11:46,Data Engineer,GLP PTE. LTD.,16 hours ago,,Full–time,"The Data Engineer will be a key player in the enterprise-wide data transformations projects. You will engage in developing and automating data processing pipelines for data modelling, analysis, and reporting from various data sources system; The primary responsibility of this position is to assist to establish the enterprise data Lake architecture under Microsoft Azure Data factory, Databricks and Synapse and deliver data driven solutions.

Job description:

• Assist in architecture design, develop, document, and implement end-to-end data pipelines and data driven solutions.

• Define roadmap to transform data architecture focusing on scalability, performance and stability for the entire data lifecycle;

• Build data flows for data acquisition, aggregation, and modelling, using both batch and streaming paradigms.

• Perform data analysis, data profiling, data cleansing, data lineage, data mapping and data transformation.

• Develop high-quality code for the core data stack including data integration hub, data warehouse and data pipelines under Azure service.

• Execute and deliver best practices in data management and data lifecycle processes, including modular development of data processes, coding and configuration standards, error handling and notification standards, auditing standards, and data archival standards.

• Implement security and standards, documenting technical specifications and operating procedures.

• Collaborate across developers as part of a SCRUM team ensuring collective team productivity

• Provide technical support for any data issues with recommendations, and resolutions.

Requirements:

• 2 years professional experience as a data engineer, software engineer, data analyst, data scientist, or related role.

• Experience with Microsoft Azure Data Integration Stack (Azure Data Lake Gen2, Azure Data Factory, Delta Lake, SSIS, SQL Server, Azure Data Warehouse), Databricks, Spark.

• Working experience in Investment or Real Estate industry, preferably with business and functional knowledge.

• Expertise building ETL and data pipelines on Databricks using data engineering languages Python and SQL on Azure.

• Advanced knowledge and experience working with Python & SQL;

• Proven experience with all aspects of the Data Pipeline (Data Sourcing, Transformations, Data Quality, Etc…).

• Experience with visual modelling tools including UML

• Proficient in using data visualization tool such as Power BI, Workiva and in standard office tools such as Excel.

• Familiar with DevOps and Agile methodology.

Recommended Skills

Agile Methodology

Apache Spark

Architectural Design

Architecture

Auditing Standards

Azure Data Factory"
29-Apr-2022 T11:46,Data Engineer,Lilith Games Sg Pte. Ltd.,16 hours ago,,Full–time,"Big Data Development Engineer (Advertising Direction)

Big Data Development Engineer (Advertising Direction)

What you will be doing:

1ã€ Responsible for the construction of Big Data Cloud Platform and the maintenance of Advertising Data Warehouse.

2ã€ Responsible for the calculation, cleaning and layering of Advertising Data based on Flink, Spark, etc., and store it through Hadoop, Clickhouse, etc.

3ã€ Responsible for Advertising Data Analysis and Advertising System Report development.

Qualifications & Skills

1ã€ Bachelor degree or above, major in computer and other related majors.

2ã€ Proficient in Python or Golang coding.

3ã€ Proficient with Mysql, Memcache, Redis, Message Queue and other common WEB components.

4ã€ Having experience in using one or more of HDFS, Hive, HBase, MongoDB, Kafka, Flink and Spark is preferred.

5ã€ Experience in operation and maintenance development is preferred.

6ã€ Familiar with Alibaba Cloud and other cloud computing resource deployment and optimization is preferredã€'

7ã€ Proficient in coding complex SQL Statements, and have the ability and experience of query optimization.

8ã€ Positive and optimistic, strong sense of responsibility, with good team communication and cooperation

å¤§æ•°æ ®å¼€å 'å·¥ç¨‹å¸ˆï¼ˆå¹¿å'Šæ–¹å 'ï¼‰

å·¥ä½œè Œè´£

1

è´Ÿè´£å¤§æ•°æ ®äº'å¹³å °æ å»ºï¼Œå¹¿å'Šæ•°æ ®ä»'åº'çš'æ å»ºä¸Žç»´æŠ¤ã€'

2

è´Ÿè´£åŸºäºŽFlinkã€ Sparkç‰å¯¹å¹¿å'Šæ•°æ ®çš'è¿›è¡Œè®¡ç®—ã€ æ¸…æ´—ã€ åˆ†å±'ç‰å·¥ä½œã€'å¹¶é€šè¿‡Hadoopã€ Clickhouseç‰è¿›è¡Œå˜å'¨ã€'

3

è´Ÿè´£å¹¿å'Šæ•°æ ®åˆ†æž å Šå¹¿å'Šç³»ç»ŸæŠ¥è¡¨çš'å¼€å 'ã€'

ä»»è Œè¦ æ±'

1.å¤§å¦æœ¬ç§'(ç»Ÿæ‹›)å Šä»¥ä¸Šå¦åŽ†ï¼Œè®¡ç®—æœºã€ é€šä¿¡ç‰ç›¸å…³ä¸'ä¸šã€'

2.ç†Ÿç»ƒæŽŒæ ¡Pythonæˆ–Golangä»£ç ç¼–å†™ã€'

3.ç†Ÿç»ƒä½¿ç'¨Mysqlã€ Memcacheã€ Redisã€ æ¶ˆæ ¯é˜Ÿåˆ—ç‰å¸¸ç'¨WEBç»'ä»¶ã€'

4.æœ‰ä½¿ç'¨HDFS, Hive, HBase, MongoDB, Kafkaï¼ŒFlink, Sparkä¸çš'ä¸€é¡¹æˆ–å¤šé¡¹çš'ç» éªŒä¼˜å…ˆã€'

5.æœ‰è¿ ç»´å¼€å 'ç» éªŒä¼˜å…ˆã€'

6.ç†Ÿæ'‰é˜¿é‡Œäº'ç‰äº'è®¡ç®—èµ'æº éƒ¨ç½²ä¸Žä¼˜åŒ–è€…ä¼˜å…ˆ

7.ç†Ÿç»ƒç¼–å†™å¤ æ 'çš'sqlè¯å ¥ï¼Œå…·å¤‡æŸ¥è¯¢ä¼˜åŒ–çš'èƒ½åŠ›å Šç» éªŒã€'

8.ç§¯æž ä¹ è§'ï¼Œè´£ä»»å¿ƒå¼ºï¼Œå·¥ä½œè®¤çœŸç»†è‡´ï¼Œå…·æœ‰è‰¯å¥½çš'å›¢é˜Ÿæ²Ÿé€šä¸Žå ä½œèƒ½åŠ›ã€"
29-Apr-2022 T11:46,Data Engineer,YASH TECHNOLOGIES SINGAPORE PTE. LIMITED,4 hours ago,,Full–time,"Technical Competence and Experience :
• Experience with the Microsoft Azure Platform (Data Lake, Data Factory, Databricks, Data Warehouse, Azure DevOps)
• Experience in DataBase and Information Modelling
• Experience with data wrangling using SQL and Python (R is a bonus)
• Experience with the Apache Spark Framework (preferably PySpark - Python)
• Experience with scripting languages such as Python and PowerShell (C#/.NET)
• Experience integrating systems and services
• Experience consuming REST APIs
• Experience developing REST APIs (preferably with Flask - Python)
• Experience with Docker containers

Soft Skills :
• Personal drive & problem-solving mentality
• Stakeholder management skills
• Ability to work across time-zones, culturally aware and able to collaborate with people from other countries.
• Team player – will collect and share information with colleagues in order to improve our services.
• Self-motivated and driven. Communicates and debates solutions to issues found Proactively contacts colleagues and takes the lead to work on problems or improvements.
• Service-minded.

Interested applicants can apply here or email to [ Email address blocked ]

Recommended Skills
• .Net Framework
• Apache Spark
• C Sharp (Programming Language)
• Data Lake
• Data Processing
• Data Warehousing"
29-Apr-2022 T11:46,Data Warehouse Engineer - KL,SearchAsia Consulting Pte Ltd,14 hours ago,,Full–time,"Responsibilities:
• Help build and maintain data warehouse platform with its associated data models and ETL processes
• Model the data and design ETL specifications to business requirements
• Design and develop data flows for data warehousing and self-serve reporting
• Automate and optimize existing data processing workloads to integrate with the data warehouse
• Monitor all data integrity performance and adopt appropriate tools
• Coordinate with Data Science team and collect all technical requirements
• Analyze query performance and perform query tuning to assist development engineers in designing and optimizing queries
Requirements:
• Bachelor’s Degree in Computer Science, Information Systems or similar
• 3+ years of Data Warehousing and Data Modeling experience.
• Hands-on experience with PostgreSQL and MYSQL Database
• Experience with all aspects of data systems including database design, ETL, aggregation strategy, and performance optimization
• Understanding best practices for building and designing ETL code and strong SQL experience with the ability to develop, tune, and debug complex SQL applications is required
Account Manager:
Kerwin Tan Kai Bin (R1331624)
kerwin.tan@searchasia.com.sg
EA: 16S8107"
29-Apr-2022 T11:46,Senior Data Engineer,Grabtaxi holdings pte ltd,9 hours ago,,Full–time,"Get to know our Team:
Data Engineering runs the code, pipeline and infrastructure that extracts, processes and prepares every piece of data generated or consumed by Grab's systems. We are a diverse team of software engineers that not only work to solve all kinds of data related problems faced by teams from all corners of Grab but we also act as a bridge that ties everyone together through data. As data in Grab never stops growing, this team also never stops, learning, innovating and expanding so that we can bring in or build the latest and best tools, technology to ensure the company's continued success.
Get to know the Role:
Data Engineers in Grab get to work on one of the largest and fastest growing datasets of any company in South East Asia. We operate in a challenging, fast paced and ever changing environment that will push you to grow and learn. You will be involved in various areas of Grab's Data Ecosystem including reporting & analytics, data infrastructure, and various other data services that are integral parts of Grab's overall technical stack.
The day-to-day activities:
• Build, deploy and manage big data solutions that can adequately handle the needs of a rapidly growing data driven company
• Spearhead the development of systems, architectures, and platforms that can scale to the 3 Vs of Big data (Volume, Velocity, Variety)
• Streamline data access and security to enable data scientists and analysts to easily access to data whenever they need to
• Build out scalable and reliable ETL pipelines and processes to ingest data from a large number and variety of data sources
• Maintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable and timely delivery of key insights for decision making
• Lead the movement cleaning and normalizing subsets of data of interest as preparatory step before deeper analysis by the data scientists
• Run Modern high performance analytical databases and computation engines like Spark, Flink, Presto, Synapse, BigQuery, Greenplum and others

The must haves:
• A degree or higher in Computer Science, Electronics or Electrical Engineering, Software Engineering, Information Technology or other related technical disciplines.
• Experience in handling large data sets (multiple PBs) and working with structured, unstructured and geographical datasets
• Designed high performance scalable infrastructure stacks for Big Data Analytics
• Deep understanding of databases and best engineering practices - include handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding how to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipeline
• Real passion for data, new data technologies, and discovering new and interesting solutions to the company's data needs
• Excellent communication skills to communicate with the product development engineers to coordinate development of data pipelines, and or any new products features that can be built on top of the results of data analysis"
29-Apr-2022 T11:46,Data Engineer,Manpower Singapore,16 hours ago,,Full–time,"Data Engineer Key Role and Responsibilities:Partner with the global and in-market data experts to discover and derive value from connecting external and internal data sources.Build (ETL) new and evolve data models and pipelines to power algorithmic based Business Intelligent solutions that addresses business problems requiring descriptive, diagnostic, predictive, and/or prescriptive analytics for pricing, promotion, trade spending, assortment, and sales performance management.Translates the algorithms and analytic models into data models as business needs evolves on a going basis after they are put into production.Develop a roadmap that scales existing and new data models, to support the portfolio of solutions.Expand into Business Intelligence solution development focused on automation and scale up of solutions.Key Skills Required:Professional Skills: R/Python programming languages MS Power BI MS Excel Data Visualisation General Skills: Client Management Project Management Diploma or B.S. in a relevant technical field (Operations Research, Computer Science, Statistics, Business Analytics, Econometrics, or Mathematics).Overall experience of 2-3 yearsAbility to manipulate and high-volume of data from varying sourcesExpert knowledge of an analysis tool such as Microsoft PowerBIProficiency in R/PythonBasic track record in working independently with minimal guidance Interested applicants, please submit your resume to : [ Email address blocked ] Abelene Marianne Kang Mrs Abelene Marianne Rozario R2089914EA License No: 02C3423 Reg No: 199505951HKang Abelene Marianne Mrs Rozario Abelene MarianneEA License No.: 02C3423 | Personnel Reg No.: R2089914 Please note that your response to this advertisement and communications with us pursuant to this advertisement will constitute informed consent to the collection, use and/or disclosure of personal data by ManpowerGroup Singapore for the purpose of carrying out its business, in compliance with the relevant provisions of the Personal Data Protection Act 2012. To learn more about ManpowerGroup's Global Privacy Policy, please visit [ Link removed ] Recommended Skills AlgorithmsAnalyticalAutomationBusiness AnalysisBusiness IntelligenceBusiness Requirements"
29-Apr-2022 T11:46,Senior Data Engineer,Techgig,16 hours ago,,Full–time,"About Job
CTC Undisclosed Job Location Singapore Experience 3 - 6 yrs
Description
This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide excellent user experiences. You will be an integral part of Payment Product Development team focusing on design and build of software solutions that leverage data to solve business problems.
The role is for a self-motivated individual with software engineering skills and expertise with Big Data technologies. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, development and test of new functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
Responsible for design, development, implementation and unit testing of applications
Work on development of new products iteratively by building quick POCs and converting ideas into real products.
Design and develop mission-critical systems delivering high-availability and performance.
Interact with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations while applying the latest available tools and technology.
Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.
Have a passion for delivering zero defect code and be responsible for insuring the team's deliverables meet or exceed the prescribed defect SLA.
Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code.
Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.
Perform other tasks on R&D, data governance, system infrastructure, and other cross team functions, on an as-needed basis
This is a hybrid position. Hybrid employees can alternate time between both home and office. Employees in hybrid roles are expected to work from the office three days a week, Monday (Sunday in some countries where Sunday is the start of the week) and Wednesdays. Wednesdays are designated as in-office collaboration days. The third day in the office will be decided based on team needs and determined in partnership with senior leadership.
Qualifications
Basic Qualifications:
BS/MS in Computer Science, Computer Engineering, or related field
Preferred Qualifications:
Successful candidates would usually have a mix of the following qualifications:
3 years of software design and development experience
Previous exposure to financial services is a plus, but not required
Extensive experience with SQL and big data technologies (Hadoop, Java, Spark, Hive etc.) tools for large scale data processing and data transformation
Experience with data visualization and business intelligence tools like Tableau, or other programs highly desired
Experience working in an Agile and Test Driven Development environment.
Experience with Kafka is highly desired
Strong knowledge of API development is highly desired
Strategic thinker and good business acumen to orient data engineering to the business needs of internal clients
Strong foundation in computer science, with strong competencies in data structures, algorithms and software design optimized for building highly distributed and parallelized systems
Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style"
29-Apr-2022 T11:46,"Data Engineer, Data Engineering (6 months Contract)",Bank of Singapore,23 hours ago,,Full–time,"At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programmes that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group's global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today!

Bank of Singapore opens doors to new opportunities.
Start your career with Bank of Singapore as a Data Engineer (6 months contract) in our Data Engineering team!
Bank of Singapore opens doors to new opportunities. At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programs that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group's global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today!
Bank of Singapore has defined an ambitious roadmap for accelerating its data excellence through the implementation of various bank-wide programs. As part of this roadmap, building the foundation of good data governance and management plays a key role in establishing trust in the data that we use.
General Description:
The incumbent will be based in the Data engineering team within the Data Hub team, supporting Bank of Singapore's data sourcing, curation, transformation, and data analytic services to accelerate their data initiatives. He/she will be responsible for expanding and optimizing BOS data pipelines as well as supporting data analytics needs.
Core activities :
• Be responsible for supporting data analytics and dashboarding with respects to the data availability / stability.
• Understanding the data and identifying any potential data issues as well as correcting issues arising from data.
• Define and implement data models suitable for dashboarding
• Power BI dashboarding support

Qualifications
Requirement :
General Knowledge & Experience
• Minimum of 6 years of working experience in data related functions
• Understanding of banking / financial services with exposure to data analytics preferred
Programming & Data
• Strong hands-on skills in SQL & PL/SQL
• Solid background in traditional structured database environments such as Teradata / Oracle
• Knowledge on Power BI dashboard functionality
• Strong skills in Power BI related technologies like power query and DAX
• Exposure to programming languages such as python
Communication & Soft Skills
• Curiosity & a real passion for understanding ""why?""
• Pragmatic ""can do"" approach to finding data based solutions to problems
• Good communication skills with ability to engage across business and technical audiences"
29-Apr-2022 T11:46,"Data Engineer, Capability Development (DART)",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Data Engineer in GovTech’s Data Science & Artificial Intelligence Division, you will be involved in architecting, managing and running advanced analytics and data platforms. For this role, you will be part of the Data Analytics Readiness Team (DART) within GovTech, where you will help drive GovTech’s Data Transformation efforts. You will assist to establish and implement effective operational workflows, data pipelines, procedures and best practices in accordance with government infrastructure and security policies to ensure that the platforms are monitored, secure, available and reliable.

We are looking for an independent and motivated engineer who has experience in both small and large-scale analytics platform projects, and in building and maintaining the infrastructure.

What you will be working on:
• Design, Architect, Deploy, and maintain solutions on Microsoft Azure or AWS using different Cloud & Big Data Technologies to provide secure and governed access to data for business users.
• Manage the full life-cycle of a data warehouse/lakehouse solutions from requirement gathering and analysis to platform selection, design of the architecture, and deployment.
• Collaborate with data stewards, data analysts and data scientists to build data pipelines from enterprise systems such as Workday to collect, clean, harmonise, merge and consolidate data sources for data warehouse/lakehouse.

Where we are looking for:
• Diploma/Degree in Computer Science or Information Technology or related disciplines
• Experience with the cloud (e.g. AWS, GCP, Azure)
• Hands-on experience in implementing Data Lake/Data Warehouse with technologies like – Databricks, Azure Synapse Analytics, SQL Database, AWS Lake formation.
• Programming & debugging skills in Python, Scala or R.
• Proficient in SQL.
• Knowledge of both SQL and NoSQL databases.
• Comfortable with DevOps tools like AWS Cloud Formation/Terraform, Docker and Git for CI/CD development.
• Prior experience with data engineering tools and frameworks like Airflow, Kafka, Hadoop, Spark, Kubernetes.
• Familiar in building REST services is good to have.

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:46,Sr Big Data Engineer,AccionLabs,14 hours ago,,Full–time,"Role:- Sr Big Data Engineer

Job Description

· 3+ years of professional software development experience with languages and systems such as Java, Python (PySpark), and version control (git), with good analytical & debugging skills.

· Big data: You have extensive experience with data analytics, and working knowledge of big data infrastructure such as Hadoop Eco System, HDFS, Spark, and AWS or Google or Azure (nice to have). You've routinely built data pipelines with gigabytes/terabytes of data and understand the challenges of manipulating such large datasets.

· Data Modeling: Flair for data, schema, data model, PL/SQL,Star & snow flake schema, how to bring efficiency in data modeling for efficient querying data for analysis, understands criticality TDD and develops data validation techniques.

· Real Time Systems: Understands evolution of databases for in-memory, NoSQL& indexing technologies along with experience on real-time & stream processing systems like kafka, Storm.

· Project management: You demonstrate excellent project and time management skills, exposure to scrum or other agile practices in JIRA.

· CS fundamentals: You have earned at least a B.S. / MS in Computer Science, or related degree AND you have a strong ethos of continuous learning.

All your information will be kept confidential according to EEO guidelines"
29-Apr-2022 T11:46,Lead Data Engineer,Ohmyhome Pte. Ltd.,16 hours ago,,Full–time,"Job Description:

-Assemble and collect data sets that meet functional and non-functional business requirements.

-Identify, design & implement internal process improvements, automating manual processes, optimizing data delivery, infrastructure for greater scalability.

-Build the infrastructure required for optimal extraction, transformation and loading of data from a variety of sources using SQL, APIs and cloud services technologies.

-Build tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.

-Build tools for analytics and data team member that assist in building and optimizing the product into an innovative leader.

-Work with data and analytics experts to strive for greater functionality in our data lake, systems and ML/Feature Engineering for AI solutions.

-Work with cross-functional departments to understand their data needs and requirements and build tools to assist them with their analytics tools.

Requirements:

-Candidate must possess at least a Bachelor's Degree, Computer Science/Information Technology or equivalent.

-At least 7 years of data engineering work, of which 3 years are in a leadership role.

-Experience with Apache Airflow or equivalent in automating data engineering workflow.

-Experience with GCP services.

-Knowledge in Machine Learning"
29-Apr-2022 T11:46,Cloud Data Engineer for IT Data Analytics Team,Garranto Pte. Ltd.,8 hours ago,,Full–time,"Type :
Full Time / Permanent Role

Client :
Tonik Bank

Location:
Singapore

Job Description ï'·
Act as a subject matter expert in data engineering and GCP data technologies. ï'·

Closely work with various business teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
Work with Agile and DevOps techniques and implementation approaches in the delivery. ï'·

Be required to showcase your GCP Data engineering experience when communicating with business team on their requirements, turning these into technical data solutions.
ï'·
Be required to build and deliver Data solutions using GCP products and offerings.

Qualifications

Liaise and be part of our extensive GCP community, contributing in the knowledge exchange learning programme of the platform.
Any Bachelor Degree in Computer Science or related fields
Minimum 5 years of experience as a data engineer in banking environment.
Possess analytical skills mental resilience and the ability to think systematically under stressful conditions.
Highly accountable and takes ownership. Outstanding work ethic, highintegrity, team player, and a lifelong learner.
Mentor other engineers define our technical culture and help build a fast-growing team.

Skill

Hands on and deep experience working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, AI Building Blocks, Looker, Cloud Data Fusion, Dataprep, etc.).
Experience in Spark /Scala / Python/Java / Kafka.
Experience in MDM, Metadata Management, Data Quality and Data Lineage tools.
E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.
Regulatory and Compliance work in Data Management.
E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
Experience with SQL and NoSQL modern data stores.

Work on diversified cloud Platforms consisting of Databases and CICD, Logging, and monitoring tools to provide stable, and reliable DevOps service.
Hands-on experience with terraform is a plus.
Build CI/CD pipeline; both design and implementation is an added advantage.

If you are interested please send your CV to (HIDDEN TEXT"
29-Apr-2022 T11:46,Data Engineer (Platform),Crypto.com,16 hours ago,,Full–time,"Singapore, Singapore / Engineering - Exchange / Full-time About : was founded in 2016 on a simple belief: it's a basic human right for everyone to control their money, data and identity. With over 10+ million users on its platform today, provides a powerful alternative to traditional financial services, turning its vision of ""cryptocurrency in every wallet"" into reality, one customer at a time. is built on a solid foundation of security, privacy and compliance and is the first cryptocurrency company in the world to have ISO27001:2013 and PCI:DSS 3.2.1, Level 1 compliance. is headquartered in Singapore with a 4,000+ strong team. For more information, please visit . Job Responsibilities Build a universal data platform, meet real-time/offline computing and storage requirements; Define the data pipeline scheme according to the demand scenario, and deliver it according to the project demand; Continue to optimize the data platform, improve the stability of the data platform, user experience, and reduce cluster and usage costs. Job Requirement Bachelor degree or above, major in computer, software engineering, mathematics, and more than 3 years of data development experience; Familiar with java//scala, understand the working principle of jvm, and have a certain understanding of network and linux operating system; Have a deep understanding of one or more technologies in the big data ecosystem (Hadoop, Spark, Flink, Kafka, Hive), and be familiar with mysql/Postgresql/oracle; Have a certain understanding of distributed system principles, calculations, storage; Good communication and logical thinking skills, good self-drive, continuous learning and updating knowledge system; Benefits What you can expect from us? We offer an attractive compensation package working in a cutting-edge field of Fintech. - Huge responsibilities from Day 1. Be the owner of your own learning curve. - The possibilities are limitless and depend on you. - You get to work in a very dynamic environment and be part of an international team. - You will get to have involvement in developing a brand new product from scratch alongside with a talented team"
29-Apr-2022 T11:46,Data Engineer,Lilith Games Sg Pte. Ltd.,16 hours ago,,Full–time,"Big Data Development Engineer (Advertising Direction)

Big Data Development Engineer (Advertising Direction)

What you will be doing:

1ã€ Responsible for the construction of Big Data Cloud Platform and the maintenance of Advertising Data Warehouse.

2ã€ Responsible for the calculation, cleaning and layering of Advertising Data based on Flink, Spark, etc., and store it through Hadoop, Clickhouse, etc.

3ã€ Responsible for Advertising Data Analysis and Advertising System Report development.

Qualifications & Skills

1ã€ Bachelor degree or above, major in computer and other related majors.

2ã€ Proficient in Python or Golang coding.

3ã€ Proficient with Mysql, Memcache, Redis, Message Queue and other common WEB components.

4ã€ Having experience in using one or more of HDFS, Hive, HBase, MongoDB, Kafka, Flink and Spark is preferred.

5ã€ Experience in operation and maintenance development is preferred.

6ã€ Familiar with Alibaba Cloud and other cloud computing resource deployment and optimization is preferredã€'

7ã€ Proficient in coding complex SQL Statements, and have the ability and experience of query optimization.

8ã€ Positive and optimistic, strong sense of responsibility, with good team communication and cooperation

å¤§æ•°æ ®å¼€å 'å·¥ç¨‹å¸ˆï¼ˆå¹¿å'Šæ–¹å 'ï¼‰

å·¥ä½œè Œè´£

1. è´Ÿè´£å¤§æ•°æ ®äº'å¹³å °æ å»ºï¼Œå¹¿å'Šæ•°æ ®ä»'åº'çš'æ å»ºä¸Žç»´æŠ¤ã€'

2. è´Ÿè´£åŸºäºŽFlinkã€ Sparkç‰å¯¹å¹¿å'Šæ•°æ ®çš'è¿›è¡Œè®¡ç®—ã€ æ¸…æ´—ã€ åˆ†å±'ç‰å·¥ä½œã€'å¹¶é€šè¿‡Hadoopã€ Clickhouseç‰è¿›è¡Œå˜å'¨ã€'

3. è´Ÿè´£å¹¿å'Šæ•°æ ®åˆ†æž å Šå¹¿å'Šç³»ç»ŸæŠ¥è¡¨çš'å¼€å 'ã€'

ä»»è Œè¦ æ±'

1.å¤§å¦æœ¬ç§'(ç»Ÿæ‹›)å Šä»¥ä¸Šå¦åŽ†ï¼Œè®¡ç®—æœºã€ é€šä¿¡ç‰ç›¸å…³ä¸'ä¸šã€'

2.ç†Ÿç»ƒæŽŒæ ¡Pythonæˆ–Golangä»£ç ç¼–å†™ã€'

3.ç†Ÿç»ƒä½¿ç'¨Mysqlã€ Memcacheã€ Redisã€ æ¶ˆæ ¯é˜Ÿåˆ—ç‰å¸¸ç'¨WEBç»'ä»¶ã€'

4.æœ‰ä½¿ç'¨HDFS, Hive, HBase, MongoDB, Kafkaï¼ŒFlink, Sparkä¸çš'ä¸€é¡¹æˆ–å¤šé¡¹çš'ç» éªŒä¼˜å…ˆã€'

5.æœ‰è¿ ç»´å¼€å 'ç» éªŒä¼˜å…ˆã€'

6.ç†Ÿæ'‰é˜¿é‡Œäº'ç‰äº'è®¡ç®—èµ'æº éƒ¨ç½²ä¸Žä¼˜åŒ–è€…ä¼˜å…ˆ

7.ç†Ÿç»ƒç¼–å†™å¤ æ 'çš'sqlè¯å ¥ï¼Œå…·å¤‡æŸ¥è¯¢ä¼˜åŒ–çš'èƒ½åŠ›å Šç» éªŒã€'

8.ç§¯æž ä¹ è§'ï¼Œè´£ä»»å¿ƒå¼ºï¼Œå·¥ä½œè®¤çœŸç»†è‡´ï¼Œå…·æœ‰è‰¯å¥½çš'å›¢é˜Ÿæ²Ÿé€šä¸Žå ä½œèƒ½åŠ›ã€"
29-Apr-2022 T11:46,Data Engineer,Real Estate Analytics Pte. Ltd.,16 hours ago,,Full–time,"Key ResponsibilitiesMaintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable and timely delivery of key insights for decision makingcleaning and normalizing subsets of data of interest as preparatory step before deeper analysis by the data scientistsRun Modern high performance analytical databases and computation enginesDesign and implement various data health checks to ensure the data quality and consistency across systemsDesign and implement data extraction solution in a distributed systemSkills requiredExperience in handling large data sets and working with structured, unstructured and geographical datasetsUnderstanding of databases and best engineering practices - include handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding how to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipelineExperience with DevOps and AWS will be an advantageReal passion for data, new data technologies, and discovering new and interesting solutions to the company's data needsExcellent communication skills to communicate with the product development engineers to coordinate development of data pipelines, and or any new products features that can be built on top of the results of data analysis"
29-Apr-2022 T11:46,Network Data Engineer,Rma Contracts Pte. Ltd.,27 minutes ago,,Full–time,"Job Title: Network Data Engineer

Job Type: Contract - 3 Years

Location: Central, Singapore

(This is an outsourced role)

Job Responsibilities
• Full-time Level 2 'Network data engineer' to perform and handle but not limited to:
• Manage network infrastructure such as internet links, traffic shapers, routers, and switches
• Support in daily operations on incident management, problem(s) / issue(s) remediation, and service(s) restoration
• Fulfilling of service request(s) following the Change Management procedure.
• Track and assess all announcements and/or advisories (from device principal, IT Security Team, Government IT Security Incident Response (GITSIR) Team, etc. on patches on vulnerabilities, software bugs, and firmware upgrades for network devices.
• Planning and applying of devices' security patches and firmware upgrades in accordance with the severity. Preparation of monthly reports on operational issues, link performance, patch status for all network equipment.
• Create and maintain documentation of network configuration, network diagram, mapping, processes, and service records.
• Any other tasks assigned by the Institute.

Qualification and Skills for Network Data Engineer:
• Relevant Diploma or bachelor's degree in Computer Engineering (or equivalent).
• Must have minimum CCNP certification (routing & switching).
• At least 3 years of strong experience supporting a campus network infrastructure, with in-depth hands-on experience on network devices such as Cisco Nexus switches, Catalyst switches, ASR Routers, Networking Monitoring Tools, etc.
• Knowledge of network compliance is an added advantage.
• Excellent problem-solving skills in a multi-tasking, fast-paced, and complex work environment. n) Good communication skills and written skills in English, positive attitude, team player, resourceful, and resolve problems independently.

If you are looking for an opportunity and your skills align with this job, please send your updated CV/resume (in Word format) to Mounika at [HIDDEN TEXT].

All Curriculum Vitae will be treated with strict confidentiality We regret to inform you that only shortlisted candidates will be notified"
29-Apr-2022 T11:46,Senior Engineering Officer (Data Centre),Singtel Group,16 hours ago,,Full–time,"At Singtel, we believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers.

We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative.

Join us and experience what it’s like to be with an Employer of Choice*. Together, let’s create a brighter digital future for all.

Awarded at the HR Fest Awards 2020.

Key Responsibilities :

DC Operations Management
• Perform shift duty to support 24x7 DC Operations or be on 24x7x365 stand-by duties for emegency.
• Perform Incident escalation (Ensure incidents are responded and attended to, else redirect for 2nd / 3rd level resolution base on criticality, impact and SLA).
• Perform Request Fulfillment to register customer requests.
• Operate Electronic Visitation Management System (Update Authorizer and customer access request accurately and promptly).
• Perform access clearance for customers / visitors / Contractors.
• Generate daily / weekly / monthly / yearly / ad-hoc reports when required.
• Remote Hand Support (Media Management, Visual Inspection, Reboot devices, staging room, Physical connect / disconnect of HDD, network cable connectivity test, insertion of fibre SFP).
• Manage equipment movement in Data Centre.
• Ability to conduct asset tagging, labeling and tracking.
• Participate and support both the internal and external audit.

DC Facilities Management

Perform Event Monitoring for all Data Centre Facilities infrastructure to ensure Data Centre is in normal function (eg.

UPS, power, temperature control, humidity, water detection sensor, etc.).
• Ensure all DC supporting infrastructure (i.e. Environment Monitoring System (EMS), Access Control Systems, Electronic Visitation Management System etc) are functioning well.
• Update and maintain all DC related documentation.
• Coordinate with various stakeholders to fix the technical issues on time to provide timely support to customers.

Change Management

Raise Change Request for Change Management approval.

Customer Management
• Communicate with both internal and external customer via voice or email.
• Understand Customer's contractual Service Level Agreements and ensure they are met.

The ideal candidate should be / possess :
• GCE O’ Level or ITE in Engineering / IT with 1-3 years of experience in DC Operations
• Well versed in Data Centre Facilities and Operations Support as well as Data Centre Operations Best Practices
• Basic Certificate of Infocomm Technology
• Familiar with ITLL. ITIL Foundation Level Certification will be an added advantage
• Ability to multi-task and work under pressure, independently and in teams
• Good communication skills (both verbal & written)
• Analytical with good problem solving skills
• Well organized and able to reschedule priorities as circumstances change
• Positive attitude and self- motivated
• Proactive and able to act on own initiative

We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated"
29-Apr-2022 T11:46,Senior Data Engineer,Genpact Consulting (singapore) Pte. Ltd.,16 hours ago,,Full–time,"• Work closely with the Product Owners and stake holders to design and build data systems and pipelines to meet the requirements of the proposed solution.

• Play an active role in leading team meetings and workshops with clients.

• Oversee day-to-day Data Engineering team's operation and performance.

• Interact directly with clients to understand project requirements and deadlines.

• Evaluate business needs and objectives.

• Designing and implementing highly performant data ingestion pipelines from multiple sources using SQL, Python, Apache Spark and Databricks.

• Responsible for deploying codes using Git/Bitbucket as per the CI/CD process.

• Design and Build datasets on Snowflake for faster reporting.

• Design Develop Maintain ETL data pipelines.

• Analyze and organize raw data.

• Research, Diagnose, and Monitor Performance Bottlenecks, etc.

• Ensure standardization of SQL coding practices and adherence to coding standards, change control, and SQL best practices

• Prepare data for prescriptive and predictive modelling.

• Responsible for data integration and data quality

• Enabling data from different sources into ready to consume datasets.

• Performing Data Validation/Exploration tasks.

• Migrating current lake data/ external data into Exasol/Databricks (Lakehouse)

• Helping business stakeholders to visualize and analyze the customized view/data and enable better decisions without any hassle.

• Partnering and coordinating with cross functional stakeholders across timezones"
29-Apr-2022 T11:46,Data Engineer (Remote Possible),Glints,14 hours ago,,Full–time,"At Glints, we are building the #1 tech-enabled recruitment and career discovery platform in Southeast Asia that helps people and organisations realise their human potential by joining great organisations, learning the right skills and building great teams.

We’re looking for a Data Engineer to design and set up data ingestion pipelines into Data Warehouse. Transforming data into consumable dataset for stakeholders, enabling analysts and stakeholders to make critical decisions through automated dashboard reporting and flagging out dataset with poor Data Quality are all crucial scopes for Data Engineers.
What You'll Be Doing

• You will be suggesting and implementing improvement to our existing data pipeline. Day to day work include understanding the requirement of stakeholder and assisting stakeholder in beginning dataset into data warehouse in the shape for stakeholder use for dashboard building.
• Being part of the Data Engineering Team, you will also be participating in the improvement of the team operation to ensure that the work process is optimized and improved if possible, thus enabling the organisation to achieve a data-driven decision making culture.

Why You Should Join Us

• In this role, you will be exposed to many aspects of Data Warehousing and Data Pipeline to serve stakeholders requirements. Stakeholders can range from Business Analysts, Product Analysts to Data Scientists who need datasets for model training.
• As the Data Team grows, depending on your inclination, there is room to develop both technically and also in people management.

Who We Are Looking For

• Has experience working with Apache Airflow and/or Apache Kafka
• Comfortable working with Kubernetes
• Knowledge in Data Modelling, especially in Database Normalization
• Experience in writing clear, well-structured and concise documentation that promotes data discoverability
• Knowledge in SQL, Docker(-compose), Python and/or GoLang
• Experience in working in Agile/Scrum development

Let’s Realise Human Potential.

We have impacted many lives since we were founded in 2013, but there’s still plenty to be done. If you’re ready to grow and make an impact, you’ve come to the right place.

What is Glints?

Glints is an online talent recruitment and career discovery platform with the enduring purpose of contributing to a world where people and organisations can realize their human potential.

Our tech-enabled approach to recruitment and career discovery connects more than 1.5 million candidates to their next dream opportunities, has supported more than 30,000 companies in finding top talents, doubled recruiter efficiency, and has raised more than US$30M+ from best venture investors in Asia to date.

Our agility and firm hold on our core purpose and values have allowed us to remain resilient and thrive through tumultuous times, and we are proud to be recognised by LinkedIn as one of the Top 10 Startups in Singapore in 2020.

Who We Are

At Glints, personal and professional growth are just as important as business growth. That's why we created the Glints Culture Code: #RIIBCOH. It defines our values, guides our decisions and actions, and is what makes us special.

Relentlessly Resourceful: Whatever it takes, just make it happen (ethically)
Integrity: Have courage, be guided by the truth, don’t be afraid
Impact: Missionaries, not mercenaries
Beginners’ Mindset: Stay humble, don’t be attached to ego
Customer Obsessed: Customers First
Ownership: Care intensely about the mission and take responsibility
High Standards: Dream big and deliver epic outcomes fast

Where We Work

Glints operates in multiple locations across Greater Southeast Asia, including Singapore, Indonesia, Vietnam, Taiwan and Malaysia.

Learn more about Glints and our culture at bit.ly/glintsculture,
Or check out our Careers Page at https://glints.com/careers"
29-Apr-2022 T11:46,Data Engineer,Morgan McKinley,7 hours ago,,Full–time,"Data Engineer

Job Summary
• Singapore
• Permanent
• BBBH811650
• Apr 05, 2022
• Competitive Job Description
Our client is looking for an experienced Data engineer to drive development of information products for data and digital transformation across their group.

Mandatory Skills:
• Master of Business Administrations or master's in quantitative fields (Computer Science, Statistics or similar) with minimum of 5 - 12 years of overall experience.
• Experience programming in Python, Java, SQL, PLSQL.
• Experience with traditional RDBMS based systems and more modern NoSQL technology stacks.
• Expertise building ETL and data pipelines on Databricks.
• Experience working with Big Data technologies such as Hadoop, Cloudera (CDH), Hortonworks (HDP), DataBricks, Spark, Delta, HDFS, HBase, Hive.
• Experience in event streaming with Kafka.
• Experience in ML Model Productionization, Docker.
• Real Time, Batch, Unstructured Data, DW, MDM, Data Marts.
• Proficient in using data visualization tool such as Tableau, Power BI, D3, AmCharts etc.
• Understanding FS industry fundamentals and business problems to find new ways to leverage data.
• Intellectual curiosity to solve data driven problems.
• Independent thoughts and unique ideas on solving business problems.
• Able to collaborate / virtually manage multi-cultural, multi-disciplined, globally dispersed teams.
• Sound understanding of functional areas and analytics problems in areas of Customer, Marketing, Channel, Pricing, Digital, Operations, HR/Corporate analytics.
• String communication skills to be able to explain highly technical problems in simple layman form.
• Ability to articulate the impact of decisions and recommend improvements. Desired Skills:
• Relevant experience in Banking and financial institutions. Job Responsibilities:
• Exceptional data engineering & visualization experience & technical skills. State-of-the-art expertise across, data/information preparation and data insight & visualization using BI (or similar tools).
• Be a data engineering & visualization technical expert. Lead and explain/educate to all levels people in these areas, Coding Databases, Data Integration, Frameworks, Deployment, Architectures and Visualization.
• Contribute to the development of in-house data products. Use your data engineering & visualization expertise plus FS domain & functional knowledge, problem solving skills and independent thinking to create in-house data analytics and data management products.
• Be a team player & an individual contributor. Work with group data office and other business function people as part of larger deliveries, as well as being able to work independently or in small teams to continuously deliver business value.
• Be a trusted partner of our client. Someone that anyone in the company can reach out to for help with creating data engineering & visualization driven business transformation. Those who are keen for the role and would like to discuss the opportunity further, please click ""Apply Now"" or email Alimpan at amukherjee@morganmckinley.com with your updated CV.

Only shortlisted candidates will be responded to, therefore if you do not receive a response within 14 days please accept this as notification that you have not been shortlisted.

Alimpan Mukherjee

Morgan McKinley Pte Ltd

EA Licence No: 11C5502

EA Registration Number: R2198629"
29-Apr-2022 T11:47,Data Engineer,Lilith Games Sg Pte. Ltd.,16 hours ago,,Full–time,"Big Data Development Engineer (Data Center)

What you will be doing:

1. Plan, design, develop and implement core data system in the business field

2. Develop and maintain real-time/ offline data processing tasks

3. Build industry-leading distributed systems such as storage and computing to provide a reliable infrastructure for massive data and large-scale business systems

4. Develop various data services, such as data developing platform, quality monitoring system,

Qualifications & Skills

1. BSc in Computer Science or related major.

2. At least 3 years of big data platform R&D.

3. Familiar with common algorithms and data structures, and be proficient in Java / Python / Scala / shell language (at least two)

4. Familiar with deployment and implementation of docker and k8s related containers.

5. Experienced working with big data technologies such as Hadoop, Flink, Hive, Spark, etc.

6. Enthusiastic about learning new technologies and persistent in pursuit of high concurrency and distributed architecture design.

7. Have good team communication and collaboration skills.

å¤§æ•°æ ®å¼€å 'å·¥ç¨‹å¸ˆï¼ˆæ•°æ ®ä¸å °æ–¹å 'ï¼‰

å·¥ä½œè Œè´£ï¼š

1.å 'ä¸Žä¸šåŠ¡é¢†åŸŸæ¸å¿ƒæ•°æ ®ä½'ç³»çš'è§'åˆ'è®¾è®¡ä»¥å Šå¼€å 'è ½åœ°

2.å 'ä¸Žå®žæ—¶/ç¦»çº¿æ•°æ ®åŠå·¥ä»»åŠ¡çš'å¼€å 'ä¸Žç»´æŠ¤

3.å 'ä¸Žå¤§æ•°æ ®åŸºç¡€æž¶æž'çš'å¼€å 'ä¸Žè¿ä»£

3.å 'ä¸Žå 'ç±»æ•°æ ®æœ åŠ¡å¼€å 'ï¼ŒåŒ…å «ä½†ä¸ é™ äºŽï¼šæ•°æ ®å¹³å °å¼€å 'ï¼Œæ•°æ ®è´¨é‡ ç›'æŽ§ï¼ŒBIç‰

å²—ä½ è¦ æ±'ï¼š

1. æœ¬ç§'å Šä»¥ä¸Šå¦åŽ†ï¼Œ3å¹´ä»¥ä¸Šå¤§æ•°æ ®å¹³å °ç'å 'ç» éªŒã€'

2. å…·æœ‰æ‰Žå®žçš'ç¼–ç¨‹åŠŸåº•ï¼Œç†Ÿæ'‰å¸¸ç'¨çš'ç®—æ³•å'Œæ•°æ ®ç»'æž'ï¼Œç²¾é€šJava/Python/Scala/Shellè¯è¨€ï¼ˆè‡³å°'ä¸¤ç§ ï¼‰

3. ç†Ÿæ'‰Hadoopç‰å¤§æ•°æ ®æ¡†æž¶ï¼Œæœ‰Flinkã€ Sparkç‰ç›¸å…³å®žæ—¶æµ è®¡ç®—æ¡†æž¶ç» éªŒä¼˜å…ˆ

4. ç†Ÿæ'‰dockerã€ k8sç›¸å…³å®¹å™¨çš'éƒ¨ç½²å®žæ–½

5. å¯¹æ–°æŠ€æœ¯ä¿ æŒ å¥½å¥‡å¿ƒï¼Œå¯¹é«˜å¹¶å 'å'Œåˆ†å¸ƒå¼ æž¶æž'æœ‰æ‰§ç €çš'è¿½æ±"
29-Apr-2022 T11:47,Senior Data Engineer,Newtone consulting,15 hours ago,,Full–time,"Job Description & Requirements

The successful candidate will provide data services for CIB as part of the Cybersecurity datalake application. You will be part of the team responsible for governance, quality, remediation and manages services across data standardisation, analytics, archiving, reporting, dashboards and data management and production support.

Role and Responsibilities:
• Troubleshoot Production issues following a ‘Follow The Sun principle’ [APAC/EMEA/NAR] support model
• Monitoring and proactive support
• Application maintenance and upgrades
• Automation of BAU tasks to improve efficiency
• Strong contribution to Data Analytics, including support in the development of custom add-ins for data collection & analysis
• Suggest improvements and provide guidance and support to the IT Security team to help them improve their monitoring & analytics capabilities

Candidate profile:
• 5 years of experience of IT Production and/or BigData
• Practical knowledge of performance and capacity management from a BigData perspective as well as strong aptitude for automation.
• Strong working knowledge of Linux (RedHat/Ubuntu)
• Strong working knowledge of Elastic stack (Elasticsearch / Logstash / Kibana / Beats) including data ingestion, management, monitoring & analytics
• Experience with Kafka (or similar experience in messaging broker software e.g. Rabbit MQ, ActiveMQ)
• Programming skills (Python or Ruby or Java)
• Experience & skills in automation tools (e.g. Ansible) & DevOps pipelines are appreciated"
29-Apr-2022 T11:47,Senior Platform and Data Engineer,Ensign Infosecurity (cybersecurity) Pte. Ltd.,27 minutes ago,,Full–time,"Duties and Responsibilities
• Familiar with Ensign's business domain and objectives to develop and deploy solutions that meet internal and customer requirements
• Responsible for the design, build and administration of multi-node data lakes, and data warehouses and data marts
• Responsible for health monitoring, solve cluster issues, patching and upgrades
• Responsible for node administration, load balancing with add/remove/recovery of nodes
• Ensure cluster stability, smooth upgrade releases and solving platform issues by investigating and applying solutions/patches
• Deliver detailed documentation and ensure quality throughout project lifecycle

Requirements
• Bachelor's degree in Computer Science/Information Systems/Computer Engineering
• Minimum 3 years of hands-on experience on cluster installation, deployment, upgrade, maintenance, troubleshooting various cluster issues and optimizing for better performance
• Hands-on experience managing multi-node big data application platforms

 - HDFS, YARN, Spark, Hive, Presto, Apache Airflow
• Hands-on experience managing multi-node messaging queue platforms

 - Kafka, Rabbit MQ, Nifi
• Hands-on experience managing multi-node containerization and orchestration platforms

 - Docker, Docker Swarm, Kubernetes
• Hands-on experience managing multi-node data warehousing platforms

 - MongoDB, MySQL, PostgreSQL, ElasticSearch
• Hands-on experience on setting up 3rd party applications

 - GitLab, Arkime
• Hands-on experience on setting up security features

- Kerberos, AD, LDAP, Keycloak
• Strong awareness of data security, data governance and performance, with an ability to deliver these key non-functional requirements"
29-Apr-2022 T11:47,Data Engineer Intern,Tencent,20 hours ago,,Internship,"Responsibilities
• Work with cross-functional teams and understand how data platform are developed and maintained at scale.
• Build scalable data pipelines (using Spark, Airflow and Presto) to move data from different applications into our data warehouse.
• Monitor and improve automated solutions to ensure quality and performance SLAs are met.
• Maintain and support existing platforms and evolve to newer technology stacks and architectures.

Qualifications
• Currently pursuing a Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.
• Coding & scripting proficiency in languages such as Python, C++, Golang.
• SQL knowledge in handling volumes of data and performance.

Bonus
• Passion in gaming.
• Scratch-build a highly scalable, available, fault-tolerant data processing systems using cloud technologies, HDFS, YARN, Map-Reduce, Hive, Kafka, Spark, and other big data technologies."
29-Apr-2022 T11:47,AI Data Engineer,Osim International Pte. Ltd.,16 hours ago,,Full–time,"The Digital Technology Department in OSIM works together as a team to ensure that the final product is programmed to meet the needs and wants of the consumer, whilst, incorporating the latest and most desirable technology to remain competitive in the market. This department comprises of Product Owners, UI/UX, Mobile Engineers, Backend Engineers, IOT as well as the Digital QA group. If you are forward-looking, able to think out of the box, and always hungry to learn about the newest technology, we want you on our team!Job ResponsibilitiesSet up and manage our AI development and production infrastructure.Build data ingest and data transformation infrastructure.Build AI models from scratch and help product managers and stakeholders understand results.Deploy AI models into production.Collaborate with Product Owner, mobile engineer and test engineer on projectImprove the AI models to accepted accuracyCreate API to fetch the output from the AI modelsCreate and maintain optimal data pipeline architectureAssemble large, complex data sets that meet business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimising data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Product Owner to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.Work with data and analytics experts to strive for greater functionality in our data systems.RequirementsAt least 1 year AI relevant experience and at least 1 year of experience in a Data Engineer related roleExperience working on Machine learning basic algorithm (e.g. decision tree)Proficient with database management and analytic tools and languages supporting data analysis and reporting - SQL, Python and ExcelExperience of using GIT command lineProactive and accurate communication skill and team workGood understanding of Data Structures and AlgorithmsGood understanding of programming skills, python with Tensorflow or PyTorchAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databasesExperience building and optimizing 'big data' data pipelines, architectures and data setsStrong analytic skills related to working with unstructured datasetsBuild processes supporting data transformation, data structures, metadata, dependency and workload managementExperience supporting and working with cross-functional teams in a dynamic environment(Plus point) Graduate degree in Computer Science, Statistics, Informatics, Information Systems(Plus point) Experience with AWS cloud services: EC2, EMR, RDS, Redshift(Plus point) Experience with object-oriented/object function scripting languages: Python(Plus point) Building user data center before is a plus(Plus point) Having AWS data certification is a plus(Plus Point) Experience working on NLP (e.g. voice recognition)(Plus Point) Experience working on computer vision (e.g. face recognition with CNN)(Plus Point) Experience of working in an Agile (scrum) team(Plus Point) Experience of cloud service e.g. AWS, Azure or Tencent services(Plus Point) Mandarin speaking to work with our Greater China development team"
29-Apr-2022 T11:47,Data Engineer,Eastspring Investments (singapore) Limited,16 hours ago,,Full–time,"At Eastspring Investments, we understand that success comes from the talent and commitment of our people

We strive to build a business that you can shape, an inclusive workplace where everyone's ideas are valued and a culture where we can thrive together

Our people stay connected and tuned in to what's happening around us, keeping us ahead of the curve

While focused on the long-term, we look to the future to bring growth, development and benefit to everyone whose lives we touch.

Key Accountabilities
Design and implement our Data and AI central data platform as well as related tools/systems for advanced business analytics and enterprise data governance
Manage data modeling design, writing, and optimizing ETL jobs
Participate in building and enhancing enterprise cloud data warehouse
Deliver and manage in-house and cloud-native data solutions to meet business requirements across firm-wide business units
Assist in creating and monitoring analytics dashboards, for different business functions
Ensure quality, integrity and accuracy of datasets through tracked, secured and auditable controls
Work with stakeholders to assist with data-related technical issues and support their data needs.
Follow and enforce best practices in software development and data engineering
Requirements:
Excellent coding skills with Python and SQL, and solid understanding of object-oriented analysis and design
Working knowledge of common algorithms and data structures, with strong analytical and problem-solving skills
Hands-on experience with Linux and shell scripting
Working experience with containerization (Docker/K8S) and task orchestration tools (Airflow/Luigi, etc.)
Experience with cloud service and tools (AWS/Azure/GCP), as well as cloud data warehouse platforms.
Experience with modern DevOps practices including version control, TDD, CI/CD, etc., for both code and configuration changes.
Basic understanding and experience with ML/AI concepts (e.g

deep learning, deep reinforcement learning, deep bayesian learning), workflows, and toolsets (Jupyter Notebook, etc.), and libraries (Numpy, Pandas, Scikit Learn, PyTorch, etc.) preferably in both cloud-native and desktop deployments.
Experience with traditional RDBMS based systems, including Data Lake, Data Warehouses and Marts, and more modern NoSQL and cloud-native big-data technology stacks such as document-oriented databases, Hadoop, columnar data files (e.g., Parquet), etc.
Familiar with REST APIs, service-oriented architectures (SOA) / microservices, virtualization, and serverless deployment architectures.
Demonstrated ability to understand, work with and deliver robust solutions in more than one programming language, framework, technology stack, runtime environment, etc..
QUALIFICATIONS / EXPERIENCE:
Degree level or higher in Computer Science or another quantitative field
5-10 year(s) technical experience showing increasing sophistication of solutions implemented, and ability to deliver
Fluency in both written and spoken English"
29-Apr-2022 T11:47,Data Engineer,Manpower Singapore,16 hours ago,,Full–time,"Data Engineer

Key Role and Responsibilities:
Partner with the global and in-market data experts to discover and derive value from connecting external and internal data sources.

Build (ETL) new and evolve data models and pipelines to power algorithmic based Business Intelligent solutions that addresses business problems requiring descriptive, diagnostic, predictive, and/or prescriptive analytics for pricing, promotion, trade spending, assortment, and sales performance management.

Translates the algorithms and analytic models into data models as business needs evolves on a going basis after they are put into production.

Develop a roadmap that scales existing and new data models, to support the portfolio of solutions.

Expand into Business Intelligence solution development focused on automation and scale up of solutions.

Key Skills Required:

Professional Skills:
R/Python programming languages
MS Power BI
MS Excel
Data Visualisation

General Skills:
Client Management
Project Management
Diploma or B.S. in a relevant technical field (Operations Research, Computer Science, Statistics, Business Analytics, Econometrics, or Mathematics).

Overall experience of 2-3 years

Ability to manipulate and high-volume of data from varying sources

Expert knowledge of an analysis tool such as Microsoft PowerBI

Proficiency in R/Python

Basic track record in working independently with minimal guidance

Interested applicants, please submit your resume to : [ Email address blocked ]
Abelene Marianne Kang Mrs Abelene Marianne Rozario R2089914
EA License No: 02C3423 Reg No: 199505951H

Kang Abelene Marianne Mrs Rozario Abelene Marianne

EA License No.: 02C3423 | Personnel Reg No.: R2089914

Please note that your response to this advertisement and communications with us pursuant to this advertisement will constitute informed consent to the collection, use and/or disclosure of personal data by ManpowerGroup Singapore for the purpose of carrying out its business, in compliance with the relevant provisions of the Personal Data Protection Act 2012. To learn more about ManpowerGroup's Global Privacy Policy, please visit [ Link removed ]

Recommended Skills

Algorithms

Analytical

Automation

Business Analysis

Business Intelligence

Business Requirements"
29-Apr-2022 T11:47,"Vp/avp, Data, Ai&ml Engineer - Financial Planning, Investment",Techgig,16 hours ago,,Full–time,"About Job
CTC Undisclosed Job Location Singapore Experience 0 - 3 yrs
Description
Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation. In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.
The Role
This is a unique opportunity to drive the data engineering, apply AI/ML and delivery of a multi-year greenfield program, to re-imagine our Financial Planning journeys and capabilities.
Job Purpose
We are looking for a dynamic Data Engineering Lead that can build a world class team that ships high quality production code daily. As an engineering lead that manage data engineer team, youll be responsible for design data pipeline, understand data from various sources, make sense of data and build data-mart that serves customers need every day.
Background we are looking for:
You love solving problems and building solutions.
You believe in Agile development and putting the customer first in anything you design; experience with SCRUM is a plus.
Youre self-motivated and have a demonstrated track record of self-learning and going above and beyond.
Youre a hands-on leader that regularly codes.
You understand the importance of thoughtful feedback, nurturing an inclusive engineering environment, championing engineering fundamentals and providing clarity and mentorship for team members to grow.
You will help design and build scalable and performant metadata driven complex data pipelines, data quality frameworks etc using the tools and methodologies adopted by the bank from time to time.
Experience in Bigdata, information retrieval ,Data mining ,AI &ML. Collaborate with multiple AI & ML product teams to understand their data and compute requirements.
Responsible to Ingest data from files, databases and well versed with streaming tools like Spark streaming/Kafka.
Process the data with Hadoop, Spark ,Pyspark, Python/Scala and ML.
Strong Debugging skills in Java/Python/Scala and Hive or Spark SQL programming languages.
Experience with RDBMS and NoSQL databases like MySQL, Maria DB, HBase.
Perform troubleshooting, analysis, and performance tuning for customers using the data warehouse, SQL engine, and data analytical platforms.
Perform root cause analysis, including implementation of hardened measures that avoid and/or mitigate impact to users in case of similar or related issues.
Actively participate in all phases of software development lifecycle: analysis, technical design, planning, development, testing/CICD, release, post production/escalation support
Strong leadership and management skills with proven stakeholder management experience.
Excellent interpersonal, verbal, and written communication skills"
29-Apr-2022 T11:47,"VP, Data Engineering Development Team Lead, Middle Office Technology...",DBS Bank Limited,12 hours ago,,Full–time,"VP, Data Engineering Development Team Lead, Middle Office Technology, Technology & Operations

Business Function
Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation. In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.
Responsibilities

• Creating complex, enterprise-transforming applications for competitive advantage
• Working with the latest tools and techniques
• Hands-on coding, code review, software architecture design
• Building & Leading highly collaborative teams to deliver good quality solutions
• Furnishing deep Financial Process and reporting domain knowledge and working closely with business stakeholders Requirements

• Proficient in at least one major programming language in data engineering space (Scala, Java)
• Proficient with SQL (MySql, PostgreSQL, Oracle, MariaDB)
• Deep understanding and Hands-on development experience on Big data toolsets such as Spark, Hadoop, Kafka and etc
• Hands-on development experience of large-scale ETL in big data eco systems
• Hands-on development experience in handling large volume of data in big data eco systems. Deep knowledge in big data process monitoring, tuning skills
• Good knowledge on code quality tools, testing automation
• Knowledgeable on release automation like Jenkin pipelines
• Knowledge about building microservices
• Knowledge on Cloud infrastructures such as Kubernetes, Pivotal cloud foundry, Docker etc
• A Bachelor's degree or higher preferably in Computer Science or IT Essential Traits:

• Obsessed with the elegance of codes
• Passionate about the latest technologies and willing to learn and share knowledge
• Comfortable with dealing with legacy codebase Apply Now
We offer a competitive salary and benefits package and the professional advantages of a dynamic environment that supports your development and recognises your achievements"
29-Apr-2022 T11:47,"Data Engineer, Quantitative Strategy",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) seeks to transform the delivery of Government Digital Services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with the public to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

Team Introduction:

The Data Science & Artificial Intelligence Division (DSAID) works with public sector agencies in using data science and AI to improve policy outcomes, service delivery and operational efficiency. We extract data-driven insights and build intelligent platforms to add value to the work of our partner agencies.  We also help public sector agencies transform by partnering them in building data science expertise, formulating data strategies and setting up the necessary data infrastructure.

 

How do we work:

 Outcome Driven - Our projects are not academic exercises. We are driven by the “so what” and make sure that our findings and models can be translated into tangible impact.

Start Small and Move Fast - We build things quickly. If it works, good — how can we scale this up further? If not, what went wrong and what can we do better next time?

Ownership - You are not just here to write code, but also to figure out what we should be building and how we should build it.

Continuous Learning - Working on new ideas often means not fully understanding what you are working on. Taking time to learn new architectures, frameworks, technologies, and even languages are not just encouraged but essential.

We are in this Together - We draw from the deep domain knowledge of our partners and best practices from our community of experts.

Read more about us from the team's blog https://medium.com/dsaid-govtech

Job Overview:

We are seeking a Data Engineer to join our Quantitative Strategy team, to be sited with one of our Agency Data Science Teams.

 

As a Data Engineer, you will be part of a data team deployed at a government agency identified as a strategic partner for DSAID, with the mandate to drive the growth of agency’s data analytics capabilities while operating in close alignment to DSAID's approach and philosophy. Your main role is to build Whole-of-Government data infrastructure at the partner agency, to power insights needed for evidence-based decision-making and enhancing the agency’s service delivery.

 

This role requires an individual with experience in both on-prem and cloud-based data engineering work, as well as good communication skills, as you will be expected to interact frequently with the partner agency's users to elicit useful business information that enables you to perform your job.

What you will be working on:
• Translate data requirements from business users and data scientists into technical data modelling specifications.
• Interview business users and system owners to elicit information relating to their data infrastructure, data assets, data policies, and use cases.
• Collaborate with partner agency’s IT teams on the following tasks:
• Propose and build ingestion pipelines to collect, clean, harmonise, merge, and consolidate data sources, whether on-prem or in cloud;
• Integrate and collate data sources with data systems;
• Day-to-day monitoring of databases and ETL systems, e.g., database capacity planning and maintenance, monitoring, and performance tuning; diagnose issues and deploy measures to prevent recurrence; ensure maximum database uptime;
• Construct, test, and update useful and reusable data models, with reference to consolidated business insights obtained from users, to serve the data science team and partner agency's needs;
• Propose and implement appropriate cloud data infrastructure in support of the end-to-end analytics deployment lifecycle, taking into account networking between cloud data infrastructure and any on-prem data centres;
• Design and build API gateways to expose data to systems via secure means.
• Research and develop new technologies and approaches for building highly available data persistence systems.
• Advice and support your team on data engineering matters.
• Own and participate in AWS data cloud migration projects (if applicable).

What we are looking for:
• A Bachelor’s Degree, preferably in Computer Science, Software Engineering, Information Technology, or related disciplines.
• Deep understanding of system design, data structure and algorithms, data modelling, data access, and data storage.
• Proficiency in writing SQL for databases such as Postgres, MSSQL, MongoDB, neo4j.
• Demonstrated ability in using cloud technologies such as AWS, Azure, and Google Cloud.
• Experience with data engineering tools and frameworks such as Airflow, Kafka, Hadoop, Spark, Kubernetes.
• Experience in benchmarking, clustering, and tuning the databases for performance, reliability.
• Experience in designing, building, and maintaining batch and real-time data pipelines.
• Experience in automation development, batch, shell, python.
• Familiarity with regular expressions and scripting languages such as bash, korn, awk.
• Familiarity with building and using CI/CD pipelines for platform development.
• Familiarity with DevOps tools such as Docker, Git, Terraform.
• Familiarity with LDAP, OAuth, API gateways.

Preferred requirements:
• Knowledge of IT infrastructure
• Experience with AWS RDS / Spark / other AWS Data Services
• Experience with installation, management, upgrades, backup and restore MSSQL DB
• Working knowledge of SSIS or comparable ETL tools
• Familiarity with government systems and government's policies relating to data governance, data management, data infrastructure, and data security

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round.
• Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:47,"Data Engineer, Condition Monitoring",SMRT Corporation Ltd,16 hours ago,,Full–time,"Job Purpose

The Data Engineer, Condition Monitoring will develop maintenance decision-making tools for improving train reliability and maintenance efficiency. The shortlisted candidate will instrument railway assets with condition-monitoring systems for data acquisition, apply engineering fundamentals and data analytics techniques to develop asset condition assessment models. This position will work closely with the engineering and maintenance divisions of SMRT.

Coding Assessment

A 30-60 min live coding assessment will be conducted during the interview. The candidate is expected to demonstrate ability to write code, in language and IDE of their choice, to complete simple data manipulation tasks and demonstrate knowledge of web development.

Responsibilities

The duties and responsibilities for Data Engineer, Condition Monitoring encompasses both data acquisition and data processing responsibilities. The list is not comprehensive and related duties and responsibilities may be assigned from time to time.

Collaborate with internal stakeholders to understand their business needs.

Understand the failure modes of the critical assets and identify appropriate parameters to monitor.

The data acquisition tasks include:
- Instrumenting critical assets with sensors to acquire asset condition data, and
- Managing and optimizing sensor data collection and transfer.

Develop automated data processing pipeline and software.

Develop simple web-based user interfaces and data visualizations.

Write testable and maintainable code and documentation for production.

Qualifications & Work Experience

Qualifications and work experience include:

Degree in Science, Technology, Engineering or Mathematics (STEM)

Minimum 2 years of experience with data acquisition, developing software or web applications

Basic knowledge of electrical circuits and fundamentals

Skills

Technical skills include:

Willing to learn and is passionate about programming, sensor instrumentation, and solution integration

Have prior experience to a high-level programming language for data analytics, such as Python or MATLAB

Experience with front-end web development (HTML, CSS, JS)

Experience with SQL or NoSQL databases (e.g., MySQL, MongoDB, etc.)

Optional: Knowledge of Microsoft Azure cloud services is highly advantageous

Optional: Knowledge of machine learning would be highly advantageous

Generic skills include:

Strong inclination and eager for continual learning and development

Critical thinking and problem-solving skills

Ability to understand and explain technical concepts and findings to non-technical stakeholders

Ability to think independently and actively propose solutions to the team

SMRT Trains Ltd was incorporated in 1987 and operates Singapore’s first mass rapid transit system. Today, we manage and operate train services on the North-South Line, East-West Line, the Circle Line, the Thomson-East Coast Line, and the Bukit Panjang Light Rail Transit. With over 5,000 employees, more than 250 trains, and 141 km of rail tracks across 108 stations, we serve millions of commuters daily"
29-Apr-2022 T11:47,Software/Data Engineer#TeSA #CLT,Aida Technologies Pte. Ltd.,8 hours ago,,Full–time,"Training Programme

This Company-Led Training (CLT) programme is a 9 to 12 months on the job training with Aida Technologies Pte Ltd to equip fresh professionals with industry skills to become an Artifical Intelligence / Machine learning (AI/ML) Engineer.
Each trainee will be trained and mentored in one or more of the following areas:

AI and ML applications
Data Management process
Machine learning, deep learning techniques and tools
Delpu AI/ML as a data product and/or data service
Minimum Entry Requirements
Singapore citizen
Diploma/Degree in Computer Science or Engineering.
Fresh and Mid-level professionals are welcome
Application Process

Interested individuals can apply directly with Aida Technologies Pte Ltd. Please email your CV to (HIDDEN TEXT)."
29-Apr-2022 T11:47,Senior Data Engineer Various seniority IT Central Up to SGD mth,Integrity Partners Pte. Ltd.,16 hours ago,,Full–time,"My client, a premier international software development company is looking out for a Senior Data Engineer to join their team.The incumbent will be supporting the product team, data analysts and business team on data related projects or initiatives.Job Responsibilities:• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and 'big data' technologies• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needsJob Requirements:• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases• 5+ years of experience in a Senior Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement• Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores• Experience with one or more object-oriented/object function scripting languages: Python, Ruby, Scala or equivalent• Experience with big data tools Hadoop, Spark, Kafka, Hive, etc. or equivalent• Experience with relational SQL and NoSQL databasesHow to ApplyInterested applicants may apply by sending in your updated résumé (in MS Word format) to my email or apply directly: Consultant: Tan Jun Jie EA personnel reg. no.R1878852 EA License No. 17C8502"
29-Apr-2022 T11:47,Cloud Data Engineer for IT Data Analytics Team,GARRANTO PTE. LTD.,16 hours ago,,Full–time,"Type : Full Time / Permanent Role

Client : Tonik Bank

Location: Singapore

Job Description 

Act as a subject matter expert in data engineering and GCP data technologies. 

Closely work with various business teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.

Work with Agile and DevOps techniques and implementation approaches in the delivery. 

Be required to showcase your GCP Data engineering experience when communicating with business team on their requirements, turning these into technical data solutions. 

Be required to build and deliver Data solutions using GCP products and offerings.

Qualifications

Liaise and be part of our extensive GCP community, contributing in the knowledge exchange learning programme of the platform.

Any Bachelor Degree in Computer Science or related fields

Minimum 5 years of experience as a data engineer in banking environment.

Possess analytical skills mental resilience and the ability to think systematically under stressful conditions.

Highly accountable and takes ownership. Outstanding work ethic, highintegrity, team player, and a lifelong learner.

Mentor other engineers define our technical culture and help build a fast-growing team.

Skill

Hands on and deep experience working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, AI Building Blocks, Looker, Cloud Data Fusion, Dataprep, etc.).

Experience in Spark /Scala / Python/Java / Kafka.

Experience in MDM, Metadata Management, Data Quality and Data Lineage tools.

E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.

Regulatory and Compliance work in Data Management.

E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.

Experience with SQL and NoSQL modern data stores.

Work on diversified cloud Platforms consisting of Databases and CICD, Logging, and monitoring tools to provide stable, and reliable DevOps service.

Hands-on experience with terraform is a plus.

Build CI/CD pipeline; both design and implementation is an added advantage.

If you are interested please send your CV to [ Email address blocked ]

Recommended Skills

Agile Methodology

Analytical

Apache Kafka

Apache Spark

Artificial Intelligence

Banking Services"
29-Apr-2022 T11:47,System Admin & Data Engineer,BioMind,16 hours ago,,Full–time,"Singapore

more than 1 year

college degree and above

Work content

maintain and design the company’s website

system administration

commission and decommission laptops/servers

onboarding and assisting engineers to set up development environments

assist DevOps team to maintain development pipelines and infrastructure

installation of company software

supporting non-technical staff in basic technical tasks

generate purchase orders for equipment

Job Requirements

Degree in Computer Science, Computer/Electrical Engineering or equivalent

Linux system administration

Bash, Python, or equivalent scripting languages

AWS, Github, Jenkins, Aliyun, cloud, ubuntu/centos, firewall, mac, network

Website design (Wordpress)"
29-Apr-2022 T11:47,Data Engineer/Scientist,Patsnap Pte. Ltd.,8 hours ago,,Full–time,"Company Introduction

PatSnap is a software company helping R&D leaders maximise the value of innovation intelligence within their R&D workflow and strategic planning.

As the global leaders in connected innovation intelligence, Patsnap use AI-powered and machine learning technology to comb through billions of datasets, and help innovators connect the dots.
Job description

We are looking for a Data Engineer to join the revolution to help us improve various business outcomes and drive innovation.
You will join a multidisciplinary team helping to shape our Product development. This is an excellent opportunity to take advantage of emerging trends and technologies to a real-world difference.

Responsibilities
Study and transform various kind of structured and unstructured data
Big data analytics
Create data pipeline for regular data updates
Analysing unstructured data using basic NLP techniques and extract important fields
Store data in databases (nosql and graphdb)
Develop NLP systems according to requirements
Perform statistical analysis of results and refine models
Remain updated in the rapidly changing field of machine learning
Deploy models and create APIs
Requirements
Proven experience as Data Engineer or similar role
Experience with big data analytics libraries such as pyspark is must
Experience with AWS and database technologies is plus
Strong communication skills
An analytical mind with problem-solving abilities
Degree in Computer Science, Mathematics, Computational Linguistics or similar field
Domain knowledge of Material sciences and/or chemistry is big plus"
29-Apr-2022 T11:47,Data Engineer,Tookitaki Holding Pte. Ltd.,16 hours ago,,Full–time,"Introducing TookitakiA leading Regtech company, Tookitaki has developed advanced machine learning-powered solutions in risk and compliance to help the banking and financial services (BFS) industry achieve sustainability in their compliance programs. Our offerings are deployed in production across global reputed financial institutions.Incorporated in November 2014 in Singapore, the company is led by a core team with cumulative 150-years' experience in financial crime, AI and big data analytics. Tookitaki's client portfolio expands across AsiaPacific, North America and Europe markets, including a Japanese multinational investment bank, a large European bank, a leading Southeast Asian bank and such other global reputed financial institutions.Backed by institutional investors such as Jungle Ventures, Viola Fintech, Illuminate Financial and Enterprise Singapore (a subsidiary of the Singapore Government), the company's accolades include:We won the first place in the MAS FinTech Awards (Singapore SME) in the regulatory compliance space from the Monetary Authority of Singapore for our approach to make the workflows in AML and Reconciliation scalable and highly auditable (beyond ML based black box approach)We are accredited by IMDA as Innovative Tech Company in 2017 and 2019We are among the 56 growth-stage Companies from around the world recognised by World Economic Forum as a Technology Pioneers 2019We are the winner in the Asian Private Banker Technology Awards 2019 for the Best AML/CTF SolutionWe bagged the AI Award for the banking category at Singapore Business Review's inaugural Technology Excellence Awards 2019We won the Most Promising Innovation by the SG:D Techblazer Awards 2019 – Jointly organised by Singapore Digital (SG:D), IMDA and SGTECHWe have participated and won in various Client Innovation Engagement Programs – Second Runner-up of the UBS Future of Finance Challenge, First Runner-up of the FinTech Challenge Vietnam 2019, Alumni of ING Fintech Village Cohort 2019 and many moreIn regulatory compliance, we focus on anti-money laundering and reconciliation and our products – Anti-Money Laundering Suite (AMLS) and Reconciliation Suite (RS) – cater to these areas, respectively.Today, regulatory compliance processes have become more complex and fluidic, increasing the chances for rule-based models to fail. Banks need to move beyond static rule-based systems and adopt a new approach to improve efficiency, effectiveness at optimal cost, ensuring sustainable compliance programs across the BFS industry. Tookitaki bridges the gap with its innovative software products – AMLS and RS.JOB ROLE: DATA ENGINEERTookitaki is looking for a Data Engineer who is familiar with the Hadoop platform and is able to design, implement and maintain optimal data/machine learning (ML) pipelines in the platform.The following are the main responsibilities of the role:Designing and implementing fine-tuned production ready data/ML pipelines in Hadoop platform.Driving optimization, testing and tooling to improve quality.Reviewing and approving high level & detailed design to ensure that the solution delivers to the business needs and align to the data & analytics architecture principles and roadmap.Understanding business requirement and solution design to develop and implement solutions that adhere to big data architectural guidelines and address business requirements.Following proper SDLC (Code review, sprint process).Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, etc.Building robust and scalable data infrastructure (both batch processing and real-time) to support needs from internal and external usersUnderstanding various data security standards and using secure data security tools to apply and adhere to the required data controls for user access in Hadoop platform.Supporting and contributing to development guidelines and standards for data ingestionWorking with data scientist and business analytics team to assist in data ingestion and data related technical issues.Designing and documenting the development & deployment flow.RequirementsExperience in developing rest API services using one of the Scala frameworksAbility to troubleshoot and optimize complex queries on the Spark platformExpert in building and optimizing 'big data' data/ML pipelines, architectures and data setsKnowledge in modelling unstructured to structured data design.Experience in Big Data access and storage techniques.Experience in doing cost estimation based on the design and development.Excellent debugging skills for the technical stack mentioned above which even includes analyzing server logs and application logs.Highly organized, self-motivated, proactive, and ability to propose best design solutions.Good time management and multitasking skills to work to deadlines by workingindependently and as a part of a team.Ability to analyse and understand complex problems.Ability to explain technical information in business terms.Ability to communicate clearly and effectively, both verbally and in writing.Strong in user requirements gathering, maintenance and supportExcellent understanding of Agile Methodology.Good experience in Data Architecture, Data Modelling, Data Security.Experience -Must have:Scala: Minimum 2 years of experienceSpark: Minimum 2 years of experienceHadoop: Minimum 2 years of experience (Security, Spark on yarn, Architectural knowledge)Hbase: Minimum 2 years of experienceHive - Minimum 2 years of experienceRDBMS (MySql / Postgres / Maria) - Minimum 2 years of experienceCI/CD Minimum 1 year of experienceQualificationsBachelor's degree in IT, Computer Science, Software Engineering, Business Analytics or equivalent with at-least 2 years of experience in big data systems such as Hadoop as well as cloud-based solutions.Job PerksAttractive variable compensation packageFlexible working hours - everything is results-orientedOpportunity to work with an award-winning organization in the hottest space in tech artificial intelligence and advanced machine learning"
29-Apr-2022 T11:47,BIG DATA ENGINEER,Technopals Pte. Ltd.,16 hours ago,,Full–time,"Job Description & RequirementsJob Scope:Design and implement relevant data models in the form of data marts stored in Operational Data Stores, Data Warehouses or Big Data platformsBuild data pipelines to bring information from source systems, harmonise and cleanse data to support analytics initiatives for core business metrics and performance trends.Perform data profiling to understand data quality and advise practical measures to address such data issues through data transformation and data loadingDive into company data to identify sources and features that will drive business objectives.Work closely with project manager and technical leads to provide regular status reporting and support them to refine issues/problem statements and propose/evaluate relevant analytics solutionsBring your experience and ideas to effective and innovative engineering, design, and strategyWork in interdisciplinary teams that combine technical, business and data science competencies that deliver work in waterfall or agile software development lifecycle methodologiesThe range of accountability, responsibility and autonomy will depend on your experience and seniority, including:Contributing to our internal networks and special interest groups Mentoring to upskill peers and juniorsJob Requirements:Diploma / Degree in Computer Science / Computer Engineering / Information Technology related field, or IT equivalent.Minimum of 3 years' experience in building large scale enterprise data pipelines using commercial and/or open-source data management tools from vendors such as Informatica, Talend, Microsoft, IBM or OracleStrong knowledge of data manipulation languages such as SQL necessary to build and maintain complex queries and data pipelinesPractical appreciation of data q/quality metrics and remediation strategiesData modelling and architecting skills including strong foundation in data warehousing concepts, data normalisation, and dimensional data modelling such as OLAPUndergraduate or graduate degree in Computer science or equivalentPossess good communications skills to understand our customers' core business objectives and build end-to-end data centric solutions to address themGood critical thinking and problem-solving abilitiesGood to Have:Experience with other aspects of data management such as data governance, metadata management, archival, data lifecycle managementProcessing of semi-structured and unstructured data sets such as NoSQL, graph and Hadoop based data storage technologies such as MongoDB, Cassandra, HBase, Hortonworks/Cloudera, Elastic Search and Neo4j using Spark, Splunk or Apache Nifi for batch or streaming dataLarge scale data loading experience moving enterprise or operational data from source systems to new applications or data analytics solutionsExperience in leveraging on cloud-based data analytics platform such as:o AWS serverless architecture in Lambda on AWS DynamoDB, EMR Redshifto Azure Data Factory or SQL Data Warehouseo GCP BigQuery/BigTable, Cloud Dataprep/Dataflow/Dataproc"
29-Apr-2022 T11:47,Lead Data Engineer,Ohmyhome Pte. Ltd.,16 hours ago,,Full–time,"Job Description:

-Assemble and collect data sets that meet functional and non-functional business requirements.

-Identify, design & implement internal process improvements, automating manual processes, optimizing data delivery, infrastructure for greater scalability.

-Build the infrastructure required for optimal extraction, transformation and loading of data from a variety of sources using SQL, APIs and cloud services technologies.

-Build tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.

-Build tools for analytics and data team member that assist in building and optimizing the product into an innovative leader.

-Work with data and analytics experts to strive for greater functionality in our data lake, systems and ML/Feature Engineering for AI solutions.

-Work with cross-functional departments to understand their data needs and requirements and build tools to assist them with their analytics tools.

Requirements:

-Candidate must possess at least a Bachelor's Degree, Computer Science/Information Technology or equivalent.

-At least 7 years of data engineering work, of which 3 years are in a leadership role.

-Experience with Apache Airflow or equivalent in automating data engineering workflow.

-Experience with GCP services.

-Knowledge in Machine Learning"
29-Apr-2022 T11:47,Backend Software Engineer - TikTok Live (Data Platform) - Singapore,ByteDance,21 hours ago,,Full–time,"Responsibilities

About us: TikTok is a true phenomenal app in the world. Our mission is to help live hosts connect users and inspire user interaction in real-time around the world. We are looking for passionate and talented engineers to join us to build up and optimize a real-time, high-performance, large-scale distributed infrastructure for live streaming at TikTok. We will be deeply involved in the developmental lifecycle of critical product features, and collaborate closely with product managers to deliver the best live streaming experience for live hosts and users. TikTok Live Data Platform team is a fast-growing team in TikTok engineering family. We are building solid data capabilities and enhancing the robust data ecosystem for TikTok Live, and aiming to maximize data value to benefit our hosts & users and power TikTok. You will:
- Plan and lead large-scale technical projects to lay the foundation for the iterative development and scale of early products
- Develop robust efficient technology products that serve 1 billion users
- Contribute to engineering strategy, tooling, processes, and culture
- Research and apply cutting-edge domain and technical knowledge into products

Qualifications

- As a world-class engineer, you have rich working experience in scalable, highly available, distributed and mission-critical systems.
- Deep understanding of computer architectures, data structures and algorithms.
- Able to work closely with diverse stakeholders and have good communication skills
- Self-driven, positive, cooperative and willing to keep learning enthusiasm at all times
- Experience in the payment or financial domain is a plus TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace.

At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too"
29-Apr-2022 T11:47,Data Engineer,Brenntag Asia Pacific Pte. Ltd.,16 hours ago,,Full–time,"At DigiB, we combine cutting-edge start-up environment with resources of a global company to make game – changing decision and drive digital transformation in the chemical industry. We design tools that help our customers, employees and suppliers to do their job in their best possible and efficient way.

We are developing Digital Products for our customers & partners. We will utilise Technology in a smart way to enable improved processes and reduce pains. We will touch on areas in Customer Service, Technology, Data, Supply Chain and Finance to name a few.

An idea of what you will do:

You will be responsible for building, maintaining & managing the cloud infrastructure

You will be responsible to setup & manage services connected to cloud infrastructure – including but not limited to AD, SSL Certificates, VPNs, Mail Servers, API configurations

You will design and deliver the production support management tools & services – Monitoring, Alerting, Capacity & Performance management

You will be responsible for setting up and managing log analysis & security monitoring analysis

You will be responsible for investigating & resolving infrastructure issues and collaborate with technology providers to resolve in a timely manner

You will administer & control access to all infrastructure services & tools

You will maintain application database & system logs backups and perform restores as needed by team

You will collaborate with IT security officers to ensure platform access & data is secured

You will be responsible for infrastructure failover assessments & disaster recovery drills

You will collaborate with technology services providers to proactively implement preventive measures and provide RCA report of infra issues

You will maintain complete inventory of all infrastructure resources

You will publish weekly reports of infrastructure usage, availability, risks and issues

You will support technology manager to assess platform reliability, robustness, security vulnerability and comparisons of cloud services and management tools

You will manage the Application code deployments, including defining and implementing any automated solutions for deployments

You will assist with the design and implementation of the continuous integration, automated testing and continuous deployment initiative

Who you are and your experience:

You have a degree or equivalent in computing/engineering from a reputable institution or relevant work experience

You have working experience building & maintaining modern technology platforms

You have hands on experience in enterprise systems & network architecture, cloud infrastructure i.e. Azure

You are familiar with enterprise systems like ERP, CRM, ECM, BPM & DWH systems

You have experience in collaborating with regional/global technology teams and diverse technology & service providers

You are able to work unsupervised & can make informed decisions

You are a hands-on team player and lead by example

You thrive in an unstructured and extremely agile environment

You have excellent communication and interpersonal skills to establish and build sustainable internal and external relationships

You should have a Growth mindset

Your Technical Skills:

You MUST have deep expertise in one of the Cloud technologies e.g. AWS, Azure

Must have hands on experience managing VMs, Containers using Unix shells (SSH)

Must have experience in setting up networking infrastructure, DNS, DMZ, VPNs, bastion, mail servers & API Gateways

You should have experience in one or more of the following API / integration services - WSO2 API, BPM, Mulesoft, TIBCO ESB

You should have experience using message queues / data streaming using Kafka, RabbitMQ, IBM MQ etc.

You should have experience with monitoring & Log analysis tools such as Grafana, Splunk etc

You have sound knowledge of Java technology stack , Spring Framework and Data Adapters

You have experience using micro-services deployment and Dockers management

You have DBA experience managing SQL & No-SQL databases and data management (PostgreSQL, Mongo, Redis)

You are familiar with API messaging specification JSON, YAML & REST

You are familiar with data engineering and supporting tools e.g. Hadoop, Spark, Kafka, Qlik

Familiarity with SAP data stream integration or similar enterprise solution"
29-Apr-2022 T11:47,[LTA-RAOM] ANALYST ENGINEER (DATA & SYS. STRATEGY/DATA STD. & INFO./DATA...,Land Transport Authority (LTA) Singapore,6 hours ago,,Full–time,"You will be part of the Digital asset management team, to manage the development of an integrated 4.0 Asset Management System and applications to deliver data-centric insights for railway assets including its performance, condition and usage, which will support the Digital asset management functions.

You will work closely with teams from different discipline within LTA, rail operators, contractors as well as system manufacturers on the interfaces and integration with various systems in the rail network. You are also required to review technical submissions and participate in the installation, testing and commissioning for the system and interfaces.

Requirements
• Tertiary qualification in Electrical, Electronics Engineering, Computer Science, IT, Mechanical, Sustainable Infrastructure Engineering or equivalent
• Strong writing, presentation, communication and interpersonal skills
• Team player with critical and logical thinking capability and must be able to work independently
• Experience in IT network and infrastructure with good knowledge of servers, data storage implementation, IT application, cloud technologies and big data technologies will be an advantage for data networking and integration
• Those with relevant working experience in railway/industrial system design, or engineering management, or operation and maintenance will be considered for senior positions
• Please select your preferred position and indicate in your resume – => 1. Data & Systems Strategy / 2. Data Standards & Information / 3. Data Networking & Integration"
29-Apr-2022 T11:47,Software Engineer - (Data Engineering),Goldman Sachs,16 hours ago,,Full–time,"OUR IMPACT

Seeking a data focused software engineer, to design and build highly scalable and resilient data solutions that will drive client insights, analytics and engagement. Liaise with business teams across the firm to understand client activities and create accurate and high quality data models for them. Drive towards consistent and easily accessible client information by developing to rigorous curation and quality standards.

Our team design, build, and operate the firm's Legend data platform in the AWS and GCP clouds. Legend is a data management and data governance platform that provides both engineers and non-engineers a single solution to develop data-centric applications and derive data-driven insights. Legend automates some of the most difficult data governance challenges and provides self-service tools to democratize data and analytics. It is available as an open source platform through GitHub ready to be used by our clients and the world fully open and free of charge. As a member of the Data Engineering team, you will be able to work on this open sourced platform which not only brings real, tangible value for Goldman Sachs but also our peers and clients as well as greater standardization and efficiency across the entire financial industry.

HOW YOU WILL FULFILL YOUR POTENTIAL
• Design & develop modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies
• Evaluate, select and acquire new internal & external data sets that contribute to business decision making
• Engineer streaming data processing pipelines
• Drive adoption of Cloud technology for data processing and warehousing
• Engage with data consumers and producers in order to design appropriate models to suit all needs

SKILLS AND EXPERIENCE WE ARE LOOKING FOR
• 5+ years of relevant work experience in a team-focused environment
• A Bachelor or Master degree in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)
• Strong object-oriented design and programming skills and experience in OO languages
• Experience or interest in functional programming languages
• Extensive experience in Java (Preferred), Python, C++, C#, Objective-C, or other OO languages
• Proven experience applying domain driven design to build complex business applications
• Deep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes
• In-depth knowledge of relational and columnar SQL databases, including database design
• General knowledge of business processes, data flows and the quantitative models that generate or consume data
• Excellent communications skills and the ability to work with subject matter experts to extract critical business concepts
• Independent thinker, willing to engage, challenge or learn
• Ability to stay commercially focused and to always push for quantifiable commercial impact
• Strong work ethic, a sense of ownership and urgency
• Strong analytical and problem solving skills
• Ability to collaborate effectively across global teams and communicate complex ideas in a simple manner

Preferred Qualifications
• Financial Services industry experience
• Experience in distributed system design
• Working knowledge of open-source tools such as Kafka, Spark

ABOUT GOLDMAN SACHS
At Goldman Sachs, we commit our people, capital and ideas to help our clients, shareholders and the communities we serve to grow. Founded in 1869, we are a leading global investment banking, securities and investment management firm. Headquartered in New York, we maintain offices around the world.
We believe who you are makes you better at what you do. We're committed to fostering and advancing diversity and inclusion in our own workplace and beyond by ensuring every individual within our firm has a number of opportunities to grow professionally and personally, from our training and development opportunities and firmwide networks to benefits, wellness and personal finance offerings and mindfulness programs. Learn more about our culture, benefits, and people at GS.com/careers.
We’re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https://www.goldmansachs.com/careers/footer/disability-statement.html

© The Goldman Sachs Group, Inc., 2020. All rights reserved.
Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Veteran/Sexual Orientation/Gender Identity"
29-Apr-2022 T11:47,"Senior Associate, Machine Learning Engineer, Group Consumer Banking and Big...",Dbs Bank Ltd.,8 hours ago,,Full–time,"Business Function

Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation.

In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.
Responsibilities

Apply cutting edge technologies and tools in big data and machine learning to build, manage and automate pipelines for machine learning models and analytics platform.

Implement machine learning algorithms and build production grade end-to-end analytics solution to solve business challenges together with Data Scientists and business team.
Implement and apply industry standard Auto ML solutions to solve real world challenges.
Create blueprint and reference architecture for various machine learning use cases.

Study and evaluate the state-of-the-art technologies, tools, and frameworks of machine learning engineering, and establish, apply and maintain best practices and principles of machine learning.
Perform code optimization, code reviews to improve the quality of Data Scientist's work.
Automate software testing by writing unit tests for all production code.
Monitor machine learning model releases, branches for different issues and user stories.
Manage available resources such as hardware, data, and personnel so deadlines are met.
Work with different stakeholders to ensure that the bank's production release process is adhered to.
Requirements
Bachelor's or master's degree in software Engineering, Computer Science or related fields.

At least 2 years of experience in data mining and machine learning on large amount of data, and multi-tier software application development and DevOps automation.
Excellent understanding of software engineering principles and design patterns.
Familiar with tools such as JIRA, Git and Jenkins.

Excellent programming skills in Python (Pandas, NumPy), SQL, bash and willingness to learn other languages as required by the project.
Experience with Spark, Hadoop and how to optimize Spark jobs that analyse huge amounts of data for better performance.
Experience with Kubernetes, PCF and services in AWS to deploy ML models as REST APIs.

Familiar with industry paradigms and standards for model development, validation and testing and have developed and implemented large-scaled industrial standard machine learning solutions from end to end.
Strong in problem-solving, being resourceful with end to end critical thinking to find out solutions even in unfamiliar scenarios.
Good communication and project management skills.
Strong interests in learning about recent developments in machine learning through own initiatives.
Apply Now

We offer a competitive salary and benefits package and the professional advantages of a dynamic environment that supports your development and recognises your achievements"
29-Apr-2022 T11:47,"Facilities Engineer (Data Centre, office hours)",Randstad Singapore,6 hours ago,,Full–time,"About The Job

This is a company whose expertise has made them the leader in their field is seeking a high-calibre Facilities Engineer in Singapore to support the data centre operations in Singapore.

The Facilities Engineer will play a crucial role in the company’s continued development and success in Singapore. Your key purpose will be to oversee the execution of the local data operations and manage the engineering operations and core infrastructure. You will be responsible for the incident reporting, vendor management, change management, documentation of operational procedures as well as the management of the day to day operations. This is a rare opportunity for a hands-on individual to be part of a growing organization that provides a stimulation work environment.

To Be Successful In The Role, You Would Have

skills and experience required.
• Ideally have at least 5 years experience in relevant Facilities Management within Data Center Operations
• Relevant qualifications in Engineering or Mechanical Engineering or Electrical Engineering are preferred.
• Have an understanding of construction, commissioning and operation of mission critical systems.
• Able to work shift hours is a must
• Great communication skills are necessary this is a client facing role.
• Ability to work independently and make logical decisions

How To Apply

To apply online, please click on the ‘apply’ function below.

Please indicate your availability, expected salary, and reason for leaving your current job in your CV.

EA: 94C3609 / R1767516

Applicants must be fully vaccinated or have a valid exemption in accordance with MOM’s regulations to allow them to enter the workplace. Applicants may be required to share verifiable COVID-19 vaccination documents or proof of a valid exemption at the point of offer. Randstad Pte. Limited and/or the Client reserves the right to withdraw an offer if the applicant fails to provide verifiable COVID-19 vaccination and/or proof of exemption documents"
29-Apr-2022 T11:47,Data Engineer,MANN+HUMMEL,16 hours ago,,Full–time,"Your challenge **Please indicate in your resume, the position you are applying for ** Working cross-functionally with business managers/product managers/engineers and data scientists to gather requirements and to understand their business processes Design, develop, deploy, manage scalable cloud infrastructures that solve business problems Designing, implementing, and managing optimal data analytics pipelines from end-to-end Making strategic data architecture recommendations Implementing data science frameworks to enable organization-wide experiments, enable data science research and development and enable deployment of production solutions Implementing systems that enable delivery of insights to business units and customers Apply dev-ops processes to deliver and maintain production level systems Apply quality processes to ensure the requirements are met Implement and manage security in accordance with industry standards Document all aspects of design, implement, test and release Your profile As a successful applicant, you would have a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field with the ability to manage stakeholders and communicate well. You will have strong experience in cloud technologies (AWS/Azure or similar) with at least 5 years of experience in a Data Engineer role. A history of working with large scale reliable data systems Proficient with cloud technologies and application of native services Experience with big data tools and delivery of big data solutions Experience in working with different types of data stores including SQL, NoSQL, warehouses, lakes, etc Experience working in Hadoop ecosystem and Spark is a plus Experience with data ingestion and data integration tools and frameworks, data pipeline and workflow management, common data science tools such as R, python, Big data technologies, Tableau Proven ability to deliver high profile activities to tight timescales Proven ability to apply analytical and creative thought Ability and desire to learn and pick up new tools and technologies for problem solving, enhancing analysis results and accuracy, and optimizing workflow efficiency Independent and possess creative problem solving skills to address business problems from different perspectives Ability to distil and communicate results to all organisational levels Practice a lean agile scrum process to continuously deliver value to customers Proven success in contributing to a team-oriented environment Ability to interact with global teams and manage the related cultural challenges Be able to interact with development teams to determine project requirements Are you full of ideas? Are you keen to take on responsibility and really achieve something? Then our doors are open to you. This is a company that lives out its values, gives people the freedom to use their own initiative, offers many development opportunities and many exciting projects – all of which awaits you here. Interested? Fill out the form below to apply. The fields marked with an asterisk (*) are required fields. Similar job offers"
29-Apr-2022 T11:47,Data Engineer Trust Safety,Tencent,16 hours ago,,Full–time,"工作职责 ·Technical research, development and operation in the field of anti-cheat/anti-Trojan horse for overseas PC and mobile games.·Responsible for the research, design and implementation of technical countermeasures by following the security compliance.·Communication and service management in overseas game anti-cheat work with other corporations. 工作要求 ·Bachelor degree or above, with data analysis/algorithm related experience·Good in data analysis capability; Experience in at least one of these areas is an advantage: reverse engineering, vulnerability analysis, data analysis/algorithms, security countermeasure.·Proficient in at least one programming language (Python, C/C++, Go), experience in data analysis implementation for security is an advantage·Passionate in games, capable to understand games logic ·Experience in game anti-cheat operation is an advantage"
29-Apr-2022 T11:47,Senior Data Engineer,ExpressVPN,8 hours ago,,Full–time,"About The Data Engineering Team

Let’s talk about data. Do you have experience building and maintaining data warehouses with big data technologies? Can you build data-intensive applications with Python, Java, or Scala? Have you managed cloud-native big data environments? If any or all of this applies to you, you may be just the Senior Data Engineer we’re looking for to join our fast-growing team. With opportunities in fast-growing London office, we’re hiring Data Engineers that are an essential conduit to other engineering teams across our consumer-facing company, as well as our Data Insights team.

Our team acts as a central nexus to connect various data producers with consumers across the company. Our customers are:
• Other engineering teams across the company that produce or consume data that need to be combined with other data sources.
• Analysts on the Data Insights team.

We Are Accountable For Delivering
• A centralized data warehouse that enables engineers and analysts across the company to ingest, anonymize, and enrich with other data sources from anywhere else in the company, persist, analyze, purge, and otherwise process their data.
• Tools, training, and coordination.
• Data applications that don’t fall into any one business unit, or where the business units don’t have sufficient capabilities themselves. For example, we team up with the data-insights team to build and operate churn-prediction models used by both humans and other systems at scale.
• Data Catalog for documenting the sources of data and what is available for use by other teams.

Our Responsibilities Include
• Building and operating the data platform service, including defining and tracking its SLA.
• Guiding various engineering teams to design models and schemas of the data to be fed into the platform, making sure they can be processed in a scalable way and used by analysts efficiently.
• Guiding data analysts on the use of the data platform.
• Building libraries/modules and reference implementations of data ingesters on several common tech stacks.
• Guarding user privacy. While all teams are responsible for ensuring compliance of their work with our privacy policy, our team also has a veto right against processing any data that might not be compliant.
• Partnering with other teams on projects to build data engineering solutions, such as for churn-prediction, payment fraud management, and other company-wide challenges.

Other Notes About Our Team
• Our tech stack currently mostly focuses on AWS Redshift, Google BigQuery, Apache Airflow and Tableau, but we imagine it will evolve significantly over time.
• We have an ever-expanding range of engineering roles on the team, covering people with backgrounds in software development, infrastructure operations, and data science.

Job Responsibilities

Your Responsibilities Will Include
• Understand the needs of your internal customers, and convert them to optimized and maintainable tech designs.
• Use your data engineering skills to design and build the ingestion, processing, storage and consumption system for data to enable other business units to make business and operational decisions using data.
• Maintain and operate the data platform, which many business units rely on to fulfill their service level targets.

Role Requirements
• At least 2 years of experience designing and operating data pipelines and databases
• Proficiency in Python, Java, or Scala with a good understanding of runtime complexities
• Proficiency in database operation and optimization, including SQL optimization
• Strong understanding of and experience in big data tools such as Hadoop, Spark, Flink, Storm
• Experience in testing ETL pipelines
• Experience in building and operating data applications in cloud environments (AWS, Azure or GCP)
• Experience in automation tools like Ansible and Terraform is a big plus
• Strong written and verbal English communication skills

Hiring process

When it comes to hiring processes, “rigorous” and “opaque” are often mistakenly conflated. For us, it’s always a mutual exchange, so we think it’s important that candidates have a clear understanding of the process and what we’re looking for. Learn more about the hiring process by visiting our careers page.

Benefits

Health and happiness go hand in hand, and we make every effort to support our team members in all facets of their lives—both inside and outside the office. Learn more about our employee benefits by visiting our careers page.

Before you apply
• At the moment, we do not sponsor visas in the UK and the EU. For Hong Kong, we require at least two years of working experience and a university degree in a related field. For Singapore, we can only sponsor visas for mid-career or above.
• Please upload your resume as a PDF and do not include any salary or compensation information in it.

ExpressVPN is one of the world’s leading providers of online privacy and security services for consumers. Started in 2009, we’ve grown to have millions of active paying customers, a team of more than 700 people worldwide, and a brand recognized by hundreds of millions of people in 18 languages and more than a hundred countries. We see huge growth in our industry, and are gaining market share through strong execution"
29-Apr-2022 T11:47,"Machine Learning Engineer, Risk Data Mining",ByteDance,21 hours ago,,Full–time,"Responsibilities

TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. The Business Risk Integrated Control (BRIC) team is missioned to:
- Protect ByteDance users, including and beyond content consumers, creators, advertisers;
- Secure platform health and community experience authenticity;
- Build infrastructures, platforms and technologies, as well as to collaborate with many cross-functional teams and stakeholders. The BRIC team works to minimize the damage of inauthentic behaviors on ByteDance platforms (e.g. TikTok, CapCut, Resso, Lark), covering multiple classical and novel community and business risk areas such as account integrity, engagement authenticity, anti spam, API abuse, growth fraud, live streaming security and financial safety (ads or e-commerce), etc. In this team you'll have a unique opportunity to have first-hand exposure to the strategy of the company in key security initiatives, especially in building scalable and robust, intelligent and privacy-safe, secure and product-friendly systems and solutions. Our challenges are not some regular day-to-day technical puzzles -
- You'll be part of a team that's developing novel solutions to first-seen challenges of a non-stop evolvement of a phenomenal product eco-system. The work needs to be fast, transferrable, while still down to the ground to making quick and solid differences.

Responsibilities
- Build machine learning solutions to respond to and mitigate business risks in ByteDance products/platforms. Such risks include and are not limited to abusive accounts, fake engagements, spammy redirection, scraping, fraud, etc.
- Improve modeling infrastructures, labels, features and algorithms towards robustness, automation and generalization, reduce modeling and operational load on risk adversaries and new product/risk ramping-ups.
- Uplevel risk machine learning excellence on privacy/compliance, interpretability, risk perception and analysis.

Qualifications

Qualifications
- Master or above degree in computer science, statistics, or other relevant, machine-learning-heavy majors.
- Solid engineering skills. Proficiency in at least two of: Linux, Hadoop, Hive, Spark, Storm.
- Strong machine learning background. Proficiency or publications in modern machine learning theories and applications such as deep neural nets, transfer/multi-task learning, reinforcement learning, time series or graph unsupervised learning.
- Ability to think critically, objectively, rationally. Reason and communicate in result-oriented, data-driven manner. High autonomy"
29-Apr-2022 T11:47,Data Engineer,HYPEBEAST,16 hours ago,,Full–time,"Responsibilities:Identify, define, and compile disparate data sources (local, cloud, 3rd party) for ingestion in data pipelines.Design, implement, and maintain data governance, dictionaries, and models.Develop and implement data pipelines & infrastructure for Hypebeast; such as building data lakes and warehouses in Google Cloud Platform (GCP), developing views with MYSQL/Postgre, implementing functional data marts using BigQuery, and visualisations via Tableau/Data Studio.Collaborate with data analyst to generate actionable insights based on your soon-to-be intimate understanding of our data.Requirements:2 years of managing databases or building data pipelines on the cloud.Proficient in SQL and relational databases. Proficient in at least one scripting language, preferably in Python or R, to clean, transform, and denormalise datasets.Some knowledge of visualisation tools such as Tableau, Data Studio, or Power BI.Experience with GCP infrastructure and customer analytics is a plus.Love learning and sharing new technical knowledge, willing to take new challenges.Possess strong sense of ownership, adaptive to change.Recent graduates with passion to develop career in Data are welcome. If you think you’ve got what it takes, please provide your cover letter, CV and expected salary.This role is located and based in Singapore. Candidate must be eligible to work in Singapore.Personal data collected is for recruitment purpose only"
29-Apr-2022 T11:47,Technical Data Engineer,OCBC Bank,16 hours ago,,Full–time,"Description

Bank of Singapore is currently looking for a qualified candidate to assist the Data Engineering Team’s operational and analytical needs. The Technical Data Enginer will work alongside Data Analytics and big data platform (Hadoop) and engineering team to provide data related support, data ingestion, data interface for unstructured/structred data, data analytics and data management. This person will also assist in building interfaces from various upstream systems and ingest the data into Micrsoft SQL server 2019 / Cloudera Hadoop data store and build the enterprise visulasation tool. This is a great opportunity for someone who is interested in innovative group with the possibility of tremendous career development in data engineering, big data management, data analytics and enterprise data visulsaisation tool.

A little more about this role:

As our Technical Data Enginer, you will be instrumental in big data coding and work in Hadoop-ecosystem. This is a brand-new position at Data Competency vertical.

Perform extensive unstructured data ingestion into Hadoop

StronG knowledge of Anaconda, Data visualisation BI tool, Python, SPARK, Java Scala, HIVE and Beeline with hands on experience

Ability to organize and lead meetings with business and operational data owners

Experience in integrating data processes with architecture requirements used across company

Understand Hadoop-ecosystem and Data Engineering activities as well as loading data from several disparate datasets and documentation

Strong ability to troubleshoot and resolve data issues

Analytical skill to perform data profiling and data visulization

Experience in Agile and Waterfall frameworkWork

Work closely with engineering and operations to document business processes

Work independently and with team members to understand database structure and business processes

Help form data management and governance processes within the data engineering team

Qualifications

What you’ll need to have:

Graduate degree in statistics, math, computer science, physics or other technical related fields; Master’s degree is preferred

Minimum of 10 years working experience in technical data analysis, data science, or data warehousing with proven business analysis experience

Experience in at least one or more languages: SPARK, Java Scala,Python

Experience writing Java Scala, Python

Experience with Hadoop

Hands on expierence or knowledge of minimum one mainstream cloud infrastructures:AWS,MS Azure and GCP; ablity to implement data lake.

Good to have Hands-on experienceon the Hadoop, MangoDB,SPARK, Scala, HIVE, Kafka ,Beelin…etc

Excellent communication skills

Passionate about data and analyzing business needs

Previous experience on a data team in an agile environment preferred

Hands-on experience on the Hadoop ecosystem, HDFS, Hadoop, Spark, Scala preferred

Develop in-depth plans and major milestones that must be approved by top management during the planning and design phases of the project"
29-Apr-2022 T11:47,Senior Data Engineer - Kafka,Xendit,5 hours ago,,Full–time,"Xendit provides payment infrastructure across Southeast Asia, with a focus on Indonesia and the Philippines. We process payments, power marketplaces, disburse payroll and loans, provide KYC solutions, prevent fraud, and help businesses grow exponentially. We serve our customers by providing a suite of world-class APIs, eCommerce platform integrations, and easy to use applications for individual entrepreneurs, SMEs, and enterprises alike.

Our main focus is building the most advanced payment rails for Southeast Asia, with a clear goal in mind — to make payments across in SEA simple, secure and easy for everyone. We serve thousands of businesses ranging from SMEs to multinational enterprises, and process millions of transactions monthly. We’ve been growing rapidly since our inception in 2015, onboarding hundreds of new customers every month, and backed by global top-10 VCs. We’re proud to be featured on among the fastest growing companies by Y-Combinator.

Mission

We are scaling the data engineering team that works on providing internal teams with secure, reliable, performative, and user-friendly access to data to support analytics & reporting, fraud detection, billing generation, and other company critical processes.

We are looking for a Senior Data Engineer who will be a key part of our team and will help bring structure to vast amounts of data, making it digestible and build scalable data platforms that enable data products, business analytics, and data science. This role requires technical expertise and willingness to learn a wide variety of technologies to develop real-time data pipelines, data product APIs, and modern & scalable data infrastructure. If you are interested in working in a fast-paced environment and like being challenged with fun data problems to solve, come join us.

Responsibilities
• Simplify access to real-time data for internal stakeholders (Kafka).
• Design and implement our real-time data pipelines using Kafka.
• Improve and maintain the data lake setup (S3, EMR, Presto).
• Set up fully automated alerting systems to proactively monitor the infrastructure (Datadog, Pagerduty).
• Continuously improve the overall performance of analytical queries.
• Build scalable data infrastructure using Terraform.
• Build customer-facing data products with a focus on reliability and scalability.
• Ensure data quality through automated audits.
• Collaborate with analysts, engineers, and business users to design solutions.
• Research innovative technologies and make continuous improvements.

You may be a good fit if
• 5+ years of relevant industry experience.
• Excellent knowledge of Python and SQL.
• Demonstrated ability to build high-volume data ingestion and streaming pipelines (e.g. Kafka, AWS Kinesis, Spark Streaming)
• Working experience with big data technologies (e.g. Spark, Presto, Hive).
• Experience designing, building, and scaling a production-ready event streaming system.
• Experience in optimizing SQL queries (e.g. data partitioning, bucketing, indexing)
• Experience in building data lake/warehouse solutions consisting of structured and unstructured data.
• Experience with IaC is a plus (e.g. Terraform, CloudFormation)
• Bachelor's degree in a technical field or equivalent work experience.
• You have built data products that have scaled on AWS or another cloud.
• You thrive on nimble, lean, fast-paced startups, like autonomy, and have proven you can push towards a goal by yourself.
• Coachable. Able to own mistakes, reflect, and take feedback with maturity and a willingness to improve.
• You communicate with clarity and precision and you are able to effectively present results"
29-Apr-2022 T11:47,Senior Data Engineer Remote APAC,Shopify,16 hours ago,,Full–time,"Job Description

Our Data Engineering group builds and maintains the platform that delivers accessible data to power decision-making at Shopify for millions of merchants. We’re hiring high-impact developers across teams:
• The Engine group organizes all merchant and Shopify data into our data lake in highly-optimized formats for fast query processing, and maintaining the security and quality of our datasets.
• The Analytics group leverages the Engine primitives to build and deliver simple and useful products that power scalable transformation of data at Shopify in batch, streaming, or for machine learning. This group is focused on making it really simple for our users to answer three questions: What happened in the past? What is happening now? And, what will happen in the future?
• The Data Experiences group builds end-user experiences for experimentation, data discovery, and business intelligence reporting.
• The Reliability group operates the data platform in a consistent and reliable manner. They build tools for other teams on Data Platform to leverage and encourage consistency as they champion reliability across the platform.

Qualifications
• An experienced technical leader with a proven track record of delivering impactful results.
• Technical engineering background in one or more areas in the next section.
• Experience with technical mentoring, coaching, and improving the technical output of the people around you.
• Exceptional communication skills and ability to translate technical concepts into easy to understand language for our stakeholders.
• Excitement for working with a remote team; you value collaborating on problems, asking questions, delivering feedback, and supporting others in their goals whether they are in your vicinity or entire cities apart.

A Senior Data Developer at Shopify typically has 4-6 years of experience in one or more of the following areas:
• Working with the internals of a distributed compute engine (Spark, Presto, DBT, or Flink/Beam)
• Query optimization, resource allocation and management, and data lake performance (Presto, SQL)
• Cloud infrastructure (Google Cloud, Kubernetes, Terraform)
• Security products and methods (Apache Ranger, Apache Knox, OAuth, IAM, Kerberos)
• Deploying and scaling ML solutions using open-source frameworks (MLFlow, TFX, H2O, etc.)
• Building full-stack applications (Ruby/Rails, React, TypeScript)
• Background and practical experience in statistics and/or computational mathematics (Bayesian and Frequentist approaches, NumPy, PyMC3, etc.)
• Modern Big-Data storage technologies (Iceberg, Hudi, Delta)

Additional Information

#LI-REMOTE

Shopify is now permanently remote, and we’re working towards a future that is digital by design. That location you see above? Consider it merely an example of hundreds of potential locations Shopify is hiring. Learn more here: https://www.shopify.com/careers/work-anywhere

Our belief is that a strong commitment to diversity & inclusion enables us to truly make commerce better for everyone. We encourage applications from Indigenous peoples, racialized people, people with disabilities, people from gender and sexually diverse communities, and/or people with intersectional identities. Please take a look at our Sustainability Reports to learn more about Shopify’s commitments to our communities, and our planet.

At Shopify, we understand that experience comes in many forms. We’re dedicated to adding new perspectives to the team - so if your experience is this close to what we’re looking for, please consider applying"
29-Apr-2022 T11:47,Data Engineer,fccsingapore,16 hours ago,,Full–time,"Job Description
On behalf of one of its clients in the R&D sector, the French Chamber of Commerce is looking for a
Data Engineer
Mission

We are looking for a savvy data engineer who is expertise in building data pipeline, wrangling data, designing database architecture, and optimizing data system. The candidate will work with a wide range of stakeholders and functional teams to identify opportunities, collect requirements, and manipulate company data to be assessable by different data users. The right candidate will be excited by the prospect of optimizing or even re-designing the company’s data architecture to support our clients next generation of products and data initiatives.

Responsibilities

Collaborate with the HQ and the external partners (university, start-ups etc).

Perform a technological watch on the new tools of Data Management and implement them as part of technological demonstrators.

Define and implement data processing architectures including databases, local or cloud communication systems as well as processing algorithms and applications for visualization of treatment results.

Coordinate with IoT specialists and technical experts from our business units to retrieve data generated by our products or internal processes and collect business needs, develop technical architecture with solutions with his/her expertise.

Build the infrastructure required for optimal ETL of data from a wide variety of data sources with SQL and big data technologies.

Create and maintain optimal data pipeline architecture and assemble large, complex datasets that meet functional / non-functional requirements.

Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

Requirements

Master’s/Bachelor’s Degree in Computer Science, Computer Engineering, Statistics, Business Analytics, Information System with minimum 2 years working experience.

Proficient at relational SQL (MS SQL, PostgreSQL, MySQL, etc.) and NoSQL (MongoDB, CosmosDB, etc.) database design on local premise and on cloud.

Experienced in performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

Experienced with Azure cloud services such as ADF, Azure Data Bricks, Azure Stream, Azure Synapse, etc.

Knowledge of LANs, WANs, Ethernet protocols, distributed computing architectures, authentication processes and IT security.

Strong analytic skills related to working with structured / unstructured datasets.

Programming languages: Python, R, C#, SQL, etc.

Strong project management and organizational skills.

Experience supporting and working with cross-functional teams in a dynamic environment.

Only Singapore-based candidates will be considered.

Requirements
Master’s/Bachelor’s Degree in Computer Science, Computer Engineering, Statistics, Business Analytics, Information System with minimum 2 years working experience. Proficient at relational SQL (MS SQL, PostgreSQL, MySQL, etc.) and NoSQL (MongoDB, CosmosDB, etc.) database design on local premise and on cloud. Experienced in performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Experienced with Azure cloud services such as ADF, Azure Data Bricks, Azure Stream, Azure Synapse, etc. Knowledge of LANs, WANs, Ethernet protocols, distributed computing architectures, authentication processes and IT security. Strong analytic skills related to working with structured / unstructured datasets. Programming languages: Python, R, C#, SQL, etc. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. Only Singapore-based candidates will be considered"
29-Apr-2022 T11:47,Data Engineer,Keystone Cable (s) Pte Ltd,16 hours ago,,Full–time,"Job ScopeTo provide the technical drive on the company's i4.0 digitalization and automation plans.To understand departmental digitalization requirements and use programming or software to reduce digital waste.Help to drive digitization / digitalization efforts for data analysis on key operational metrics; to increase operational capacity, efficiency, productivity and manpower utilisation.To design and build data solutions that support company's data and analytics strategy in driving business insights.To work closely with ERP, MES and other technology vendors to implement and support the company's implementations.RequirementsRespectfulGood communication skillsGrowth mindsetWilling to accept new challenges and learnTechnical requirements: Industrial control and automation, Power and PLC systemsAcademic qualifications in computer science or related fieldsMin programming skills: Python, SQL databaseUnderstanding of data governance, data security and data analytics"
29-Apr-2022 T11:47,Data Engineer,Leap29,16 hours ago,,Full–time,"Job Title: Data Engineer (12-month contract, conversion to perm after contract)
Start Date: As Per Notice
Location: Singapore
Salary: Market Competitive

Job Overview:
My Client is one of the world’s leading Consumer Goods organisation and are seeking a Data Engineer to join their team in Singapore. The role will be expected to blend the expertise of Product knowledge and Data Engineering, responsible for transforming business requirements into sophisticated product features and insights. The position will be an individual contributor role and to be successful, the candidate should thrive leading discussions with both technical and commercial business teams.

Key Accountabilities
• Partner with global and market data experts to discover value from exploiting external/ internal data sources.
• Consult commercial business leaders on how the product can best add value to their business growth.
• Build (ETL) new and evolve data models and pipelines to power algorithmic based Business Intelligent solutions that addresses business problems requiring descriptive, diagnostic, predictive, and/or prescriptive analytics for pricing, promotion, trade spending, assortment, and sales performance management.
• Translate the algorithms and analytic models into data models as business needs evolves on a going basis after they are put into production.
• Partner with operational stakeholders to ensure End-to-end data pipeline is functional and compliant.
• Develop roadmap that scales existing and new data models to support the portfolio of markets or solutions.

Relevant Experience:
• Between 4 to 8 years of strong Data Engineering experience
• Diploma or B.S. in a relevant technical field (Operations Research, Computer Science, Statistics, Business Analytics, Econometrics, or Mathematics).
• Ability to manipulate and high-volume of data from varying sources
• Proficiency in R/ Python, PySpark will be ideal
• Basic knowledge of MS Azure architecture/ Databricks/ Data Factory pipeline deployments
• Proficiency of an analysis tool such as Microsoft PowerBI
• Basic Strong track record in working independently with minimal guidance

If this is something of interest to you, please send across your CV Ms Roopinder on rkaur@leap29.sg for review. Only shortlisted candidates will be notified.

Good luck!

To help Leap29 find you that perfect job, we need to store and process your personal information.
That means that your details will be entered into Leap29's database and our consultants may contact you from time to time with relevant job opportunities.
By applying you're confirming you're happy for us to do that"
29-Apr-2022 T11:47,Head of Data Platform and Engineering,ShopBack,6 hours ago,,Full–time,"About Us

ShopBack : Better Shopping, Every Day.

The ShopBack Group is Asia-Pacific’s leading shopping and rewards platform, serving over 30 million shoppers across ten markets. Growing from a team of six back in 2014 to over seven hundred today, ShopBackers across the region come together with a singular mission: to make shopping rewarding, delightful, and accessible for all.

Joining forces with leading buy now, pay later (BNPL) player hoolah and with the launch of ShopBack Pay, the Group now offers shoppers a responsible and convenient payment option at checkout.

More than half a billion shopping trips start with ShopBack each year. The Group powers over US$3.5 billion in annual sales for over 10,000 online and in-store merchant partners, across categories ranging from fashion, beauty, F&B, electronics, travel and more. If you are passionate about building and scaling up businesses in this fast-growing landscape, come and join our growing ShopBack team!

Responsibilities
• Grow develop, and manage a team of talented Data Engineers
• Maintain high quality engineering standards
• Develop and deliver high-impact, forward-looking roadmaps for your team
• Help solve cross-team problems at scale and delight internal customers
• Drive vision to build a modern Data Platform at ShopBack
• Communicating fearlessly to build trust with diverse stakeholders
• Drive continuous improvement in the efficiency and flexibility of the services

Requirements
• 5+ years of data engineering and data platform experience
• Significant prior success developing and deploying RecSys at scale
• Excellent people manager with a record of recruiting, managing, and retaining talent
• Full stack experience in data platform components
• Robust project management skills, including work estimation, prioritization, planning, tracking, and retrospectives
• Ability to develop ML Product with good engineering practice and mindset
• Strong desire to solve tough problems with scientific rigour at scale
• Experience with big data technologies like Hadoop, Spark, Hive, Presto, etc.
• Strong skills in any programming language like Java, Scala or Python.
• Experience working on both real-time and batch processing data pipelines
• Bachelor's degree or equivalent experience in computer science, engineering, or another technical discipline (MS/PhD is a plus"
29-Apr-2022 T11:47,Data Engineer,Ntuc Learninghub Pte. Ltd.,16 hours ago,,Full–time,"Job Responsibilities:Design, develop, implement, support and maintain old and new data models, data warehouses, cubes, ETL packages and core data infrastructure crucial to the needs of the business.Retrieve, cleanse, validate and analyse data using different techniques in order to retain the data integrity and availability for Business Supports.Work closely with internal stakeholders to collect and understand business requirements then build, design and manage interpret solutions for either reporting or dashboard solutions requiredWork with management and stakeholders to prioritize business and information needsTransform business requirement to data requirement, data analysis and profilingDesign data models for use cases and data warehousingPerform data cleaning and ensure data quality and integrity in data warehouseRoot cause analysis of data issues and coming up with stable long terms solutionsSupport data store's inbound and/or outbound developmentPerform data acceptance testing together with usersImplement and build a reliable data warehouse which meets the business needs of all stakeholder and according to data warehouse architecture guidelinesIdentify data from different source systems and build the target data modelsEnsure business rules and data definition are standardised across users and reports/dashboardsEnsure company's data policy, data security standard and data governance guidelines are adhered toRequirement:Strong experience with Talend Studio and related toolsDiploma/BS in any relevant field (e.g., Statistics, mathematics, computer science or information technology).At least 3-5 years relevant working experience as Data Engineer/Business Intelligence.Highly proficient in composing, maintaining and optimising complex SQL queries.Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Strong knowledge of and experience on data management, structure, retrieval and ETL processes (structured and unstructured data sources)Knowledge of ODS an advantageAbility to present information in a meaningful and structured manner (report and dashboard)Familiarity with programming languages such as Java, Python and its libraries"
29-Apr-2022 T11:47,"VP, Data Engineer",JobCart,16 hours ago,,Full–time,"Job Details

VP, Data Engineer

VP, Data Engineer

Posting Date: 10-Mar-2022

Location: Alexandra, Singapore, SG

Company: United Overseas Bank Limited

About UOB
United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices.
Our history spans more than 80 years. Over this time, we have been guided by our values - Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.

About the Department
The Technology and Operations function is comprised of five teams of specialists with distinct capabilities business partnership, technology, operations, risk governance and planning support and services. We work closely together to harness the power of technology to support our physical and digital banking services and operations. This includes developing, centralising and standardising technology systems as well as banking operations in Singapore and overseas branches.

Job Responsibilities

The Data Engineer will be directly responsible for the end-to-end software development and support for all work related to projects, quarterly change requests, L3 production fixes. This includes software product implementation and administration, application design, development, implementation, testing and support. You will be expected to work on Finance & Risk Analytics .

You will also be responsible for quality assurance of the team's delivery in conformance with the Bank-defined software delivery methodology and tools . You will partner with other technology functions to help deliver required technology solutions:

Create frameworks, technical features which helps in faster operationalisation of Data models, Analytical models (including AI/ML) and user generated contents (dashboards, reports etc)

Effectively partner with citizen data scientists in enabling faster adoption of AL/ML model based systems

Independently install, customise and integrate software packages and programs

Carry out POCs involving new data technologies

Design and develop application frameworks for data integration

Create technical documents such as solution design, program specifications for target solutions

Perform design and development of applications which may not be limited to: Software Applications, Data Integration, User Interfaces, Automation

Maintain and recommend software improvements to ensure a platform centric management of software applications

Performance tuning

Work with production support team members to conduct root cause analysis of issues, review new and existing code and/or perform unit testing

Perform tasks as part of a cross functional development team using agile or other methodologies and utilising project management software

What will help you succeed in this role

Adopt an uncompromising attitude when it comes to quality and help raise bar of products and team members

Be a team player who communicates effectively and professionally with both internal and external customers

Identify ideas to improve system performance and impact availability

Embrace tackling and resolving complex technical design issues

Possess strong problem solving and decision-making skills while exercising good judgment

Strong analytical and problem-solving skills

Ability to work on multiple projects at a time

Be able to work under pressure and manage deadlines or unexpected changes in expectations or requirements

Good communication skills - ability to convey technical information to non-technical audience

Ability to understand the big picture

Ability to develop long lasting relationships with all levels

Deep understanding and experience in software development cycle, including Agile based rapid delivery

Collaborate with business and IT to analyse, elicit and review business requirements

Facilitate communication between vendor, project team, business stakeholders and internal IT team

Ability to work in a team distributed across multiple locations

Job Requirements

Functional Skills: Data Lake, EDW, Data Mart, Data Integration & Visualisation
Hands-on experience in implementing large scale data warehouse & analytics platforms in financial services industry; with good functional knowledge of products & services offered in Retail bank/Wholesale/Global Markets covering some of the following analytics domains:

Setting up and running BI tools oriented platform

Design and develop QlikSense & Microsoft Power BI applications

Design and develop Applications in SAS, Microsoft-R, Python

Integration of BI tools with data stores (EDW, data marts)

Experience in Data Modeling, Data mapping for Data Warehouse and Data Marts solutions

Expertise in FSLDM or similar industry models

Experience in financial domain - Retail , Wholesale, Compliance, Digital

Experience in analytics - Finance Analytics, Credit Risk Analytics, Credit Scoring, Financial Crime Analytics

Expertise in design of role based fine grained access control

Designing cloud ready data solutions, Virtualization

Technical Skills: Data Lake, EDW, Data marts, Data Integration & Visualisation

Expertise in implementing Teradata based EDW and Data mart solution using ETL tool such as (Informatica Power Centre, BDM), GCFR based framework, BTEQ scripts)

Expertise in implementing Hadoop based Data mart using Spark based framework (Java, Scala, Pyspark)

Good knowledge and working experience in data loading into Teradata using different load pattern (TPT, MLOAD, FAST Load), data compression (MVC, BLC)

Expertise in implementing Data Governance tools; Data Lineage ( Informatica MM, EDC), Business Glossary and Data Quality

Expertise in implementing Reference data management (Data standardization, Hierarchy maintenance) using tools ( MDM,RDM)

Good knowledge and working experience in Teradata FSLM or similar industry standard data model

Expertise in Cloudera CDH / CDP components

Good knowledge in developing Spark based ingestion framework (Java, Scala, Pyspark)

Experience in building and operationalising feature pipeline to support AI/ML model execution, data pipelines for supporting large scale data warehouse/data marts

Any TWO technical certifications

Cloudera Hadoop distribution - Hive, Impala, Spark, Kudo, Kafka, Flume

Teradata - Bteq, Query Grid, GCFR, MDM, Data Mover, BAR

Informatica Data Integration - PC, IDR, BDM, MM, IDQ, EDC

Data modelling tools - Erwin

QlikSense

Microsoft Power BI - SSAS, SSRS

Microsoft - R

Data science workbenches - Cloudera Workbench, Jupyter, DataRobot, , IBM DSX

Data Virtualization tool - Denodo, Dremio

AS400

Language - SQL, Java, Python, Scala, Pyspark

Automation / scripting - CtrlM, Shell Scripting, Groovy

Any ONE as added advantage

CI/CD software, Testing Tools - Jenkins, SonarQube

Version Control Tool - Aldon+LMe, CA Endeavor

Deployment Tool kit - Jenkins

Service or Incident Management (IcM) Tools - Remedy

Source Code Repository Tool - Bitbucket

Scheduling Tool - Control-M

Defect Management Tool - JIRA

Application Testing tool - QuerySurge

Cloud certification

Platforms provided by FICO, Experian, SAS for credit and portfolio management

Be a part of UOB Family
UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.

Apply now and make a difference.

Technology - Project Management
Technology - Infrastructure
Technology - Information security"
29-Apr-2022 T11:47,Senior Data Engineer,Singtel,16 hours ago,,Full–time,"Jobscope Develop big data solutions for near real-time stream processing, as well as batch processing on the Big Data platform Work with business domain experts, data scientists, and solution designers to identify data relevant for analysis and develop Data solutions Fine-tuning of new and existing data pipelines Schedule and maintain data pipelines Drive optimization, testing and tooling to improve data quality Assemble large, complex data sets that meet functional / non-functional business requirements Develop APIs to support high throughput data processing, feature engineering, and use cases Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc Review and approve high level & detailed design to ensure that the solution delivers to the business needs and align to the data & analytics architecture principles and roadmap Understand various data security standards and use secure data security tools to apply and adhere to the required data controls for user access Support and contribute to development guidelines and standards for data ingestion Adapt and learn new technologies surrounding Data Platform The Ideal Candidate should possess the following: Bachelor’s Degree in IT, Computer Science, Software Engineering, Business Analytics or equivalent. Minimum of 6 years of experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following technologies: Hadoop distributions Spark NoSQL data warehouses such as HBase, Cassandra Excellent software & data engineering principles and design patterns Proficient in creating and maintaining complex data pipelines end-to-end while maintaining high reliability and security Excellent hands-on experience in Scala or Python Excellent hands-on experience with SQL and Spark Experience with Kafka Experience with CI/CD tools and environment Experience migrating from on-premise data stores to cloud solutions Good working experience with one or more major cloud vendors (ie: Azure,AWS, GCP) Experience working in Telco Data Warehouse and / or Data Lake advantageous Highly organized, selfmotivated, pro-active, and able to plan Ability to analyze and understand complex problems Ability to explain technical information in business terms Ability to communicate clearly and effectively, both verbally and in writing Strong in User Requirements Gathering, Maintenance and Support Good experience managing users and vendors Experience with Agile Methodology We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated"
29-Apr-2022 T11:49,Data Engineer,ITCONNECTUS PTE. LTD.,16 hours ago,$5K–$13K a month,Full–time,"Immediate joiners

Requirements
• Technical Data Architect who is strong on Data Warehousing, Big Data, Data Analytics & Data Science for solutioning
• Strong Google Cloud Platform Data Components – BigQuery, BigTable, CloudSQL, Dataproc, Data Flow, Data Fusion, Etc
• Demonstrate extensive skills and success in the implementation of technology projects within a professional environment, with a particular focus on data engineering
• Experienced in delivering Medium to Big Enterprise Data Projects
• GCP Experience including and not limited to MPP systems, Database systems, ETL and ELT systems and Data Flow compute Good to have skills
• A high-level understanding of AI and ML technologies and desire to develop your skills in these areas of emerging technology.
• Should be able to advise the best of breed for the client solutions; Skills Needed
• The Data Engineer coaches the junior data engineering personnel position by bringing them up to speed and help them get better understanding of overall Data ecosystem.
• Prior experience developing, building and deploying on GCP
• Working on Solution deck, IP build, client meetings on requirement gathering"
29-Apr-2022 T11:49,Data Engineer,Asm Technology Singapore Pte Ltd,16 hours ago,,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform. We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets. You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).

Your Responsibilities

Manage and support on-premise and Cloud-based data lake and warehouse systems

Design, build, support and optimize new and existing data structure and ETL processes

Build scalable and efficient data pipelines & services to help analytics teams to process the data

Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results

Liaise with third party tool providers to understand and improve data workflow

Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms

Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification

Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines

At least 2 years' experience in data engineering, automation and integration is preferred

Strong programming and scripting skills in Python and other modern programming languages

Strong data management, schema design and SQL development skills

Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)

Self-motivated and proactive, willing to learn new things

Good communication skills and strong team player

What our preferred candidates have

Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions

Understand and experienced with Cloud platform, eg. Microsoft Azure, AWS, GCP

Business intelligence and reporting tools, eg. Power BI, Tableau, Qlik, etc

Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)

REST/Web API development and management

Knowledge in Statistical software is an advantage

Experience In building machine learning models is a plus"
29-Apr-2022 T11:49,"Principal Data Engineer, Data Engineering",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Principal Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you are responsible for helping to shape the direction of the data engineering domain in GovTech; working with other domain experts, the delivery teams, vendors, and partners to influence technology roadmaps and the adoption of suitable data solutions across government. In addition, you will teach, mentor, and proactively grow fellow data engineers within the organization.

You will be leading multiple teams of data engineers to drive impact at scale, by building whole-of-government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will architect, design and build next-generation central data infrastructure to galvanise digitalisation in the public sector. Taking a more integrated approach to lay the foundation infrastructure and driving pervasive adoption of advanced data technologies throughout the economy and society, you will architect solutions that enable public agencies and sectoral partners to better integrate, exploit and understand their data to improve operations and decision-making, and deliver high-quality digital services to citizens and businesses in Singapore.

Join us on this journey to make a difference for the nation and our future generations, if you are:
• Experienced leading data pipeline building and data wrangling and enjoys optimizing data systems and building them from the ground up as part of multiple cross-functional teams,
• Have strong project management and organizational skills, self-directed and comfortable supporting the data needs across multiple teams, systems and products,
• Excited by the prospect of optimizing or even re-designing central and agencies’ data architecture to support next generation of government products and data initiatives, and
• Recognized expert with proven technical leadership, experienced working in a agile team to build systems from architecture to deployment for cloud and on-prem infrastructure, and
• Have the desire to serve the public good through the use of technology.

What you will be working on:
• Take the driving seat and lead our data engineers in complex technical projects to:
• Expand and optimize our data and data pipeline architecture, as well as optimize data flow systems for cross functional teams in a secured and scalable manner.
• Assemble large, complex data sets that meet functional / non-functional business requirements, transforming data into formats that are easy to consume and analyse.
• Identify, design, and implement internal process improvements, e.g., automating manual processes, optimizing data delivery, re-designing infra for greater scalability.
• Build the infra required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and industry standard Infra-as-Code tools.
• Build analytics tools that utilize the data pipeline to provide actionable insights into key performance metrics and support our next gen of products and data initiatives.
• Work with stakeholders including the Executive, Product, Data and Design teams to support their data and data pipelining use cases, and ensure optimal and secured data delivery architecture is consistent across projects, products and platforms.
• Drive innovative initiatives that uplift data capability across the government sector:
• Develop a strategic sense of key organisations, software and hardware tools, methodologies, and best practices in the data engineering landscape.
• Be the bridge between engineering and product line management and translate market requirements into product definitions and architecture designs.
• Analyse and solve complex data engineering problems, and translate architecture designs into implementations that satisfy the market requirements.
• Educate the team on new technological advances and work alongside them in proof-of-concept research projects and subsequent agile product delivery.
• Provide expert perspective to help shape data-related strategy and initiatives, and be our technical ambassador by writing internal blogs or publishing white papers.

What we are looking for:
• Candidate with 8+ years of experience in a Lead Data Engineer role, who has attained a Bachelor’s degree in Computer Science or other relevant engineering degree.
• In-depth working knowledge across a variety of relational SQL and NoSQL databases, including efficient query authoring, performance optimisation and troubleshooting.
• Experience building and optimizing data, data pipelines, and architectures, with processes to support transformation, data structures, metadata, dependency and workload management.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for further improvement.
• Working knowledge of message queuing, stream processing, and scalable data stores.
• A successful history of processing and extracting value from large datasets from different sources and systems, with strong analytics skills related to working with unstructured data.
• You should also have experience using the following software/tools:
• Big data and stream-processing tools: Hadoop, Spark, Beam, Flink, Kafka, etc.
• Data pipelining and workflow management tools: Luigi, Airflow, etc
• Cloud computing environments and services: AWS, GCP, Azure
• DevSecOps tools: Git, Kubernetes, Docker, Terraform etc
• Programming languages: Python, Java, C++, Scala, etc

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours”"
29-Apr-2022 T11:49,Data Engineer,Thakral One,16 hours ago,$8.5K–$10.5K a month,Full–time,"• Work with Banking team to understand existing SAS code logic written by techno-functional users
• Develop the SAS logic in Big Data environment, using spark, scala and pyspark utilizing client Hadoop ecosystem of the inhouse Data Lake
• Perform unit testing and system integration testing
• Work closely with Business Analysts team to review the test results and obtain sign off
• Deploy the new code in the client Production environment
• Prepare necessary design/operations documentation for future usage
• Perform peers Code quality review and be gatekeeper for quality checks
• 5-8 years of application development experience in Spark, Spark SQL, Scala is a must
• Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc
• Technical proficiency on data mining techniques and performance optimization
• Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL)
• Experience on SAS will be beneficial but not mandatory
• Handling of reporting packages (Tableau, QlikView) is nice to have
• AWS experience is nice to have
• Degree in Computer Science or Engineering is a must
• Good problem diagnosis and creative problem-solving skills
• Passion to learn and master diverse new technologies in the open-source community
• Accuracy and attention to detail
• Team-working, Verbal and Written communication skills

Desired Skills and Experience
Tableau, PySpark, Scala, Big Data, Pipelines, Unit Testing, Hadoop, Application Development, MariaDB, Spark, Data Mining, SQL, Attention to Detail, Python, System Integration Testing, S3"
29-Apr-2022 T11:49,Data Engineer,POWER IT SERVICES,13 hours ago,,Full–time,"Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

· Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

· A successful history of manipulating, processing and extracting value from large datasets.

· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science Statistics, Informatics, Information Systems or another quantitative field.

They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:49,Data Engineer,ITCONNECTUS PTE. LTD.,16 hours ago,$5K–$13K a month,Full–time,"Immediate joiners

Requirements
• Technical Data Architect who is strong on Data Warehousing, Big Data, Data Analytics & Data Science for solutioning
• Strong Google Cloud Platform Data Components – BigQuery, BigTable, CloudSQL, Dataproc, Data Flow, Data Fusion, Etc
• Demonstrate extensive skills and success in the implementation of technology projects within a professional environment, with a particular focus on data engineering
• Experienced in delivering Medium to Big Enterprise Data Projects
• GCP Experience including and not limited to MPP systems, Database systems, ETL and ELT systems and Data Flow compute Good to have skills
• A high-level understanding of AI and ML technologies and desire to develop your skills in these areas of emerging technology.
• Should be able to advise the best of breed for the client solutions; Skills Needed
• The Data Engineer coaches the junior data engineering personnel position by bringing them up to speed and help them get better understanding of overall Data ecosystem.
• Prior experience developing, building and deploying on GCP
• Working on Solution deck, IP build, client meetings on requirement gathering"
29-Apr-2022 T11:49,Data Engineer,Asm Technology Singapore Pte Ltd,16 hours ago,,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform. We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets. You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).

Your Responsibilities

Manage and support on-premise and Cloud-based data lake and warehouse systems

Design, build, support and optimize new and existing data structure and ETL processes

Build scalable and efficient data pipelines & services to help analytics teams to process the data

Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results

Liaise with third party tool providers to understand and improve data workflow

Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms

Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification

Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines

At least 2 years' experience in data engineering, automation and integration is preferred

Strong programming and scripting skills in Python and other modern programming languages

Strong data management, schema design and SQL development skills

Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)

Self-motivated and proactive, willing to learn new things

Good communication skills and strong team player

What our preferred candidates have

Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions

Understand and experienced with Cloud platform, eg. Microsoft Azure, AWS, GCP

Business intelligence and reporting tools, eg. Power BI, Tableau, Qlik, etc

Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)

REST/Web API development and management

Knowledge in Statistical software is an advantage

Experience In building machine learning models is a plus"
29-Apr-2022 T11:49,"Principal Data Engineer, Data Engineering",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Principal Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you are responsible for helping to shape the direction of the data engineering domain in GovTech; working with other domain experts, the delivery teams, vendors, and partners to influence technology roadmaps and the adoption of suitable data solutions across government. In addition, you will teach, mentor, and proactively grow fellow data engineers within the organization.

You will be leading multiple teams of data engineers to drive impact at scale, by building whole-of-government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will architect, design and build next-generation central data infrastructure to galvanise digitalisation in the public sector. Taking a more integrated approach to lay the foundation infrastructure and driving pervasive adoption of advanced data technologies throughout the economy and society, you will architect solutions that enable public agencies and sectoral partners to better integrate, exploit and understand their data to improve operations and decision-making, and deliver high-quality digital services to citizens and businesses in Singapore.

Join us on this journey to make a difference for the nation and our future generations, if you are:
• Experienced leading data pipeline building and data wrangling and enjoys optimizing data systems and building them from the ground up as part of multiple cross-functional teams,
• Have strong project management and organizational skills, self-directed and comfortable supporting the data needs across multiple teams, systems and products,
• Excited by the prospect of optimizing or even re-designing central and agencies’ data architecture to support next generation of government products and data initiatives, and
• Recognized expert with proven technical leadership, experienced working in a agile team to build systems from architecture to deployment for cloud and on-prem infrastructure, and
• Have the desire to serve the public good through the use of technology.

What you will be working on:
• Take the driving seat and lead our data engineers in complex technical projects to:
• Expand and optimize our data and data pipeline architecture, as well as optimize data flow systems for cross functional teams in a secured and scalable manner.
• Assemble large, complex data sets that meet functional / non-functional business requirements, transforming data into formats that are easy to consume and analyse.
• Identify, design, and implement internal process improvements, e.g., automating manual processes, optimizing data delivery, re-designing infra for greater scalability.
• Build the infra required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and industry standard Infra-as-Code tools.
• Build analytics tools that utilize the data pipeline to provide actionable insights into key performance metrics and support our next gen of products and data initiatives.
• Work with stakeholders including the Executive, Product, Data and Design teams to support their data and data pipelining use cases, and ensure optimal and secured data delivery architecture is consistent across projects, products and platforms.
• Drive innovative initiatives that uplift data capability across the government sector:
• Develop a strategic sense of key organisations, software and hardware tools, methodologies, and best practices in the data engineering landscape.
• Be the bridge between engineering and product line management and translate market requirements into product definitions and architecture designs.
• Analyse and solve complex data engineering problems, and translate architecture designs into implementations that satisfy the market requirements.
• Educate the team on new technological advances and work alongside them in proof-of-concept research projects and subsequent agile product delivery.
• Provide expert perspective to help shape data-related strategy and initiatives, and be our technical ambassador by writing internal blogs or publishing white papers.

What we are looking for:
• Candidate with 8+ years of experience in a Lead Data Engineer role, who has attained a Bachelor’s degree in Computer Science or other relevant engineering degree.
• In-depth working knowledge across a variety of relational SQL and NoSQL databases, including efficient query authoring, performance optimisation and troubleshooting.
• Experience building and optimizing data, data pipelines, and architectures, with processes to support transformation, data structures, metadata, dependency and workload management.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for further improvement.
• Working knowledge of message queuing, stream processing, and scalable data stores.
• A successful history of processing and extracting value from large datasets from different sources and systems, with strong analytics skills related to working with unstructured data.
• You should also have experience using the following software/tools:
• Big data and stream-processing tools: Hadoop, Spark, Beam, Flink, Kafka, etc.
• Data pipelining and workflow management tools: Luigi, Airflow, etc
• Cloud computing environments and services: AWS, GCP, Azure
• DevSecOps tools: Git, Kubernetes, Docker, Terraform etc
• Programming languages: Python, Java, C++, Scala, etc

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours”"
29-Apr-2022 T11:49,Data Engineer,Thakral One,16 hours ago,$8.5K–$10.5K a month,Full–time,"• Work with Banking team to understand existing SAS code logic written by techno-functional users
• Develop the SAS logic in Big Data environment, using spark, scala and pyspark utilizing client Hadoop ecosystem of the inhouse Data Lake
• Perform unit testing and system integration testing
• Work closely with Business Analysts team to review the test results and obtain sign off
• Deploy the new code in the client Production environment
• Prepare necessary design/operations documentation for future usage
• Perform peers Code quality review and be gatekeeper for quality checks
• 5-8 years of application development experience in Spark, Spark SQL, Scala is a must
• Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc
• Technical proficiency on data mining techniques and performance optimization
• Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL)
• Experience on SAS will be beneficial but not mandatory
• Handling of reporting packages (Tableau, QlikView) is nice to have
• AWS experience is nice to have
• Degree in Computer Science or Engineering is a must
• Good problem diagnosis and creative problem-solving skills
• Passion to learn and master diverse new technologies in the open-source community
• Accuracy and attention to detail
• Team-working, Verbal and Written communication skills

Desired Skills and Experience
Tableau, PySpark, Scala, Big Data, Pipelines, Unit Testing, Hadoop, Application Development, MariaDB, Spark, Data Mining, SQL, Attention to Detail, Python, System Integration Testing, S3"
29-Apr-2022 T11:49,Data Engineer,POWER IT SERVICES,13 hours ago,,Full–time,"Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

· Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

· A successful history of manipulating, processing and extracting value from large datasets.

· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science Statistics, Informatics, Information Systems or another quantitative field.

They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:49,Staff Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,16 hours ago,$8K–$13K a month,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 5 years of experience in Data Engineering role and have good knowledge / working experience in:

• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.

• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.

• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.

• Functional programming languages, e.g. Scala.

• Virtualization and container environment such as Docker and Kubernetes.

•Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:49,Data Engineering Lead,Stemly Pte. Ltd.,16 hours ago,,Full–time,"We are looking for a Senior Data Engineer with a track record of building enterprise grade data platform for cloud products .

This is an exciting opportunity for a technologist to build an innovative SaaS product using state-of-the-art and modern cloud technology stack.

As a Data Engineering Lead, you will be working on all aspects of data, from platform and infra build out to pipeline engineering and writing tooling / services for augmenting and fronting the core platform.

The data platform interfaces with large systems to ingest data (batch, micro-batch) to create statistical / machine learning models in a distributed setting.

You will work closely with data scientists, product managers, legal, compliance and business stakeholders.

You will also evaluate new technologies / frameworks and contribute to various open source big data technologies. You are encouraged to think out of the box and have fun exploring the latest patterns and designs in the fields of software and data engineering.

Your responsibilities
• Architecting and implementing a robust data platform for Stemly's products. Design and develop extremely efficient, reliable and observable data pipelines using technologies such as Airflow, Kubernetes, ElasticSearch, Kafka / KubeMQ, Postgres, ClickHouse etc.
• Design and deliver the next-gen data lifecycle management suite of tools / frameworks , including ingestion and consumption on the top of the data lake to support real-time, API-based and serverless use-cases, along with batch (mini / micro)
• Build and expose metadata catalogue for the Data Lake for easy exploration, profiling as well as lineage requirements
• Work on the development of the data platform iteratively by building quick POCs and converting ideas into real products
• Make data discoverable and easy to use for Data Scientists and Analysts. Enable Data Science teams to test and productize various ML models
• Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.

Support your colleagues by reviewing code and designs
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner

About you
• 8+ years of professional experience in software development in Python and Pandas
• Extensive experience with object-oriented design, coding and testing patterns, including experience with engineering software platforms and data infrastructures
• Must have sound understanding for Big Data components & administration fundamentals
• Hands-on in building a complete data platform using various open source technologies. Must have the knowledge to build metadata, lineage, observability and discoverability for data platform
• Experienced in DevOps best practices like CI / CD, containerization, blue-green deployments, secrets management etc. in the Data ecosystem
• Expert in the principles of distributed computing and data modelling
• Expert in building optimized SQL queries
• Conversant with the latest developments in the areas of Machine Learning models and efficiently supporting the data scientists is a plus
• Experience with Airflow, ClickHouse, Postgres, Kubernetes, GraphDB is a big plus
• Familiarity with Google Cloud Platform (e.g. GCS, Cloud Composer, BigQuery) is a plus
• Outstanding communication and interpersonal skills

About us

We are scientists and engineers with a strong passion for cutting-edge technology and decision science. Our mission is to enable enterprises to reduce time to decision with state-of-the-art forecasting and optimization.

Our algorithms are based on automatic machine learning techniques and allow for the discovery of optimal forecasting models.

We unlock sizable business and financial impact in terms of lower lost demand, decrease in inventory, lower working capital and financing costs.

How to apply

You may already know if you're a fit, but perhaps you're worried about some of the requirements We're looking for smart and passionate Engineers;

if you're the right candidate, we're flexible"
29-Apr-2022 T11:49,Data Engineer,Eastspring Investments (singapore) Limited,16 hours ago,,Full–time,"At Eastspring Investments, we understand that success comes from the talent and commitment of our people. We strive to build a business that you can shape, an inclusive workplace where everyone's ideas are valued and a culture where we can thrive together. Our people stay connected and tuned in to what's happening around us, keeping us ahead of the curve. While focused on the long-term, we look to the future to bring growth, development and benefit to everyone whose lives we touch.

Key Accountabilities

Main responsibilities:

Design and implement our Data and AI central data platform as well as related tools/systems for advanced business analytics and enterprise data governance

Manage data modeling design, writing, and optimizing ETL jobs

Participate in building and enhancing enterprise cloud data warehouse

Deliver and manage in-house and cloud-native data solutions to meet business requirements across firm-wide business units

Assist in creating and monitoring analytics dashboards, for different business functions

Ensure quality, integrity and accuracy of datasets through tracked, secured and auditable controls

Work with stakeholders to assist with data-related technical issues and support their data needs.

Follow and enforce best practices in software development and data engineering

Requirements:

Excellent coding skills with Python and SQL, and solid understanding of object-oriented analysis and design

Working knowledge of common algorithms and data structures, with strong analytical and problem-solving skills

Hands-on experience with Linux and shell scripting

Working experience with containerization (Docker/K8S) and task orchestration tools (Airflow/Luigi, etc.)

Experience with cloud service and tools (AWS/Azure/GCP), as well as cloud data warehouse platforms.

Experience with modern DevOps practices including version control, TDD, CI/CD, etc., for both code and configuration changes.

Basic understanding and experience with ML/AI concepts (e.g. deep learning, deep reinforcement learning, deep bayesian learning), workflows, and toolsets (Jupyter Notebook, etc.), and libraries (Numpy, Pandas, Scikit Learn, PyTorch, etc.) preferably in both cloud-native and desktop deployments.

Experience with traditional RDBMS based systems, including Data Lake, Data Warehouses and Marts, and more modern NoSQL and cloud-native big-data technology stacks such as document-oriented databases, Hadoop, columnar data files (e.g., Parquet), etc.

Familiar with REST APIs, service-oriented architectures (SOA) / microservices, virtualization, and serverless deployment architectures.

Demonstrated ability to understand, work with and deliver robust solutions in more than one programming language, framework, technology stack, runtime environment, etc..

QUALIFICATIONS / EXPERIENCE:

Degree level or higher in Computer Science or another quantitative field

1-5 year(s) technical experience showing increasing sophistication of solutions implemented, and ability to deliver

Fluency in both written and spoken English"
29-Apr-2022 T11:49,Data Engineer - APAC,Tamr,16 hours ago,,Full–time,"Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE
who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being
customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation
This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.
Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion
Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:49,"Data Engineer, Data Engineering",Bank of Singapore,17 hours ago,,Full–time,"At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programmes that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group’s global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today

Bank of Singapore opens doors to new opportunities.
Start your career with Bank of Singapore as a Data Engineer (6 months contract) in our Data Engineering team
Bank of Singapore opens doors to new opportunities. At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programs that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group's global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today
Bank of Singapore has defined an ambitious roadmap for accelerating its data excellence through the implementation of various bank-wide programs. As part of this roadmap, building the foundation of good data governance and management plays a key role in establishing trust in the data that we use.
General Description:
The incumbent will be based in the Data engineering team within the Data Hub team, supporting Bank of Singapore's data sourcing, curation, transformation, and data analytic services to accelerate their data initiatives. He/she will be responsible for expanding and optimizing BOS data pipelines as well as supporting data analytics needs.
Core activities :
• Be responsible for supporting data analytics and dashboarding with respects to the data availability / stability.
• Understanding the data and identifying any potential data issues as well as correcting issues arising from data.
• Define and implement data models suitable for dashboarding
• Power BI dashboarding support

Qualifications
Requirement :
General Knowledge & Experience
• Minimum of 6 years of working experience in data related functions
• Understanding of banking / financial services with exposure to data analytics preferred
Programming & Data
• Strong hands-on skills in SQL & PL/SQL
• Solid background in traditional structured database environments such as Teradata / Oracle
• Knowledge on Power BI dashboard functionality
• Strong skills in Power BI related technologies like power query and DAX
• Exposure to programming languages such as python
Communication & Soft Skills
• Curiosity & a real passion for understanding ""why?""
• Pragmatic ""can do"" approach to finding data based solutions to problems
• Good communication skills with ability to engage across business and technical audiences"
29-Apr-2022 T11:49,Data Engineer,ITCONNECTUS PTE. LTD.,16 hours ago,$5K–$13K a month,Full–time,"Immediate joiners

Requirements
• Technical Data Architect who is strong on Data Warehousing, Big Data, Data Analytics & Data Science for solutioning
• Strong Google Cloud Platform Data Components – BigQuery, BigTable, CloudSQL, Dataproc, Data Flow, Data Fusion, Etc
• Demonstrate extensive skills and success in the implementation of technology projects within a professional environment, with a particular focus on data engineering
• Experienced in delivering Medium to Big Enterprise Data Projects
• GCP Experience including and not limited to MPP systems, Database systems, ETL and ELT systems and Data Flow compute Good to have skills
• A high-level understanding of AI and ML technologies and desire to develop your skills in these areas of emerging technology.
• Should be able to advise the best of breed for the client solutions; Skills Needed
• The Data Engineer coaches the junior data engineering personnel position by bringing them up to speed and help them get better understanding of overall Data ecosystem.
• Prior experience developing, building and deploying on GCP
• Working on Solution deck, IP build, client meetings on requirement gathering"
29-Apr-2022 T11:49,Data Engineer,Asm Technology Singapore Pte Ltd,16 hours ago,,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform. We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets. You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).

Your Responsibilities

Manage and support on-premise and Cloud-based data lake and warehouse systems

Design, build, support and optimize new and existing data structure and ETL processes

Build scalable and efficient data pipelines & services to help analytics teams to process the data

Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results

Liaise with third party tool providers to understand and improve data workflow

Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms

Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification

Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines

At least 2 years' experience in data engineering, automation and integration is preferred

Strong programming and scripting skills in Python and other modern programming languages

Strong data management, schema design and SQL development skills

Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)

Self-motivated and proactive, willing to learn new things

Good communication skills and strong team player

What our preferred candidates have

Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions

Understand and experienced with Cloud platform, eg. Microsoft Azure, AWS, GCP

Business intelligence and reporting tools, eg. Power BI, Tableau, Qlik, etc

Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)

REST/Web API development and management

Knowledge in Statistical software is an advantage

Experience In building machine learning models is a plus"
29-Apr-2022 T11:49,"Principal Data Engineer, Data Engineering",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Principal Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you are responsible for helping to shape the direction of the data engineering domain in GovTech; working with other domain experts, the delivery teams, vendors, and partners to influence technology roadmaps and the adoption of suitable data solutions across government. In addition, you will teach, mentor, and proactively grow fellow data engineers within the organization.

You will be leading multiple teams of data engineers to drive impact at scale, by building whole-of-government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will architect, design and build next-generation central data infrastructure to galvanise digitalisation in the public sector. Taking a more integrated approach to lay the foundation infrastructure and driving pervasive adoption of advanced data technologies throughout the economy and society, you will architect solutions that enable public agencies and sectoral partners to better integrate, exploit and understand their data to improve operations and decision-making, and deliver high-quality digital services to citizens and businesses in Singapore.

Join us on this journey to make a difference for the nation and our future generations, if you are:
• Experienced leading data pipeline building and data wrangling and enjoys optimizing data systems and building them from the ground up as part of multiple cross-functional teams,
• Have strong project management and organizational skills, self-directed and comfortable supporting the data needs across multiple teams, systems and products,
• Excited by the prospect of optimizing or even re-designing central and agencies’ data architecture to support next generation of government products and data initiatives, and
• Recognized expert with proven technical leadership, experienced working in a agile team to build systems from architecture to deployment for cloud and on-prem infrastructure, and
• Have the desire to serve the public good through the use of technology.

What you will be working on:
• Take the driving seat and lead our data engineers in complex technical projects to:
• Expand and optimize our data and data pipeline architecture, as well as optimize data flow systems for cross functional teams in a secured and scalable manner.
• Assemble large, complex data sets that meet functional / non-functional business requirements, transforming data into formats that are easy to consume and analyse.
• Identify, design, and implement internal process improvements, e.g., automating manual processes, optimizing data delivery, re-designing infra for greater scalability.
• Build the infra required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and industry standard Infra-as-Code tools.
• Build analytics tools that utilize the data pipeline to provide actionable insights into key performance metrics and support our next gen of products and data initiatives.
• Work with stakeholders including the Executive, Product, Data and Design teams to support their data and data pipelining use cases, and ensure optimal and secured data delivery architecture is consistent across projects, products and platforms.
• Drive innovative initiatives that uplift data capability across the government sector:
• Develop a strategic sense of key organisations, software and hardware tools, methodologies, and best practices in the data engineering landscape.
• Be the bridge between engineering and product line management and translate market requirements into product definitions and architecture designs.
• Analyse and solve complex data engineering problems, and translate architecture designs into implementations that satisfy the market requirements.
• Educate the team on new technological advances and work alongside them in proof-of-concept research projects and subsequent agile product delivery.
• Provide expert perspective to help shape data-related strategy and initiatives, and be our technical ambassador by writing internal blogs or publishing white papers.

What we are looking for:
• Candidate with 8+ years of experience in a Lead Data Engineer role, who has attained a Bachelor’s degree in Computer Science or other relevant engineering degree.
• In-depth working knowledge across a variety of relational SQL and NoSQL databases, including efficient query authoring, performance optimisation and troubleshooting.
• Experience building and optimizing data, data pipelines, and architectures, with processes to support transformation, data structures, metadata, dependency and workload management.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for further improvement.
• Working knowledge of message queuing, stream processing, and scalable data stores.
• A successful history of processing and extracting value from large datasets from different sources and systems, with strong analytics skills related to working with unstructured data.
• You should also have experience using the following software/tools:
• Big data and stream-processing tools: Hadoop, Spark, Beam, Flink, Kafka, etc.
• Data pipelining and workflow management tools: Luigi, Airflow, etc
• Cloud computing environments and services: AWS, GCP, Azure
• DevSecOps tools: Git, Kubernetes, Docker, Terraform etc
• Programming languages: Python, Java, C++, Scala, etc

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours”"
29-Apr-2022 T11:49,Data Engineer,Thakral One,16 hours ago,$8.5K–$10.5K a month,Full–time,"• Work with Banking team to understand existing SAS code logic written by techno-functional users
• Develop the SAS logic in Big Data environment, using spark, scala and pyspark utilizing client Hadoop ecosystem of the inhouse Data Lake
• Perform unit testing and system integration testing
• Work closely with Business Analysts team to review the test results and obtain sign off
• Deploy the new code in the client Production environment
• Prepare necessary design/operations documentation for future usage
• Perform peers Code quality review and be gatekeeper for quality checks
• 5-8 years of application development experience in Spark, Spark SQL, Scala is a must
• Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc
• Technical proficiency on data mining techniques and performance optimization
• Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL)
• Experience on SAS will be beneficial but not mandatory
• Handling of reporting packages (Tableau, QlikView) is nice to have
• AWS experience is nice to have
• Degree in Computer Science or Engineering is a must
• Good problem diagnosis and creative problem-solving skills
• Passion to learn and master diverse new technologies in the open-source community
• Accuracy and attention to detail
• Team-working, Verbal and Written communication skills

Desired Skills and Experience
Tableau, PySpark, Scala, Big Data, Pipelines, Unit Testing, Hadoop, Application Development, MariaDB, Spark, Data Mining, SQL, Attention to Detail, Python, System Integration Testing, S3"
29-Apr-2022 T11:49,Data Engineer,POWER IT SERVICES,13 hours ago,,Full–time,"Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

· Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

· A successful history of manipulating, processing and extracting value from large datasets.

· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science Statistics, Informatics, Information Systems or another quantitative field.

They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:49,Staff Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,16 hours ago,$8K–$13K a month,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 5 years of experience in Data Engineering role and have good knowledge / working experience in:

• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.

• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.

• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.

• Functional programming languages, e.g. Scala.

• Virtualization and container environment such as Docker and Kubernetes.

•Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:49,Data Engineering Lead,Stemly Pte. Ltd.,16 hours ago,,Full–time,"We are looking for a Senior Data Engineer with a track record of building enterprise grade data platform for cloud products .

This is an exciting opportunity for a technologist to build an innovative SaaS product using state-of-the-art and modern cloud technology stack.

As a Data Engineering Lead, you will be working on all aspects of data, from platform and infra build out to pipeline engineering and writing tooling / services for augmenting and fronting the core platform.

The data platform interfaces with large systems to ingest data (batch, micro-batch) to create statistical / machine learning models in a distributed setting.

You will work closely with data scientists, product managers, legal, compliance and business stakeholders.

You will also evaluate new technologies / frameworks and contribute to various open source big data technologies. You are encouraged to think out of the box and have fun exploring the latest patterns and designs in the fields of software and data engineering.

Your responsibilities
• Architecting and implementing a robust data platform for Stemly's products. Design and develop extremely efficient, reliable and observable data pipelines using technologies such as Airflow, Kubernetes, ElasticSearch, Kafka / KubeMQ, Postgres, ClickHouse etc.
• Design and deliver the next-gen data lifecycle management suite of tools / frameworks , including ingestion and consumption on the top of the data lake to support real-time, API-based and serverless use-cases, along with batch (mini / micro)
• Build and expose metadata catalogue for the Data Lake for easy exploration, profiling as well as lineage requirements
• Work on the development of the data platform iteratively by building quick POCs and converting ideas into real products
• Make data discoverable and easy to use for Data Scientists and Analysts. Enable Data Science teams to test and productize various ML models
• Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.

Support your colleagues by reviewing code and designs
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner

About you
• 8+ years of professional experience in software development in Python and Pandas
• Extensive experience with object-oriented design, coding and testing patterns, including experience with engineering software platforms and data infrastructures
• Must have sound understanding for Big Data components & administration fundamentals
• Hands-on in building a complete data platform using various open source technologies. Must have the knowledge to build metadata, lineage, observability and discoverability for data platform
• Experienced in DevOps best practices like CI / CD, containerization, blue-green deployments, secrets management etc. in the Data ecosystem
• Expert in the principles of distributed computing and data modelling
• Expert in building optimized SQL queries
• Conversant with the latest developments in the areas of Machine Learning models and efficiently supporting the data scientists is a plus
• Experience with Airflow, ClickHouse, Postgres, Kubernetes, GraphDB is a big plus
• Familiarity with Google Cloud Platform (e.g. GCS, Cloud Composer, BigQuery) is a plus
• Outstanding communication and interpersonal skills

About us

We are scientists and engineers with a strong passion for cutting-edge technology and decision science. Our mission is to enable enterprises to reduce time to decision with state-of-the-art forecasting and optimization.

Our algorithms are based on automatic machine learning techniques and allow for the discovery of optimal forecasting models.

We unlock sizable business and financial impact in terms of lower lost demand, decrease in inventory, lower working capital and financing costs.

How to apply

You may already know if you're a fit, but perhaps you're worried about some of the requirements We're looking for smart and passionate Engineers;

if you're the right candidate, we're flexible"
29-Apr-2022 T11:49,Data Engineer,Eastspring Investments (singapore) Limited,16 hours ago,,Full–time,"At Eastspring Investments, we understand that success comes from the talent and commitment of our people. We strive to build a business that you can shape, an inclusive workplace where everyone's ideas are valued and a culture where we can thrive together. Our people stay connected and tuned in to what's happening around us, keeping us ahead of the curve. While focused on the long-term, we look to the future to bring growth, development and benefit to everyone whose lives we touch.

Key Accountabilities

Main responsibilities:

Design and implement our Data and AI central data platform as well as related tools/systems for advanced business analytics and enterprise data governance

Manage data modeling design, writing, and optimizing ETL jobs

Participate in building and enhancing enterprise cloud data warehouse

Deliver and manage in-house and cloud-native data solutions to meet business requirements across firm-wide business units

Assist in creating and monitoring analytics dashboards, for different business functions

Ensure quality, integrity and accuracy of datasets through tracked, secured and auditable controls

Work with stakeholders to assist with data-related technical issues and support their data needs.

Follow and enforce best practices in software development and data engineering

Requirements:

Excellent coding skills with Python and SQL, and solid understanding of object-oriented analysis and design

Working knowledge of common algorithms and data structures, with strong analytical and problem-solving skills

Hands-on experience with Linux and shell scripting

Working experience with containerization (Docker/K8S) and task orchestration tools (Airflow/Luigi, etc.)

Experience with cloud service and tools (AWS/Azure/GCP), as well as cloud data warehouse platforms.

Experience with modern DevOps practices including version control, TDD, CI/CD, etc., for both code and configuration changes.

Basic understanding and experience with ML/AI concepts (e.g. deep learning, deep reinforcement learning, deep bayesian learning), workflows, and toolsets (Jupyter Notebook, etc.), and libraries (Numpy, Pandas, Scikit Learn, PyTorch, etc.) preferably in both cloud-native and desktop deployments.

Experience with traditional RDBMS based systems, including Data Lake, Data Warehouses and Marts, and more modern NoSQL and cloud-native big-data technology stacks such as document-oriented databases, Hadoop, columnar data files (e.g., Parquet), etc.

Familiar with REST APIs, service-oriented architectures (SOA) / microservices, virtualization, and serverless deployment architectures.

Demonstrated ability to understand, work with and deliver robust solutions in more than one programming language, framework, technology stack, runtime environment, etc..

QUALIFICATIONS / EXPERIENCE:

Degree level or higher in Computer Science or another quantitative field

1-5 year(s) technical experience showing increasing sophistication of solutions implemented, and ability to deliver

Fluency in both written and spoken English"
29-Apr-2022 T11:49,Data Engineer - APAC,Tamr,16 hours ago,,Full–time,"Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE
who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being
customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation
This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.
Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion
Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:49,"Data Engineer, Data Engineering",Bank of Singapore,17 hours ago,,Full–time,"At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programmes that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group’s global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today

Bank of Singapore opens doors to new opportunities.
Start your career with Bank of Singapore as a Data Engineer (6 months contract) in our Data Engineering team
Bank of Singapore opens doors to new opportunities. At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programs that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group's global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today
Bank of Singapore has defined an ambitious roadmap for accelerating its data excellence through the implementation of various bank-wide programs. As part of this roadmap, building the foundation of good data governance and management plays a key role in establishing trust in the data that we use.
General Description:
The incumbent will be based in the Data engineering team within the Data Hub team, supporting Bank of Singapore's data sourcing, curation, transformation, and data analytic services to accelerate their data initiatives. He/she will be responsible for expanding and optimizing BOS data pipelines as well as supporting data analytics needs.
Core activities :
• Be responsible for supporting data analytics and dashboarding with respects to the data availability / stability.
• Understanding the data and identifying any potential data issues as well as correcting issues arising from data.
• Define and implement data models suitable for dashboarding
• Power BI dashboarding support

Qualifications
Requirement :
General Knowledge & Experience
• Minimum of 6 years of working experience in data related functions
• Understanding of banking / financial services with exposure to data analytics preferred
Programming & Data
• Strong hands-on skills in SQL & PL/SQL
• Solid background in traditional structured database environments such as Teradata / Oracle
• Knowledge on Power BI dashboard functionality
• Strong skills in Power BI related technologies like power query and DAX
• Exposure to programming languages such as python
Communication & Soft Skills
• Curiosity & a real passion for understanding ""why?""
• Pragmatic ""can do"" approach to finding data based solutions to problems
• Good communication skills with ability to engage across business and technical audiences"
29-Apr-2022 T11:49,Senior Data Engineer,Toptal,4 hours ago,,Full–time,"About The Job

Toptal developers work with speed and efficiency to deliver the highest quality of work. We are looking for someone who is passionate about their client’s business, and ready to work on exciting projects with Fortune 500 companies and Silicon Valley startups, with great rates and zero hassles. If you are looking for a place to advance your career, enhance your skill set, and build connections around the globe, Toptal is right for you.

About Toptal

Toptal is an exclusive network of top freelancers from around the world. Fortune 500 companies and Silicon Valley startups hire Toptal for their most important projects. Toptal is one of the fastest-growing fully remote networks and empowers freelance software developers, designers, finance experts, product managers, and project managers worldwide to grow and excel in their freelance careers.

Toptal clients vary in sizes and industries, from enterprise organizations and big tech companies to Silicon Valley startups and renowned universities. Once you enter the network, our matchers will contact you with project opportunities that fit your expertise and preferences. We have experts in over 120 countries who get to work remotely on projects that meet their career ambitions.

About The Role

As a Data Engineer, your main goal is to be one step ahead of data scientists and analysts. You will support them by providing infrastructure and tools they can use to deliver end-to-end solutions to business problems that can be developed rapidly and maintained easily. This is more than building and maintaining ETL pipelines. We need innovation, creativity, and solutions that will have a significant impact on the client’s velocity.

Requirements
• 3+ years of professional experience in software development
• Working experience with Python and Pandas.
• Familiarity with the basic principles of distributed computing and data modeling.
• Extensive experience with object-oriented design and coding and testing patterns, including experience with engineering software platforms and data infrastructures.
• Working experience with Airflow and Luigi is a big plus.
• Working experience with Scala is a plus.
• Familiarity with Google Cloud Platform (e.g. GCS and BigQuery) is a plus.
• Working experience with Dimensional Modeling and Rails is a plus.
• Outstanding communication and interpersonal skills.
• Full-time availability is a strong advantage

If you’re interested in pursuing an engaging career working on full-time freelance jobs for exclusive clients, take the next step by clicking apply and filling out the short form to get started.

#RemoteJobDataEngineering"
29-Apr-2022 T11:49,Data Engineer,Tech Mahindra Limited (singapore Branch),8 hours ago,,Full–time,"What to expect:
Manage & develop data warehouse/data lake and pipeline plans.
Develop and uphold best practices with respect to documentation & data protocols
Design, build and launch new data models in production.
To also develop new data extraction, transformation and loading processes in production.

Work in a cross-functional team, Interfacing with engineers, product managers and product designers to understand data needs to define and review technical specifications.
Build machine learning/reasoning models at scale.
How To Succeed:
Degree in Computer Science, Information Systems, Computer Engineering, Mathematics, or related disciplines.

At least 3 years of experience in data engineering, with experience in Kotlin/Scala/ Python, and experiences in Amazon Web Services stack, such as ECS, Kinesis, EMR, and DynamoDB.
Experienced in building production-grade models using machine learning frameworks such as Tensorflow, Scikit-learn, PySpark and/or others is required.
Experienced in custom ETL & ELT & Streaming big data design, implementation & maintenance and data warehouse/data lake space.
Experienced/willing to use Infrastructure-as-code for deploying pipelines.
Hands-on and deep experience with schema design and dimensional data modelling.
Ability to write efficient SQL statements.
Ability to analyse data to identify deliverables, gaps and inconsistencies.
Excellent communication skills including the ability to identify and communicate data-driven insights.
Experience in using Hadoop, Spark, HBase, Hive and Pig, is a plus.
Experience in software engineering, with experience in Kotlin or Java, and Spring Boot is a plus"
29-Apr-2022 T11:49,Data Engineer (Cyber security/ Linux/ Big Data/ East),TRUST RECRUIT PTE. LTD.,1 hour ago,,Full–time,"• Leading company specialised in Big Data & Cyber Security industry
• Good learning opportunity & project exposure
• No exp is welcomed
• Learn / Know Programming Language
Responsibilities:
• Study business and application specific requirements, review corporate technology to design and implement the big data architecture and solution to meet business requirements.
• Provide big data analysis consultancy and advisory to customers in the areas of cyber security, application monitoring, networking monitoring, infrastructure monitoring or business analytics.
• Conduct proof of concept/value on big data analytics related solution
Requirements:
• Knowledge in networking, windows servers and Linux servers
• Knowledge in Cybersecurity related solutions
• Knowledge in programming languages

HOW TO APPLY: Interested applicants, kindly send your resume in MS WORD format to ref17@trustrecruit.com.sg or please click on “Apply Now” and provide the below details in your resume.
We regret only shortlisted candidates will be notified.
Important Note: Trust Recruit Pte Ltd is committed to safeguarding your personal data in accordance with the Personal Data Protection Act (PDPA).
Please read our privacy statement on our corporate website www.trustrecruit.com.sg.
Trust Recruit Pte Ltd
EA License No: 19C9950
EA Personnel: VIvian Chung Fhui Tiin
EA Personnel Reg No: R22106436"
29-Apr-2022 T11:49,Data Engineer,Asm Technology Singapore Pte Ltd,16 hours ago,,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform

We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets

You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).
Your Responsibilities
Manage and support on-premise and Cloud-based data lake and warehouse systems
Design, build, support and optimize new and existing data structure and ETL processes
Build scalable and efficient data pipelines & services to help analytics teams to process the data
Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results
Liaise with third party tool providers to understand and improve data workflow
Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms
Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification
Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines
At least 2 years' experience in data engineering, automation and integration is preferred
Strong programming and scripting skills in Python and other modern programming languages
Strong data management, schema design and SQL development skills
Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)
Self-motivated and proactive, willing to learn new things
Good communication skills and strong team player

What our preferred candidates have
Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions
Understand and experienced with Cloud platform, eg

Microsoft Azure, AWS, GCP
Business intelligence and reporting tools, eg

Power BI, Tableau, Qlik, etc
Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)
REST/Web API development and management
Knowledge in Statistical software is an advantage
Experience In building machine learning models is a plus"
29-Apr-2022 T11:49,"Data Engineer (5 days, Orchard, $3500-4000)",MACHSPEED HUMAN RESOURCES PTE. LTD.,9 hours ago,,Full–time,"Leading System Integrator company providing IT solutions for network, IT infrastructure, analytics, cyber security and managed services.

Job Responsibilities
• Design, engineer, configure and administer BI project based on given functional and technical requirements
• Collaborate with pre-sales, project and relevant internal teams to deliver the optimal technical solution to client's business problems
• Develop real-time and batch data ingesting and processing pipelines to be used for analysis, machine learning, dashboards, alerts and visualizations
• Review and monitor ETL tasks and performance
• Support testing and deployment
• Provide recommendations and implementation changes to optimize in the customer environment.
• Write and develop custom scripts as needed

Requirements
• Minimum Degree/Diploma in Computer Science, Engineering or IT with at least 2 years of relevant experience
• Hands-on experience in scripting/programming
• Possess knowledge in Networking and Servers(Windows and Linux)
• Possess CEH/ECSA/CISSP/ECSA/CompTIA Security+ certification will be an added advantage
• Working hours: 9am - 6pm, Mondays to Fridays
• Salary range: $3500 - 4000

We thank you for your interest in this position, applicants with relevant experience please send Microsoft word resume with expected salary to

apply@machspeed.com.sg

for faster processing, alternatively, please visit https://www.facebook.com/MachspeedHR for more jobs available, thank you.

You may also call +6563362530 (Look for BingCheng) to find out more

Thank you very much.

Agency License No. 12C6200

EA Registration No: R1437671"
29-Apr-2022 T11:49,Senior Data Engineer,GRASSHOPPER PTE. LTD.,16 hours ago,,Full–time,"OVERVIEW:

We seek a Senior Data Engineer to support our high-frequency trading and quantitative research. At Grasshopper we recognize that effective research and trading strategies are built upon a solid foundation in data collection, storage and service. Grasshopper trades at a number of exchanges globally, and our success depends on our ability to scale by leveraging the potential of data.

You will, in close collaboration with stakeholders, design and develop highly scalable and fault-tolerant data pipelines in a cloud environment focusing on scalability and performance, as well as the next generation of tools that empower users to generate insights on the data they work with.

RESPONSIBILITIES:

• Work with a team of data engineers across locations, managing project schedules.

• Expand the imagination of data stakeholders on what is possible and educate them in leveraging data they already have.

• Work proactively with traders, researchers and various members outside the data team to meet their needs for timely, accurate and complete data sets.

• Build highly scalable data pipelines to process and analyse billions of messages in real time.

• Set strong technical/architectural/cultural foundations, advocate good design in software development as well as data engineering, and lead by example.

SKILLS & KNOWLEDGE REQUIREMENTS:

• Strong technical leadership qualities, good at working with both people and with code.

• Extensive experience with data modelling and designing/supporting both streaming and batch ETL pipelines.

• Extensive experience in SQL and databases.

• Domain knowledge in finance, especially in buy-side trading, is not required but good to have.

• Proficiency in at least two mainstream programming languages (preferably Python, Java, C++).

• Proficiency in a programming language of a non-OOP paradigm (e.g. functional/logic programming).

• Experience with FP libraries like scalaz/cats/ZIO is a plus.

• Demonstrates good judgment in software design balancing design principles with pragmatics, with an emphasis on simplicity. See Rich Hickey’s Simple Made Easy talk: [ Link removed ]

• Good to have experience in Google BigQuery.

• Experience working in some cloud services platform (e.g. Google Cloud, AWS, Azure).

• Experience with messaging middleware such as Solace or Kafka.

• Experience with machine learning is good to have.

WHO WE ARE LOOKING FOR:

We are looking for team-players who are creative in their approach to problem solving. They take the initiative to explore different ways to resolve an issue, and systematically find the most efficient and effective way to do it. They are adept at expressing their ideas and solutions into clean and maintainable code.

THE ENVIRONMENT WE OFFER:

As a growing firm with a tightly knit team, we respect and listen to all our employees. You will get the chance to make an impact by having your voice heard by everyone, including the management.

Our employees enjoy a high level of autonomy at work. We focus on substance, not form - as long as you can perform, you will be recognized and rewarded. We are also dedicated to supporting our staff and ensuring they develop holistically to maximize their potential in the long-term.

We also provide flexible working arrangement as required and a casual and fun work environment to boot!

Recommended Skills

Amazon Web Services

Apache Kafka

C++ (Programming Language)

Cloud Computing

Constraint Logic Programming

Creativity"
29-Apr-2022 T11:49,Data Engineer,Supernova,16 hours ago,,Full–time,"About Supernova

We are the fastest growing lifestyle & beauty omni-channel ecommerce company. We build iconic beauty and lifestyle brands for a new generation of women worldwide. Independently owned with a global marketing reach, we connect with over 15 million women on social media every week. Headquartered in Singapore, we currently have offices in Berlin, London and LA. Our brands include Sand & Sky and Coco & Eve.

About Supernova's Internal Engine

Together we create Superstars! Our three company pillars are guided by this motto: Our product and brands are of the highest quality and within the top five in their category; our customers are the focus of our actions and their lives improve with our products; our talent and employees grow and develop within Supernova.

This means that investing in hiring, developing and retaining the brightest minds in the world is a top priority. Living our core values of #MakeAnImpact, #TalkAboutIt, #SeekNewOpportunities, #DelightAndSurprise and #MoveForwardTogether creates a unique culture across five different time zones and allows us to realise the enormous potential of this exciting industry. If you're an ambitious, smart, natural collaborator who likes taking risks, influencing, and innovating in a challenging hyper-growth environment, we'd love to talk to you about joining our team.

The role

This role provides tremendous opportunity for the right candidate: to capture the energy and rapid pace of a fast growing start up, while being responsible for a sizeable and growing part of our global footprint.

As Supernova's Data Engineer, you are responsible for the design and implementation of modern, scalable data-centric services and solutions.

You have strong data infrastructure and architecture skills, with a proven track record of handling high data volumes to serve batch as well as streaming needs.

This is a remote position, working as part of a strong and growing Data Team.

Responsibilities:
• Define and build data pipelines from a wide variety of data sources that will enable faster, better, data-informed decision-making within the business.
• Develop custom integrations via REST/SOAP/event-driven architecture.
• Work with stakeholders including the Brand, Product, Ops and Marketing teams to assist with data-related issues and support their data infrastructure needs.
• Build tools for effective maintenance and monitoring of the data infrastructure.
• Contribute to an ongoing effort to improve data reliability, efficiency and quality.
• Performance tuning of data pipelines jobs for optimal end-user experience.

Your Profile:
• Minimum Bachelor's degree in a quantitative field is required e.g. Computer Science, Statistics, Mathematics
• 5+ years of experience in data engineering roles using Python / Java.
• Experience in building the solution architecture, provisioning of infrastructure and delivering data-centric services and applications either in GCP or AWS.
• Strong experience in Event Driven Architecture using Kafka or Pub/Sub etc.
• Extensive knowledge in ELT processing and workflow orchestration using Airflow/ Nifi or similar tools.
• Experience with development ecosystems such as Git, Docker, Jenkins and CI/CD.
• Preferably with a background in Startup / Ecommerce.
• Ability to plan your work and commit to deadlines.
• Ability to work with cross-functional teams to deliver quality results.

This position is available immediately. All applications will be treated confidentially.

Please note that we can only contact successful applicants.

Supernova is an equal opportunity employer. Our values define the working culture and environment we strive to create – fair, diverse, respectful and welcoming of different views. We embrace a culture reflecting a variety of perspectives, insights and backgrounds to drive innovation. We build talented and diverse teams to drive business results and encourage our people to develop to their full potential. We promote harmonious working relationships and encourage a learning and development culture. We welcome applications from talented people coming from all cultures, countries, races, genders, sexual orientations, disabilities, beliefs and generations and are committed to providing a working environment free from harassment, discrimination and retaliation.

See our Applicant Data Protection policy for information on how we collect, use and process your personal data: http://www.supernova.xyz/applicant-data-protection-notice"
29-Apr-2022 T11:49,Data Engineer 3 - AWS & Python (Contractual),The Economist,16 hours ago,,Contractor,"What will you experience

At Economist Intelligence Unit (EIU) we believe having the right work-life balance is super important; striking balance between your personal and professional life is critical to wellbeing and happiness. We offer flexible working and have recently shifted to a 'remote first' working policy with a minimum expectation of coming to the office two days a month, however you can come in more often if you wish to.

How you will contribute:
• Build data pipelines: Architecting, creating and maintaining data pipelines and ETL processes in AWS via Python, Glue and Lambda
• Support and Transition: Support and optimise our current desktop data tool set and Excel analysis pipeline to a transformative Cloud scale Big Data Architecture environment.
• Work in an agile environment: within a collaborative agile product team using Kanban
• Collaborate across departments: Work in close relationship with data science teams and with business (economists/data) analysts in refining their data requirements for various initiatives and data consumption requirements.
• Educate and train: Required to train colleagues such as data scientists, analysts, and stakeholders in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases.
• Participate in ensuring compliance and governance during data use: To ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives.
• Become a data and analytics evangelist: This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals.

To succeed in this role it would be an advantage if you possess:
• Experience with programing in Python, and Lambda functions
• Knowledge of building bespoke ETL solutions, and extracting data using Data APIs
• MS SQL Server (data modelling, T-SQL, and SSIS) for managing business data and reporting
• Prior experience in design and developing microservice architecture
• Ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management.
• A combination of IT skills, data governance skills, analytics skills and economics knowledge
• An advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (postgraduation diploma or related) or a related quantitative field or equivalent work experience.
• Experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms"
29-Apr-2022 T11:49,Senior Spark/Hadoop Engineer,Ridik Pte. Ltd.,16 hours ago,,Full–time,"Requirements
• Must have application development experience in Spark, Spark SQL, Scala is a must
• Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop / HDFS, S3, Colibra, Claudera Workbench, etc.
• Technical proficiency on data mining techniques and performance optimization
• Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL)
• Experience on SAS will be beneficial but not mandatory
• Handling of reporting packages (Tableau, QlikView) is nice to have
• AWS experience is nice to have
• Degree in Computer Science or Engineering is a must
• Good problem diagnosis and creative problem-solving skills
• Passion to learn and master diverse new technologies in the open-source community
• Accuracy and attention to detail
• Team-working, Verbal and Written communication skills"
29-Apr-2022 T11:49,Senior Data Engineer,Kkr Singapore Pte. Ltd.,8 hours ago,,Full–time,"Position Summary

We are looking for a savvy Data Engineer to be based in Hong Kong / Singapore, to join our growing team of data and analytics experts.

The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.
The ideal candidate is an experienced cloud-based services, data architecture, data engineering, data pipeline development (ETL), and analytical tools. We are seeking an engineer who enjoys optimizing data systems and building enterprise scale data services.

The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.
Skill / Experience Required
Bachelor's Degree in Computer Science/Engineering or a related discipline.
10+ years Development experience
Experience in Python and related open source modules
Experience in Python development including Web application frameworks such as Flask / FAST API
Experience working with RESTful API Services
Strong database skills with a thorough understanding of relational database and understanding of Object Oriented databases
Exposure with the AWS Stack / RDS / is preferred
Knowledge of open source solutions and trending technologies
Good communication and written skills
Ability to be self-sufficient and proactive individual contributor
Exposure to Private/Public Markets

Desirable
Understanding of Object Oriented Programming and Design Patterns
Knowledge of web standards, security, accessibility, browser compatibility
Knowledge of JavaScript, HTML5 and awareness of frameworks such as
Experience in a Business Intelligence tool e.g. Tableau and Dremio
Exposure to ML / AI and python libraries (pandas, NumPy, SciPy) and concepts"
29-Apr-2022 T11:49,Data Engineer,Percept Solutions Pte. Ltd.,16 hours ago,,Full–time,"We are looking for an IT professional who has gathered some years of working experience in Business Intelligence, Data Analytics, Report Development, SQL and DWH design.Role Responsibilities:Use SQL and/or programming language and tools to perform tasks such as data discovery, data QA, and data preparation based on requirements provided by the business.Build data assets in AWS and aligning the data architecture based on business requirements and working towards a common data model.Support the Business Analyst in translating business requirements to technical requirementsSupport Data Analytics Projects & Initiatives, like Campaign Automation with AIConstantly searching for optimizations, improvements and innovations (e.g. predictive AI)Skills and Competencies:Data Engineering & Data Analytics Expert, DWH Design, SQL, ETL, AWS, Tableau, QliSense, Power BIIndependent and reliable working style as well as enjoying team collaborationGood understanding about Agile software development methods and IT security is a plusBusiness Knowledge of Captive or Financial Services, Banking is a big plusTo apply please click the Apply button or send us your updated profile to [HIDDEN TEXT]EA Licence No.:18S9405 / EA Reg. No.:R1330864Percept Solutions is undergoing a growth phase and are on the lookout for talent. Applicants are encouraged to follow Percept Solutions on LinkedIn @ https://www.linkedin.com/company/percept-solutions/ to stay up to date on our upcoming roles and events"
29-Apr-2022 T11:49,Data Engineer,Azendian Solutions Pte. Ltd.,17 hours ago,,Full–time,"Responsibilities:
• Responsible for data analysis and generate reports for estate sub-system performance
• Develop data set processes
• Develop analytics programs, machine learning algorithms and statistical method, to make raw data useful for smart estate application
• Identify ways to improve data reliability, efficiency and quality
• Conduct research and explore for innovative solutions
• Monitor compliance to project requirement, performance standards and specifications
• Perform overall quality control of the work and report regularly on project status
• Cooperate and communicate effectively with product development and operation team for implementation

Requirements:
• Relevant Bachelor's degree or equivalent in electrical/electronic engineering or computing
• 3+ years' related field and project planning experience
• Excellent data science knowledge is required
• Programming skill in Python and Java experience is required
• Strong analytic skill is required
• Knowledge in data management is required
• Knowledge in ETL is preferred
• Must have strong written and verbal communication skills
• Able to work comfortably and independently in a fast-paced environment"
29-Apr-2022 T11:49,Data Engineering Lead (Remote Possible),Glints,15 hours ago,,Full–time,"At Glints, we are building the #1 tech-enabled recruitment and career discovery platform in Southeast Asia that helps people and organisations realise their human potential by joining great organisations, learning the right skills and building great teams.

We’re looking for a Data Engineering Lead to join our Data Engineer team, to
improve Data Governance and promote Data Democratization in the company
What You’ll Be Doing

• Identifying organizational needs for Data Engineering and developing a roadmap within the organization
• To promote ownership of data across the organization
• To improve the quality and integrity of Data in Glints
• To improve discoverability of data and encourage usage of data
• To develop security around data usage
• Conveying organization need to Data Engineering Team for implementation of the solution
• Mentor and groom members of the team to be technical leaders; Hire and expand the team if needed

Why You Should Join Us

• Opportunity to determine Data Engineering Roadmap and realise it with a team of enthusiastic engineers

Who We Are Looking For

• Proficient with Python and Scrum Framework
• Experience with leading Data Engineering Team in improving Data Warehouse
• Managerial or Leadership experience in mentoring and grooming Data Engineering Team
• Able to understand the business equation of the companies and identify levers which the Data Engineering Team can pull

Let’s Realise Human Potential.

We have impacted many lives since we were founded in 2013, but there’s still plenty to be done. If you’re ready to grow and make an impact, you’ve come to the right place.

What is Glints?

Glints is an online talent recruitment and career discovery platform with the enduring purpose of contributing to a world where people and organisations can realize their human potential.

Our tech-enabled approach to recruitment and career discovery connects more than 1.5 million candidates to their next dream opportunities, has supported more than 30,000 companies in finding top talents, doubled recruiter efficiency, and has raised more than US$30M+ from best venture investors in Asia to date.

Our agility and firm hold on our core purpose and values have allowed us to remain resilient and thrive through tumultuous times, and we are proud to be recognised by LinkedIn as one of the Top 10 Startups in Singapore in 2020.

Who We Are

At Glints, personal and professional growth are just as important as business growth. That's why we created the Glints Culture Code: #RIIBCOH. It defines our values, guides our decisions and actions, and is what makes us special.

Relentlessly Resourceful: Whatever it takes, just make it happen (ethically)
Integrity: Have courage, be guided by the truth, don’t be afraid
Impact: Missionaries, not mercenaries
Beginners’ Mindset: Stay humble, don’t be attached to ego
Customer Obsessed: Customers First
Ownership: Care intensely about the mission and take responsibility
High Standards: Dream big and deliver epic outcomes fast

Where We Work

Glints operates in multiple locations across Greater Southeast Asia, including Singapore, Indonesia, Vietnam, Taiwan and Malaysia.

Learn more about Glints and our culture at bit.ly/glintsculture,
Or check out our Careers Page at https://glints.com/careers"
29-Apr-2022 T11:49,Staff Data Engineer,Techgig,16 hours ago,,Full–time,"About Job
CTC Undisclosed Job Location Singapore Experience 8 - 11 yrs
Description
Job Description and Responsibilities
We are developing and executing a shared strategic vision for Loyalty platforms and products that enable Visa to be the world-leading data-driven payments company. As a Staff Data Engineer, you will work closely with a world-class team of Development and Test Engineers.
This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide excellent user experiences. You will be an integral part of the Loyalty development team focusing on design and building of software solutions that leverage data to solve business problems.
Strategic thinker and good business acumen to orient data engineering to the business needs of internal clients.
Lead development and oversee programming testing functions to ensure that projects are securely delivered while fulfilling expectations.
Identify opportunities for further enhancements and refinements to standards and processes.
Lead by example: demonstrating accountability, mentoring junior team members, and by contributing to departmental procedures, best practices, and standards.
The candidate will be extensively involved in hands-on activities including POCs, design, documentation, coding, unit testing, implementation, and documentation of solutions for new development, system enhancements, and production support.
Candidate must be flexible and willing to switch tasks based on team's needs.
Development and programming functions to ensure that projects are delivered on time and within budget with good code quality.
Work with architects, systems analysts, project managers, QA, and other developers to successfully implement business requirements while applying the latest available tools and technology.
Responsible for the architecture, design, development, implementation of data-based software applications. This includes working with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations.
Interact with business units to define requirements/modifications and use case to utilize in designing appropriate solutions.
Provide recommendation on scope and scale of effort required to develop solution.
Design, develop, document, and implement new programs and subprograms, as well as enhancements, modifications, and corrections to existing software - Develop testing and debugging routines.
Create documentation and procedures for installation and maintenance.
Build and maintain relationship with global and virtual teams and third parties on software development or support issues.
Identify opportunities for further enhancements and refinements to standards, best practices, and development methodologies.
Work directly with Architects, System Analysts, Dev leads and QA team leads to manage the technical aspects of a development pipeline.
This is a hybrid position. Hybrid employees can alternate time between both home and office. Employees in hybrid roles are expected to work from the office three days a week, Monday (Sunday in some countries where Sunday is the start of the week) and Wednesdays. Wednesdays are designated as in-office collaboration days. The third day in the office will be decided based on team needs and determined in partnership with senior leadership.
Qualifications
Preferred Qualifications
Masters Degree in Computer Science or related field with 6 years of relevant experience or BS Degree with 8 years of experience.
Previous exposure to financial services is a plus, but not required.
Extensive experience in architecting and developing real-time applications that are fault-tolerant, scalable and can handle high volumes.
Experience in best practices for API development and design patterns.
Quick learner: self-starter, detailed and thorough.
Experience in all phases of software development life cycle including project management, functional requirements definition, technical design, development, testing, quality assurance, system certification, systems implementation, and system validation.
Consistently able to assess and evaluate problems in a production environment and manage risk to the service when recommending change.
Strong secure coding practices.
Good Knowledge on Hadoop framework and related Big Data Technologies (HDFS, Map Reduce, Spark, HBase, Kafka).
Strong knowledge in Java or Scala or Python.
Strong knowledge of database concepts, systems architecture, and data structures is a must.
Java/J2EE/Angular, Spring Cloud, Microservices and strong knowledge on API development is desired.
Experience working in an Agile and Test-Driven Development environment.
Process oriented with strong analytical and problem-solving skills.
Work independently and mentor others in the team and with minimal supervision.
Ability to juggle multiple projects and change direction mid-course based on business drivers.
Ability to work independently in a high throughput environment.
Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style"
29-Apr-2022 T11:49,"Facilities Engineer (Data Centre, office hours)",Randstad Singapore,7 hours ago,,Full–time,"about the job

This is a company whose expertise has made them the leader in their field is seeking a high-calibre Facilities Engineer in Singapore to support the data centre operations in Singapore.

The Facilities Engineer will play a crucial role in the company’s continued development and success in Singapore. Your key purpose will be to oversee the execution of the local data operations and manage the engineering operations and core infrastructure. You will be responsible for the incident reporting, vendor management, change management, documentation of operational procedures as well as the management of the day to day operations. This is a rare opportunity for a hands-on individual to be part of a growing organization that provides a stimulation work environment.

skills and experience required.

To be successful in the role, you would have:
• Ideally have at least 5 years experience in relevant Facilities Management within Data Center Operations
• Relevant qualifications in Engineering or Mechanical Engineering or Electrical Engineering are preferred.
• Have an understanding of construction, commissioning and operation of mission critical systems.
• Able to work shift hours is a must
• Great communication skills are necessary this is a client facing role.
• Ability to work independently and make logical decisions

how to apply

To apply online, please click on the ‘apply’ function below.

Please indicate your availability, expected salary, and reason for leaving your current job in your CV.

EA: 94C3609 / R1767516

Applicants must be fully vaccinated or have a valid exemption in accordance with MOM’s regulations to allow them to enter the workplace. Applicants may be required to share verifiable COVID-19 vaccination documents or proof of a valid exemption at the point of offer. Randstad Pte. Limited and/or the Client reserves the right to withdraw an offer if the applicant fails to provide verifiable COVID-19 vaccination and/or proof of exemption documents.

skills

no additional skills required

qualifications

no additional qualifications required

education

Bachelor Degree"
29-Apr-2022 T11:49,Data Engineer,Huxley,16 hours ago,,Full–time,"We are currently looking to fill 2 Data Engineer positions focusing on data modelling and risk related projects:

Data Engineer should have the following:

3+ year of experience developing machine models

Proficient in either Python, Java and/or Scala

Master's Degree with working experience OR Phd

Understanding of deep learning is a plus

Data Engineer will be responsible for:

Building Models reliable on machine learning, graph mining and other data driven technologies. Identify potential threats in KYC, Payments, Credit, Reward etc

Maintain the Models to meet business expectations.

Work on data that analyses customer feedback and satisfaction analysis by leveraging on machine learning techniques.

Collaborate effectively with various stakeholders

Please note that this opportunity is open for Singaporeans only."
29-Apr-2022 T11:49,Principal Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,9 hours ago,,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 10-12 years of experience in Data Engineering role and have good knowledge / working experience in:
• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.
• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.
• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.
• Functional programming languages, e.g. Scala.
• Virtualization and container environment such as Docker and Kubernetes.
• Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:49,Data Engineer,"nSearch Global Pte Ltd, EA Licence No: 10C3636",16 hours ago,,Full–time,"Our client, one of Asia-Pacific’s leading organizations is looking for:
Data Engineer

Responsibilities:

Perform data extraction, cleaning, transformation, and flow. Web scraping may be also a part of the work scope in data extraction Design, build, launch and maintain efficient and reliable large-scale batch and real-time data pipelines with data processing frameworks Collaborate with Project Manager, Visualisation Developer, Data Scientist, UX Designers and Data Analyst to build scalable data-driven products Work in an Agile Environment that practices Continuous Integration and Delivery Support and advise best practices in data management standards, policies and procedures

Requirements:

Interest in being the bridge between engineering and analytics Knowledge about system design, data structure and algorithms Familiar with data modelling, data access, and data storage infrastructure like Data Mart, Data Lake, and Data Warehouse Proficient in general data cleaning and transformation (e.g. SQL, pandas, R, etc) Comfortable in at least one scripting language (eg. Python) Familiar with rest API and web requests/protocols in general Proficient with processing data from open-standard file format rchange format (e.g. XML, JSON) Good in building ETL pipeline (eg.

Talend Data Integration, Informatica Power

Center, IBM DataStage, SAS Data Integration) Good in database design and various databases (e.g. MS SQL Server, S, IBM DB2, Postgres etc) ----------------------------------------------------------------------------------------------------------------- Interested applicants can also email CV at harry (for faster processing, please state the exact job / position title applied “Data Engineer”) Only shortlisted candidates will be notified.

------------------------------------------------------------------------------------------------------------------ EA License Number: 10C3636 EA Personnel Name:
Arora, Hardeep EA Personnel Registration Number: R1111454"
29-Apr-2022 T11:49,Data Engineer,"Huxley APAC, EA Licence No: 09C5506",23 hours ago,,Full–time,"Huxley is currently working with a digital assets company that is rapidly expanding in the region. This is an exciting opportunity for highly motivated individuals keen to work in a busy environment and contribute with ideas.

We are currently looking to fill 2 Data Engineer positions focusing on data modelling and risk related projects:

Data Engineer should have the following:
• 3+ year of experience developing machine models
• Proficient in either Python, Java and/or Scala
• Master's Degree with working experience OR Phd
• Understanding of deep learning is a plus

Data Engineer will be responsible for:
• Building Models reliable on machine learning, graph mining and other data driven technologies. Identify potential threats in KYC, Payments, Credit, Reward etc
• Maintain the Models to meet business expectations.
• Work on data that analyses customer feedback and satisfaction analysis by leveraging on machine learning techniques.
• Collaborate effectively with various stakeholders

Please note that this opportunity is open for Singaporeans only.

If you are interested in the above-mentioned Data Engineer role located in Singapore, do contact me and attach your latest CV. I will be glad to provide additional information and guide you through the next steps where necessary.

Consultant Details:

Anastasija Horoscuka

EA License No.: R

Huxley is a trading division of SThree Pte Limited

(Registration Number: E | SThree Pte Limited Licence Number 16S8216 | Huxley Licence Number J)

As per Ministry of Manpower (MoM) requirements, if you're suitable for any roles that we will be putting you forward for, we will need to request for your identification number. Please be assured that this will not be disclosed with our client or any other parties other than MoM.

Award winner of:

Recruitment Agency of the Year by Asia Recruitment Awards 2019 | Best Client Services by Asia Recruitment Awards 2017 | Best Overseas Operation by Global Recruiters 2017 | Highly Commended for Best Large Recruitment Business 2017 | Commended for Best In-House Training by Global Recruiters 2017"
29-Apr-2022 T11:49,Senior Data Engineer – AI & IoT,Holloberg,9 hours ago,,Full–time,"Senior Data Engineer – AI & IoT Jobs in Singapore at Holloberg

Title: Senior Data Engineer – AI & IoT

Company: Holloberg

Location: Singapore

We have partnered with a growing Technology company to look for a Senior Data Engineer. An opening for someone looking to be involved with cutting edge innovative technologies such as Artificial Intelligence and Internet of Things, and gain exposure in a dynamic environment. It’s a great opportunity for someone to join a multicultural team with great company culture.

Responsibilities:

Create and maintain optimal data pipeline architecture.

Assemble complex data sets to meet specific requirements.

Creating data systems that ingest data from various sources.

Implement flows with distributed systems and cloud architecture.

Write efficient, well documented, and highly readable code.

Schedule/automate data pipelines and monitor their performance.

Write ad-hoc queries in order to perform analysis

Interact with the teams to gather requirements and explain his work

Requirements:

Experience with Data Warehousing and Databases (MySQL, PostgreSQL or MongoDB),

Having experience with Python would be a plus.

If you’d like to hear more, please apply and/or contact us at:www.holloberg.com

Business Registration No: 201805144DEA Registration No: R1653145License No: 18C9522

Job Types: Full-time, Permanent

Salary: $7,000.00 – $9,000.00 per month

Benefits:

Cell phone reimbursement

Dental insurance

Health insurance

Vision insurance

Work from home

Schedule:

Flexible hours

Monday to Friday

Supplemental Pay:

Performance bonus

Yearly bonus"
29-Apr-2022 T11:49,Data Engineer,Achieve Group,20 hours ago,,Full–time,"Job Description
• Up to $8.5k basic
• Experienced in Health Informatics/ data management is advantageous
• Multi-award-winning healthcare IT leader

Our client played a key role in helping all major public healthcare institutions in Asia Pacific, transforming healthcare through smart technology, to achieve international benchmarks for advanced technology used in patient care.

Job Description
• Support translation of data business needs into technical system requirements
• Build data flow channels and processing systems to extract, transform, load and integrate data from various sources
• Develop complex code, scripts and data pipelines to process structured and unstructured data
• Make data science code production ready to ensure it is maintainable, scalable and debuggable.
• Deploy machine learning models to production and build the framework to monitor the performance of the model.
• Test data system configurations
• Support the handling and logging of errors
• Monitor data system performance
• Develop tools to improve data flows between internal/external systems and the data lake/warehouse

Requirements
• Degree/Master in Computer Science, Information Technology, Computer Engineering or equivalent.
• Demonstrate good, in-depth knowledge in relevant Extract-Transform-Load (ETL) hardware/software products, frameworks and methodologies.
• Experience with at least two of the following areas:
• databases (e.g. Oracle, MS SQL, MySQL, Teradata)
• big data (e.g. Hadoop ecosystem)
• ETL development using ETL tools (e.g. Informatica, IBM Datastage, Talend)
• data repository design (e.g. operational data stores, dimensional data stores, data marts)
• data interrogation techniques (e.g. SQL, NoSQL).
• structured and unstructured data analytics.
• batch and real-time data ingestion and processing
• data quality tools and processes.
• data transformation and terminology equivalence mapping.
• Experience in data modelling for analytics (e.g. star schemas, snowflake schemas).
• Experience with data acquisition tools (e.g., ETL, real-time data capture, and change data capture).

How to Apply

Simply submit your application by emailing a detailed copy of your updated Resume in MS Word Format to

or call your friendly Consultant, Esther Quak Soo Chin (Reg. No.: R21101335)(ICS), at 6590 9907 for a confidential discussion.

or Click the ""Apply Now"" button at the bottom of the page

YOUR SUCCESS IS OUR ACHIEVEMENT

Notice:

We regret that only short-listed candidates will be notified. All applications will be treated with the strictest confidence.

By submitting any application or resume to us, you will be deemed to have read & agreed to the terms of our Privacy Policy, and consented to us collecting, using, retaining and disclosing your personal information to prospective employers for their consideration, and for our marketing EDMs which you may opt out by unsubscribing in the mailer. You may refer and access our website at for more information.

Cessation of Collection of full NRIC Numbers:

In compliance with the Personal Data Protection Act and commitment to protect candidates' personal data, Achieve Group will cease to collect, process or use full NRIC numbers during our screening and job application process.

Kindly ensure your resumes provided to us does not contain your full NRIC number and full home address during your job application"
29-Apr-2022 T11:49,"Manager, Data Informatics",Twillio,16 hours ago,,Full–time,"See yourself at Twilio Join the team as our next Manager, Data Informatics. Who we are & why we're hiring Twilio powers real-time business communications and data solutions that help build better applications and customer experiences. Although we're headquartered in San Francisco, we have presence throughout South America, Europe, Asia and Australia. We're on a journey to becoming a globally anti-racist, anti-oppressive, anti-bias company that actively opposes racism and all forms of oppression and bias. At Twilio, we support wherever we do business. We employ thousands of Twilions worldwide, and we're looking for more builders, creators, and visionaries to help fuel our growth momentum. About the job This position is needed tomanage a team of data analysts to optimize processes and improve/sustain systems. Twilio is growing rapidly and seeking a Manager of Data Analytics team to be a key player in the process optimization and data integrity and processes evolution to be more efficient across customers/sales/carrier relations and operations teams. You will work with Data Analysts, Systems Admins, Developers and Business Users to ensure the data integrity and process optimization projects, be accountable and improve efficiencies throughout Twilio. Reporting to the Head of PGE PMO & Process/Systems, this position is critical in supporting Twilio's growth efforts throughout the entire carrier's onboarding lifecycle and super network operations. Responsibilities In this role, you'll: Live and have proven success partnering cross-functionally inside and outside the R&D and GTM orgs to align resources to accomplish multiple projects. They also have: Able to maintain and ensure data integrity across the business. Knows how to create and maintain reports and dashboards using Salesforce, looker and google analytics. Able to improve the customer experience and optimize the internal processes. Develop tools using Salesforce and integrated systems (i.e. DocuSign, Aptus Agreements +) Driven teammate with good communication skills and enthusiasm for collaboration. Manage the PLC of Salesforce from requirements gathering, implementation, testing to enabling users Impeccable attention to detail in a fast-paced work environment. Problem solving skill is a must and willingness to learn. Develop and maintain project plans and end-to-end documentation. Qualifications Not all applicants will have skills that match a job description exactly. Twilio values diverse experiences in other industries, and we encourage everyone who meets the required qualifications to apply. While having desired qualifications make for a strong candidate, we encourage applicants with alternative experiences to also apply. If your career is just starting or hasn't followed a traditional path, don't let that stop you from considering Twilio. We are always looking for people who will bring something new to the table! Required: 3+ years of managerial experience with a bachelor's degree in Business, Mathematics, Statistics, Computer Science, Engineering, Economics or related field. Experience analyzing and tracking performance indicators across projects. Be a professional for data, analytics, and testing to ensure accurate and proper interpretation of core business metrics and consumer behavior. Experience working independently and as part of a team. Will take the initiative to drive projects forward even under ambiguous circumstances. Require a SFDC certification. Desired: Experience with BI analytics Excellent written and verbal communication skills. Ability to influence and build effective working relationships with all levels of the organization. Location This role will be based in our Singaporeoffice. Approximately 10% travel is anticipated. What We Offer There are many benefits to working at Twilio, including, in addition to competitive pay, things like generous time-off, ample parental and wellness leave, healthcare, a retirement savings program, and much more. Offerings vary by location. Twilio thinks big. Do you We like to solve problems, take initiative, pitch in when needed, and are always up for trying new things. That's why we seek out colleagues who embody our values - something we call. Additionally, we empower employees to build by supporting their volunteering and donation efforts. So, if you're ready to unleash your full potential, do your best work, and be the best version of yourself, apply now! If this role isn't what you're looking for, . The successful candidate's starting salary will be determined based on permissible, non-discriminatory factors such as skills, experience, and geographic location within the state. This role is also eligible to participate in Twilio's equity plan and for the following benefits: health care insurance, 401(k) retirement account, paid sick time, paid personal time off, paid parental leave"
29-Apr-2022 T11:49,Data Engineer,Brenntag Asia Pacific Pte. Ltd.,16 hours ago,,Full–time,"At DigiB, we combine cutting-edge start-up environment with resources of a global company to make game – changing decision and drive digital transformation in the chemical industry

We design tools that help our customers, employees and suppliers to do their job in their best possible and efficient way.

We are developing Digital Products for our customers & partners

We will utilise Technology in a smart way to enable improved processes and reduce pains

We will touch on areas in Customer Service, Technology, Data, Supply Chain and Finance to name a few.

An idea of what you will do:

You will be responsible for building, maintaining & managing the cloud infrastructure
You will be responsible to setup & manage services connected to cloud infrastructure – including but not limited to AD, SSL Certificates, VPNs, Mail Servers, API configurations
You will design and deliver the production support management tools & services – Monitoring, Alerting, Capacity & Performance management
You will be responsible for setting up and managing log analysis & security monitoring analysis
You will be responsible for investigating & resolving infrastructure issues and collaborate with technology providers to resolve in a timely manner
You will administer & control access to all infrastructure services & tools
You will maintain application database & system logs backups and perform restores as needed by team
You will collaborate with IT security officers to ensure platform access & data is secured
You will be responsible for infrastructure failover assessments & disaster recovery drills
You will collaborate with technology services providers to proactively implement preventive measures and provide RCA report of infra issues
You will maintain complete inventory of all infrastructure resources
You will publish weekly reports of infrastructure usage, availability, risks and issues
You will support technology manager to assess platform reliability, robustness, security vulnerability and comparisons of cloud services and management tools
You will manage the Application code deployments, including defining and implementing any automated solutions for deployments
You will assist with the design and implementation of the continuous integration, automated testing and continuous deployment initiative
Who you are and your experience:
You have a degree or equivalent in computing/engineering from a reputable institution or relevant work experience
You have working experience building & maintaining modern technology platforms
You have hands on experience in enterprise systems & network architecture, cloud infrastructure i.e

Azure
You are familiar with enterprise systems like ERP, CRM, ECM, BPM & DWH systems
You have experience in collaborating with regional/global technology teams and diverse technology & service providers
You are able to work unsupervised & can make informed decisions
You are a hands-on team player and lead by example
You thrive in an unstructured and extremely agile environment
You have excellent communication and interpersonal skills to establish and build sustainable internal and external relationships
You should have a Growth mindset
Your Technical Skills:
You MUST have deep expertise in one of the Cloud technologies e.g

AWS, Azure
Must have hands on experience managing VMs, Containers using Unix shells (SSH)
Must have experience in setting up networking infrastructure, DNS, DMZ, VPNs, bastion, mail servers & API Gateways
You should have experience in one or more of the following API / integration services - WSO2 API, BPM, Mulesoft, TIBCO ESB
You should have experience using message queues / data streaming using Kafka, RabbitMQ, IBM MQ etc.
You should have experience with monitoring & Log analysis tools such as Grafana, Splunk etc
You have sound knowledge of Java technology stack , Spring Framework and Data Adapters
You have experience using micro-services deployment and Dockers management
You have DBA experience managing SQL & No-SQL databases and data management (PostgreSQL, Mongo, Redis)
You are familiar with API messaging specification JSON, YAML & REST
You are familiar with data engineering and supporting tools e.g

Hadoop, Spark, Kafka, Qlik
Familiarity with SAP data stream integration or similar enterprise solution"
29-Apr-2022 T11:49,Data Engineer,Geniebook,16 hours ago,,Full–time,"Loved by over 150,000 users, Geniebook is Singapore’s largest online learning platform for English, Mathematics and Science (EMS) syllabus. From AI-personalised worksheets to live classes and teacher chats, we are a powerful suite of complementary learning products designed to help students accelerate their academic performance. We’re looking out for amazing global talents to scale Geniebook exponentially across Southeast Asia, and we have many exciting roles opening up across a wide range of functions.

So join us today to inspire a new generation of learners and help even more students learn smarter and do better!

Job Scope:
• Design, build and maintain reliable and efficient pipelines to extract and process data from multiple sources to suit diverse data consumers and business units
• Work closely with the Engineering team to make sure the data handling infrastructure and pipelines are up-to-date, efficient, and in line with industry standards.

Job Requirements:
• Bachelor's degree or equivalent experience in quantitive field (Statistics, Mathematics, Computer Science, Engineering, etc.)
• At least 1 - 2 years of related experience
• Knowledge of database theories and design
• Experience in relational and non-relational database management systems, such as MySQL, NoSQL, or similar
• Strong programming skills, particularly in Python, C++
• Familiarity with distributed processing and managing of big data using Apache Hadoop, MapReduce, Apache Hive, or similar frameworks
• Previous exposure to ETL tools and technologies
• Proven competency in creating and maintaining a Data Warehouse or Data Science Pipeline
• Awareness of data storing, maintenance, and migrations costs.
• Ability to work in a multidisciplinary team"
29-Apr-2022 T11:49,"AVP, Data Engineer - BI Tools",United Overseas Bank,5 hours ago,,Full–time,"AVP, Data Engineer - BI Tools

AVP, Data Engineer - BI Tools

Posting Date: 25-Apr-2022

Location: Alexandra, Singapore, SG

Company: United Overseas Bank Limited

About UOB
United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices.
Our history spans more than 80 years. Over this time, we have been guided by our values - Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.

About the Department
The Technology and Operations function is comprised of five teams of specialists with distinct capabilities: business partnership, technology, operations, risk governance and planning support and services. We work closely together to harness the power of technology to support our physical and digital banking services and operations. This includes developing, centralising and standardising technology systems as well as banking operations in Singapore and overseas branches.

Job Responsibilities
You will be responsible for the end-to-end software development and support for all work related to projects, quarterly change requests, L3 production fixes. This includes software product implementation and administration, application design, development, implementation, testing and support. You will be expected to work on Finance & Risk Analytics.
You will also be responsible for quality assurance of the team's delivery in conformance with the Bank-defined software delivery methodology and tools. You will partner with other technology functions to help deliver required technology solutions.

Other responsibilities include:

• Create frameworks, technical features which helps in faster operationalisation of Data models, Analytical models(including AI/ML) and user generated contents (dashboards, reports etc.)

• Effectively partner with citizen data scientists in enabling faster adoption of AL/ML model based systems

• Independently install, customise and integrate software packages and programs

• Carry out POCs involving new data technologies

• Design and develop application frameworks for data integration

• Create technical documents such as solution design, program specifications for target solutions

• Perform design and development of applications which may not be limited to: Software Applications, Data Integration, User Interfaces, Automation

• Maintain and recommend software improvements to ensure a platform centric management of software applications

• Performance tuning

• Work with production support team members to conduct root cause analysis of issues, review new and existing code and/or perform unit testing

• Perform tasks as part of a cross functional development team using agile or other methodologies and utilising project management software

Job Requirements
Functional skillsets
Data Lake, EDW, Data Mart, Data Integration & Visualization
Hands-on experience in implementing large scale data warehouse & analytics platforms in financial services industry with good functional knowledge of products & services offered in Retail bank / Wholesale / Global Markets covering some of the following analytics domains:

• Setting up and running BI tools oriented platform
• Design and develop QlikSense & Microsoft Power BI applications
• Design and develop Applications in SAS, Microsoft-R, Python
• Integration of BI tools with data stores (EDW, data marts )
• Experience in Data Modeling, Data mapping for Data Warehouse and Data Marts solutions
• Experience in financial domain - Retail , Wholesale, Compliance, Digital
• Expertise in design of role based fine grained access control
• Designing cloud ready data solutions, Virtualization
Technical skillsets

• Expertise in installing and configuring of BI/Analytical tools like Qlik, Power BI, SAS, Microsoft R
• Expertise in administration (QMC, powershell scripting), infrastructure design of such tools
• Integration with other data systems like EDW, Data mart, CRM, Digital
• Expertise in building BI semantic layer (Qlik - QVD, PowerBI - Direct Query, SSAS, In Memory SSAS modelling, SSRS, Excel Add In)
• Creating application framework specific to BI tools to speed up user adoption
• Good working experience in security features of BI tools including integration with Active Directory, solutions to protect data in motion and at rest
• Good working experience in fine tuning on line BI queries, BI models, analysis of DAX queries
• Reports/Dashboard/Application development expertise using QLIK, PowerBI, SAS, Microsoft R, Python
2 to 3 technical certifications from enclosed list:

• Cloudera Hadoop distribution - Hive, Impala, Spark, Kudo, Kafka, Flume
• Teradata - Bteq, Query Grid, GCFR, MDM, Data Mover, BAR
• Informatica Data Integration - PC, IDR, BDM, MM, IDQ, EDC
• Data modelling tools (Erwin)
• QlikSense
• Microsoft Power BI - Direct Query, SSAS, SSRS, Excel Add In
• Microsoft - R
• Data science workbenches - Cloudera Workbench, Jupyter, DataRobot, H2O.AI, IBM DSX
• Data Virtualization tool - Denodo, Dremio
• AS400
• Language - SQL, Java, Python, Scala, Pyspark
• Automation / scripting - CtrlM, Shell Scripting, Groovy
Experience for an added advantage:

• CI/CD software, Testing Tools - Jenkins, SonarQube
• Version Control Tool - Aldon+LMe, CA Endeavor
• Deployment Tool kit -Jenkins
• Service or Incident Management (IcM) Tools - Remedy
• Source Code Repository Tool - Bitbucket
• Scheduling Tool - Control-M
• Defect Management Tool - JIRA
• Application Testing tool - QuerySurge
• Cloud certification
• Platforms provided by FICO, Experian, SAS for credit and portfolio management

Be a part of UOB Family
UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.

Apply now and make a difference"
29-Apr-2022 T11:49,Principal Data Engineer,SingTel,29 minutes ago,,Full–time,"Job Description :

At Singtel, we believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers. We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative.
Join us and experience what it's like to be with an Employer of Choice*. Together, let's create a brighter digital future for all. *Awarded at the HR Fest Awards 2020.
We are seeking a Principal Data Engineer who will be responsible to lead the data engineering solutions, deliveries, and operations. Ensure the efficient and sustainable operation of Singtel Unified Data Platform and Event Streaming Platform, and to build and maintain large-scale, highly available, high-performance distributed systems based on system availability and performance. You will be part of Group IT Data & Platform Management team. In this position you will work with business, IT, and data professionals.
The role covers the following key objectives:
• Lead a team of data engineers and be a mentor to the team on development best practices
• Take ownership of the team deliverables, delegate the development tasks efficiently to the team and ensure timely delivery
• Provide solution oversight to delivery architects and data engineers
• Additional to being the lead of the development team, contribute as a senior individual contributor on complex modules of the platform architecture, design, and development
• Drive, lead and align new capabilities in data engineering and data integration across Singtel Group
• Develop new data solutions and accelerators that help to deploy our data platform and engineering services at scale
• Lead, manage and run data platform operations and application support
• Setup and operate site reliable engineering (SRE) for data engineering to achieve efficient, stable, and sustainable operations
• Plan and manage annual budget for both capital expenditures and operating expenses
• Define standards and guidelines for development and operations
• Contribute technical and thought leadership to Singtel data platform and engineering initiatives, programs, and roadmap
• Build and maintain strong relationship with business leaders, Group IT domains (departments) and IT service providers to deliver value via data
• Present and pitch at relevant senior leadership levels and /or executive steering committees
• Promote an engineering-centric and devops culture through building relationships with development & operations and driving enhancements to the end-to-end release process\
• Build, hire and retain a high-performance data engineering team

Requirements
• Bachelor's degree in Business Management, IT, Computer Science or equivalent.
• At least 12 years of experience in big data and / or data warehouse, including at least 8 years of experience in leading data engineering and operations
• Expert in building and optimizing data pipelines using Scala, Spark
• Experience in managing large scale data warehouse / data lake in both on-prem and cloud with high availability and scalability
• Experience in data management, data architecture and design
• Experience in event streaming with Kafka
• Strong technical knowledge of data integrations including a data engineering framework
• Strong knowledge of SQL, Scala, Spark, Hadoop
• Hands-on experience in handling incident, problem, configuration, capacity, and availability management
• Experience with DevOps tools and environment
• Experience with Cloud environment - AWS, Azure, GCP
• Experience with monitoring and load balancing tools
• Experience with SRE
• Experience in managing and driving outsourced vendors to delivery and operations objectives
• Experience in leading complex and major change initiatives and programs
• Strong background in operational and capital finances, and IT budget development"
29-Apr-2022 T11:49,Senior Data Engineer,Quantexa,21 hours ago,,Full–time,"Founded in 2016 with only a handful of individuals, Quantexa was built with a purpose that through a greater understanding of context, better decisions can be made. 6 years, 10 locations and 500+ employees later we still believe that today. We connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data.

Due to the continuous success and high demand from our customers, we are looking for a Senior Data Engineer with a proven track record to join the Quantexa family. 🚀

What does a Senior Data Engineer role at Quantexa look like?

In order to be a successful data Engineer at Quantexa, you’ll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.

Being Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. You’ll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service. 🥇

We want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala, Data Fusion and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but don’t worry If that’s not your strongest language or if you haven’t used it before, we make sure that every Quantexan goes through our training academy so they’re comfortable and confident with using our platform.

Requirements

• We’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.
• The desire to learn and code in Scala

• Experience in working in an Agile environment
• Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.
• A strong coding background in either Java, Python or Scala
• Experience of building data processing pipelines for use in production “hands off” batch systems, including either traditional ETL pipelines and/or analytics pipelines.
• Passion and drive to grow within one of the UK’s fastest growing scale-ups.
• Consulting or business facing skills and a desire to work with customers.

Benefits

Why join Quantexa?

We know that just having an excellent glass door rating isn’t enough, so we’ve put together a competitive package as a way of saying thank you for all your hard work and dedication.

We offer:

• Competitive salary 💰
• Company bonus
• Private healthcare with Sigma plus life insurance and critical illness
• Free Calm App Subscription #1 app for meditation, relaxation and sleep 🧘‍♀️
• Annual leave, national holidays + birthday off! 🌴
• Ongoing personal development
• Great WeWork Office Space & Company wide socials"
29-Apr-2022 T11:49,Data Engineer,St Engineering Unmanned & Integrated Systems Pte. Ltd.,16 hours ago,,Full–time,"We are looking for an experienced AI Engineer to join our multidisciplinary team in transforming our MRO business, using deep learning and neuro-linguistic programming (NLP) to help us improve various business outcomes and drive innovation.Job Responsibilities:Perform research and development (R&D) and processes to meet the needs of our AI strategyWork with operation teams to identify and prioritize key areas of the business where AI solutions can drive significant business benefitDesign and develop AI Solutions for MRO business Optimization.Work on functional design, process design (including scenario design, flow mapping), prototyping, testing, training, and defining support procedures, working with Group Engineering Centre and Operation Teams.Document and articulate solution architecture and lessons learned for each exploration and accelerated incubationJob Requirements:Degree in Computer science, Electrical Engineering / Mechanical Engineering or equivalent2+ years of experience in applying AI to practical and comprehensive technology solutionsProficient in C, C++ and Python programmingFamiliar with Windows and Linux programming environmentsProven experience with ML, deep learning, Tensorflow, NLPStrong interpersonal and communication skills.Ability to contribute as a team player or independently.Ability to demonstrate a high level of initiative and resourcefulness.Location: Ang Mo KioSingaporean only"
29-Apr-2022 T11:49,Data Engineer,Tata Consultancy Services,16 hours ago,,Full–time,"Required Qualifications3-5 years' experience in ETL workflows on Big Data platform using Informatica Data Engineering Quality (version 10.X)Experience in data profiling, cleansing, parsing, standardization, verification, matching and data quality exception handling.Strong technical skills in SQL/PLSQL, HiveQL, SparkSQLAble to apply best-practice optimization techniques on ETL workflowsExperience and understanding of testing, coding, design, documentation and change management procedures.Strong work ethic with a highly positive, hands-on, can-do attitude and flexible team player.Ability to manage tasks independently, take ownership of responsibilities and work with minimal supervision."
29-Apr-2022 T11:49,"AVP, Software Engineer, Data Technology, Technology and Operations",DBS Bank,16 hours ago,,Full–time,"Business Function

Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation.

In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.

Responsibilities
• Build the enterprise data platform and tooling for the users of the data platform.
• Incident management : troubleshoot business critical incidents. Analyse patterns of production incidents, develop permanent remediation plans, and implement automation to prevent future incidents from occurring through software engineering.
• Programming and scripting together with development teams to automate everything to remove the toil.
• Setting up strategies for observability, automating monitoring and auto-remediation of known issues.
• Work with remote teams.

Requirements
• Bachelor's degree in Computer Science or a related technical field involving software or systems engineering, or equivalent practical experience.
• Experience programming in at least one JVM languages (Java, Scala, Clojure).
• Experience programming in another language C, C++, Python, Javascript or Go.
• Strong experience in managing at least two of the following : Kafka, YARN, Spark, Cassandra, Elasticsearch, Kubernetes, Ansible.
• Strong experience in problem solving and analyzing global scale distributed systems including at least two of the above.
• Strong experience in managing at least one of the following monitoring solutions : Open-Telemetry, Prometheus, Grafana.
• Expertise in Unix / Linux systems, IP networking, performance and application issues.
• Experience in designing, analyzing and building automation and tools for large scale systems.
• Ability to debug, optimize code, and automate routine tasks.
• Knowledge of Linux systems internals.
• Hands-on technical experience.
• Effective communication skills.
• Experience with software product development.
• Experience with Java application servers and JVM configuration.
• Experience writing Kubernetes controllers and operators.
• Ability to work independently and make decisions under minimal supervision.
• Experience in building solutions with AWS, Google, Azures and other cloud services.

Apply Now

We offer a competitive salary and benefits package and the professional advantages of a dynamic environment that supports your development and recognises your achievements"
29-Apr-2022 T11:49,Data Engineer Big Data,ADECCO PERSONNEL PTE LTD,22 hours ago,,,"Job Description
• Development efforts in an agile software development environment, participate in sprint planning, task breakdown, and daily stand ups
• Work with product owners and architect to understand objectives and translate these into a system level design and implementation
• Implement designs that meet quality standards, coding standards, and provide a rich user experience across platforms
• Design and implement new frameworksand software that meets DBS’s standards in performance, reliability, and maintainability
• Create rapid prototypes to vet new design, interaction, and integration concepts.
• Collaborate across cross locations IT teams in delivering software components
• Investigate and resolve SIT and UAT defects
• Troubleshoot and solve production issues related to performance and reliability throughout the software stack
• Proactively identify bottlenecks in the system and work to resolve these issues before they become a problem

Requirements
• 2 – 5 years of Actimize development (WLF, CDD, SAM)
• Relevant AML experience in banking environment
• Able to perform linux / windows scripting.
• Experience in Java development will have an advantage
• Candidate with operational experience, preferably in support of critical application systems, will have an advantage

Saghana Sithara | Registration Number: R1550224"
29-Apr-2022 T11:49,Data Engineer - APAC,Tamr,22 hours ago,,Full–time,"Company Description

Tamr is the enterprise data mastering company trusted by large enterprises like Blackstone, the US Air Force, Toyota, and GSK. The company’s patented software platform uses machine learning supplemented with human feedback to master and prepare data across myriad silos to deliver previously unavailable business-changing insights. With a co-founding team led by Andy Palmer (founding CEO of Vertica) and Mike Stonebraker (Turing Award winner) and backed by top-tier investors such as NEA and GV, Tamr is transforming how companies get value from their data.

Job Description

Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE

who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being

customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation

This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.

Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion

Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:49,Data Engineer,Endowus,6 hours ago,,Full–time,"About us
Endowus is Asia's leading fee-only digital wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping all investors grow their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.
Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones. Our mission is clear: help people invest better so they can live easier today, and better tomorrow.

The people you will work with
The team has deep domain knowledge in finance and technology, bringing together decades of experience at Goldman Sachs, Morgan Stanley, UBS, Credit Suisse, The Blackstone Group, AQR, Grab, Dropbox, Lyft, Redmart, Carousell, Bytedance, Grab, Kakao, Alibaba, and more See our leadership team here . We practise inclusion and treasure our diversity in background and experience. A diverse team is our biggest asset and we look for people who share our belief in Endowus' clear mission.

Investors, recognition, licensing
Endowus is backed by global leading strategic and venture capital investors including UBS, Samsung Ventures, EDBI, Prosus Ventures, ZVC, Singtel Innov8, Lightspeed Venture Partners, and SoftBank Ventures Asia.

Endowus has been recognised by the industry with the following awards: Singapore's Rising Star and Fintech Innovation (Asia Asset Management's Best of the Best Awards 2021), LinkedIn Top Start-ups 2021, WealthTech of the Year (Asia FinTech Awards 2021), and the top 15 Singapore's Best Workplaces Award (Great Place to Work).
Endowus is licensed by the Monetary Authority of Singapore (MAS).

About this role; responsibilities & ownership
• We are looking for a Data Engineer who thrives in a fast-paced environment and enjoys driving innovation through rapid prototyping and iterative development.
• Being part of Endowus's Technology team, you will build end-to-end product features that you are confident of delighting user experiences.
• You will need to leverage the entire technology stack to realise these goals and will be part of a team that is constantly tackling difficult questions of scale, architecture, and interaction. Requirements & qualifications

• Bachelors' or above in Computer Science, a related field, or equivalent professional experience
• 3-6 years of experience in designing and implementing key components for highly scalable, distributed data collection and analysis systems built for handling large volumes of data in cloud
• Advanced working SQL knowledge and experience working with relational databases or BigQuery, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimising data pipelines and data sets in Apache Flink or Apache Spark to answer specific business questions.
• Experience in at least one of the JVM languages such as Java, Scala.
• Experience Build processes supporting data transformation, data structures, metadata, dependency and workload management. (experience in Airflow is Plus)
• Experience with distributed databases, such as Cassandra, and the key issues affecting their performance and reliability.
• Hands-on development with key technologies including Scala, Apache Flink and other relevant distributed computing languages, frameworks, and libraries
• Independence and self-reliance while being a proactive team player with excellent communication skills.
• Self-driven, has a strong sense of ownership and able to work and learn independently
• Comfortable with numbers and motivated by steep learning curves Nice to haves
• Experience with message broker systems, such as Apache Kafka.
• Developed mission-critical products, e.g. trading platforms, medical applications
• Basic knowledge of finance and trading
• Preferably some exposure to CQRS / Event Sourcing patterns
• Familiarity with Docker, Kubernetes and Terraform Remote Okay
• We are open to hiring remotely in Asia time zones. Benefits & perks
• Stock options in a fast growing company *
• Employee discount for investing at Endowus
• Flexible working hours and locations so you can live life and your best work - you are trusted to be responsible
• Competitive Staff Benefits; Annual Leave, Medical insurance, Dental *
• Productivity Equipment; Omnidesk Standing Desk & Ergonomic Chairs, Dell UltraSharp USB Type-C Monitor, high-end computer peripherals
• Choice of Apple or PC equipment
• Additional S$250 /HKD 1450 budget for your favourite hardware to boost productivity * Note: * is only applicable to Full-Time employees"
29-Apr-2022 T11:49,Data Engineer,GLP PTE. LTD.,16 hours ago,,Full–time,"The Data Engineer will be a key player in the enterprise-wide data transformations projects. You will engage in developing and automating data processing pipelines for data modelling, analysis, and reporting from various data sources system; The primary responsibility of this position is to assist to establish the enterprise data Lake architecture under Microsoft Azure Data factory, Databricks and Synapse and deliver data driven solutions.

Job description:

• Assist in architecture design, develop, document, and implement end-to-end data pipelines and data driven solutions.

• Define roadmap to transform data architecture focusing on scalability, performance and stability for the entire data lifecycle;

• Build data flows for data acquisition, aggregation, and modelling, using both batch and streaming paradigms.

• Perform data analysis, data profiling, data cleansing, data lineage, data mapping and data transformation.

• Develop high-quality code for the core data stack including data integration hub, data warehouse and data pipelines under Azure service.

• Execute and deliver best practices in data management and data lifecycle processes, including modular development of data processes, coding and configuration standards, error handling and notification standards, auditing standards, and data archival standards.

• Implement security and standards, documenting technical specifications and operating procedures.

• Collaborate across developers as part of a SCRUM team ensuring collective team productivity

• Provide technical support for any data issues with recommendations, and resolutions.

Requirements:

• 2 years professional experience as a data engineer, software engineer, data analyst, data scientist, or related role.

• Experience with Microsoft Azure Data Integration Stack (Azure Data Lake Gen2, Azure Data Factory, Delta Lake, SSIS, SQL Server, Azure Data Warehouse), Databricks, Spark.

• Working experience in Investment or Real Estate industry, preferably with business and functional knowledge.

• Expertise building ETL and data pipelines on Databricks using data engineering languages Python and SQL on Azure.

• Advanced knowledge and experience working with Python & SQL;

• Proven experience with all aspects of the Data Pipeline (Data Sourcing, Transformations, Data Quality, Etc…).

• Experience with visual modelling tools including UML

• Proficient in using data visualization tool such as Power BI, Workiva and in standard office tools such as Excel.

• Familiar with DevOps and Agile methodology.

Recommended Skills

Agile Methodology

Apache Spark

Architectural Design

Architecture

Auditing Standards

Azure Data Factory"
29-Apr-2022 T11:49,Data Engineer,Lilith Games Sg Pte. Ltd.,16 hours ago,,Full–time,"Big Data Development Engineer (Advertising Direction)

Big Data Development Engineer (Advertising Direction)

What you will be doing:

1ã€ Responsible for the construction of Big Data Cloud Platform and the maintenance of Advertising Data Warehouse.

2ã€ Responsible for the calculation, cleaning and layering of Advertising Data based on Flink, Spark, etc., and store it through Hadoop, Clickhouse, etc.

3ã€ Responsible for Advertising Data Analysis and Advertising System Report development.

Qualifications & Skills

1ã€ Bachelor degree or above, major in computer and other related majors.

2ã€ Proficient in Python or Golang coding.

3ã€ Proficient with Mysql, Memcache, Redis, Message Queue and other common WEB components.

4ã€ Having experience in using one or more of HDFS, Hive, HBase, MongoDB, Kafka, Flink and Spark is preferred.

5ã€ Experience in operation and maintenance development is preferred.

6ã€ Familiar with Alibaba Cloud and other cloud computing resource deployment and optimization is preferredã€'

7ã€ Proficient in coding complex SQL Statements, and have the ability and experience of query optimization.

8ã€ Positive and optimistic, strong sense of responsibility, with good team communication and cooperation

å¤§æ•°æ ®å¼€å 'å·¥ç¨‹å¸ˆï¼ˆå¹¿å'Šæ–¹å 'ï¼‰

å·¥ä½œè Œè´£

1

è´Ÿè´£å¤§æ•°æ ®äº'å¹³å °æ å»ºï¼Œå¹¿å'Šæ•°æ ®ä»'åº'çš'æ å»ºä¸Žç»´æŠ¤ã€'

2

è´Ÿè´£åŸºäºŽFlinkã€ Sparkç‰å¯¹å¹¿å'Šæ•°æ ®çš'è¿›è¡Œè®¡ç®—ã€ æ¸…æ´—ã€ åˆ†å±'ç‰å·¥ä½œã€'å¹¶é€šè¿‡Hadoopã€ Clickhouseç‰è¿›è¡Œå˜å'¨ã€'

3

è´Ÿè´£å¹¿å'Šæ•°æ ®åˆ†æž å Šå¹¿å'Šç³»ç»ŸæŠ¥è¡¨çš'å¼€å 'ã€'

ä»»è Œè¦ æ±'

1.å¤§å¦æœ¬ç§'(ç»Ÿæ‹›)å Šä»¥ä¸Šå¦åŽ†ï¼Œè®¡ç®—æœºã€ é€šä¿¡ç‰ç›¸å…³ä¸'ä¸šã€'

2.ç†Ÿç»ƒæŽŒæ ¡Pythonæˆ–Golangä»£ç ç¼–å†™ã€'

3.ç†Ÿç»ƒä½¿ç'¨Mysqlã€ Memcacheã€ Redisã€ æ¶ˆæ ¯é˜Ÿåˆ—ç‰å¸¸ç'¨WEBç»'ä»¶ã€'

4.æœ‰ä½¿ç'¨HDFS, Hive, HBase, MongoDB, Kafkaï¼ŒFlink, Sparkä¸çš'ä¸€é¡¹æˆ–å¤šé¡¹çš'ç» éªŒä¼˜å…ˆã€'

5.æœ‰è¿ ç»´å¼€å 'ç» éªŒä¼˜å…ˆã€'

6.ç†Ÿæ'‰é˜¿é‡Œäº'ç‰äº'è®¡ç®—èµ'æº éƒ¨ç½²ä¸Žä¼˜åŒ–è€…ä¼˜å…ˆ

7.ç†Ÿç»ƒç¼–å†™å¤ æ 'çš'sqlè¯å ¥ï¼Œå…·å¤‡æŸ¥è¯¢ä¼˜åŒ–çš'èƒ½åŠ›å Šç» éªŒã€'

8.ç§¯æž ä¹ è§'ï¼Œè´£ä»»å¿ƒå¼ºï¼Œå·¥ä½œè®¤çœŸç»†è‡´ï¼Œå…·æœ‰è‰¯å¥½çš'å›¢é˜Ÿæ²Ÿé€šä¸Žå ä½œèƒ½åŠ›ã€"
29-Apr-2022 T11:49,Data Engineer,YASH TECHNOLOGIES SINGAPORE PTE. LIMITED,4 hours ago,,Full–time,"Technical Competence and Experience :
• Experience with the Microsoft Azure Platform (Data Lake, Data Factory, Databricks, Data Warehouse, Azure DevOps)
• Experience in DataBase and Information Modelling
• Experience with data wrangling using SQL and Python (R is a bonus)
• Experience with the Apache Spark Framework (preferably PySpark - Python)
• Experience with scripting languages such as Python and PowerShell (C#/.NET)
• Experience integrating systems and services
• Experience consuming REST APIs
• Experience developing REST APIs (preferably with Flask - Python)
• Experience with Docker containers

Soft Skills :
• Personal drive & problem-solving mentality
• Stakeholder management skills
• Ability to work across time-zones, culturally aware and able to collaborate with people from other countries.
• Team player – will collect and share information with colleagues in order to improve our services.
• Self-motivated and driven. Communicates and debates solutions to issues found Proactively contacts colleagues and takes the lead to work on problems or improvements.
• Service-minded.

Interested applicants can apply here or email to [ Email address blocked ]

Recommended Skills
• .Net Framework
• Apache Spark
• C Sharp (Programming Language)
• Data Lake
• Data Processing
• Data Warehousing"
29-Apr-2022 T11:49,Data Warehouse Engineer - KL,SearchAsia Consulting Pte Ltd,14 hours ago,,Full–time,"Responsibilities:
• Help build and maintain data warehouse platform with its associated data models and ETL processes
• Model the data and design ETL specifications to business requirements
• Design and develop data flows for data warehousing and self-serve reporting
• Automate and optimize existing data processing workloads to integrate with the data warehouse
• Monitor all data integrity performance and adopt appropriate tools
• Coordinate with Data Science team and collect all technical requirements
• Analyze query performance and perform query tuning to assist development engineers in designing and optimizing queries
Requirements:
• Bachelor’s Degree in Computer Science, Information Systems or similar
• 3+ years of Data Warehousing and Data Modeling experience.
• Hands-on experience with PostgreSQL and MYSQL Database
• Experience with all aspects of data systems including database design, ETL, aggregation strategy, and performance optimization
• Understanding best practices for building and designing ETL code and strong SQL experience with the ability to develop, tune, and debug complex SQL applications is required
Account Manager:
Kerwin Tan Kai Bin (R1331624)
kerwin.tan@searchasia.com.sg
EA: 16S8107"
29-Apr-2022 T11:49,Senior Data Engineer,Grabtaxi holdings pte ltd,9 hours ago,,Full–time,"Get to know our Team:
Data Engineering runs the code, pipeline and infrastructure that extracts, processes and prepares every piece of data generated or consumed by Grab's systems. We are a diverse team of software engineers that not only work to solve all kinds of data related problems faced by teams from all corners of Grab but we also act as a bridge that ties everyone together through data. As data in Grab never stops growing, this team also never stops, learning, innovating and expanding so that we can bring in or build the latest and best tools, technology to ensure the company's continued success.
Get to know the Role:
Data Engineers in Grab get to work on one of the largest and fastest growing datasets of any company in South East Asia. We operate in a challenging, fast paced and ever changing environment that will push you to grow and learn. You will be involved in various areas of Grab's Data Ecosystem including reporting & analytics, data infrastructure, and various other data services that are integral parts of Grab's overall technical stack.
The day-to-day activities:
• Build, deploy and manage big data solutions that can adequately handle the needs of a rapidly growing data driven company
• Spearhead the development of systems, architectures, and platforms that can scale to the 3 Vs of Big data (Volume, Velocity, Variety)
• Streamline data access and security to enable data scientists and analysts to easily access to data whenever they need to
• Build out scalable and reliable ETL pipelines and processes to ingest data from a large number and variety of data sources
• Maintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable and timely delivery of key insights for decision making
• Lead the movement cleaning and normalizing subsets of data of interest as preparatory step before deeper analysis by the data scientists
• Run Modern high performance analytical databases and computation engines like Spark, Flink, Presto, Synapse, BigQuery, Greenplum and others

The must haves:
• A degree or higher in Computer Science, Electronics or Electrical Engineering, Software Engineering, Information Technology or other related technical disciplines.
• Experience in handling large data sets (multiple PBs) and working with structured, unstructured and geographical datasets
• Designed high performance scalable infrastructure stacks for Big Data Analytics
• Deep understanding of databases and best engineering practices - include handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding how to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipeline
• Real passion for data, new data technologies, and discovering new and interesting solutions to the company's data needs
• Excellent communication skills to communicate with the product development engineers to coordinate development of data pipelines, and or any new products features that can be built on top of the results of data analysis"
29-Apr-2022 T11:49,Data Engineer,Manpower Singapore,16 hours ago,,Full–time,"Data Engineer Key Role and Responsibilities:Partner with the global and in-market data experts to discover and derive value from connecting external and internal data sources.Build (ETL) new and evolve data models and pipelines to power algorithmic based Business Intelligent solutions that addresses business problems requiring descriptive, diagnostic, predictive, and/or prescriptive analytics for pricing, promotion, trade spending, assortment, and sales performance management.Translates the algorithms and analytic models into data models as business needs evolves on a going basis after they are put into production.Develop a roadmap that scales existing and new data models, to support the portfolio of solutions.Expand into Business Intelligence solution development focused on automation and scale up of solutions.Key Skills Required:Professional Skills: R/Python programming languages MS Power BI MS Excel Data Visualisation General Skills: Client Management Project Management Diploma or B.S. in a relevant technical field (Operations Research, Computer Science, Statistics, Business Analytics, Econometrics, or Mathematics).Overall experience of 2-3 yearsAbility to manipulate and high-volume of data from varying sourcesExpert knowledge of an analysis tool such as Microsoft PowerBIProficiency in R/PythonBasic track record in working independently with minimal guidance Interested applicants, please submit your resume to : [ Email address blocked ] Abelene Marianne Kang Mrs Abelene Marianne Rozario R2089914EA License No: 02C3423 Reg No: 199505951HKang Abelene Marianne Mrs Rozario Abelene MarianneEA License No.: 02C3423 | Personnel Reg No.: R2089914 Please note that your response to this advertisement and communications with us pursuant to this advertisement will constitute informed consent to the collection, use and/or disclosure of personal data by ManpowerGroup Singapore for the purpose of carrying out its business, in compliance with the relevant provisions of the Personal Data Protection Act 2012. To learn more about ManpowerGroup's Global Privacy Policy, please visit [ Link removed ] Recommended Skills AlgorithmsAnalyticalAutomationBusiness AnalysisBusiness IntelligenceBusiness Requirements"
29-Apr-2022 T11:49,Senior Data Engineer,Techgig,16 hours ago,,Full–time,"About Job
CTC Undisclosed Job Location Singapore Experience 3 - 6 yrs
Description
This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide excellent user experiences. You will be an integral part of Payment Product Development team focusing on design and build of software solutions that leverage data to solve business problems.
The role is for a self-motivated individual with software engineering skills and expertise with Big Data technologies. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, development and test of new functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
Responsible for design, development, implementation and unit testing of applications
Work on development of new products iteratively by building quick POCs and converting ideas into real products.
Design and develop mission-critical systems delivering high-availability and performance.
Interact with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations while applying the latest available tools and technology.
Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.
Have a passion for delivering zero defect code and be responsible for insuring the team's deliverables meet or exceed the prescribed defect SLA.
Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code.
Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.
Perform other tasks on R&D, data governance, system infrastructure, and other cross team functions, on an as-needed basis
This is a hybrid position. Hybrid employees can alternate time between both home and office. Employees in hybrid roles are expected to work from the office three days a week, Monday (Sunday in some countries where Sunday is the start of the week) and Wednesdays. Wednesdays are designated as in-office collaboration days. The third day in the office will be decided based on team needs and determined in partnership with senior leadership.
Qualifications
Basic Qualifications:
BS/MS in Computer Science, Computer Engineering, or related field
Preferred Qualifications:
Successful candidates would usually have a mix of the following qualifications:
3 years of software design and development experience
Previous exposure to financial services is a plus, but not required
Extensive experience with SQL and big data technologies (Hadoop, Java, Spark, Hive etc.) tools for large scale data processing and data transformation
Experience with data visualization and business intelligence tools like Tableau, or other programs highly desired
Experience working in an Agile and Test Driven Development environment.
Experience with Kafka is highly desired
Strong knowledge of API development is highly desired
Strategic thinker and good business acumen to orient data engineering to the business needs of internal clients
Strong foundation in computer science, with strong competencies in data structures, algorithms and software design optimized for building highly distributed and parallelized systems
Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style"
29-Apr-2022 T11:49,"Data Engineer, Data Engineering (6 months Contract)",Bank of Singapore,23 hours ago,,Full–time,"At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programmes that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group's global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today!

Bank of Singapore opens doors to new opportunities.
Start your career with Bank of Singapore as a Data Engineer (6 months contract) in our Data Engineering team!
Bank of Singapore opens doors to new opportunities. At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programs that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group's global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today!
Bank of Singapore has defined an ambitious roadmap for accelerating its data excellence through the implementation of various bank-wide programs. As part of this roadmap, building the foundation of good data governance and management plays a key role in establishing trust in the data that we use.
General Description:
The incumbent will be based in the Data engineering team within the Data Hub team, supporting Bank of Singapore's data sourcing, curation, transformation, and data analytic services to accelerate their data initiatives. He/she will be responsible for expanding and optimizing BOS data pipelines as well as supporting data analytics needs.
Core activities :
• Be responsible for supporting data analytics and dashboarding with respects to the data availability / stability.
• Understanding the data and identifying any potential data issues as well as correcting issues arising from data.
• Define and implement data models suitable for dashboarding
• Power BI dashboarding support

Qualifications
Requirement :
General Knowledge & Experience
• Minimum of 6 years of working experience in data related functions
• Understanding of banking / financial services with exposure to data analytics preferred
Programming & Data
• Strong hands-on skills in SQL & PL/SQL
• Solid background in traditional structured database environments such as Teradata / Oracle
• Knowledge on Power BI dashboard functionality
• Strong skills in Power BI related technologies like power query and DAX
• Exposure to programming languages such as python
Communication & Soft Skills
• Curiosity & a real passion for understanding ""why?""
• Pragmatic ""can do"" approach to finding data based solutions to problems
• Good communication skills with ability to engage across business and technical audiences"
29-Apr-2022 T11:49,"Data Engineer, Capability Development (DART)",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Data Engineer in GovTech’s Data Science & Artificial Intelligence Division, you will be involved in architecting, managing and running advanced analytics and data platforms. For this role, you will be part of the Data Analytics Readiness Team (DART) within GovTech, where you will help drive GovTech’s Data Transformation efforts. You will assist to establish and implement effective operational workflows, data pipelines, procedures and best practices in accordance with government infrastructure and security policies to ensure that the platforms are monitored, secure, available and reliable.

We are looking for an independent and motivated engineer who has experience in both small and large-scale analytics platform projects, and in building and maintaining the infrastructure.

What you will be working on:
• Design, Architect, Deploy, and maintain solutions on Microsoft Azure or AWS using different Cloud & Big Data Technologies to provide secure and governed access to data for business users.
• Manage the full life-cycle of a data warehouse/lakehouse solutions from requirement gathering and analysis to platform selection, design of the architecture, and deployment.
• Collaborate with data stewards, data analysts and data scientists to build data pipelines from enterprise systems such as Workday to collect, clean, harmonise, merge and consolidate data sources for data warehouse/lakehouse.

Where we are looking for:
• Diploma/Degree in Computer Science or Information Technology or related disciplines
• Experience with the cloud (e.g. AWS, GCP, Azure)
• Hands-on experience in implementing Data Lake/Data Warehouse with technologies like – Databricks, Azure Synapse Analytics, SQL Database, AWS Lake formation.
• Programming & debugging skills in Python, Scala or R.
• Proficient in SQL.
• Knowledge of both SQL and NoSQL databases.
• Comfortable with DevOps tools like AWS Cloud Formation/Terraform, Docker and Git for CI/CD development.
• Prior experience with data engineering tools and frameworks like Airflow, Kafka, Hadoop, Spark, Kubernetes.
• Familiar in building REST services is good to have.

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:49,Sr Big Data Engineer,AccionLabs,14 hours ago,,Full–time,"Role:- Sr Big Data Engineer

Job Description

· 3+ years of professional software development experience with languages and systems such as Java, Python (PySpark), and version control (git), with good analytical & debugging skills.

· Big data: You have extensive experience with data analytics, and working knowledge of big data infrastructure such as Hadoop Eco System, HDFS, Spark, and AWS or Google or Azure (nice to have). You've routinely built data pipelines with gigabytes/terabytes of data and understand the challenges of manipulating such large datasets.

· Data Modeling: Flair for data, schema, data model, PL/SQL,Star & snow flake schema, how to bring efficiency in data modeling for efficient querying data for analysis, understands criticality TDD and develops data validation techniques.

· Real Time Systems: Understands evolution of databases for in-memory, NoSQL& indexing technologies along with experience on real-time & stream processing systems like kafka, Storm.

· Project management: You demonstrate excellent project and time management skills, exposure to scrum or other agile practices in JIRA.

· CS fundamentals: You have earned at least a B.S. / MS in Computer Science, or related degree AND you have a strong ethos of continuous learning.

All your information will be kept confidential according to EEO guidelines"
29-Apr-2022 T11:49,Lead Data Engineer,Ohmyhome Pte. Ltd.,16 hours ago,,Full–time,"Job Description:

-Assemble and collect data sets that meet functional and non-functional business requirements.

-Identify, design & implement internal process improvements, automating manual processes, optimizing data delivery, infrastructure for greater scalability.

-Build the infrastructure required for optimal extraction, transformation and loading of data from a variety of sources using SQL, APIs and cloud services technologies.

-Build tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.

-Build tools for analytics and data team member that assist in building and optimizing the product into an innovative leader.

-Work with data and analytics experts to strive for greater functionality in our data lake, systems and ML/Feature Engineering for AI solutions.

-Work with cross-functional departments to understand their data needs and requirements and build tools to assist them with their analytics tools.

Requirements:

-Candidate must possess at least a Bachelor's Degree, Computer Science/Information Technology or equivalent.

-At least 7 years of data engineering work, of which 3 years are in a leadership role.

-Experience with Apache Airflow or equivalent in automating data engineering workflow.

-Experience with GCP services.

-Knowledge in Machine Learning"
29-Apr-2022 T11:49,Cloud Data Engineer for IT Data Analytics Team,Garranto Pte. Ltd.,8 hours ago,,Full–time,"Type :
Full Time / Permanent Role

Client :
Tonik Bank

Location:
Singapore

Job Description ï'·
Act as a subject matter expert in data engineering and GCP data technologies. ï'·

Closely work with various business teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
Work with Agile and DevOps techniques and implementation approaches in the delivery. ï'·

Be required to showcase your GCP Data engineering experience when communicating with business team on their requirements, turning these into technical data solutions.
ï'·
Be required to build and deliver Data solutions using GCP products and offerings.

Qualifications

Liaise and be part of our extensive GCP community, contributing in the knowledge exchange learning programme of the platform.
Any Bachelor Degree in Computer Science or related fields
Minimum 5 years of experience as a data engineer in banking environment.
Possess analytical skills mental resilience and the ability to think systematically under stressful conditions.
Highly accountable and takes ownership. Outstanding work ethic, highintegrity, team player, and a lifelong learner.
Mentor other engineers define our technical culture and help build a fast-growing team.

Skill

Hands on and deep experience working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, AI Building Blocks, Looker, Cloud Data Fusion, Dataprep, etc.).
Experience in Spark /Scala / Python/Java / Kafka.
Experience in MDM, Metadata Management, Data Quality and Data Lineage tools.
E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.
Regulatory and Compliance work in Data Management.
E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
Experience with SQL and NoSQL modern data stores.

Work on diversified cloud Platforms consisting of Databases and CICD, Logging, and monitoring tools to provide stable, and reliable DevOps service.
Hands-on experience with terraform is a plus.
Build CI/CD pipeline; both design and implementation is an added advantage.

If you are interested please send your CV to (HIDDEN TEXT"
29-Apr-2022 T11:49,Data Engineer (Platform),Crypto.com,16 hours ago,,Full–time,"Singapore, Singapore / Engineering - Exchange / Full-time About : was founded in 2016 on a simple belief: it's a basic human right for everyone to control their money, data and identity. With over 10+ million users on its platform today, provides a powerful alternative to traditional financial services, turning its vision of ""cryptocurrency in every wallet"" into reality, one customer at a time. is built on a solid foundation of security, privacy and compliance and is the first cryptocurrency company in the world to have ISO27001:2013 and PCI:DSS 3.2.1, Level 1 compliance. is headquartered in Singapore with a 4,000+ strong team. For more information, please visit . Job Responsibilities Build a universal data platform, meet real-time/offline computing and storage requirements; Define the data pipeline scheme according to the demand scenario, and deliver it according to the project demand; Continue to optimize the data platform, improve the stability of the data platform, user experience, and reduce cluster and usage costs. Job Requirement Bachelor degree or above, major in computer, software engineering, mathematics, and more than 3 years of data development experience; Familiar with java//scala, understand the working principle of jvm, and have a certain understanding of network and linux operating system; Have a deep understanding of one or more technologies in the big data ecosystem (Hadoop, Spark, Flink, Kafka, Hive), and be familiar with mysql/Postgresql/oracle; Have a certain understanding of distributed system principles, calculations, storage; Good communication and logical thinking skills, good self-drive, continuous learning and updating knowledge system; Benefits What you can expect from us? We offer an attractive compensation package working in a cutting-edge field of Fintech. - Huge responsibilities from Day 1. Be the owner of your own learning curve. - The possibilities are limitless and depend on you. - You get to work in a very dynamic environment and be part of an international team. - You will get to have involvement in developing a brand new product from scratch alongside with a talented team"
29-Apr-2022 T11:49,Data Engineer,Lilith Games Sg Pte. Ltd.,16 hours ago,,Full–time,"Big Data Development Engineer (Advertising Direction)

Big Data Development Engineer (Advertising Direction)

What you will be doing:

1ã€ Responsible for the construction of Big Data Cloud Platform and the maintenance of Advertising Data Warehouse.

2ã€ Responsible for the calculation, cleaning and layering of Advertising Data based on Flink, Spark, etc., and store it through Hadoop, Clickhouse, etc.

3ã€ Responsible for Advertising Data Analysis and Advertising System Report development.

Qualifications & Skills

1ã€ Bachelor degree or above, major in computer and other related majors.

2ã€ Proficient in Python or Golang coding.

3ã€ Proficient with Mysql, Memcache, Redis, Message Queue and other common WEB components.

4ã€ Having experience in using one or more of HDFS, Hive, HBase, MongoDB, Kafka, Flink and Spark is preferred.

5ã€ Experience in operation and maintenance development is preferred.

6ã€ Familiar with Alibaba Cloud and other cloud computing resource deployment and optimization is preferredã€'

7ã€ Proficient in coding complex SQL Statements, and have the ability and experience of query optimization.

8ã€ Positive and optimistic, strong sense of responsibility, with good team communication and cooperation

å¤§æ•°æ ®å¼€å 'å·¥ç¨‹å¸ˆï¼ˆå¹¿å'Šæ–¹å 'ï¼‰

å·¥ä½œè Œè´£

1. è´Ÿè´£å¤§æ•°æ ®äº'å¹³å °æ å»ºï¼Œå¹¿å'Šæ•°æ ®ä»'åº'çš'æ å»ºä¸Žç»´æŠ¤ã€'

2. è´Ÿè´£åŸºäºŽFlinkã€ Sparkç‰å¯¹å¹¿å'Šæ•°æ ®çš'è¿›è¡Œè®¡ç®—ã€ æ¸…æ´—ã€ åˆ†å±'ç‰å·¥ä½œã€'å¹¶é€šè¿‡Hadoopã€ Clickhouseç‰è¿›è¡Œå˜å'¨ã€'

3. è´Ÿè´£å¹¿å'Šæ•°æ ®åˆ†æž å Šå¹¿å'Šç³»ç»ŸæŠ¥è¡¨çš'å¼€å 'ã€'

ä»»è Œè¦ æ±'

1.å¤§å¦æœ¬ç§'(ç»Ÿæ‹›)å Šä»¥ä¸Šå¦åŽ†ï¼Œè®¡ç®—æœºã€ é€šä¿¡ç‰ç›¸å…³ä¸'ä¸šã€'

2.ç†Ÿç»ƒæŽŒæ ¡Pythonæˆ–Golangä»£ç ç¼–å†™ã€'

3.ç†Ÿç»ƒä½¿ç'¨Mysqlã€ Memcacheã€ Redisã€ æ¶ˆæ ¯é˜Ÿåˆ—ç‰å¸¸ç'¨WEBç»'ä»¶ã€'

4.æœ‰ä½¿ç'¨HDFS, Hive, HBase, MongoDB, Kafkaï¼ŒFlink, Sparkä¸çš'ä¸€é¡¹æˆ–å¤šé¡¹çš'ç» éªŒä¼˜å…ˆã€'

5.æœ‰è¿ ç»´å¼€å 'ç» éªŒä¼˜å…ˆã€'

6.ç†Ÿæ'‰é˜¿é‡Œäº'ç‰äº'è®¡ç®—èµ'æº éƒ¨ç½²ä¸Žä¼˜åŒ–è€…ä¼˜å…ˆ

7.ç†Ÿç»ƒç¼–å†™å¤ æ 'çš'sqlè¯å ¥ï¼Œå…·å¤‡æŸ¥è¯¢ä¼˜åŒ–çš'èƒ½åŠ›å Šç» éªŒã€'

8.ç§¯æž ä¹ è§'ï¼Œè´£ä»»å¿ƒå¼ºï¼Œå·¥ä½œè®¤çœŸç»†è‡´ï¼Œå…·æœ‰è‰¯å¥½çš'å›¢é˜Ÿæ²Ÿé€šä¸Žå ä½œèƒ½åŠ›ã€"
29-Apr-2022 T11:49,Data Engineer,Real Estate Analytics Pte. Ltd.,16 hours ago,,Full–time,"Key ResponsibilitiesMaintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable and timely delivery of key insights for decision makingcleaning and normalizing subsets of data of interest as preparatory step before deeper analysis by the data scientistsRun Modern high performance analytical databases and computation enginesDesign and implement various data health checks to ensure the data quality and consistency across systemsDesign and implement data extraction solution in a distributed systemSkills requiredExperience in handling large data sets and working with structured, unstructured and geographical datasetsUnderstanding of databases and best engineering practices - include handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding how to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipelineExperience with DevOps and AWS will be an advantageReal passion for data, new data technologies, and discovering new and interesting solutions to the company's data needsExcellent communication skills to communicate with the product development engineers to coordinate development of data pipelines, and or any new products features that can be built on top of the results of data analysis"
29-Apr-2022 T11:49,Network Data Engineer,Rma Contracts Pte. Ltd.,29 minutes ago,,Full–time,"Job Title: Network Data Engineer

Job Type: Contract - 3 Years

Location: Central, Singapore

(This is an outsourced role)

Job Responsibilities
• Full-time Level 2 'Network data engineer' to perform and handle but not limited to:
• Manage network infrastructure such as internet links, traffic shapers, routers, and switches
• Support in daily operations on incident management, problem(s) / issue(s) remediation, and service(s) restoration
• Fulfilling of service request(s) following the Change Management procedure.
• Track and assess all announcements and/or advisories (from device principal, IT Security Team, Government IT Security Incident Response (GITSIR) Team, etc. on patches on vulnerabilities, software bugs, and firmware upgrades for network devices.
• Planning and applying of devices' security patches and firmware upgrades in accordance with the severity. Preparation of monthly reports on operational issues, link performance, patch status for all network equipment.
• Create and maintain documentation of network configuration, network diagram, mapping, processes, and service records.
• Any other tasks assigned by the Institute.

Qualification and Skills for Network Data Engineer:
• Relevant Diploma or bachelor's degree in Computer Engineering (or equivalent).
• Must have minimum CCNP certification (routing & switching).
• At least 3 years of strong experience supporting a campus network infrastructure, with in-depth hands-on experience on network devices such as Cisco Nexus switches, Catalyst switches, ASR Routers, Networking Monitoring Tools, etc.
• Knowledge of network compliance is an added advantage.
• Excellent problem-solving skills in a multi-tasking, fast-paced, and complex work environment. n) Good communication skills and written skills in English, positive attitude, team player, resourceful, and resolve problems independently.

If you are looking for an opportunity and your skills align with this job, please send your updated CV/resume (in Word format) to Mounika at [HIDDEN TEXT].

All Curriculum Vitae will be treated with strict confidentiality We regret to inform you that only shortlisted candidates will be notified"
29-Apr-2022 T11:49,Senior Engineering Officer (Data Centre),Singtel Group,16 hours ago,,Full–time,"At Singtel, we believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers.

We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative.

Join us and experience what it’s like to be with an Employer of Choice*. Together, let’s create a brighter digital future for all.

Awarded at the HR Fest Awards 2020.

Key Responsibilities :

DC Operations Management
• Perform shift duty to support 24x7 DC Operations or be on 24x7x365 stand-by duties for emegency.
• Perform Incident escalation (Ensure incidents are responded and attended to, else redirect for 2nd / 3rd level resolution base on criticality, impact and SLA).
• Perform Request Fulfillment to register customer requests.
• Operate Electronic Visitation Management System (Update Authorizer and customer access request accurately and promptly).
• Perform access clearance for customers / visitors / Contractors.
• Generate daily / weekly / monthly / yearly / ad-hoc reports when required.
• Remote Hand Support (Media Management, Visual Inspection, Reboot devices, staging room, Physical connect / disconnect of HDD, network cable connectivity test, insertion of fibre SFP).
• Manage equipment movement in Data Centre.
• Ability to conduct asset tagging, labeling and tracking.
• Participate and support both the internal and external audit.

DC Facilities Management

Perform Event Monitoring for all Data Centre Facilities infrastructure to ensure Data Centre is in normal function (eg.

UPS, power, temperature control, humidity, water detection sensor, etc.).
• Ensure all DC supporting infrastructure (i.e. Environment Monitoring System (EMS), Access Control Systems, Electronic Visitation Management System etc) are functioning well.
• Update and maintain all DC related documentation.
• Coordinate with various stakeholders to fix the technical issues on time to provide timely support to customers.

Change Management

Raise Change Request for Change Management approval.

Customer Management
• Communicate with both internal and external customer via voice or email.
• Understand Customer's contractual Service Level Agreements and ensure they are met.

The ideal candidate should be / possess :
• GCE O’ Level or ITE in Engineering / IT with 1-3 years of experience in DC Operations
• Well versed in Data Centre Facilities and Operations Support as well as Data Centre Operations Best Practices
• Basic Certificate of Infocomm Technology
• Familiar with ITLL. ITIL Foundation Level Certification will be an added advantage
• Ability to multi-task and work under pressure, independently and in teams
• Good communication skills (both verbal & written)
• Analytical with good problem solving skills
• Well organized and able to reschedule priorities as circumstances change
• Positive attitude and self- motivated
• Proactive and able to act on own initiative

We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated"
29-Apr-2022 T11:49,Senior Data Engineer,Genpact Consulting (singapore) Pte. Ltd.,16 hours ago,,Full–time,"• Work closely with the Product Owners and stake holders to design and build data systems and pipelines to meet the requirements of the proposed solution.

• Play an active role in leading team meetings and workshops with clients.

• Oversee day-to-day Data Engineering team's operation and performance.

• Interact directly with clients to understand project requirements and deadlines.

• Evaluate business needs and objectives.

• Designing and implementing highly performant data ingestion pipelines from multiple sources using SQL, Python, Apache Spark and Databricks.

• Responsible for deploying codes using Git/Bitbucket as per the CI/CD process.

• Design and Build datasets on Snowflake for faster reporting.

• Design Develop Maintain ETL data pipelines.

• Analyze and organize raw data.

• Research, Diagnose, and Monitor Performance Bottlenecks, etc.

• Ensure standardization of SQL coding practices and adherence to coding standards, change control, and SQL best practices

• Prepare data for prescriptive and predictive modelling.

• Responsible for data integration and data quality

• Enabling data from different sources into ready to consume datasets.

• Performing Data Validation/Exploration tasks.

• Migrating current lake data/ external data into Exasol/Databricks (Lakehouse)

• Helping business stakeholders to visualize and analyze the customized view/data and enable better decisions without any hassle.

• Partnering and coordinating with cross functional stakeholders across timezones"
29-Apr-2022 T11:49,Data Engineer (Remote Possible),Glints,15 hours ago,,Full–time,"At Glints, we are building the #1 tech-enabled recruitment and career discovery platform in Southeast Asia that helps people and organisations realise their human potential by joining great organisations, learning the right skills and building great teams.

We’re looking for a Data Engineer to design and set up data ingestion pipelines into Data Warehouse. Transforming data into consumable dataset for stakeholders, enabling analysts and stakeholders to make critical decisions through automated dashboard reporting and flagging out dataset with poor Data Quality are all crucial scopes for Data Engineers.
What You'll Be Doing

• You will be suggesting and implementing improvement to our existing data pipeline. Day to day work include understanding the requirement of stakeholder and assisting stakeholder in beginning dataset into data warehouse in the shape for stakeholder use for dashboard building.
• Being part of the Data Engineering Team, you will also be participating in the improvement of the team operation to ensure that the work process is optimized and improved if possible, thus enabling the organisation to achieve a data-driven decision making culture.

Why You Should Join Us

• In this role, you will be exposed to many aspects of Data Warehousing and Data Pipeline to serve stakeholders requirements. Stakeholders can range from Business Analysts, Product Analysts to Data Scientists who need datasets for model training.
• As the Data Team grows, depending on your inclination, there is room to develop both technically and also in people management.

Who We Are Looking For

• Has experience working with Apache Airflow and/or Apache Kafka
• Comfortable working with Kubernetes
• Knowledge in Data Modelling, especially in Database Normalization
• Experience in writing clear, well-structured and concise documentation that promotes data discoverability
• Knowledge in SQL, Docker(-compose), Python and/or GoLang
• Experience in working in Agile/Scrum development

Let’s Realise Human Potential.

We have impacted many lives since we were founded in 2013, but there’s still plenty to be done. If you’re ready to grow and make an impact, you’ve come to the right place.

What is Glints?

Glints is an online talent recruitment and career discovery platform with the enduring purpose of contributing to a world where people and organisations can realize their human potential.

Our tech-enabled approach to recruitment and career discovery connects more than 1.5 million candidates to their next dream opportunities, has supported more than 30,000 companies in finding top talents, doubled recruiter efficiency, and has raised more than US$30M+ from best venture investors in Asia to date.

Our agility and firm hold on our core purpose and values have allowed us to remain resilient and thrive through tumultuous times, and we are proud to be recognised by LinkedIn as one of the Top 10 Startups in Singapore in 2020.

Who We Are

At Glints, personal and professional growth are just as important as business growth. That's why we created the Glints Culture Code: #RIIBCOH. It defines our values, guides our decisions and actions, and is what makes us special.

Relentlessly Resourceful: Whatever it takes, just make it happen (ethically)
Integrity: Have courage, be guided by the truth, don’t be afraid
Impact: Missionaries, not mercenaries
Beginners’ Mindset: Stay humble, don’t be attached to ego
Customer Obsessed: Customers First
Ownership: Care intensely about the mission and take responsibility
High Standards: Dream big and deliver epic outcomes fast

Where We Work

Glints operates in multiple locations across Greater Southeast Asia, including Singapore, Indonesia, Vietnam, Taiwan and Malaysia.

Learn more about Glints and our culture at bit.ly/glintsculture,
Or check out our Careers Page at https://glints.com/careers"
29-Apr-2022 T11:49,Data Engineer,Morgan McKinley,7 hours ago,,Full–time,"Data Engineer

Job Summary
• Singapore
• Permanent
• BBBH811650
• Apr 05, 2022
• Competitive Job Description
Our client is looking for an experienced Data engineer to drive development of information products for data and digital transformation across their group.

Mandatory Skills:
• Master of Business Administrations or master's in quantitative fields (Computer Science, Statistics or similar) with minimum of 5 - 12 years of overall experience.
• Experience programming in Python, Java, SQL, PLSQL.
• Experience with traditional RDBMS based systems and more modern NoSQL technology stacks.
• Expertise building ETL and data pipelines on Databricks.
• Experience working with Big Data technologies such as Hadoop, Cloudera (CDH), Hortonworks (HDP), DataBricks, Spark, Delta, HDFS, HBase, Hive.
• Experience in event streaming with Kafka.
• Experience in ML Model Productionization, Docker.
• Real Time, Batch, Unstructured Data, DW, MDM, Data Marts.
• Proficient in using data visualization tool such as Tableau, Power BI, D3, AmCharts etc.
• Understanding FS industry fundamentals and business problems to find new ways to leverage data.
• Intellectual curiosity to solve data driven problems.
• Independent thoughts and unique ideas on solving business problems.
• Able to collaborate / virtually manage multi-cultural, multi-disciplined, globally dispersed teams.
• Sound understanding of functional areas and analytics problems in areas of Customer, Marketing, Channel, Pricing, Digital, Operations, HR/Corporate analytics.
• String communication skills to be able to explain highly technical problems in simple layman form.
• Ability to articulate the impact of decisions and recommend improvements. Desired Skills:
• Relevant experience in Banking and financial institutions. Job Responsibilities:
• Exceptional data engineering & visualization experience & technical skills. State-of-the-art expertise across, data/information preparation and data insight & visualization using BI (or similar tools).
• Be a data engineering & visualization technical expert. Lead and explain/educate to all levels people in these areas, Coding Databases, Data Integration, Frameworks, Deployment, Architectures and Visualization.
• Contribute to the development of in-house data products. Use your data engineering & visualization expertise plus FS domain & functional knowledge, problem solving skills and independent thinking to create in-house data analytics and data management products.
• Be a team player & an individual contributor. Work with group data office and other business function people as part of larger deliveries, as well as being able to work independently or in small teams to continuously deliver business value.
• Be a trusted partner of our client. Someone that anyone in the company can reach out to for help with creating data engineering & visualization driven business transformation. Those who are keen for the role and would like to discuss the opportunity further, please click ""Apply Now"" or email Alimpan at amukherjee@morganmckinley.com with your updated CV.

Only shortlisted candidates will be responded to, therefore if you do not receive a response within 14 days please accept this as notification that you have not been shortlisted.

Alimpan Mukherjee

Morgan McKinley Pte Ltd

EA Licence No: 11C5502

EA Registration Number: R2198629"
29-Apr-2022 T11:49,Data Engineer,Lilith Games Sg Pte. Ltd.,16 hours ago,,Full–time,"Big Data Development Engineer (Data Center)

What you will be doing:

1. Plan, design, develop and implement core data system in the business field

2. Develop and maintain real-time/ offline data processing tasks

3. Build industry-leading distributed systems such as storage and computing to provide a reliable infrastructure for massive data and large-scale business systems

4. Develop various data services, such as data developing platform, quality monitoring system,

Qualifications & Skills

1. BSc in Computer Science or related major.

2. At least 3 years of big data platform R&D.

3. Familiar with common algorithms and data structures, and be proficient in Java / Python / Scala / shell language (at least two)

4. Familiar with deployment and implementation of docker and k8s related containers.

5. Experienced working with big data technologies such as Hadoop, Flink, Hive, Spark, etc.

6. Enthusiastic about learning new technologies and persistent in pursuit of high concurrency and distributed architecture design.

7. Have good team communication and collaboration skills.

å¤§æ•°æ ®å¼€å 'å·¥ç¨‹å¸ˆï¼ˆæ•°æ ®ä¸å °æ–¹å 'ï¼‰

å·¥ä½œè Œè´£ï¼š

1.å 'ä¸Žä¸šåŠ¡é¢†åŸŸæ¸å¿ƒæ•°æ ®ä½'ç³»çš'è§'åˆ'è®¾è®¡ä»¥å Šå¼€å 'è ½åœ°

2.å 'ä¸Žå®žæ—¶/ç¦»çº¿æ•°æ ®åŠå·¥ä»»åŠ¡çš'å¼€å 'ä¸Žç»´æŠ¤

3.å 'ä¸Žå¤§æ•°æ ®åŸºç¡€æž¶æž'çš'å¼€å 'ä¸Žè¿ä»£

3.å 'ä¸Žå 'ç±»æ•°æ ®æœ åŠ¡å¼€å 'ï¼ŒåŒ…å «ä½†ä¸ é™ äºŽï¼šæ•°æ ®å¹³å °å¼€å 'ï¼Œæ•°æ ®è´¨é‡ ç›'æŽ§ï¼ŒBIç‰

å²—ä½ è¦ æ±'ï¼š

1. æœ¬ç§'å Šä»¥ä¸Šå¦åŽ†ï¼Œ3å¹´ä»¥ä¸Šå¤§æ•°æ ®å¹³å °ç'å 'ç» éªŒã€'

2. å…·æœ‰æ‰Žå®žçš'ç¼–ç¨‹åŠŸåº•ï¼Œç†Ÿæ'‰å¸¸ç'¨çš'ç®—æ³•å'Œæ•°æ ®ç»'æž'ï¼Œç²¾é€šJava/Python/Scala/Shellè¯è¨€ï¼ˆè‡³å°'ä¸¤ç§ ï¼‰

3. ç†Ÿæ'‰Hadoopç‰å¤§æ•°æ ®æ¡†æž¶ï¼Œæœ‰Flinkã€ Sparkç‰ç›¸å…³å®žæ—¶æµ è®¡ç®—æ¡†æž¶ç» éªŒä¼˜å…ˆ

4. ç†Ÿæ'‰dockerã€ k8sç›¸å…³å®¹å™¨çš'éƒ¨ç½²å®žæ–½

5. å¯¹æ–°æŠ€æœ¯ä¿ æŒ å¥½å¥‡å¿ƒï¼Œå¯¹é«˜å¹¶å 'å'Œåˆ†å¸ƒå¼ æž¶æž'æœ‰æ‰§ç €çš'è¿½æ±"
29-Apr-2022 T11:49,Senior Data Engineer,Newtone consulting,15 hours ago,,Full–time,"Job Description & Requirements

The successful candidate will provide data services for CIB as part of the Cybersecurity datalake application. You will be part of the team responsible for governance, quality, remediation and manages services across data standardisation, analytics, archiving, reporting, dashboards and data management and production support.

Role and Responsibilities:
• Troubleshoot Production issues following a ‘Follow The Sun principle’ [APAC/EMEA/NAR] support model
• Monitoring and proactive support
• Application maintenance and upgrades
• Automation of BAU tasks to improve efficiency
• Strong contribution to Data Analytics, including support in the development of custom add-ins for data collection & analysis
• Suggest improvements and provide guidance and support to the IT Security team to help them improve their monitoring & analytics capabilities

Candidate profile:
• 5 years of experience of IT Production and/or BigData
• Practical knowledge of performance and capacity management from a BigData perspective as well as strong aptitude for automation.
• Strong working knowledge of Linux (RedHat/Ubuntu)
• Strong working knowledge of Elastic stack (Elasticsearch / Logstash / Kibana / Beats) including data ingestion, management, monitoring & analytics
• Experience with Kafka (or similar experience in messaging broker software e.g. Rabbit MQ, ActiveMQ)
• Programming skills (Python or Ruby or Java)
• Experience & skills in automation tools (e.g. Ansible) & DevOps pipelines are appreciated"
29-Apr-2022 T11:49,Senior Platform and Data Engineer,Ensign Infosecurity (cybersecurity) Pte. Ltd.,29 minutes ago,,Full–time,"Duties and Responsibilities
• Familiar with Ensign's business domain and objectives to develop and deploy solutions that meet internal and customer requirements
• Responsible for the design, build and administration of multi-node data lakes, and data warehouses and data marts
• Responsible for health monitoring, solve cluster issues, patching and upgrades
• Responsible for node administration, load balancing with add/remove/recovery of nodes
• Ensure cluster stability, smooth upgrade releases and solving platform issues by investigating and applying solutions/patches
• Deliver detailed documentation and ensure quality throughout project lifecycle

Requirements
• Bachelor's degree in Computer Science/Information Systems/Computer Engineering
• Minimum 3 years of hands-on experience on cluster installation, deployment, upgrade, maintenance, troubleshooting various cluster issues and optimizing for better performance
• Hands-on experience managing multi-node big data application platforms

 - HDFS, YARN, Spark, Hive, Presto, Apache Airflow
• Hands-on experience managing multi-node messaging queue platforms

 - Kafka, Rabbit MQ, Nifi
• Hands-on experience managing multi-node containerization and orchestration platforms

 - Docker, Docker Swarm, Kubernetes
• Hands-on experience managing multi-node data warehousing platforms

 - MongoDB, MySQL, PostgreSQL, ElasticSearch
• Hands-on experience on setting up 3rd party applications

 - GitLab, Arkime
• Hands-on experience on setting up security features

- Kerberos, AD, LDAP, Keycloak
• Strong awareness of data security, data governance and performance, with an ability to deliver these key non-functional requirements"
29-Apr-2022 T11:49,Data Engineer Intern,Tencent,20 hours ago,,Internship,"Responsibilities
• Work with cross-functional teams and understand how data platform are developed and maintained at scale.
• Build scalable data pipelines (using Spark, Airflow and Presto) to move data from different applications into our data warehouse.
• Monitor and improve automated solutions to ensure quality and performance SLAs are met.
• Maintain and support existing platforms and evolve to newer technology stacks and architectures.

Qualifications
• Currently pursuing a Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.
• Coding & scripting proficiency in languages such as Python, C++, Golang.
• SQL knowledge in handling volumes of data and performance.

Bonus
• Passion in gaming.
• Scratch-build a highly scalable, available, fault-tolerant data processing systems using cloud technologies, HDFS, YARN, Map-Reduce, Hive, Kafka, Spark, and other big data technologies."
29-Apr-2022 T11:49,AI Data Engineer,Osim International Pte. Ltd.,16 hours ago,,Full–time,"The Digital Technology Department in OSIM works together as a team to ensure that the final product is programmed to meet the needs and wants of the consumer, whilst, incorporating the latest and most desirable technology to remain competitive in the market. This department comprises of Product Owners, UI/UX, Mobile Engineers, Backend Engineers, IOT as well as the Digital QA group. If you are forward-looking, able to think out of the box, and always hungry to learn about the newest technology, we want you on our team!Job ResponsibilitiesSet up and manage our AI development and production infrastructure.Build data ingest and data transformation infrastructure.Build AI models from scratch and help product managers and stakeholders understand results.Deploy AI models into production.Collaborate with Product Owner, mobile engineer and test engineer on projectImprove the AI models to accepted accuracyCreate API to fetch the output from the AI modelsCreate and maintain optimal data pipeline architectureAssemble large, complex data sets that meet business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimising data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Product Owner to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.Work with data and analytics experts to strive for greater functionality in our data systems.RequirementsAt least 1 year AI relevant experience and at least 1 year of experience in a Data Engineer related roleExperience working on Machine learning basic algorithm (e.g. decision tree)Proficient with database management and analytic tools and languages supporting data analysis and reporting - SQL, Python and ExcelExperience of using GIT command lineProactive and accurate communication skill and team workGood understanding of Data Structures and AlgorithmsGood understanding of programming skills, python with Tensorflow or PyTorchAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databasesExperience building and optimizing 'big data' data pipelines, architectures and data setsStrong analytic skills related to working with unstructured datasetsBuild processes supporting data transformation, data structures, metadata, dependency and workload managementExperience supporting and working with cross-functional teams in a dynamic environment(Plus point) Graduate degree in Computer Science, Statistics, Informatics, Information Systems(Plus point) Experience with AWS cloud services: EC2, EMR, RDS, Redshift(Plus point) Experience with object-oriented/object function scripting languages: Python(Plus point) Building user data center before is a plus(Plus point) Having AWS data certification is a plus(Plus Point) Experience working on NLP (e.g. voice recognition)(Plus Point) Experience working on computer vision (e.g. face recognition with CNN)(Plus Point) Experience of working in an Agile (scrum) team(Plus Point) Experience of cloud service e.g. AWS, Azure or Tencent services(Plus Point) Mandarin speaking to work with our Greater China development team"
29-Apr-2022 T11:49,Data Engineer,Eastspring Investments (singapore) Limited,16 hours ago,,Full–time,"At Eastspring Investments, we understand that success comes from the talent and commitment of our people

We strive to build a business that you can shape, an inclusive workplace where everyone's ideas are valued and a culture where we can thrive together

Our people stay connected and tuned in to what's happening around us, keeping us ahead of the curve

While focused on the long-term, we look to the future to bring growth, development and benefit to everyone whose lives we touch.

Key Accountabilities
Design and implement our Data and AI central data platform as well as related tools/systems for advanced business analytics and enterprise data governance
Manage data modeling design, writing, and optimizing ETL jobs
Participate in building and enhancing enterprise cloud data warehouse
Deliver and manage in-house and cloud-native data solutions to meet business requirements across firm-wide business units
Assist in creating and monitoring analytics dashboards, for different business functions
Ensure quality, integrity and accuracy of datasets through tracked, secured and auditable controls
Work with stakeholders to assist with data-related technical issues and support their data needs.
Follow and enforce best practices in software development and data engineering
Requirements:
Excellent coding skills with Python and SQL, and solid understanding of object-oriented analysis and design
Working knowledge of common algorithms and data structures, with strong analytical and problem-solving skills
Hands-on experience with Linux and shell scripting
Working experience with containerization (Docker/K8S) and task orchestration tools (Airflow/Luigi, etc.)
Experience with cloud service and tools (AWS/Azure/GCP), as well as cloud data warehouse platforms.
Experience with modern DevOps practices including version control, TDD, CI/CD, etc., for both code and configuration changes.
Basic understanding and experience with ML/AI concepts (e.g

deep learning, deep reinforcement learning, deep bayesian learning), workflows, and toolsets (Jupyter Notebook, etc.), and libraries (Numpy, Pandas, Scikit Learn, PyTorch, etc.) preferably in both cloud-native and desktop deployments.
Experience with traditional RDBMS based systems, including Data Lake, Data Warehouses and Marts, and more modern NoSQL and cloud-native big-data technology stacks such as document-oriented databases, Hadoop, columnar data files (e.g., Parquet), etc.
Familiar with REST APIs, service-oriented architectures (SOA) / microservices, virtualization, and serverless deployment architectures.
Demonstrated ability to understand, work with and deliver robust solutions in more than one programming language, framework, technology stack, runtime environment, etc..
QUALIFICATIONS / EXPERIENCE:
Degree level or higher in Computer Science or another quantitative field
5-10 year(s) technical experience showing increasing sophistication of solutions implemented, and ability to deliver
Fluency in both written and spoken English"
29-Apr-2022 T11:49,Data Engineer,Manpower Singapore,16 hours ago,,Full–time,"Data Engineer

Key Role and Responsibilities:
Partner with the global and in-market data experts to discover and derive value from connecting external and internal data sources.

Build (ETL) new and evolve data models and pipelines to power algorithmic based Business Intelligent solutions that addresses business problems requiring descriptive, diagnostic, predictive, and/or prescriptive analytics for pricing, promotion, trade spending, assortment, and sales performance management.

Translates the algorithms and analytic models into data models as business needs evolves on a going basis after they are put into production.

Develop a roadmap that scales existing and new data models, to support the portfolio of solutions.

Expand into Business Intelligence solution development focused on automation and scale up of solutions.

Key Skills Required:

Professional Skills:
R/Python programming languages
MS Power BI
MS Excel
Data Visualisation

General Skills:
Client Management
Project Management
Diploma or B.S. in a relevant technical field (Operations Research, Computer Science, Statistics, Business Analytics, Econometrics, or Mathematics).

Overall experience of 2-3 years

Ability to manipulate and high-volume of data from varying sources

Expert knowledge of an analysis tool such as Microsoft PowerBI

Proficiency in R/Python

Basic track record in working independently with minimal guidance

Interested applicants, please submit your resume to : [ Email address blocked ]
Abelene Marianne Kang Mrs Abelene Marianne Rozario R2089914
EA License No: 02C3423 Reg No: 199505951H

Kang Abelene Marianne Mrs Rozario Abelene Marianne

EA License No.: 02C3423 | Personnel Reg No.: R2089914

Please note that your response to this advertisement and communications with us pursuant to this advertisement will constitute informed consent to the collection, use and/or disclosure of personal data by ManpowerGroup Singapore for the purpose of carrying out its business, in compliance with the relevant provisions of the Personal Data Protection Act 2012. To learn more about ManpowerGroup's Global Privacy Policy, please visit [ Link removed ]

Recommended Skills

Algorithms

Analytical

Automation

Business Analysis

Business Intelligence

Business Requirements"
29-Apr-2022 T11:49,"Vp/avp, Data, Ai&ml Engineer - Financial Planning, Investment",Techgig,16 hours ago,,Full–time,"About Job
CTC Undisclosed Job Location Singapore Experience 0 - 3 yrs
Description
Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation. In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.
The Role
This is a unique opportunity to drive the data engineering, apply AI/ML and delivery of a multi-year greenfield program, to re-imagine our Financial Planning journeys and capabilities.
Job Purpose
We are looking for a dynamic Data Engineering Lead that can build a world class team that ships high quality production code daily. As an engineering lead that manage data engineer team, youll be responsible for design data pipeline, understand data from various sources, make sense of data and build data-mart that serves customers need every day.
Background we are looking for:
You love solving problems and building solutions.
You believe in Agile development and putting the customer first in anything you design; experience with SCRUM is a plus.
Youre self-motivated and have a demonstrated track record of self-learning and going above and beyond.
Youre a hands-on leader that regularly codes.
You understand the importance of thoughtful feedback, nurturing an inclusive engineering environment, championing engineering fundamentals and providing clarity and mentorship for team members to grow.
You will help design and build scalable and performant metadata driven complex data pipelines, data quality frameworks etc using the tools and methodologies adopted by the bank from time to time.
Experience in Bigdata, information retrieval ,Data mining ,AI &ML. Collaborate with multiple AI & ML product teams to understand their data and compute requirements.
Responsible to Ingest data from files, databases and well versed with streaming tools like Spark streaming/Kafka.
Process the data with Hadoop, Spark ,Pyspark, Python/Scala and ML.
Strong Debugging skills in Java/Python/Scala and Hive or Spark SQL programming languages.
Experience with RDBMS and NoSQL databases like MySQL, Maria DB, HBase.
Perform troubleshooting, analysis, and performance tuning for customers using the data warehouse, SQL engine, and data analytical platforms.
Perform root cause analysis, including implementation of hardened measures that avoid and/or mitigate impact to users in case of similar or related issues.
Actively participate in all phases of software development lifecycle: analysis, technical design, planning, development, testing/CICD, release, post production/escalation support
Strong leadership and management skills with proven stakeholder management experience.
Excellent interpersonal, verbal, and written communication skills"
29-Apr-2022 T11:49,"VP, Data Engineering Development Team Lead, Middle Office Technology...",DBS Bank Limited,12 hours ago,,Full–time,"VP, Data Engineering Development Team Lead, Middle Office Technology, Technology & Operations

Business Function
Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation. In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.
Responsibilities

• Creating complex, enterprise-transforming applications for competitive advantage
• Working with the latest tools and techniques
• Hands-on coding, code review, software architecture design
• Building & Leading highly collaborative teams to deliver good quality solutions
• Furnishing deep Financial Process and reporting domain knowledge and working closely with business stakeholders Requirements

• Proficient in at least one major programming language in data engineering space (Scala, Java)
• Proficient with SQL (MySql, PostgreSQL, Oracle, MariaDB)
• Deep understanding and Hands-on development experience on Big data toolsets such as Spark, Hadoop, Kafka and etc
• Hands-on development experience of large-scale ETL in big data eco systems
• Hands-on development experience in handling large volume of data in big data eco systems. Deep knowledge in big data process monitoring, tuning skills
• Good knowledge on code quality tools, testing automation
• Knowledgeable on release automation like Jenkin pipelines
• Knowledge about building microservices
• Knowledge on Cloud infrastructures such as Kubernetes, Pivotal cloud foundry, Docker etc
• A Bachelor's degree or higher preferably in Computer Science or IT Essential Traits:

• Obsessed with the elegance of codes
• Passionate about the latest technologies and willing to learn and share knowledge
• Comfortable with dealing with legacy codebase Apply Now
We offer a competitive salary and benefits package and the professional advantages of a dynamic environment that supports your development and recognises your achievements"
29-Apr-2022 T11:49,"Data Engineer, Quantitative Strategy",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) seeks to transform the delivery of Government Digital Services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with the public to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

Team Introduction:

The Data Science & Artificial Intelligence Division (DSAID) works with public sector agencies in using data science and AI to improve policy outcomes, service delivery and operational efficiency. We extract data-driven insights and build intelligent platforms to add value to the work of our partner agencies.  We also help public sector agencies transform by partnering them in building data science expertise, formulating data strategies and setting up the necessary data infrastructure.

 

How do we work:

 Outcome Driven - Our projects are not academic exercises. We are driven by the “so what” and make sure that our findings and models can be translated into tangible impact.

Start Small and Move Fast - We build things quickly. If it works, good — how can we scale this up further? If not, what went wrong and what can we do better next time?

Ownership - You are not just here to write code, but also to figure out what we should be building and how we should build it.

Continuous Learning - Working on new ideas often means not fully understanding what you are working on. Taking time to learn new architectures, frameworks, technologies, and even languages are not just encouraged but essential.

We are in this Together - We draw from the deep domain knowledge of our partners and best practices from our community of experts.

Read more about us from the team's blog https://medium.com/dsaid-govtech

Job Overview:

We are seeking a Data Engineer to join our Quantitative Strategy team, to be sited with one of our Agency Data Science Teams.

 

As a Data Engineer, you will be part of a data team deployed at a government agency identified as a strategic partner for DSAID, with the mandate to drive the growth of agency’s data analytics capabilities while operating in close alignment to DSAID's approach and philosophy. Your main role is to build Whole-of-Government data infrastructure at the partner agency, to power insights needed for evidence-based decision-making and enhancing the agency’s service delivery.

 

This role requires an individual with experience in both on-prem and cloud-based data engineering work, as well as good communication skills, as you will be expected to interact frequently with the partner agency's users to elicit useful business information that enables you to perform your job.

What you will be working on:
• Translate data requirements from business users and data scientists into technical data modelling specifications.
• Interview business users and system owners to elicit information relating to their data infrastructure, data assets, data policies, and use cases.
• Collaborate with partner agency’s IT teams on the following tasks:
• Propose and build ingestion pipelines to collect, clean, harmonise, merge, and consolidate data sources, whether on-prem or in cloud;
• Integrate and collate data sources with data systems;
• Day-to-day monitoring of databases and ETL systems, e.g., database capacity planning and maintenance, monitoring, and performance tuning; diagnose issues and deploy measures to prevent recurrence; ensure maximum database uptime;
• Construct, test, and update useful and reusable data models, with reference to consolidated business insights obtained from users, to serve the data science team and partner agency's needs;
• Propose and implement appropriate cloud data infrastructure in support of the end-to-end analytics deployment lifecycle, taking into account networking between cloud data infrastructure and any on-prem data centres;
• Design and build API gateways to expose data to systems via secure means.
• Research and develop new technologies and approaches for building highly available data persistence systems.
• Advice and support your team on data engineering matters.
• Own and participate in AWS data cloud migration projects (if applicable).

What we are looking for:
• A Bachelor’s Degree, preferably in Computer Science, Software Engineering, Information Technology, or related disciplines.
• Deep understanding of system design, data structure and algorithms, data modelling, data access, and data storage.
• Proficiency in writing SQL for databases such as Postgres, MSSQL, MongoDB, neo4j.
• Demonstrated ability in using cloud technologies such as AWS, Azure, and Google Cloud.
• Experience with data engineering tools and frameworks such as Airflow, Kafka, Hadoop, Spark, Kubernetes.
• Experience in benchmarking, clustering, and tuning the databases for performance, reliability.
• Experience in designing, building, and maintaining batch and real-time data pipelines.
• Experience in automation development, batch, shell, python.
• Familiarity with regular expressions and scripting languages such as bash, korn, awk.
• Familiarity with building and using CI/CD pipelines for platform development.
• Familiarity with DevOps tools such as Docker, Git, Terraform.
• Familiarity with LDAP, OAuth, API gateways.

Preferred requirements:
• Knowledge of IT infrastructure
• Experience with AWS RDS / Spark / other AWS Data Services
• Experience with installation, management, upgrades, backup and restore MSSQL DB
• Working knowledge of SSIS or comparable ETL tools
• Familiarity with government systems and government's policies relating to data governance, data management, data infrastructure, and data security

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round.
• Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:49,"Data Engineer, Condition Monitoring",SMRT Corporation Ltd,16 hours ago,,Full–time,"Job Purpose

The Data Engineer, Condition Monitoring will develop maintenance decision-making tools for improving train reliability and maintenance efficiency. The shortlisted candidate will instrument railway assets with condition-monitoring systems for data acquisition, apply engineering fundamentals and data analytics techniques to develop asset condition assessment models. This position will work closely with the engineering and maintenance divisions of SMRT.

Coding Assessment

A 30-60 min live coding assessment will be conducted during the interview. The candidate is expected to demonstrate ability to write code, in language and IDE of their choice, to complete simple data manipulation tasks and demonstrate knowledge of web development.

Responsibilities

The duties and responsibilities for Data Engineer, Condition Monitoring encompasses both data acquisition and data processing responsibilities. The list is not comprehensive and related duties and responsibilities may be assigned from time to time.

Collaborate with internal stakeholders to understand their business needs.

Understand the failure modes of the critical assets and identify appropriate parameters to monitor.

The data acquisition tasks include:
- Instrumenting critical assets with sensors to acquire asset condition data, and
- Managing and optimizing sensor data collection and transfer.

Develop automated data processing pipeline and software.

Develop simple web-based user interfaces and data visualizations.

Write testable and maintainable code and documentation for production.

Qualifications & Work Experience

Qualifications and work experience include:

Degree in Science, Technology, Engineering or Mathematics (STEM)

Minimum 2 years of experience with data acquisition, developing software or web applications

Basic knowledge of electrical circuits and fundamentals

Skills

Technical skills include:

Willing to learn and is passionate about programming, sensor instrumentation, and solution integration

Have prior experience to a high-level programming language for data analytics, such as Python or MATLAB

Experience with front-end web development (HTML, CSS, JS)

Experience with SQL or NoSQL databases (e.g., MySQL, MongoDB, etc.)

Optional: Knowledge of Microsoft Azure cloud services is highly advantageous

Optional: Knowledge of machine learning would be highly advantageous

Generic skills include:

Strong inclination and eager for continual learning and development

Critical thinking and problem-solving skills

Ability to understand and explain technical concepts and findings to non-technical stakeholders

Ability to think independently and actively propose solutions to the team

SMRT Trains Ltd was incorporated in 1987 and operates Singapore’s first mass rapid transit system. Today, we manage and operate train services on the North-South Line, East-West Line, the Circle Line, the Thomson-East Coast Line, and the Bukit Panjang Light Rail Transit. With over 5,000 employees, more than 250 trains, and 141 km of rail tracks across 108 stations, we serve millions of commuters daily"
29-Apr-2022 T11:49,Software/Data Engineer#TeSA #CLT,Aida Technologies Pte. Ltd.,8 hours ago,,Full–time,"Training Programme

This Company-Led Training (CLT) programme is a 9 to 12 months on the job training with Aida Technologies Pte Ltd to equip fresh professionals with industry skills to become an Artifical Intelligence / Machine learning (AI/ML) Engineer.
Each trainee will be trained and mentored in one or more of the following areas:

AI and ML applications
Data Management process
Machine learning, deep learning techniques and tools
Delpu AI/ML as a data product and/or data service
Minimum Entry Requirements
Singapore citizen
Diploma/Degree in Computer Science or Engineering.
Fresh and Mid-level professionals are welcome
Application Process

Interested individuals can apply directly with Aida Technologies Pte Ltd. Please email your CV to (HIDDEN TEXT)."
29-Apr-2022 T11:49,Senior Data Engineer Various seniority IT Central Up to SGD mth,Integrity Partners Pte. Ltd.,16 hours ago,,Full–time,"My client, a premier international software development company is looking out for a Senior Data Engineer to join their team.The incumbent will be supporting the product team, data analysts and business team on data related projects or initiatives.Job Responsibilities:• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and 'big data' technologies• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needsJob Requirements:• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases• 5+ years of experience in a Senior Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement• Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores• Experience with one or more object-oriented/object function scripting languages: Python, Ruby, Scala or equivalent• Experience with big data tools Hadoop, Spark, Kafka, Hive, etc. or equivalent• Experience with relational SQL and NoSQL databasesHow to ApplyInterested applicants may apply by sending in your updated résumé (in MS Word format) to my email or apply directly: Consultant: Tan Jun Jie EA personnel reg. no.R1878852 EA License No. 17C8502"
29-Apr-2022 T11:49,Cloud Data Engineer for IT Data Analytics Team,GARRANTO PTE. LTD.,16 hours ago,,Full–time,"Type : Full Time / Permanent Role

Client : Tonik Bank

Location: Singapore

Job Description 

Act as a subject matter expert in data engineering and GCP data technologies. 

Closely work with various business teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.

Work with Agile and DevOps techniques and implementation approaches in the delivery. 

Be required to showcase your GCP Data engineering experience when communicating with business team on their requirements, turning these into technical data solutions. 

Be required to build and deliver Data solutions using GCP products and offerings.

Qualifications

Liaise and be part of our extensive GCP community, contributing in the knowledge exchange learning programme of the platform.

Any Bachelor Degree in Computer Science or related fields

Minimum 5 years of experience as a data engineer in banking environment.

Possess analytical skills mental resilience and the ability to think systematically under stressful conditions.

Highly accountable and takes ownership. Outstanding work ethic, highintegrity, team player, and a lifelong learner.

Mentor other engineers define our technical culture and help build a fast-growing team.

Skill

Hands on and deep experience working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, AI Building Blocks, Looker, Cloud Data Fusion, Dataprep, etc.).

Experience in Spark /Scala / Python/Java / Kafka.

Experience in MDM, Metadata Management, Data Quality and Data Lineage tools.

E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.

Regulatory and Compliance work in Data Management.

E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.

Experience with SQL and NoSQL modern data stores.

Work on diversified cloud Platforms consisting of Databases and CICD, Logging, and monitoring tools to provide stable, and reliable DevOps service.

Hands-on experience with terraform is a plus.

Build CI/CD pipeline; both design and implementation is an added advantage.

If you are interested please send your CV to [ Email address blocked ]

Recommended Skills

Agile Methodology

Analytical

Apache Kafka

Apache Spark

Artificial Intelligence

Banking Services"
29-Apr-2022 T11:49,System Admin & Data Engineer,BioMind,16 hours ago,,Full–time,"Singapore

more than 1 year

college degree and above

Work content

maintain and design the company’s website

system administration

commission and decommission laptops/servers

onboarding and assisting engineers to set up development environments

assist DevOps team to maintain development pipelines and infrastructure

installation of company software

supporting non-technical staff in basic technical tasks

generate purchase orders for equipment

Job Requirements

Degree in Computer Science, Computer/Electrical Engineering or equivalent

Linux system administration

Bash, Python, or equivalent scripting languages

AWS, Github, Jenkins, Aliyun, cloud, ubuntu/centos, firewall, mac, network

Website design (Wordpress)"
29-Apr-2022 T11:49,Data Engineer/Scientist,Patsnap Pte. Ltd.,8 hours ago,,Full–time,"Company Introduction

PatSnap is a software company helping R&D leaders maximise the value of innovation intelligence within their R&D workflow and strategic planning.

As the global leaders in connected innovation intelligence, Patsnap use AI-powered and machine learning technology to comb through billions of datasets, and help innovators connect the dots.
Job description

We are looking for a Data Engineer to join the revolution to help us improve various business outcomes and drive innovation.
You will join a multidisciplinary team helping to shape our Product development. This is an excellent opportunity to take advantage of emerging trends and technologies to a real-world difference.

Responsibilities
Study and transform various kind of structured and unstructured data
Big data analytics
Create data pipeline for regular data updates
Analysing unstructured data using basic NLP techniques and extract important fields
Store data in databases (nosql and graphdb)
Develop NLP systems according to requirements
Perform statistical analysis of results and refine models
Remain updated in the rapidly changing field of machine learning
Deploy models and create APIs
Requirements
Proven experience as Data Engineer or similar role
Experience with big data analytics libraries such as pyspark is must
Experience with AWS and database technologies is plus
Strong communication skills
An analytical mind with problem-solving abilities
Degree in Computer Science, Mathematics, Computational Linguistics or similar field
Domain knowledge of Material sciences and/or chemistry is big plus"
29-Apr-2022 T11:49,Data Engineer,Tookitaki Holding Pte. Ltd.,16 hours ago,,Full–time,"Introducing TookitakiA leading Regtech company, Tookitaki has developed advanced machine learning-powered solutions in risk and compliance to help the banking and financial services (BFS) industry achieve sustainability in their compliance programs. Our offerings are deployed in production across global reputed financial institutions.Incorporated in November 2014 in Singapore, the company is led by a core team with cumulative 150-years' experience in financial crime, AI and big data analytics. Tookitaki's client portfolio expands across AsiaPacific, North America and Europe markets, including a Japanese multinational investment bank, a large European bank, a leading Southeast Asian bank and such other global reputed financial institutions.Backed by institutional investors such as Jungle Ventures, Viola Fintech, Illuminate Financial and Enterprise Singapore (a subsidiary of the Singapore Government), the company's accolades include:We won the first place in the MAS FinTech Awards (Singapore SME) in the regulatory compliance space from the Monetary Authority of Singapore for our approach to make the workflows in AML and Reconciliation scalable and highly auditable (beyond ML based black box approach)We are accredited by IMDA as Innovative Tech Company in 2017 and 2019We are among the 56 growth-stage Companies from around the world recognised by World Economic Forum as a Technology Pioneers 2019We are the winner in the Asian Private Banker Technology Awards 2019 for the Best AML/CTF SolutionWe bagged the AI Award for the banking category at Singapore Business Review's inaugural Technology Excellence Awards 2019We won the Most Promising Innovation by the SG:D Techblazer Awards 2019 – Jointly organised by Singapore Digital (SG:D), IMDA and SGTECHWe have participated and won in various Client Innovation Engagement Programs – Second Runner-up of the UBS Future of Finance Challenge, First Runner-up of the FinTech Challenge Vietnam 2019, Alumni of ING Fintech Village Cohort 2019 and many moreIn regulatory compliance, we focus on anti-money laundering and reconciliation and our products – Anti-Money Laundering Suite (AMLS) and Reconciliation Suite (RS) – cater to these areas, respectively.Today, regulatory compliance processes have become more complex and fluidic, increasing the chances for rule-based models to fail. Banks need to move beyond static rule-based systems and adopt a new approach to improve efficiency, effectiveness at optimal cost, ensuring sustainable compliance programs across the BFS industry. Tookitaki bridges the gap with its innovative software products – AMLS and RS.JOB ROLE: DATA ENGINEERTookitaki is looking for a Data Engineer who is familiar with the Hadoop platform and is able to design, implement and maintain optimal data/machine learning (ML) pipelines in the platform.The following are the main responsibilities of the role:Designing and implementing fine-tuned production ready data/ML pipelines in Hadoop platform.Driving optimization, testing and tooling to improve quality.Reviewing and approving high level & detailed design to ensure that the solution delivers to the business needs and align to the data & analytics architecture principles and roadmap.Understanding business requirement and solution design to develop and implement solutions that adhere to big data architectural guidelines and address business requirements.Following proper SDLC (Code review, sprint process).Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, etc.Building robust and scalable data infrastructure (both batch processing and real-time) to support needs from internal and external usersUnderstanding various data security standards and using secure data security tools to apply and adhere to the required data controls for user access in Hadoop platform.Supporting and contributing to development guidelines and standards for data ingestionWorking with data scientist and business analytics team to assist in data ingestion and data related technical issues.Designing and documenting the development & deployment flow.RequirementsExperience in developing rest API services using one of the Scala frameworksAbility to troubleshoot and optimize complex queries on the Spark platformExpert in building and optimizing 'big data' data/ML pipelines, architectures and data setsKnowledge in modelling unstructured to structured data design.Experience in Big Data access and storage techniques.Experience in doing cost estimation based on the design and development.Excellent debugging skills for the technical stack mentioned above which even includes analyzing server logs and application logs.Highly organized, self-motivated, proactive, and ability to propose best design solutions.Good time management and multitasking skills to work to deadlines by workingindependently and as a part of a team.Ability to analyse and understand complex problems.Ability to explain technical information in business terms.Ability to communicate clearly and effectively, both verbally and in writing.Strong in user requirements gathering, maintenance and supportExcellent understanding of Agile Methodology.Good experience in Data Architecture, Data Modelling, Data Security.Experience -Must have:Scala: Minimum 2 years of experienceSpark: Minimum 2 years of experienceHadoop: Minimum 2 years of experience (Security, Spark on yarn, Architectural knowledge)Hbase: Minimum 2 years of experienceHive - Minimum 2 years of experienceRDBMS (MySql / Postgres / Maria) - Minimum 2 years of experienceCI/CD Minimum 1 year of experienceQualificationsBachelor's degree in IT, Computer Science, Software Engineering, Business Analytics or equivalent with at-least 2 years of experience in big data systems such as Hadoop as well as cloud-based solutions.Job PerksAttractive variable compensation packageFlexible working hours - everything is results-orientedOpportunity to work with an award-winning organization in the hottest space in tech artificial intelligence and advanced machine learning"
29-Apr-2022 T11:49,BIG DATA ENGINEER,Technopals Pte. Ltd.,16 hours ago,,Full–time,"Job Description & RequirementsJob Scope:Design and implement relevant data models in the form of data marts stored in Operational Data Stores, Data Warehouses or Big Data platformsBuild data pipelines to bring information from source systems, harmonise and cleanse data to support analytics initiatives for core business metrics and performance trends.Perform data profiling to understand data quality and advise practical measures to address such data issues through data transformation and data loadingDive into company data to identify sources and features that will drive business objectives.Work closely with project manager and technical leads to provide regular status reporting and support them to refine issues/problem statements and propose/evaluate relevant analytics solutionsBring your experience and ideas to effective and innovative engineering, design, and strategyWork in interdisciplinary teams that combine technical, business and data science competencies that deliver work in waterfall or agile software development lifecycle methodologiesThe range of accountability, responsibility and autonomy will depend on your experience and seniority, including:Contributing to our internal networks and special interest groups Mentoring to upskill peers and juniorsJob Requirements:Diploma / Degree in Computer Science / Computer Engineering / Information Technology related field, or IT equivalent.Minimum of 3 years' experience in building large scale enterprise data pipelines using commercial and/or open-source data management tools from vendors such as Informatica, Talend, Microsoft, IBM or OracleStrong knowledge of data manipulation languages such as SQL necessary to build and maintain complex queries and data pipelinesPractical appreciation of data q/quality metrics and remediation strategiesData modelling and architecting skills including strong foundation in data warehousing concepts, data normalisation, and dimensional data modelling such as OLAPUndergraduate or graduate degree in Computer science or equivalentPossess good communications skills to understand our customers' core business objectives and build end-to-end data centric solutions to address themGood critical thinking and problem-solving abilitiesGood to Have:Experience with other aspects of data management such as data governance, metadata management, archival, data lifecycle managementProcessing of semi-structured and unstructured data sets such as NoSQL, graph and Hadoop based data storage technologies such as MongoDB, Cassandra, HBase, Hortonworks/Cloudera, Elastic Search and Neo4j using Spark, Splunk or Apache Nifi for batch or streaming dataLarge scale data loading experience moving enterprise or operational data from source systems to new applications or data analytics solutionsExperience in leveraging on cloud-based data analytics platform such as:o AWS serverless architecture in Lambda on AWS DynamoDB, EMR Redshifto Azure Data Factory or SQL Data Warehouseo GCP BigQuery/BigTable, Cloud Dataprep/Dataflow/Dataproc"
29-Apr-2022 T11:49,Lead Data Engineer,Ohmyhome Pte. Ltd.,16 hours ago,,Full–time,"Job Description:

-Assemble and collect data sets that meet functional and non-functional business requirements.

-Identify, design & implement internal process improvements, automating manual processes, optimizing data delivery, infrastructure for greater scalability.

-Build the infrastructure required for optimal extraction, transformation and loading of data from a variety of sources using SQL, APIs and cloud services technologies.

-Build tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.

-Build tools for analytics and data team member that assist in building and optimizing the product into an innovative leader.

-Work with data and analytics experts to strive for greater functionality in our data lake, systems and ML/Feature Engineering for AI solutions.

-Work with cross-functional departments to understand their data needs and requirements and build tools to assist them with their analytics tools.

Requirements:

-Candidate must possess at least a Bachelor's Degree, Computer Science/Information Technology or equivalent.

-At least 7 years of data engineering work, of which 3 years are in a leadership role.

-Experience with Apache Airflow or equivalent in automating data engineering workflow.

-Experience with GCP services.

-Knowledge in Machine Learning"
29-Apr-2022 T11:49,Backend Software Engineer - TikTok Live (Data Platform) - Singapore,ByteDance,21 hours ago,,Full–time,"Responsibilities

About us: TikTok is a true phenomenal app in the world. Our mission is to help live hosts connect users and inspire user interaction in real-time around the world. We are looking for passionate and talented engineers to join us to build up and optimize a real-time, high-performance, large-scale distributed infrastructure for live streaming at TikTok. We will be deeply involved in the developmental lifecycle of critical product features, and collaborate closely with product managers to deliver the best live streaming experience for live hosts and users. TikTok Live Data Platform team is a fast-growing team in TikTok engineering family. We are building solid data capabilities and enhancing the robust data ecosystem for TikTok Live, and aiming to maximize data value to benefit our hosts & users and power TikTok. You will:
- Plan and lead large-scale technical projects to lay the foundation for the iterative development and scale of early products
- Develop robust efficient technology products that serve 1 billion users
- Contribute to engineering strategy, tooling, processes, and culture
- Research and apply cutting-edge domain and technical knowledge into products

Qualifications

- As a world-class engineer, you have rich working experience in scalable, highly available, distributed and mission-critical systems.
- Deep understanding of computer architectures, data structures and algorithms.
- Able to work closely with diverse stakeholders and have good communication skills
- Self-driven, positive, cooperative and willing to keep learning enthusiasm at all times
- Experience in the payment or financial domain is a plus TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace.

At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too"
29-Apr-2022 T11:49,Data Engineer,Brenntag Asia Pacific Pte. Ltd.,16 hours ago,,Full–time,"At DigiB, we combine cutting-edge start-up environment with resources of a global company to make game – changing decision and drive digital transformation in the chemical industry. We design tools that help our customers, employees and suppliers to do their job in their best possible and efficient way.

We are developing Digital Products for our customers & partners. We will utilise Technology in a smart way to enable improved processes and reduce pains. We will touch on areas in Customer Service, Technology, Data, Supply Chain and Finance to name a few.

An idea of what you will do:

You will be responsible for building, maintaining & managing the cloud infrastructure

You will be responsible to setup & manage services connected to cloud infrastructure – including but not limited to AD, SSL Certificates, VPNs, Mail Servers, API configurations

You will design and deliver the production support management tools & services – Monitoring, Alerting, Capacity & Performance management

You will be responsible for setting up and managing log analysis & security monitoring analysis

You will be responsible for investigating & resolving infrastructure issues and collaborate with technology providers to resolve in a timely manner

You will administer & control access to all infrastructure services & tools

You will maintain application database & system logs backups and perform restores as needed by team

You will collaborate with IT security officers to ensure platform access & data is secured

You will be responsible for infrastructure failover assessments & disaster recovery drills

You will collaborate with technology services providers to proactively implement preventive measures and provide RCA report of infra issues

You will maintain complete inventory of all infrastructure resources

You will publish weekly reports of infrastructure usage, availability, risks and issues

You will support technology manager to assess platform reliability, robustness, security vulnerability and comparisons of cloud services and management tools

You will manage the Application code deployments, including defining and implementing any automated solutions for deployments

You will assist with the design and implementation of the continuous integration, automated testing and continuous deployment initiative

Who you are and your experience:

You have a degree or equivalent in computing/engineering from a reputable institution or relevant work experience

You have working experience building & maintaining modern technology platforms

You have hands on experience in enterprise systems & network architecture, cloud infrastructure i.e. Azure

You are familiar with enterprise systems like ERP, CRM, ECM, BPM & DWH systems

You have experience in collaborating with regional/global technology teams and diverse technology & service providers

You are able to work unsupervised & can make informed decisions

You are a hands-on team player and lead by example

You thrive in an unstructured and extremely agile environment

You have excellent communication and interpersonal skills to establish and build sustainable internal and external relationships

You should have a Growth mindset

Your Technical Skills:

You MUST have deep expertise in one of the Cloud technologies e.g. AWS, Azure

Must have hands on experience managing VMs, Containers using Unix shells (SSH)

Must have experience in setting up networking infrastructure, DNS, DMZ, VPNs, bastion, mail servers & API Gateways

You should have experience in one or more of the following API / integration services - WSO2 API, BPM, Mulesoft, TIBCO ESB

You should have experience using message queues / data streaming using Kafka, RabbitMQ, IBM MQ etc.

You should have experience with monitoring & Log analysis tools such as Grafana, Splunk etc

You have sound knowledge of Java technology stack , Spring Framework and Data Adapters

You have experience using micro-services deployment and Dockers management

You have DBA experience managing SQL & No-SQL databases and data management (PostgreSQL, Mongo, Redis)

You are familiar with API messaging specification JSON, YAML & REST

You are familiar with data engineering and supporting tools e.g. Hadoop, Spark, Kafka, Qlik

Familiarity with SAP data stream integration or similar enterprise solution"
29-Apr-2022 T11:49,[LTA-RAOM] ANALYST ENGINEER (DATA & SYS. STRATEGY/DATA STD. & INFO./DATA...,Land Transport Authority (LTA) Singapore,6 hours ago,,Full–time,"You will be part of the Digital asset management team, to manage the development of an integrated 4.0 Asset Management System and applications to deliver data-centric insights for railway assets including its performance, condition and usage, which will support the Digital asset management functions.

You will work closely with teams from different discipline within LTA, rail operators, contractors as well as system manufacturers on the interfaces and integration with various systems in the rail network. You are also required to review technical submissions and participate in the installation, testing and commissioning for the system and interfaces.

Requirements
• Tertiary qualification in Electrical, Electronics Engineering, Computer Science, IT, Mechanical, Sustainable Infrastructure Engineering or equivalent
• Strong writing, presentation, communication and interpersonal skills
• Team player with critical and logical thinking capability and must be able to work independently
• Experience in IT network and infrastructure with good knowledge of servers, data storage implementation, IT application, cloud technologies and big data technologies will be an advantage for data networking and integration
• Those with relevant working experience in railway/industrial system design, or engineering management, or operation and maintenance will be considered for senior positions
• Please select your preferred position and indicate in your resume – => 1. Data & Systems Strategy / 2. Data Standards & Information / 3. Data Networking & Integration"
29-Apr-2022 T11:49,Software Engineer - (Data Engineering),Goldman Sachs,16 hours ago,,Full–time,"OUR IMPACT

Seeking a data focused software engineer, to design and build highly scalable and resilient data solutions that will drive client insights, analytics and engagement. Liaise with business teams across the firm to understand client activities and create accurate and high quality data models for them. Drive towards consistent and easily accessible client information by developing to rigorous curation and quality standards.

Our team design, build, and operate the firm's Legend data platform in the AWS and GCP clouds. Legend is a data management and data governance platform that provides both engineers and non-engineers a single solution to develop data-centric applications and derive data-driven insights. Legend automates some of the most difficult data governance challenges and provides self-service tools to democratize data and analytics. It is available as an open source platform through GitHub ready to be used by our clients and the world fully open and free of charge. As a member of the Data Engineering team, you will be able to work on this open sourced platform which not only brings real, tangible value for Goldman Sachs but also our peers and clients as well as greater standardization and efficiency across the entire financial industry.

HOW YOU WILL FULFILL YOUR POTENTIAL
• Design & develop modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies
• Evaluate, select and acquire new internal & external data sets that contribute to business decision making
• Engineer streaming data processing pipelines
• Drive adoption of Cloud technology for data processing and warehousing
• Engage with data consumers and producers in order to design appropriate models to suit all needs

SKILLS AND EXPERIENCE WE ARE LOOKING FOR
• 5+ years of relevant work experience in a team-focused environment
• A Bachelor or Master degree in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)
• Strong object-oriented design and programming skills and experience in OO languages
• Experience or interest in functional programming languages
• Extensive experience in Java (Preferred), Python, C++, C#, Objective-C, or other OO languages
• Proven experience applying domain driven design to build complex business applications
• Deep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes
• In-depth knowledge of relational and columnar SQL databases, including database design
• General knowledge of business processes, data flows and the quantitative models that generate or consume data
• Excellent communications skills and the ability to work with subject matter experts to extract critical business concepts
• Independent thinker, willing to engage, challenge or learn
• Ability to stay commercially focused and to always push for quantifiable commercial impact
• Strong work ethic, a sense of ownership and urgency
• Strong analytical and problem solving skills
• Ability to collaborate effectively across global teams and communicate complex ideas in a simple manner

Preferred Qualifications
• Financial Services industry experience
• Experience in distributed system design
• Working knowledge of open-source tools such as Kafka, Spark

ABOUT GOLDMAN SACHS
At Goldman Sachs, we commit our people, capital and ideas to help our clients, shareholders and the communities we serve to grow. Founded in 1869, we are a leading global investment banking, securities and investment management firm. Headquartered in New York, we maintain offices around the world.
We believe who you are makes you better at what you do. We're committed to fostering and advancing diversity and inclusion in our own workplace and beyond by ensuring every individual within our firm has a number of opportunities to grow professionally and personally, from our training and development opportunities and firmwide networks to benefits, wellness and personal finance offerings and mindfulness programs. Learn more about our culture, benefits, and people at GS.com/careers.
We’re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https://www.goldmansachs.com/careers/footer/disability-statement.html

© The Goldman Sachs Group, Inc., 2020. All rights reserved.
Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Veteran/Sexual Orientation/Gender Identity"
29-Apr-2022 T11:49,"Senior Associate, Machine Learning Engineer, Group Consumer Banking and Big...",Dbs Bank Ltd.,8 hours ago,,Full–time,"Business Function

Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation.

In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.
Responsibilities

Apply cutting edge technologies and tools in big data and machine learning to build, manage and automate pipelines for machine learning models and analytics platform.

Implement machine learning algorithms and build production grade end-to-end analytics solution to solve business challenges together with Data Scientists and business team.
Implement and apply industry standard Auto ML solutions to solve real world challenges.
Create blueprint and reference architecture for various machine learning use cases.

Study and evaluate the state-of-the-art technologies, tools, and frameworks of machine learning engineering, and establish, apply and maintain best practices and principles of machine learning.
Perform code optimization, code reviews to improve the quality of Data Scientist's work.
Automate software testing by writing unit tests for all production code.
Monitor machine learning model releases, branches for different issues and user stories.
Manage available resources such as hardware, data, and personnel so deadlines are met.
Work with different stakeholders to ensure that the bank's production release process is adhered to.
Requirements
Bachelor's or master's degree in software Engineering, Computer Science or related fields.

At least 2 years of experience in data mining and machine learning on large amount of data, and multi-tier software application development and DevOps automation.
Excellent understanding of software engineering principles and design patterns.
Familiar with tools such as JIRA, Git and Jenkins.

Excellent programming skills in Python (Pandas, NumPy), SQL, bash and willingness to learn other languages as required by the project.
Experience with Spark, Hadoop and how to optimize Spark jobs that analyse huge amounts of data for better performance.
Experience with Kubernetes, PCF and services in AWS to deploy ML models as REST APIs.

Familiar with industry paradigms and standards for model development, validation and testing and have developed and implemented large-scaled industrial standard machine learning solutions from end to end.
Strong in problem-solving, being resourceful with end to end critical thinking to find out solutions even in unfamiliar scenarios.
Good communication and project management skills.
Strong interests in learning about recent developments in machine learning through own initiatives.
Apply Now

We offer a competitive salary and benefits package and the professional advantages of a dynamic environment that supports your development and recognises your achievements"
29-Apr-2022 T11:49,"Facilities Engineer (Data Centre, office hours)",Randstad Singapore,6 hours ago,,Full–time,"About The Job

This is a company whose expertise has made them the leader in their field is seeking a high-calibre Facilities Engineer in Singapore to support the data centre operations in Singapore.

The Facilities Engineer will play a crucial role in the company’s continued development and success in Singapore. Your key purpose will be to oversee the execution of the local data operations and manage the engineering operations and core infrastructure. You will be responsible for the incident reporting, vendor management, change management, documentation of operational procedures as well as the management of the day to day operations. This is a rare opportunity for a hands-on individual to be part of a growing organization that provides a stimulation work environment.

To Be Successful In The Role, You Would Have

skills and experience required.
• Ideally have at least 5 years experience in relevant Facilities Management within Data Center Operations
• Relevant qualifications in Engineering or Mechanical Engineering or Electrical Engineering are preferred.
• Have an understanding of construction, commissioning and operation of mission critical systems.
• Able to work shift hours is a must
• Great communication skills are necessary this is a client facing role.
• Ability to work independently and make logical decisions

How To Apply

To apply online, please click on the ‘apply’ function below.

Please indicate your availability, expected salary, and reason for leaving your current job in your CV.

EA: 94C3609 / R1767516

Applicants must be fully vaccinated or have a valid exemption in accordance with MOM’s regulations to allow them to enter the workplace. Applicants may be required to share verifiable COVID-19 vaccination documents or proof of a valid exemption at the point of offer. Randstad Pte. Limited and/or the Client reserves the right to withdraw an offer if the applicant fails to provide verifiable COVID-19 vaccination and/or proof of exemption documents"
29-Apr-2022 T11:49,Data Engineer,MANN+HUMMEL,16 hours ago,,Full–time,"Your challenge **Please indicate in your resume, the position you are applying for ** Working cross-functionally with business managers/product managers/engineers and data scientists to gather requirements and to understand their business processes Design, develop, deploy, manage scalable cloud infrastructures that solve business problems Designing, implementing, and managing optimal data analytics pipelines from end-to-end Making strategic data architecture recommendations Implementing data science frameworks to enable organization-wide experiments, enable data science research and development and enable deployment of production solutions Implementing systems that enable delivery of insights to business units and customers Apply dev-ops processes to deliver and maintain production level systems Apply quality processes to ensure the requirements are met Implement and manage security in accordance with industry standards Document all aspects of design, implement, test and release Your profile As a successful applicant, you would have a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field with the ability to manage stakeholders and communicate well. You will have strong experience in cloud technologies (AWS/Azure or similar) with at least 5 years of experience in a Data Engineer role. A history of working with large scale reliable data systems Proficient with cloud technologies and application of native services Experience with big data tools and delivery of big data solutions Experience in working with different types of data stores including SQL, NoSQL, warehouses, lakes, etc Experience working in Hadoop ecosystem and Spark is a plus Experience with data ingestion and data integration tools and frameworks, data pipeline and workflow management, common data science tools such as R, python, Big data technologies, Tableau Proven ability to deliver high profile activities to tight timescales Proven ability to apply analytical and creative thought Ability and desire to learn and pick up new tools and technologies for problem solving, enhancing analysis results and accuracy, and optimizing workflow efficiency Independent and possess creative problem solving skills to address business problems from different perspectives Ability to distil and communicate results to all organisational levels Practice a lean agile scrum process to continuously deliver value to customers Proven success in contributing to a team-oriented environment Ability to interact with global teams and manage the related cultural challenges Be able to interact with development teams to determine project requirements Are you full of ideas? Are you keen to take on responsibility and really achieve something? Then our doors are open to you. This is a company that lives out its values, gives people the freedom to use their own initiative, offers many development opportunities and many exciting projects – all of which awaits you here. Interested? Fill out the form below to apply. The fields marked with an asterisk (*) are required fields. Similar job offers"
29-Apr-2022 T11:49,Data Engineer Trust Safety,Tencent,16 hours ago,,Full–time,"工作职责 ·Technical research, development and operation in the field of anti-cheat/anti-Trojan horse for overseas PC and mobile games.·Responsible for the research, design and implementation of technical countermeasures by following the security compliance.·Communication and service management in overseas game anti-cheat work with other corporations. 工作要求 ·Bachelor degree or above, with data analysis/algorithm related experience·Good in data analysis capability; Experience in at least one of these areas is an advantage: reverse engineering, vulnerability analysis, data analysis/algorithms, security countermeasure.·Proficient in at least one programming language (Python, C/C++, Go), experience in data analysis implementation for security is an advantage·Passionate in games, capable to understand games logic ·Experience in game anti-cheat operation is an advantage"
29-Apr-2022 T11:49,Senior Data Engineer,ExpressVPN,8 hours ago,,Full–time,"About The Data Engineering Team

Let’s talk about data. Do you have experience building and maintaining data warehouses with big data technologies? Can you build data-intensive applications with Python, Java, or Scala? Have you managed cloud-native big data environments? If any or all of this applies to you, you may be just the Senior Data Engineer we’re looking for to join our fast-growing team. With opportunities in fast-growing London office, we’re hiring Data Engineers that are an essential conduit to other engineering teams across our consumer-facing company, as well as our Data Insights team.

Our team acts as a central nexus to connect various data producers with consumers across the company. Our customers are:
• Other engineering teams across the company that produce or consume data that need to be combined with other data sources.
• Analysts on the Data Insights team.

We Are Accountable For Delivering
• A centralized data warehouse that enables engineers and analysts across the company to ingest, anonymize, and enrich with other data sources from anywhere else in the company, persist, analyze, purge, and otherwise process their data.
• Tools, training, and coordination.
• Data applications that don’t fall into any one business unit, or where the business units don’t have sufficient capabilities themselves. For example, we team up with the data-insights team to build and operate churn-prediction models used by both humans and other systems at scale.
• Data Catalog for documenting the sources of data and what is available for use by other teams.

Our Responsibilities Include
• Building and operating the data platform service, including defining and tracking its SLA.
• Guiding various engineering teams to design models and schemas of the data to be fed into the platform, making sure they can be processed in a scalable way and used by analysts efficiently.
• Guiding data analysts on the use of the data platform.
• Building libraries/modules and reference implementations of data ingesters on several common tech stacks.
• Guarding user privacy. While all teams are responsible for ensuring compliance of their work with our privacy policy, our team also has a veto right against processing any data that might not be compliant.
• Partnering with other teams on projects to build data engineering solutions, such as for churn-prediction, payment fraud management, and other company-wide challenges.

Other Notes About Our Team
• Our tech stack currently mostly focuses on AWS Redshift, Google BigQuery, Apache Airflow and Tableau, but we imagine it will evolve significantly over time.
• We have an ever-expanding range of engineering roles on the team, covering people with backgrounds in software development, infrastructure operations, and data science.

Job Responsibilities

Your Responsibilities Will Include
• Understand the needs of your internal customers, and convert them to optimized and maintainable tech designs.
• Use your data engineering skills to design and build the ingestion, processing, storage and consumption system for data to enable other business units to make business and operational decisions using data.
• Maintain and operate the data platform, which many business units rely on to fulfill their service level targets.

Role Requirements
• At least 2 years of experience designing and operating data pipelines and databases
• Proficiency in Python, Java, or Scala with a good understanding of runtime complexities
• Proficiency in database operation and optimization, including SQL optimization
• Strong understanding of and experience in big data tools such as Hadoop, Spark, Flink, Storm
• Experience in testing ETL pipelines
• Experience in building and operating data applications in cloud environments (AWS, Azure or GCP)
• Experience in automation tools like Ansible and Terraform is a big plus
• Strong written and verbal English communication skills

Hiring process

When it comes to hiring processes, “rigorous” and “opaque” are often mistakenly conflated. For us, it’s always a mutual exchange, so we think it’s important that candidates have a clear understanding of the process and what we’re looking for. Learn more about the hiring process by visiting our careers page.

Benefits

Health and happiness go hand in hand, and we make every effort to support our team members in all facets of their lives—both inside and outside the office. Learn more about our employee benefits by visiting our careers page.

Before you apply
• At the moment, we do not sponsor visas in the UK and the EU. For Hong Kong, we require at least two years of working experience and a university degree in a related field. For Singapore, we can only sponsor visas for mid-career or above.
• Please upload your resume as a PDF and do not include any salary or compensation information in it.

ExpressVPN is one of the world’s leading providers of online privacy and security services for consumers. Started in 2009, we’ve grown to have millions of active paying customers, a team of more than 700 people worldwide, and a brand recognized by hundreds of millions of people in 18 languages and more than a hundred countries. We see huge growth in our industry, and are gaining market share through strong execution"
29-Apr-2022 T11:49,"Machine Learning Engineer, Risk Data Mining",ByteDance,21 hours ago,,Full–time,"Responsibilities

TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo. The Business Risk Integrated Control (BRIC) team is missioned to:
- Protect ByteDance users, including and beyond content consumers, creators, advertisers;
- Secure platform health and community experience authenticity;
- Build infrastructures, platforms and technologies, as well as to collaborate with many cross-functional teams and stakeholders. The BRIC team works to minimize the damage of inauthentic behaviors on ByteDance platforms (e.g. TikTok, CapCut, Resso, Lark), covering multiple classical and novel community and business risk areas such as account integrity, engagement authenticity, anti spam, API abuse, growth fraud, live streaming security and financial safety (ads or e-commerce), etc. In this team you'll have a unique opportunity to have first-hand exposure to the strategy of the company in key security initiatives, especially in building scalable and robust, intelligent and privacy-safe, secure and product-friendly systems and solutions. Our challenges are not some regular day-to-day technical puzzles -
- You'll be part of a team that's developing novel solutions to first-seen challenges of a non-stop evolvement of a phenomenal product eco-system. The work needs to be fast, transferrable, while still down to the ground to making quick and solid differences.

Responsibilities
- Build machine learning solutions to respond to and mitigate business risks in ByteDance products/platforms. Such risks include and are not limited to abusive accounts, fake engagements, spammy redirection, scraping, fraud, etc.
- Improve modeling infrastructures, labels, features and algorithms towards robustness, automation and generalization, reduce modeling and operational load on risk adversaries and new product/risk ramping-ups.
- Uplevel risk machine learning excellence on privacy/compliance, interpretability, risk perception and analysis.

Qualifications

Qualifications
- Master or above degree in computer science, statistics, or other relevant, machine-learning-heavy majors.
- Solid engineering skills. Proficiency in at least two of: Linux, Hadoop, Hive, Spark, Storm.
- Strong machine learning background. Proficiency or publications in modern machine learning theories and applications such as deep neural nets, transfer/multi-task learning, reinforcement learning, time series or graph unsupervised learning.
- Ability to think critically, objectively, rationally. Reason and communicate in result-oriented, data-driven manner. High autonomy"
29-Apr-2022 T11:49,Data Engineer,HYPEBEAST,16 hours ago,,Full–time,"Responsibilities:Identify, define, and compile disparate data sources (local, cloud, 3rd party) for ingestion in data pipelines.Design, implement, and maintain data governance, dictionaries, and models.Develop and implement data pipelines & infrastructure for Hypebeast; such as building data lakes and warehouses in Google Cloud Platform (GCP), developing views with MYSQL/Postgre, implementing functional data marts using BigQuery, and visualisations via Tableau/Data Studio.Collaborate with data analyst to generate actionable insights based on your soon-to-be intimate understanding of our data.Requirements:2 years of managing databases or building data pipelines on the cloud.Proficient in SQL and relational databases. Proficient in at least one scripting language, preferably in Python or R, to clean, transform, and denormalise datasets.Some knowledge of visualisation tools such as Tableau, Data Studio, or Power BI.Experience with GCP infrastructure and customer analytics is a plus.Love learning and sharing new technical knowledge, willing to take new challenges.Possess strong sense of ownership, adaptive to change.Recent graduates with passion to develop career in Data are welcome. If you think you’ve got what it takes, please provide your cover letter, CV and expected salary.This role is located and based in Singapore. Candidate must be eligible to work in Singapore.Personal data collected is for recruitment purpose only"
29-Apr-2022 T11:49,Technical Data Engineer,OCBC Bank,16 hours ago,,Full–time,"Description

Bank of Singapore is currently looking for a qualified candidate to assist the Data Engineering Team’s operational and analytical needs. The Technical Data Enginer will work alongside Data Analytics and big data platform (Hadoop) and engineering team to provide data related support, data ingestion, data interface for unstructured/structred data, data analytics and data management. This person will also assist in building interfaces from various upstream systems and ingest the data into Micrsoft SQL server 2019 / Cloudera Hadoop data store and build the enterprise visulasation tool. This is a great opportunity for someone who is interested in innovative group with the possibility of tremendous career development in data engineering, big data management, data analytics and enterprise data visulsaisation tool.

A little more about this role:

As our Technical Data Enginer, you will be instrumental in big data coding and work in Hadoop-ecosystem. This is a brand-new position at Data Competency vertical.

Perform extensive unstructured data ingestion into Hadoop

StronG knowledge of Anaconda, Data visualisation BI tool, Python, SPARK, Java Scala, HIVE and Beeline with hands on experience

Ability to organize and lead meetings with business and operational data owners

Experience in integrating data processes with architecture requirements used across company

Understand Hadoop-ecosystem and Data Engineering activities as well as loading data from several disparate datasets and documentation

Strong ability to troubleshoot and resolve data issues

Analytical skill to perform data profiling and data visulization

Experience in Agile and Waterfall frameworkWork

Work closely with engineering and operations to document business processes

Work independently and with team members to understand database structure and business processes

Help form data management and governance processes within the data engineering team

Qualifications

What you’ll need to have:

Graduate degree in statistics, math, computer science, physics or other technical related fields; Master’s degree is preferred

Minimum of 10 years working experience in technical data analysis, data science, or data warehousing with proven business analysis experience

Experience in at least one or more languages: SPARK, Java Scala,Python

Experience writing Java Scala, Python

Experience with Hadoop

Hands on expierence or knowledge of minimum one mainstream cloud infrastructures:AWS,MS Azure and GCP; ablity to implement data lake.

Good to have Hands-on experienceon the Hadoop, MangoDB,SPARK, Scala, HIVE, Kafka ,Beelin…etc

Excellent communication skills

Passionate about data and analyzing business needs

Previous experience on a data team in an agile environment preferred

Hands-on experience on the Hadoop ecosystem, HDFS, Hadoop, Spark, Scala preferred

Develop in-depth plans and major milestones that must be approved by top management during the planning and design phases of the project"
29-Apr-2022 T11:49,Senior Data Engineer - Kafka,Xendit,5 hours ago,,Full–time,"Xendit provides payment infrastructure across Southeast Asia, with a focus on Indonesia and the Philippines. We process payments, power marketplaces, disburse payroll and loans, provide KYC solutions, prevent fraud, and help businesses grow exponentially. We serve our customers by providing a suite of world-class APIs, eCommerce platform integrations, and easy to use applications for individual entrepreneurs, SMEs, and enterprises alike.

Our main focus is building the most advanced payment rails for Southeast Asia, with a clear goal in mind — to make payments across in SEA simple, secure and easy for everyone. We serve thousands of businesses ranging from SMEs to multinational enterprises, and process millions of transactions monthly. We’ve been growing rapidly since our inception in 2015, onboarding hundreds of new customers every month, and backed by global top-10 VCs. We’re proud to be featured on among the fastest growing companies by Y-Combinator.

Mission

We are scaling the data engineering team that works on providing internal teams with secure, reliable, performative, and user-friendly access to data to support analytics & reporting, fraud detection, billing generation, and other company critical processes.

We are looking for a Senior Data Engineer who will be a key part of our team and will help bring structure to vast amounts of data, making it digestible and build scalable data platforms that enable data products, business analytics, and data science. This role requires technical expertise and willingness to learn a wide variety of technologies to develop real-time data pipelines, data product APIs, and modern & scalable data infrastructure. If you are interested in working in a fast-paced environment and like being challenged with fun data problems to solve, come join us.

Responsibilities
• Simplify access to real-time data for internal stakeholders (Kafka).
• Design and implement our real-time data pipelines using Kafka.
• Improve and maintain the data lake setup (S3, EMR, Presto).
• Set up fully automated alerting systems to proactively monitor the infrastructure (Datadog, Pagerduty).
• Continuously improve the overall performance of analytical queries.
• Build scalable data infrastructure using Terraform.
• Build customer-facing data products with a focus on reliability and scalability.
• Ensure data quality through automated audits.
• Collaborate with analysts, engineers, and business users to design solutions.
• Research innovative technologies and make continuous improvements.

You may be a good fit if
• 5+ years of relevant industry experience.
• Excellent knowledge of Python and SQL.
• Demonstrated ability to build high-volume data ingestion and streaming pipelines (e.g. Kafka, AWS Kinesis, Spark Streaming)
• Working experience with big data technologies (e.g. Spark, Presto, Hive).
• Experience designing, building, and scaling a production-ready event streaming system.
• Experience in optimizing SQL queries (e.g. data partitioning, bucketing, indexing)
• Experience in building data lake/warehouse solutions consisting of structured and unstructured data.
• Experience with IaC is a plus (e.g. Terraform, CloudFormation)
• Bachelor's degree in a technical field or equivalent work experience.
• You have built data products that have scaled on AWS or another cloud.
• You thrive on nimble, lean, fast-paced startups, like autonomy, and have proven you can push towards a goal by yourself.
• Coachable. Able to own mistakes, reflect, and take feedback with maturity and a willingness to improve.
• You communicate with clarity and precision and you are able to effectively present results"
29-Apr-2022 T11:49,Senior Data Engineer Remote APAC,Shopify,16 hours ago,,Full–time,"Job Description

Our Data Engineering group builds and maintains the platform that delivers accessible data to power decision-making at Shopify for millions of merchants. We’re hiring high-impact developers across teams:
• The Engine group organizes all merchant and Shopify data into our data lake in highly-optimized formats for fast query processing, and maintaining the security and quality of our datasets.
• The Analytics group leverages the Engine primitives to build and deliver simple and useful products that power scalable transformation of data at Shopify in batch, streaming, or for machine learning. This group is focused on making it really simple for our users to answer three questions: What happened in the past? What is happening now? And, what will happen in the future?
• The Data Experiences group builds end-user experiences for experimentation, data discovery, and business intelligence reporting.
• The Reliability group operates the data platform in a consistent and reliable manner. They build tools for other teams on Data Platform to leverage and encourage consistency as they champion reliability across the platform.

Qualifications
• An experienced technical leader with a proven track record of delivering impactful results.
• Technical engineering background in one or more areas in the next section.
• Experience with technical mentoring, coaching, and improving the technical output of the people around you.
• Exceptional communication skills and ability to translate technical concepts into easy to understand language for our stakeholders.
• Excitement for working with a remote team; you value collaborating on problems, asking questions, delivering feedback, and supporting others in their goals whether they are in your vicinity or entire cities apart.

A Senior Data Developer at Shopify typically has 4-6 years of experience in one or more of the following areas:
• Working with the internals of a distributed compute engine (Spark, Presto, DBT, or Flink/Beam)
• Query optimization, resource allocation and management, and data lake performance (Presto, SQL)
• Cloud infrastructure (Google Cloud, Kubernetes, Terraform)
• Security products and methods (Apache Ranger, Apache Knox, OAuth, IAM, Kerberos)
• Deploying and scaling ML solutions using open-source frameworks (MLFlow, TFX, H2O, etc.)
• Building full-stack applications (Ruby/Rails, React, TypeScript)
• Background and practical experience in statistics and/or computational mathematics (Bayesian and Frequentist approaches, NumPy, PyMC3, etc.)
• Modern Big-Data storage technologies (Iceberg, Hudi, Delta)

Additional Information

#LI-REMOTE

Shopify is now permanently remote, and we’re working towards a future that is digital by design. That location you see above? Consider it merely an example of hundreds of potential locations Shopify is hiring. Learn more here: https://www.shopify.com/careers/work-anywhere

Our belief is that a strong commitment to diversity & inclusion enables us to truly make commerce better for everyone. We encourage applications from Indigenous peoples, racialized people, people with disabilities, people from gender and sexually diverse communities, and/or people with intersectional identities. Please take a look at our Sustainability Reports to learn more about Shopify’s commitments to our communities, and our planet.

At Shopify, we understand that experience comes in many forms. We’re dedicated to adding new perspectives to the team - so if your experience is this close to what we’re looking for, please consider applying"
29-Apr-2022 T11:49,Data Engineer,fccsingapore,16 hours ago,,Full–time,"Job Description
On behalf of one of its clients in the R&D sector, the French Chamber of Commerce is looking for a
Data Engineer
Mission

We are looking for a savvy data engineer who is expertise in building data pipeline, wrangling data, designing database architecture, and optimizing data system. The candidate will work with a wide range of stakeholders and functional teams to identify opportunities, collect requirements, and manipulate company data to be assessable by different data users. The right candidate will be excited by the prospect of optimizing or even re-designing the company’s data architecture to support our clients next generation of products and data initiatives.

Responsibilities

Collaborate with the HQ and the external partners (university, start-ups etc).

Perform a technological watch on the new tools of Data Management and implement them as part of technological demonstrators.

Define and implement data processing architectures including databases, local or cloud communication systems as well as processing algorithms and applications for visualization of treatment results.

Coordinate with IoT specialists and technical experts from our business units to retrieve data generated by our products or internal processes and collect business needs, develop technical architecture with solutions with his/her expertise.

Build the infrastructure required for optimal ETL of data from a wide variety of data sources with SQL and big data technologies.

Create and maintain optimal data pipeline architecture and assemble large, complex datasets that meet functional / non-functional requirements.

Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

Requirements

Master’s/Bachelor’s Degree in Computer Science, Computer Engineering, Statistics, Business Analytics, Information System with minimum 2 years working experience.

Proficient at relational SQL (MS SQL, PostgreSQL, MySQL, etc.) and NoSQL (MongoDB, CosmosDB, etc.) database design on local premise and on cloud.

Experienced in performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

Experienced with Azure cloud services such as ADF, Azure Data Bricks, Azure Stream, Azure Synapse, etc.

Knowledge of LANs, WANs, Ethernet protocols, distributed computing architectures, authentication processes and IT security.

Strong analytic skills related to working with structured / unstructured datasets.

Programming languages: Python, R, C#, SQL, etc.

Strong project management and organizational skills.

Experience supporting and working with cross-functional teams in a dynamic environment.

Only Singapore-based candidates will be considered.

Requirements
Master’s/Bachelor’s Degree in Computer Science, Computer Engineering, Statistics, Business Analytics, Information System with minimum 2 years working experience. Proficient at relational SQL (MS SQL, PostgreSQL, MySQL, etc.) and NoSQL (MongoDB, CosmosDB, etc.) database design on local premise and on cloud. Experienced in performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Experienced with Azure cloud services such as ADF, Azure Data Bricks, Azure Stream, Azure Synapse, etc. Knowledge of LANs, WANs, Ethernet protocols, distributed computing architectures, authentication processes and IT security. Strong analytic skills related to working with structured / unstructured datasets. Programming languages: Python, R, C#, SQL, etc. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. Only Singapore-based candidates will be considered"
29-Apr-2022 T11:49,Data Engineer,Keystone Cable (s) Pte Ltd,16 hours ago,,Full–time,"Job ScopeTo provide the technical drive on the company's i4.0 digitalization and automation plans.To understand departmental digitalization requirements and use programming or software to reduce digital waste.Help to drive digitization / digitalization efforts for data analysis on key operational metrics; to increase operational capacity, efficiency, productivity and manpower utilisation.To design and build data solutions that support company's data and analytics strategy in driving business insights.To work closely with ERP, MES and other technology vendors to implement and support the company's implementations.RequirementsRespectfulGood communication skillsGrowth mindsetWilling to accept new challenges and learnTechnical requirements: Industrial control and automation, Power and PLC systemsAcademic qualifications in computer science or related fieldsMin programming skills: Python, SQL databaseUnderstanding of data governance, data security and data analytics"
29-Apr-2022 T11:49,Data Engineer,Leap29,16 hours ago,,Full–time,"Job Title: Data Engineer (12-month contract, conversion to perm after contract)
Start Date: As Per Notice
Location: Singapore
Salary: Market Competitive

Job Overview:
My Client is one of the world’s leading Consumer Goods organisation and are seeking a Data Engineer to join their team in Singapore. The role will be expected to blend the expertise of Product knowledge and Data Engineering, responsible for transforming business requirements into sophisticated product features and insights. The position will be an individual contributor role and to be successful, the candidate should thrive leading discussions with both technical and commercial business teams.

Key Accountabilities
• Partner with global and market data experts to discover value from exploiting external/ internal data sources.
• Consult commercial business leaders on how the product can best add value to their business growth.
• Build (ETL) new and evolve data models and pipelines to power algorithmic based Business Intelligent solutions that addresses business problems requiring descriptive, diagnostic, predictive, and/or prescriptive analytics for pricing, promotion, trade spending, assortment, and sales performance management.
• Translate the algorithms and analytic models into data models as business needs evolves on a going basis after they are put into production.
• Partner with operational stakeholders to ensure End-to-end data pipeline is functional and compliant.
• Develop roadmap that scales existing and new data models to support the portfolio of markets or solutions.

Relevant Experience:
• Between 4 to 8 years of strong Data Engineering experience
• Diploma or B.S. in a relevant technical field (Operations Research, Computer Science, Statistics, Business Analytics, Econometrics, or Mathematics).
• Ability to manipulate and high-volume of data from varying sources
• Proficiency in R/ Python, PySpark will be ideal
• Basic knowledge of MS Azure architecture/ Databricks/ Data Factory pipeline deployments
• Proficiency of an analysis tool such as Microsoft PowerBI
• Basic Strong track record in working independently with minimal guidance

If this is something of interest to you, please send across your CV Ms Roopinder on rkaur@leap29.sg for review. Only shortlisted candidates will be notified.

Good luck!

To help Leap29 find you that perfect job, we need to store and process your personal information.
That means that your details will be entered into Leap29's database and our consultants may contact you from time to time with relevant job opportunities.
By applying you're confirming you're happy for us to do that"
29-Apr-2022 T11:49,Head of Data Platform and Engineering,ShopBack,6 hours ago,,Full–time,"About Us

ShopBack : Better Shopping, Every Day.

The ShopBack Group is Asia-Pacific’s leading shopping and rewards platform, serving over 30 million shoppers across ten markets. Growing from a team of six back in 2014 to over seven hundred today, ShopBackers across the region come together with a singular mission: to make shopping rewarding, delightful, and accessible for all.

Joining forces with leading buy now, pay later (BNPL) player hoolah and with the launch of ShopBack Pay, the Group now offers shoppers a responsible and convenient payment option at checkout.

More than half a billion shopping trips start with ShopBack each year. The Group powers over US$3.5 billion in annual sales for over 10,000 online and in-store merchant partners, across categories ranging from fashion, beauty, F&B, electronics, travel and more. If you are passionate about building and scaling up businesses in this fast-growing landscape, come and join our growing ShopBack team!

Responsibilities
• Grow develop, and manage a team of talented Data Engineers
• Maintain high quality engineering standards
• Develop and deliver high-impact, forward-looking roadmaps for your team
• Help solve cross-team problems at scale and delight internal customers
• Drive vision to build a modern Data Platform at ShopBack
• Communicating fearlessly to build trust with diverse stakeholders
• Drive continuous improvement in the efficiency and flexibility of the services

Requirements
• 5+ years of data engineering and data platform experience
• Significant prior success developing and deploying RecSys at scale
• Excellent people manager with a record of recruiting, managing, and retaining talent
• Full stack experience in data platform components
• Robust project management skills, including work estimation, prioritization, planning, tracking, and retrospectives
• Ability to develop ML Product with good engineering practice and mindset
• Strong desire to solve tough problems with scientific rigour at scale
• Experience with big data technologies like Hadoop, Spark, Hive, Presto, etc.
• Strong skills in any programming language like Java, Scala or Python.
• Experience working on both real-time and batch processing data pipelines
• Bachelor's degree or equivalent experience in computer science, engineering, or another technical discipline (MS/PhD is a plus"
29-Apr-2022 T11:49,Data Engineer,Ntuc Learninghub Pte. Ltd.,16 hours ago,,Full–time,"Job Responsibilities:Design, develop, implement, support and maintain old and new data models, data warehouses, cubes, ETL packages and core data infrastructure crucial to the needs of the business.Retrieve, cleanse, validate and analyse data using different techniques in order to retain the data integrity and availability for Business Supports.Work closely with internal stakeholders to collect and understand business requirements then build, design and manage interpret solutions for either reporting or dashboard solutions requiredWork with management and stakeholders to prioritize business and information needsTransform business requirement to data requirement, data analysis and profilingDesign data models for use cases and data warehousingPerform data cleaning and ensure data quality and integrity in data warehouseRoot cause analysis of data issues and coming up with stable long terms solutionsSupport data store's inbound and/or outbound developmentPerform data acceptance testing together with usersImplement and build a reliable data warehouse which meets the business needs of all stakeholder and according to data warehouse architecture guidelinesIdentify data from different source systems and build the target data modelsEnsure business rules and data definition are standardised across users and reports/dashboardsEnsure company's data policy, data security standard and data governance guidelines are adhered toRequirement:Strong experience with Talend Studio and related toolsDiploma/BS in any relevant field (e.g., Statistics, mathematics, computer science or information technology).At least 3-5 years relevant working experience as Data Engineer/Business Intelligence.Highly proficient in composing, maintaining and optimising complex SQL queries.Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Strong knowledge of and experience on data management, structure, retrieval and ETL processes (structured and unstructured data sources)Knowledge of ODS an advantageAbility to present information in a meaningful and structured manner (report and dashboard)Familiarity with programming languages such as Java, Python and its libraries"
29-Apr-2022 T11:49,"VP, Data Engineer",JobCart,16 hours ago,,Full–time,"Job Details

VP, Data Engineer

VP, Data Engineer

Posting Date: 10-Mar-2022

Location: Alexandra, Singapore, SG

Company: United Overseas Bank Limited

About UOB
United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices.
Our history spans more than 80 years. Over this time, we have been guided by our values - Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.

About the Department
The Technology and Operations function is comprised of five teams of specialists with distinct capabilities business partnership, technology, operations, risk governance and planning support and services. We work closely together to harness the power of technology to support our physical and digital banking services and operations. This includes developing, centralising and standardising technology systems as well as banking operations in Singapore and overseas branches.

Job Responsibilities

The Data Engineer will be directly responsible for the end-to-end software development and support for all work related to projects, quarterly change requests, L3 production fixes. This includes software product implementation and administration, application design, development, implementation, testing and support. You will be expected to work on Finance & Risk Analytics .

You will also be responsible for quality assurance of the team's delivery in conformance with the Bank-defined software delivery methodology and tools . You will partner with other technology functions to help deliver required technology solutions:

Create frameworks, technical features which helps in faster operationalisation of Data models, Analytical models (including AI/ML) and user generated contents (dashboards, reports etc)

Effectively partner with citizen data scientists in enabling faster adoption of AL/ML model based systems

Independently install, customise and integrate software packages and programs

Carry out POCs involving new data technologies

Design and develop application frameworks for data integration

Create technical documents such as solution design, program specifications for target solutions

Perform design and development of applications which may not be limited to: Software Applications, Data Integration, User Interfaces, Automation

Maintain and recommend software improvements to ensure a platform centric management of software applications

Performance tuning

Work with production support team members to conduct root cause analysis of issues, review new and existing code and/or perform unit testing

Perform tasks as part of a cross functional development team using agile or other methodologies and utilising project management software

What will help you succeed in this role

Adopt an uncompromising attitude when it comes to quality and help raise bar of products and team members

Be a team player who communicates effectively and professionally with both internal and external customers

Identify ideas to improve system performance and impact availability

Embrace tackling and resolving complex technical design issues

Possess strong problem solving and decision-making skills while exercising good judgment

Strong analytical and problem-solving skills

Ability to work on multiple projects at a time

Be able to work under pressure and manage deadlines or unexpected changes in expectations or requirements

Good communication skills - ability to convey technical information to non-technical audience

Ability to understand the big picture

Ability to develop long lasting relationships with all levels

Deep understanding and experience in software development cycle, including Agile based rapid delivery

Collaborate with business and IT to analyse, elicit and review business requirements

Facilitate communication between vendor, project team, business stakeholders and internal IT team

Ability to work in a team distributed across multiple locations

Job Requirements

Functional Skills: Data Lake, EDW, Data Mart, Data Integration & Visualisation
Hands-on experience in implementing large scale data warehouse & analytics platforms in financial services industry; with good functional knowledge of products & services offered in Retail bank/Wholesale/Global Markets covering some of the following analytics domains:

Setting up and running BI tools oriented platform

Design and develop QlikSense & Microsoft Power BI applications

Design and develop Applications in SAS, Microsoft-R, Python

Integration of BI tools with data stores (EDW, data marts)

Experience in Data Modeling, Data mapping for Data Warehouse and Data Marts solutions

Expertise in FSLDM or similar industry models

Experience in financial domain - Retail , Wholesale, Compliance, Digital

Experience in analytics - Finance Analytics, Credit Risk Analytics, Credit Scoring, Financial Crime Analytics

Expertise in design of role based fine grained access control

Designing cloud ready data solutions, Virtualization

Technical Skills: Data Lake, EDW, Data marts, Data Integration & Visualisation

Expertise in implementing Teradata based EDW and Data mart solution using ETL tool such as (Informatica Power Centre, BDM), GCFR based framework, BTEQ scripts)

Expertise in implementing Hadoop based Data mart using Spark based framework (Java, Scala, Pyspark)

Good knowledge and working experience in data loading into Teradata using different load pattern (TPT, MLOAD, FAST Load), data compression (MVC, BLC)

Expertise in implementing Data Governance tools; Data Lineage ( Informatica MM, EDC), Business Glossary and Data Quality

Expertise in implementing Reference data management (Data standardization, Hierarchy maintenance) using tools ( MDM,RDM)

Good knowledge and working experience in Teradata FSLM or similar industry standard data model

Expertise in Cloudera CDH / CDP components

Good knowledge in developing Spark based ingestion framework (Java, Scala, Pyspark)

Experience in building and operationalising feature pipeline to support AI/ML model execution, data pipelines for supporting large scale data warehouse/data marts

Any TWO technical certifications

Cloudera Hadoop distribution - Hive, Impala, Spark, Kudo, Kafka, Flume

Teradata - Bteq, Query Grid, GCFR, MDM, Data Mover, BAR

Informatica Data Integration - PC, IDR, BDM, MM, IDQ, EDC

Data modelling tools - Erwin

QlikSense

Microsoft Power BI - SSAS, SSRS

Microsoft - R

Data science workbenches - Cloudera Workbench, Jupyter, DataRobot, , IBM DSX

Data Virtualization tool - Denodo, Dremio

AS400

Language - SQL, Java, Python, Scala, Pyspark

Automation / scripting - CtrlM, Shell Scripting, Groovy

Any ONE as added advantage

CI/CD software, Testing Tools - Jenkins, SonarQube

Version Control Tool - Aldon+LMe, CA Endeavor

Deployment Tool kit - Jenkins

Service or Incident Management (IcM) Tools - Remedy

Source Code Repository Tool - Bitbucket

Scheduling Tool - Control-M

Defect Management Tool - JIRA

Application Testing tool - QuerySurge

Cloud certification

Platforms provided by FICO, Experian, SAS for credit and portfolio management

Be a part of UOB Family
UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.

Apply now and make a difference.

Technology - Project Management
Technology - Infrastructure
Technology - Information security"
29-Apr-2022 T11:49,Senior Data Engineer,Singtel,16 hours ago,,Full–time,"Jobscope Develop big data solutions for near real-time stream processing, as well as batch processing on the Big Data platform Work with business domain experts, data scientists, and solution designers to identify data relevant for analysis and develop Data solutions Fine-tuning of new and existing data pipelines Schedule and maintain data pipelines Drive optimization, testing and tooling to improve data quality Assemble large, complex data sets that meet functional / non-functional business requirements Develop APIs to support high throughput data processing, feature engineering, and use cases Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc Review and approve high level & detailed design to ensure that the solution delivers to the business needs and align to the data & analytics architecture principles and roadmap Understand various data security standards and use secure data security tools to apply and adhere to the required data controls for user access Support and contribute to development guidelines and standards for data ingestion Adapt and learn new technologies surrounding Data Platform The Ideal Candidate should possess the following: Bachelor’s Degree in IT, Computer Science, Software Engineering, Business Analytics or equivalent. Minimum of 6 years of experience with highly distributed, scalable, concurrent and low latency systems working with one or more of the following technologies: Hadoop distributions Spark NoSQL data warehouses such as HBase, Cassandra Excellent software & data engineering principles and design patterns Proficient in creating and maintaining complex data pipelines end-to-end while maintaining high reliability and security Excellent hands-on experience in Scala or Python Excellent hands-on experience with SQL and Spark Experience with Kafka Experience with CI/CD tools and environment Experience migrating from on-premise data stores to cloud solutions Good working experience with one or more major cloud vendors (ie: Azure,AWS, GCP) Experience working in Telco Data Warehouse and / or Data Lake advantageous Highly organized, selfmotivated, pro-active, and able to plan Ability to analyze and understand complex problems Ability to explain technical information in business terms Ability to communicate clearly and effectively, both verbally and in writing Strong in User Requirements Gathering, Maintenance and Support Good experience managing users and vendors Experience with Agile Methodology We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated"
29-Apr-2022 T11:49,,,16 hours ago,,Full–time,"Data Engineer – Standard Chartered nexus-(2200007998)

Job

:Technology

Primary Location

:Asia-Singapore-Singapore

Schedule

:Full-time

Employee Status

:Permanent

Posting Date

:03/Apr/2022, 11:32:08 PM

Unposting Date

:04/May/2022, 6:59:00 PM

Want to push the boundaries of traditional technology, finance and operations? At nexus, courage is a defining trait for all team members. Courage to challenge the status quo, to do better, to deliver against the grain and to achieve spectacular results in massive ambiguity. We want to change how ecosystems interact and only have the blueprint for the baby steps – come join us to build out the rest of it! The idea is to do amazing things and have a lot of fun while doing it.

We are looking for a Data Engineer to define, implement and run the Data platforms and analytics for an online banking project at Standard Chartered nexus!

What will you be doing:

Choosing the right technologies for our use cases, deploy and operate.

Setting up Data stores structured, semi structured and non-structured.

Secure data at rest via encryption

Implement tool to access securely multiple data sources

Implement solutions to run real-time analytics

Use container technologies

What do you need to be successful in this role?

Experience in one of the following: Elastic Search, Cassandra, Hadoop, Mongo DB

Experience in Spark and Presto/Trino

Worked with microservice based architectures

Experience of Unix/Linux environments is plus

Experience of Agile/Scrum development methodologies is a plus

Cloud knowledge a big plus (AWS/GCP) – (Kubernetes/Docker)

Be nice, respectful, able to work in a team

Willingness to learn

About Standard Chartered nexus

Standard Chartered nexus is a white label plug-and-play banking solution powered by Standard Chartered Bank. We take pride in redefining how we bank customers globally by digitally marrying ecosystems to banks. Our Banking-as-a-Service (BaaS) solution is the gateway that enables convenient access to financial services. We believe in challenging the status quo and thinking outside the box to deliver innovative solutions. Let’s shape the future of banking together.

The Standard Chartered nexus family is made up of a bunch of fun, hardworking and results-oriented individuals. Most importantly, nexaurs value teamwork, and we champion a respectful, open and trusting work environment. If you are interested to be part of the team in re-wiring the DNA of banking, join us today. #YesWeCanLAH"
29-Apr-2022 T11:49,Senior Data Engineer APAC,foodpanda,16 hours ago,,Full–time,"foodpanda is the largest food and grocery delivery platform in Asia, outside of China. Operating in more than 400 cities across 12 markets, foodpanda continues to expand and grow in its core food delivery business as well as in new verticals like grocery deliveries, with a strong tech infrastructure at its core. From our restaurants-partners, cloud kitchens and cloud grocery stores — foodpanda is just one tap away, delivering everything you need quickly and conveniently to your doorstep.We are looking for a (Senior) Data Engineer to join our growing data team to help Foodpanda make smart, data-driven business decisions. If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you! What's on your plate: Defines and builds data pipelines that will enable faster, better, data-informed decision-making within the businessArchitect, build, and deploy new data models that provide intuitive analytics across the businessManage and own the entire data processing system (internally, as well as interfacing with other teams or third party services), provide advice on any necessary infrastructure changesExperiment with, select, and implement cutting edge Big Data tools and frameworks required to provide requested capabilitiesPerform all needed data transformation to populate a reporting optimised data warehouseMentoring junior team members through code review and enablement trainingParticipate in building machine learning models, deliver analyses and insights to support business decision making What you bring to the table: Ability to write clean, structured, and high performance SQL and Python codeStrong experience with big data, Data Warehouse technologiesStrong oral and written communication skillsStrong business mindset and ability to grasp business requirements from stakeholdersKnowledge of Machine Learning is a plusExperience in machine learning operations, or setting up environments for data scientists/machine learning engineers would be advantageous What we can offer you: A vibrant and international team with multi-cultural and diverse backgrounds.Solving challenges with inspiring colleagues in an all hands-on deck environment.Management team that recognizes top performers, welcome our newbies, and shares a love for good food.Competitive package, incentives, allowances, food perks, insurance, pension and more"
29-Apr-2022 T11:49,Software Engineer - (Data Engineering),Goldman Sachs,6 hours ago,,Full–time,"Job Description

OUR IMPACT

Seeking a data focused software engineer, to design and build highly scalable and resilient data solutions that will drive client insights, analytics and engagement. Liaise with business teams across the firm to understand client activities and create accurate and high quality data models for them. Drive towards consistent and easily accessible client information by developing to rigorous curation and quality standards.

Our team design, build, and operate the firm's Legend data platform in the AWS and GCP clouds. Legend is a data management and data governance platform that provides both engineers and non-engineers a single solution to develop data-centric applications and derive data-driven insights. Legend automates some of the most difficult data governance challenges and provides self-service tools to democratize data and analytics. It is available as an open source platform through GitHub ready to be used by our clients and the world fully open and free of charge. As a member of the Data Engineering team, you will be able to work on this open sourced platform which not only brings real, tangible value for Goldman Sachs but also our peers and clients as well as greater standardization and efficiency across the entire financial industry.

How You Will Fulfill Your Potential
• Design & develop modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies
• Evaluate, select and acquire new internal & external data sets that contribute to business decision making
• Engineer streaming data processing pipelines
• Drive adoption of Cloud technology for data processing and warehousing
• Engage with data consumers and producers in order to design appropriate models to suit all needs

Skills And Experience We Are Looking For
• 5+ years of relevant work experience in a team-focused environment
• A Bachelor or Master degree in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)
• Strong object-oriented design and programming skills and experience in OO languages
• Experience or interest in functional programming languages
• Extensive experience in Java (Preferred), Python, C++, C#, Objective-C, or other OO languages
• Proven experience applying domain driven design to build complex business applications
• Deep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes
• In-depth knowledge of relational and columnar SQL databases, including database design
• General knowledge of business processes, data flows and the quantitative models that generate or consume data
• Excellent communications skills and the ability to work with subject matter experts to extract critical business concepts
• Independent thinker, willing to engage, challenge or learn
• Ability to stay commercially focused and to always push for quantifiable commercial impact
• Strong work ethic, a sense of ownership and urgency
• Strong analytical and problem solving skills
• Ability to collaborate effectively across global teams and communicate complex ideas in a simple manner

Preferred Qualifications
• Financial Services industry experience
• Experience in distributed system design
• Working knowledge of open-source tools such as Kafka, Spark

About Goldman Sachs

At Goldman Sachs, we commit our people, capital and ideas to help our clients, shareholders and the communities we serve to grow. Founded in 1869, we are a leading global investment banking, securities and investment management firm. Headquartered in New York, we maintain offices around the world.

We believe who you are makes you better at what you do. We're committed to fostering and advancing diversity and inclusion in our own workplace and beyond by ensuring every individual within our firm has a number of opportunities to grow professionally and personally, from our training and development opportunities and firmwide networks to benefits, wellness and personal finance offerings and mindfulness programs. Learn more about our culture, benefits, and people at GS.com/careers .

We’re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https:// www.goldmansachs.com/careers/footer/disability-statement.html

© The Goldman Sachs Group, Inc., 2020. All rights reserved.

Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Veteran/Sexual Orientation/Gender Identity"
29-Apr-2022 T11:49,Senior Data Engineer Data Warehouse,Search Staffing Services Pte. Ltd.,16 hours ago,,Full–time,"We are repesenting our client on the following role.Leader – Business Data Analysis (Senior Engineer level)Responsibilities:Lead a team of 5+ data specialists and digital solution developers to manage business data (products, BOMs, materials, suppliers, sales, etc.)Gathers data from various sources, cleanse data for integrity and accuracy, applies various data analysis tools to provide meaningful information and insights to facilitate for better decision-making, and to create digital solutions for process automation, working efficiency improvementStrategize team development, communicate expectations, set goals, monitor and review performancePerform daily team management – request prioritization, task delegation, progress monitor, stakeholder communication, problem solving, team motivation, etcLiaise with various departments and stakeholders to create collaborative work relationships and solid rapportsProvide technical expertise in data infrastructure setup such as data system architecting, database development, data warehousing, etcGather data from various sources, create data models and schemas, transform and organise them into usable formatsUse statistical tools to derive meaningful information/insights from raw data, analyse data to find answers to specific questionsAssess quality and meanings of data, cleanse data issues for integrity and accuracyCreating data visualisations and reports for other stakeholders to comprehend business facts and trends to make better decisionsExploit business tools, software and technology advances to create digital solutions for business process automation and working efficiency improvementWork with relevant stakeholders to identify process improvement opportunities, propose system modifications, and devise data governance strategiesCreate appropriate documentation that allows others to understand data analysis processes and steps to foster wide adoption of data processing techniquesRequirements:Bachelor's degree with 5+ years of experience in data warehouse or database development & deployment with profound understanding of relational databases, practical knowledge of SQL and RDMS – SQL Server, Azure SQL, Dataverse, Access, etc3+ years of data analytic experience with good proficiency in Microsoft PowerBI and Excel – Power Query (M language), Power Pivot, Pivot Table, Data Model, DAX, Visual Reporting, VBA programmingGood understanding of Product BOM and SAP ERP/PLM systemExperienced in project management and team collaborationExperience in MS Power Platform development with Power Automate and Power Apps, and MS SharePoint development are highly desirableKnowledge of engineering documentation & control, product development, manufacturing operation, and supply chain management are desirableExperienced in resource management – recruitment, organization, motivation and performance reviewInterested applicants, kindly send in a copy of your updated resume in WORD document to [HIDDEN TEXT] stating your current and expected remuneration together with notice period required to current employer.    EA License No: 12C6254EA Personnel Registration No: R1113411Join us on Telegram here: https://t.me/Searchstaffingjobs"
29-Apr-2022 T11:49,Software Engineer (Data Science) / Sr,Micron Technology,7 hours ago,,Full–time,"Our vision is to transform how the world uses information to enrich life for all.

Join an inclusive team passionate about one thing: using their expertise in the relentless pursuit of innovation for customers and partners. The solutions we build help make everything from virtual reality experiences to breakthroughs in neural networks possible. We do it all while committing to integrity, sustainability, and giving back to our communities. Because doing so can fuel the very innovation we are pursuing.

JR19130 Software Engineer (Data Science) / Sr

Responsibilities Include, But Not Limited To

Build Next Generation Data Systems
• Work with cross functional teams to define and drive data infrastructure roadmap and architecture
• Design, architect, and develop a scalable and supportable data pipeline which includes data warehousing, data integration and ETL processing
• Document artifacts related to software development, design, test cases, user training, release.
• Lead/Participate in custom data integration projects with internal data systems and/or external sub con manufacturing and logistics systems
• Collaborate with IT architectures/teams to design and prioritize infrastructure delivery with processes, deliverables aligned to roadmap.
• Enable end-users direct access to native and/or transformed data (eg. via ReST services)

Enforce Operational Data Quality
• Establish software development best practices and tools to support data integrity, cleanup, and validation across all relevant data sources
• Enable continuous data quality improvements to minimize support requirements
• Implement central logging and monitoring systems with logic for usage reports and audits
• Collaborate with IT to ensure effective incident, problem and change management processes

Design and Develop Data Analytics
• Develop expertise in data mining and analytic methods
• Solve complex optimization problems using linear and nonlinear optimization techniques
• Develop and generalize data science solutions in a production data pipeline using statistical and machine learning techniques
• Determine statistical validity and significance (pick out signals from noise)
• Apply statistical methods to detect anomalies, trends and shifts

Develop Modern Visualization Solutions
• Automate business reporting needs and organize into user friendly dashboards in multiple platforms (desktop/web/mobile)
• Facilitate interactive design sessions with customers
• Use experimentation methods and design tests to optimize key user experiences

Ensure a Safe, Compliant, and Ethical Work Environment
• Comply with company safety, labor, and ethics policies
• Oversee execution of mandatory training for direct reports
• Encourage a culture of safety and recognize contributions made to supporting a safe work environments
• Participate in process improvement activities
• Identify errors before they get to the next step
• Provide timely feedback when quality, safety, or ethical issues arise
• Follow and support company-wide and department safety procedures to maintain a clean, safe, and accident-free work environment
• Promote high integrity processes that will enable immediate sharing of problems, issues and solutions with management team

Qualification
• Bachelor’s Degree in Computer Science or Information Technology
• Experience in developing software/web application (Java, C#, JavaScript) and business intelligent solutions (Apache Nifi, Tableau or Microsoft Power Platform)
• Strong knowledge of relational databases, table design, SQL query and optimization of queries.
• Familiar with database management system (Microsoft SQL, Hana or Snowflake)
• Knowledge of Agile development process
• Strong problem-solving skills with ability to multi-task and manage multiple projects simultaneously
• Possess the ability to work in several areas of a development life cycle as part of a team or independently
• Self-motivated, strong ownership and ability to work independently with minimal supervision
• Excellent oral and written communication skills
• Very high attention to detail and data accuracy and quality
• Strong team player with experience working on projects with global stakeholders
• Knowledge in Manufacturing Execution System, Supply Planning Optimization system, SAP enterprise master data and understanding of semiconductor manufacturing process will be an advantage

About Micron Technology, Inc.

We are an industry leader in innovative memory and storage solutions transforming how the world uses information to enrich life for all. With a relentless focus on our customers, technology leadership, and manufacturing and operational excellence, Micron delivers a rich portfolio of high-performance DRAM, NAND, and NOR memory and storage products through our Micron® and Crucial® brands. Every day, the innovations that our people create fuel the data economy, enabling advances in artificial intelligence and 5G applications that unleash opportunities — from the data center to the intelligent edge and across the client and mobile user experience.

Please note that in order to assist in providing a safe and healthy workplace for all Micron team members, new employment offers for jobs based in India, Malaysia, Singapore, and the U.S., are contingent upon the applicant’s provision of a copy of their COVID-19 vaccination document to Micron on a confidential basis prior to their scheduled start date confirming that they have completed the COVID-19 vaccination process, subject to any written request for medical or religious accommodation and to the extent permitted by applicable law.

To learn more, please visit micron.com/careers

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.

To request assistance with the application process and/or for reasonable accommodations, please contact hrsupport_sg@micron.com

Micron Prohibits the use of child labor and complies with all applicable laws, rules, regulations, and other international and industry labor standards.

Micron does not charge candidates any recruitment fees or unlawfully collect any other payment from candidates as consideration for their employment with Micron"
29-Apr-2022 T11:49,Information Technology Senior Data Sciences Analytics Engineer Data...,Singapore Airlines Limited,16 hours ago,,Full–time,"Job DescriptionWe have multiple senior data engineer positions available in the Data Engineering team. The senior data engineer is a senior software developer with strong software engineering skills who is responsible for building custom open-source-based data ingestion and MLOps platforms. He/she has deep appreciation of the complexity of the data engineering process, such as the challenges of data ingestion involving large or near-real-time datasets, the maintenance of high data quality, and the importance of automation for increasing pipeline robustness and reducing the need for human intervention.Responsibilities:Be an effective distributed-system implementer in the following core activities:Design and develop data engineering services and their ecosystem using distributed databases (relational, columnar, graph, in-memory); orchestration (Apache Airflow); and distributed stream/batch data processing (Kafka, Kinesis, Spark).Design and develop MLOps production pipelines; provide technical support to data scientists/ML engineers by getting their ML/DL models deployed at scale and meeting SLAs on both cloud and on-premises GPU and CPU instances.Design data models for mission-critical, high-volume, near-real-time/batch data; build idempotent/atomic production data pipelines to make data ingestion more fault tolerant.Design and develop intuitive, highly automated, self-service data platform functions for business users.Explore, evaluate and champion the introduction of next-generation technologies in the data-ingestion workflow. Participate in project planning and provide technical guidance on cloud architecture for data projects.RequirementsBS in Computer Science or other related discipline is required. Advanced degrees in Computer Science (PhD, MS) are highly desirable.3+ years of relevant industry experience in some or most of the following technical areas:Advanced programming skills in Python. Conversant with data structures and algorithm design.Experience in building data pipelines (including data collection, warehousing, processing, analysis, monitoring, and governance) using open-source data ingestion platforms.Intermediate-level knowledge and experience with AWS cloud components and best practices. Good understanding in deploying data stores such as S3, RedShift, Elasticache, PostgreSQL, and ClickHouse.Prior experience in modern software development is required (such as web frontend UI, backend API microservices, understanding of CI/CD and Scrum/Kanban agile development). Strong grasp on object-oriented or functional programming (using e.g. Python, Java, Scala, or C"
29-Apr-2022 T11:49,"Data Engineer Intern, Trust and Safety (2023 graduate)",Tencent,16 hours ago,,Internship,"About Tencent Games

Tencent Games was launched in 2003 and is now the largest gaming company in the world, achieving game-related revenues of $29.3 billion in 2020. As a leading global organization for game development, publishing, and operations, we are dedicated to providing high-quality interactive entertainment experiences to players and currently offer more than 140 games including PUBG MOBILE, Call of Duty Mobile, and League of Legends to hundreds of millions of players across more than 200 countries and regions. Tencent Games offers talented individuals the opportunity to help shape and build something unique within the gaming industry as a whole.

About the Team

The Interactive Entertainment Group (IEG) at Tencent is responsible for the R&D, operation, and development of the company's interactive entertainment business including games and eSports. Through online gaming, live broadcasts, and offline eSports, IEG assists the company in leading the global interactive entertainment market to create better interactive entertainment content experiences for users.

Responsibilities:

As a Data Engineer Intern, you will:

Perform complex queries and analyses to prototype and derive insights that help identify and fight abuse across our products, improve decision-making and help tell ours fairplay policy enforcement story.

Conduct end-to-end analysis that includes data discovery, data readiness assessments, technical requirements development, and clarification with stakeholders at all levels.

Design, develop, maintain reliable metrics and dashboards, and build expertise on how to centralize information in a consistent and qualitative manner.

Collaborate with multi-functional teams, receive mentorship, and gain insight into our value-driven process. Our goal is both to support your growth and development while empowering you for a successful start to your career.

Qualifications and Skills ：

Bachelor’s, Master’s, or PhD in Computer Science, Software Engineering, Computer Engineering, Electrical Engineering, Mathematics or a related technical field, and a strong record of academic achievement

Experience querying datasets using SQL and building dashboards

Experience solving for repeatability and scalability of data and metrics

Ability to analyze data and create succinct presentations tailored to the audiences

Ability to multitask and prioritize in a fast-paced environment

What we offer:
• The opportunity to work with a set of hardworking and dedicated peers, all the way from engineering and QA to product management and customer support.
• A constant stream of new things for you to learn and an opportunity for growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe mentors help both sides of the equation"
29-Apr-2022 T11:49,Data Engineer Trading,MICHAEL PAGE INTERNATIONAL PTE LTD,16 hours ago,,Full–time,"Permanent role Career Advancement OpportunityAbout Our ClientOur client is a fintech company that is a trusted crypto exchange brand. They are looking for a Data Engineer that has experience in the finance industry.Job Description Collaborate with trading applications and backend developers to advise and fulfill data requirements in terms of consistency, latency and scalability Responsible for the implementation and maintenance of relevant database technologies and data quality to meet these data needs Collaborate with existing data team to design and implement data synchronization Work with development team for the maintenance and enhancement of latency-sensitive market data feed componentsThe Successful Applicant 3 years' working experience in data engineering, database administration, AWS data solutions or relevant field Proficient in Python, R, Julia (at least one), SQL, Linux and C++, Java or Rust (at least one) Proficient in data pipeline development involving cloud data solutions such as Redshift, Snowflake or databases such as SQL, kdb+ or other time series databases Experience in market data gateway protocols such as ITCH or socket and web socket programming or real-time messaging frameworks such as Apache Kafka, Aeron, Chronicle is highly desirableWhat's on Offer Permanent role Career Advancement Opportunity Opportunity to work on cutting edge technologies Recommended Skills Amazon RedshiftApache KafkaBackendC++ (Programming Language)Data PipelineData Quality"
29-Apr-2022 T11:49,"Data Center Controls Engineer, Security",Google,5 hours ago,,Full–time,"Google will be prioritizing applicants who have a current right to work in Singapore, and do not require Google's sponsorship of a visa.

Minimum qualifications:
• Bachelor's degree in Engineering, Physics, or Computer Science or equivalent practical experience.
• 8 years of experience with facility control systems.
• Experience as a controls engineering lead for the software, security, and/or network aspects of control system design and product development.
• Experience in industrial control systems design, network design, security, and system administration.

Preferred qualifications:
• Master's or PhD degree in Engineering, Physics, or Computer Science.
• CISSP, CISA, GIAC-GSNA, or GIAC-GCCC certification.
• Experience with the software lifecycle (design, development, release, and support) for distributed controls software and configuration.
• Experience in bash shell scripting and working in virtualized server environments (Windows and Linux). Programming skills in Go, Python C++, Java or similar in an industrial controls context.
• Ability to travel, domestically or internationally, up to 20%.
• Knowledge of large-scale data centers monitoring and control systems, and their integration and validation.

About The Job

Our thirst for technology is a part of everything we do. The Data Center Engineering team takes the physical design of our data centers into the future. Our lab mirrors a research and development department -- cutting-edge strategies are born, tested and tested again. Along with a team of great minds, you take on complex topics like how we use power or how to run state-of-the-art, environmentally-friendly facilities. You're a visionary who optimizes for efficiencies and never stops seeking improvements -- even small changes that can make a huge impact. You generate ideas, communicate recommendations to senior-level executives and drive implementation alongside facilities technicians.

With your technical expertise, you ensure compliance with codes and standards, develop infrastructure improvements and serve as an expert in your specialty (e.g., cooling, electrical).

As a Data Center Controls Engineer, you would have experience in control system and network security design, development, and maintenance, specifically in the area of complex and mission-critical facilities, including data centers and/or related industries such as chemicals, pharmaceuticals, nuclear, or semiconductors. You will be involved in the development of large-scale facility monitoring and controls infrastructure, from site assessments and concept design to piloting and construction and upgrades.

Behind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible.

Responsibilities
• Develop monitoring and control system designs, related hardware and software configurations, statistical process controls, and OT/IT stack interfaces.
• Build and manage control system software configuration management systems, tools, and processes for version control, regression testing, release, and distribution to data center deployment and upgrade projects worldwide.
• Manage end-to-end product lifecycle of controls software and system standards; update internal design specifications, drawings and standards to the latest configurations.
• Understand and apply cutting-edge industry trends to Google’s data center infrastructure.
• Communicate, collaborate, and negotiate with internal and external partners from requirements to impactful results.

Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form"
29-Apr-2022 T11:49,SAVE,SnapHunt Pte Ltd,16 hours ago,,Full–time,"Work within a company with a solid track record of success

Work alongside & learn from best in class talent

Excellent career development opportunities

Our Client is a global leader in Engineering and R&D (ER&D) services.

The Job

You will be responsible for :

Defining, developing and maintaining reports to support decision making.

Processing & Interpreting data to get actionable insights.

Working closely with business users to understand their data analysis needs/requirements.

Identifying trends, doing follow-up analysis, preparing visualizations.

Ideal Candidate

Degree in Computer Science, Applied Mathematics, Engineering or related field.

3+ years experience, ideally within a Data Analyst role.

Good presentation and communication skills and the ability to present you findings clearly and accessibly in the form of reports and presentations to senior colleagues.

Strong knowledge of Python, C# and PL / SQL.

Expertise in Data Visualisation Tools (Tableau, Qlikview, Hue Data Visualisation and Predictive Modeling would be highly valuable.

Good knowledge of Oracle DB databases.

Strong analytical skills and are comfortable dealing with numerical data

Ref: PJQMLZJ6NV

Recommended Skills

.Net Framework

C Sharp (Programming Language)

Java (Programming Language)

Pl/Sql

Sql (Programming Language)

Unix"
29-Apr-2022 T11:49,Senior Data Engineer,U3 Infotech Pte. Ltd.,16 hours ago,,Full–time,"Job description (In Detail) :Work with Lead Data Engineer to design and implement the data layer. This includes data modelling, data pipeline (ETL).Working experience with AWS services and technologies highly preferred.Working proficiency in SQL and Stored Procedures (MSSQL or Oracle).Working knowledge and experience Data Modelling, Data Mart design and ETL tools (Informatica).Possess experience and skills in programming in PL/SQL or T-SQL, ETL."
29-Apr-2022 T11:49,VPAVP Big Data Engineer Spark Development and Support Middle Office...,DBS Bank,16 hours ago,,Full–time,"Business Function Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation. In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels. Responsibilities T&O Middle Office Technology is looking to hire an experienced Spark Developer having strong exposure in Level 2 Support for enterprise class Big Data Applications. Requirements University Graduate in Computer Science or Related discipline 7 to 10 years of experience in developing, maintaining and supporting Enterprise Class Big Data Applications, preferably in banking environments Expertise in Big Data technologies/tools/platforms such as Hadoop, HDFS, Hive, Impala, HDFS, Presto, Spark, Hive, Impala, Zeppelin, Yarn, Cloudera, Hortonworks Experience in Relational Databases Knowledge and experience in coding and performance tuning of Spark jobs. Java Spark preferred but not compulsory Establish and adhere to best practices relating to Apache Spark programming Responsible for understanding functional/non-functional requirements and implement them in Apache Spark Experience in usage of CI/CD tools – Jenkins, Git, Bitbucket Knowledge and experience in Apache Spark batch and streaming framework Knowledge of working with different file formats like Parquet, ORC, AVRO and JSON Knowledge of Micro Services architecture Knowledge of S3 is added advantage Ability to work proactively, independently and with cross-functional and cross-reginal teams Strong communication and analytical skills and experience in Agile projects Apply Now We offer a competitive salary and benefits package and the professional advantages of a dynamic environment that supports your development and recognises your achievements"
29-Apr-2022 T11:49,Principal Data Engineer,Keppel Logistics Pte. Ltd.,16 hours ago,,Full–time,"Job DescriptionThe Data Scientist Team will lead the running of analytical experiments in a methodical manner, and will regularly evaluate alternate models via theoretical approaches.Under general supervision, support the research and development of statistical learning models for data analysis.Support in the collaboration with relevant departments to understand company needs and devise possible solutions.Stay updated on latest technology trends.Support in the communication of results and ideas to key decision makers and stakeholders.Support implementation of new statistical or other mathematical methodologies as needed for specific models or analysis.Support joint development efforts optimization through appropriate database use and project design.Job RequirementsMaster's Degree in Computer Science, Statistics, Applied Math or related field.At least 3 years' practical experience with SAS, ETL, data processing, database programming and data analytics.Basic background in data mining and statistical analysis.Able to understand various data structures and common methods in data transformation.Excellent pattern recognition and predictive modeling skills.Exposure to programming languages such as Java/Python an asset.In accordance with the requirements set by the Singaporean Government and due to the nature of the role requiring work onsite, it is a requirement for all new employees, as well as contractors, to be fully vaccinated* against COVID-19. *Fully vaccinated means individuals have completed the full regime of an approved COVID-19 Vaccine, including the respective post-vaccination period to ensure the vaccine has become full effective, according to prevailing vaccination guidelines as advised by Ministry of Health (MOH).Anyone who is unable to be vaccinated due to an approved and/or recognized exemption condition may apply for special consideration.Applicants will have to provide full vaccination details during recruitment process, in compliance with company's regulations"
29-Apr-2022 T11:49,Lead Data Engineer,Singtel Group,16 hours ago,,Full–time,"Singtel, Asia’s leading communications technology group, provides an extensive range of telecommunications and digital services to millions of consumers and businesses across Asia, Australia, Africa and the USA. With over 140 years of innovation behind us, we continue to push boundaries in our networks and services, to enrich lives and transform businesses.

Our core values – Customer Focus, Challenger Spirit, Teamwork, Integrity, and Personal Excellence – shape the way we work. We are passionate about making a difference and have an open and inclusive culture where everyone is empowered to do their best. Our diverse business means you will enjoy unique opportunities and rewarding experiences to learn and grow your career in a dynamic industry.

Join us and experience what it’s like to be with an Employer of Choice*. Together, let’s create a brighter digital future for all. *Awarded at the HR Fest Awards 2020

About the role

This role is accountable to define the big data architecture, design, build and run data pipelines under Singtel Data & Analytics within Group IT:
• Define and govern big data architecture
• Provide DevOps architecture implementation and operational support
• Manage the automation, design, engineering and development work related to data pipelines
• Drive optimization, testing and tooling to improve data quality
• Review and approve solution design for data pipelines
• Ensure that proposed solutions are aligned and conformed to the big data architecture guidelines and roadmap
• Evaluate and renew implemented data pipelines solutions to ensure their relevance and effectiveness in supporting business needs and growth

Responsibilities
• Establish big data (data lake) architecture along with standards, guidelines and best practices
• Develop and maintain big data architecture blueprint for Group IT
• Build and maintain continuous integration and continuous deployment of data pipelines
• Understand business requirement and solution design to develop and implement solutions that adhere to big data architectural guidelines and address business requirements
• Fine-tuning of new and existing data pipelines, Drive optimization, testing and tooling to improve data quality
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc
• Build robust and scalable data infrastructure (both batch processing and real-time) to support needs from internal and external users
• Review and approve high level & detailed design to ensure that the solution delivers to the business needs and align to the data & analytics architecture principles and roadmap
• Provide guidance and direction to project delivery and operation teams in regards to big data architecture and solution design.
• Understand various data security standards and use secure data security tools to apply and adhere to the required data controls for user access in Hadoop platform
• Lead the identification of technologies and tools for big data and analytics platform

The ideal candidate should possess:
• Bachelor’s degree in IT, Computer Science, Software Engineering, Business Analytics or equivalent
• Minimum 10 years of experience in data warehousing / distributed system such as Hadoop
• Minimum 5 years of experience in solution architect and design of distributed system such as Hadoop
• Minimum 5 years of hands on experience in DevOps development for big data platform
• Experience with relational SQL and NoSQL DB
• Expert in building and optimizing ‘big data’ data pipelines, architectures and data sets
• Experience in Scala or Python
• Experience in ETL and / or data wrangling tools for big data environment
• Ability to troubleshoot and optimize complex queries on the Spark platform
• Knowledgeable on structured and unstructured data design / modelling, data access and data storage techniques

We believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers. We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative"
29-Apr-2022 T11:49,Senior Data Engineer FinTech,foodpanda,16 hours ago,,Full–time,"At foodpanda, we're on a mission to redefine how tech, food and people are connected. Headquartered in Singapore, foodpanda offers a quick and convenient way to get food, groceries and more delivered to your doorstep at the touch of a button. Through our network of local restaurants, retailers and pandamart stores, we provide thousands of exciting on-demand options to customers in over 400 cities across 12 markets in Asia. foodpanda is part of Delivery Hero, a world leader in food delivery and related services. We are looking for a Senior Data Enginner, to join our growing team that who will work closely with our fintech squads based in Singapore. You will be part of an international team of highly talented and motivated people. This role reports directly to the Director of Data & Product Analytics. What's on your plate: Build and execute SQL queries & database schema as required by the businessWork with software engineers and architects to design the target data architecture as per business requirementsArchitect, build and deploy data models with adequate documentation and validationImplement data processing and pipelining queries that will enable data-informed decision-making within the businessEnable data consumers such as analytics, data products, machine learning etc. by actively seeking opportunities to automate data pipelinesContinuously experiment state of the art technologies and proactively seek opportunities to improve the data ecosystemActively document the codes, models and data dictionaries that are being developed. What you bring to the table: Strong knowledge on SQL, data structures and database schemasExperience in data pipelining tools such as AirflowGood working knowledge of Python / Java or any other programming languageKnowledgeable in big data, Data Warehouse technologiesKnowledgeable in agile tool sets such as Jira, Confluence etc.Preferable qualifications: Experience in Google Cloud PlatformWorking knowledge on NoSQL datastores such as MongoDB, DynamoDB etc. What we can offer you: A vibrant and international team with multicultural and diverse backgrounds.Solving challenges with inspiring colleagues in an all hands-on deck environment.Management team that recognizes top performers, welcomes our newbies, and shares a love for good food.Competitive package, incentives, allowances, food perks, insurance, pension and more"
29-Apr-2022 T11:49,Big Data Engineer,Unisoft Infotech Pte Ltd,16 hours ago,,Full–time,"ï'· At least 6 to 8 years of working experience, preferably in banking environmentsï'· Familiarity with key technologies and concepts, e.g. Hadoop, Hive, Spark, Scala, Java arenecessaryï'· Familiarity with Cloudera echo systems.ï'· Candidate with experience in Analytics and big data architecture.ï'· Familiarity with key technologies and concepts, SQL & Big Data.ï'· Knowledge of products in Banking and Financial Services.ï'· Ability to write clear documentation of procedures, and ability to write specifications fordata models to be developed.ï'· Detail-oriented and able to follow clear methodologies for troubleshooting anddevelopment.ï'· Expert knowledge of data Architecture in financial service data models.ï'· Expert knowledge of Erwin data modeler tool. Certifications would be a plus.ï'· Expert in Data Analysis and Data Profiling.ï'· Efficient in prioritising work and multi-tasking"
29-Apr-2022 T11:49,Data Science Product Development,Centre for Strategic Infocomm Technologies,16 hours ago,,Full–time,"wog-icon-smiley-face,wog-dynamic-form,wog-checkbox-field,wog-email-field,wog-radio-field,wog-select-field,wog-textarea-field,wog-close-button,wog-linear-scale-field,wog-modern-feedback,wog-question-section,wog-rating,wog-connection-error,wog-floating-action-button,wog-tabbed-widget,wog-thank-you,wog-sentiments{visibility:hidden}.hydrated{visibility:inherit}CSIT | Job Detail // // // // A Singapore Government Agency Website Click logo to go back to the homepage About CSIT Who We Are Core Values Our Technology Programmes Cybersecurity Software Engineering Data Analytics Cloud Infrastructure and Services Internships & Scholarships CSIT Undergraduate Scholarship CSIT Diploma Scholarship CSIT Computing Scholarship Internships Join Us Career at CSIT Job Opportunities CSIT Events TISC 2021 TISC Home Prizes Terms and Conditions FAQ Past Winners TISC 2021 Summary Interview with community challenge creators Seach within this service Button to search and go to search results page Job Details Home Join Us Job Opportunities Job Detail Data Science Product Development Opportunities include: Product Manager (Information Management Systems) (Senior) Data Engineer (Information Management Systems) (Senior) Full Stack Software Engineer (Information Management Systems) Software Engineer (Threat Prioritisation) Senior Software Engineer (Threat Prioritisation) Engineering Manager (Threat Prioritisation) Software Engineer (Threat Research) Senior Software Engineer (Threat Research) Engineering Manager (Threat Research) Software Engineer (Profiling & Analytics System) Senior Software Engineer (Profiling & Analytics System) View Related Jobs  Centre for Strategic Infocomm Technologies Contact Us Feedback Reach.gov.sg Privacy Statement Terms of Use Rate this Website Sitemap Report Vulnerability © 2022 Government of Singapore Last Updated: 07 Jul 2021 This site is best viewed using IE10, Mozilla Firefox 60 or Google Chrome 70 & above."
29-Apr-2022 T11:50,Data Engineer,Mangtas,16 hours ago,,Full–time,"Mangtas is a marketplace for B2B services, disrupting a 700B industry by making outsourcing reliable. Our mission is to connect businesses with the global vendor ecosystem. We facilitate the process of finding, contracting, and engaging outsourcing services around the globe for corporations small and large. To that extent, we have built a vendor marketplace (www.mangtas.com) to provide a connection point for all vendor-related opportunities. We are currently focusing on services that help our clients create “Tech of the Future” – applying artificial intelligence & analytics, virtual reality & metaverses, gaming & gamification, blockchain & NFTs, IoT & robotics, cyber security and scaled microservices to name a fewThe RoleYou will be responsible for : Design, develop and maintain an infrastructure for streaming, processing, and storage of data. Build tools for effective maintenance and monitoring of the data infrastructure. Contribute to key data pipeline architecture decisions and lead the implementation of major initiatives.Work closely with stakeholders to develop scalable and performant solutions for their data requirements, including extraction, transformation, and loading of data from a range of data sources. Develop the team’s data capabilities - share knowledge, enforce best practices and encourage data-driven decisions. Develop data retention policies, and backup strategies and ensure that the firm’s data is stored redundantly and securely. Ideal ProfileSolid Computer Science fundamentals, excellent problem-solving skills and a strong understanding of distributed computing principles. At least 3 years of experience in a similar role, with a proven track record of building scalable and performant data infrastructure. Expert SQL knowledge and deep experience working with relational and NoSQL databases. Advanced knowledge of Apache Kafka and demonstrated proficiency in Hadoop v2, HDFS, and MapReduce. Experience with stream-processing systems (e.g. Storm, Spark Streaming), big data querying tools (e.g. Pig, Hive, Spark) and data serialization frameworks (e.g. Protobuf, Thrift, Avro). Bachelor’s or Master’s degree in Computer Science or related field from a top university. Able to work within the GMT+8 time zoneWhat's on Offer?An exciting and passionate working environment within a young and fast-growing companyThe opportunity to work with a high performing teamA competitive salary packageThe ability to work from anywhere in the world (assuming a stable internet connection)The chance of being a fundamental part of the team and make a difference"
29-Apr-2022 T11:50,Data Engineer Intern Trust and Safety graduate,Tencent,16 hours ago,,Full–time,"About Tencent GamesTencent Games was launched in 2003 and is now the largest gaming company in the world, achieving game-related revenues of $29.3 billion in 2020. As a leading global organization for game development, publishing, and operations, we are dedicated to providing high-quality interactive entertainment experiences to players and currently offer more than 140 games including PUBG MOBILE, Call of Duty Mobile, and League of Legends to hundreds of millions of players across more than 200 countries and regions. Tencent Games offers talented individuals the opportunity to help shape and build something unique within the gaming industry as a whole.About the Team The Interactive Entertainment Group (IEG) at Tencent is responsible for the R&D, operation, and development of the company's interactive entertainment business including games and eSports. Through online gaming, live broadcasts, and offline eSports, IEG assists the company in leading the global interactive entertainment market to create better interactive entertainment content experiences for users.Responsibilities:As a Data Engineer Intern, you will: Perform complex queries and analyses to prototype and derive insights that help identify and fight abuse across our products, improve decision-making and help tell ours fairplay policy enforcement story.Conduct end-to-end analysis that includes data discovery, data readiness assessments, technical requirements development, and clarification with stakeholders at all levels.Design, develop, maintain reliable metrics and dashboards, and build expertise on how to centralize information in a consistent and qualitative manner.Collaborate with multi-functional teams, receive mentorship, and gain insight into our value-driven process. Our goal is both to support your growth and development while empowering you for a successful start to your career. Qualifications and Skills： Bachelor’s, Master’s, or PhD in Computer Science, Software Engineering, Computer Engineering, Electrical Engineering, Mathematics or a related technical field, and a strong record of academic achievementExperience querying datasets using SQL and building dashboardsExperience solving for repeatability and scalability of data and metricsAbility to analyze data and create succinct presentations tailored to the audiencesAbility to multitask and prioritize in a fast-paced environment What we offer:* The opportunity to work with a set of hardworking and dedicated peers, all the way from engineering and QA to product management and customer support.* A constant stream of new things for you to learn and an opportunity for growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe mentors help both sides of the equation"
29-Apr-2022 T11:50,Internship Data Engineer Intern APAC,foodpanda,16 hours ago,,Full–time,"This is a full-time internship opportunity starting in May 2022At foodpanda, we're on a mission to redefine how tech, food, people and culture are connected. Operating in more than 400 cities across 12 locations worldwide, foodpanda continues to expand and grow in its core food delivery business as well as in new verticals like grocery deliveries, with a strong tech infrastructure at its core. From our restaurants-partners, cloud kitchens and cloud grocery stores — foodpanda is just one tap away, delivering everything you need quickly and conveniently to your doorstep.We are looking for undergraduates with a passion for data and insights to join our APAC Regional teams in Singapore! You will be empowered to find the most effective way of using data to help foodpanda make smart, data-driven business decisions. This is an amazing opportunity for individuals who would like to learn, thrive and hone their analytical skills. If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!What's on the menu for you: Defines and builds data pipelines that will enable faster, better, data-informed decision-making within the businessArchitect, build, and deploy new data models that provide intuitive analytics across the businessManage and own the entire data processing system (internally, as well as interfacing with other teams or third party services), provide advice on any necessary infrastructure changesExperiment with, select, and implement cutting edge Big Data tools and frameworks required to provide requested capabilitiesPerform all needed data transformation to populate a reporting optimised data warehouseParticipate in building machine learning models, deliver analyses and insights to support business decision making What you bring to the table: Penultimate or final year undergraduate students pursuing bachelor’s degree in Computer Science, Engineering, Data Analytics, Mathematics, or related disciplineAbility to write clean, structured, and high performance SQL codeStrong oral and written communication skillsStrong business mindset and ability to grasp business requirements from stakeholdersChampion of data and visualization with strong presentation and story-telling skillsKnowledge and experience with BI tools (Tableau, Data Studio), big data, Data Warehouse technologies, Python or R.Knowledge of Machine Learning, Big Data, Data Pipelines, or setting up environments for data scientists/machine learning engineers is a plus. What we Offer: A dynamic and challenging work environment.A company committed to developing you personally and professionally.A great working atmosphere with regular company and team events.A vibrant and international team committed to diversity and inclusion.Responsibility from day one in a fast growing and global company.Other benefits include free food and learning and development opportunities"
29-Apr-2022 T11:50,Data Engineer Intern,Tencent,16 hours ago,,Full–time,"ResponsibilitiesWork with cross-functional teams and understand how data platform are developed and maintained at scale.Build scalable data pipelines (using Spark, Airflow and Presto) to move data from different applications into our data warehouse.Monitor and improve automated solutions to ensure quality and performance SLAs are met.Maintain and support existing platforms and evolve to newer technology stacks and architectures. Qualifications Currently pursuing a Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.Coding & scripting proficiency in languages such as Python, C++, Golang.SQL knowledge in handling volumes of data and performance. Bonus Passion in gaming.Scratch-build a highly scalable, available, fault-tolerant data processing systems using cloud technologies, HDFS, YARN, Map-Reduce, Hive, Kafka, Spark, and other big data technologies."
29-Apr-2022 T11:50,Senior Associate Big Data Engineer Middle Office Technology Finance...,DBS Bank,16 hours ago,,Full–time,"Business Function Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation. In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels. Responsibilities Deliver solutions for online data extraction, data integration and data management. Develop and maintain high-performance, scalable utilities to support technology research and data transformation. Optimize data processing protocols and systems for better efficiency and maintainability. Contribute to the establishment and maintenance of distributed computing platform and big data services Writing of documents that clearly explain how algorithms should be implemented, verified and validated. Requirements Experience in a field encompassing Distributed computing Big Data Analytics, Data Transformation (6-8 years) Relevant industry experience in large scale crawling on cloud platforms would be favourably considered Knowledge on Core Java / Scala preferably Hands on experience in Spark is must. Hands on experience on HDFS (Hadoop), spark, impala, hive. Hands on experience on database technology. Computer Science fundamentals in algorithm design, problem solving, and complexity analysis Knowledge of professional software engineering practices & best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations Able to perform Unix / Linux scripting. Able to write the programs in Spark. Monitor and address issues relating to capacity constraints and performance related items. Analyse and perform performance tuning for jobs and Queries. Apply Now We offer a competitive salary and benefits package and the professional advantages of a dynamic environment that supports your development and recognises your achievements"
29-Apr-2022 T11:51,Data Engineer,ITCONNECTUS PTE. LTD.,16 hours ago,$5K–$13K a month,Full–time,"Immediate joiners

Requirements
• Technical Data Architect who is strong on Data Warehousing, Big Data, Data Analytics & Data Science for solutioning
• Strong Google Cloud Platform Data Components – BigQuery, BigTable, CloudSQL, Dataproc, Data Flow, Data Fusion, Etc
• Demonstrate extensive skills and success in the implementation of technology projects within a professional environment, with a particular focus on data engineering
• Experienced in delivering Medium to Big Enterprise Data Projects
• GCP Experience including and not limited to MPP systems, Database systems, ETL and ELT systems and Data Flow compute Good to have skills
• A high-level understanding of AI and ML technologies and desire to develop your skills in these areas of emerging technology.
• Should be able to advise the best of breed for the client solutions; Skills Needed
• The Data Engineer coaches the junior data engineering personnel position by bringing them up to speed and help them get better understanding of overall Data ecosystem.
• Prior experience developing, building and deploying on GCP
• Working on Solution deck, IP build, client meetings on requirement gathering"
29-Apr-2022 T11:51,Data Engineer,Asm Technology Singapore Pte Ltd,16 hours ago,,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform. We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets. You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).

Your Responsibilities

Manage and support on-premise and Cloud-based data lake and warehouse systems

Design, build, support and optimize new and existing data structure and ETL processes

Build scalable and efficient data pipelines & services to help analytics teams to process the data

Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results

Liaise with third party tool providers to understand and improve data workflow

Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms

Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification

Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines

At least 2 years' experience in data engineering, automation and integration is preferred

Strong programming and scripting skills in Python and other modern programming languages

Strong data management, schema design and SQL development skills

Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)

Self-motivated and proactive, willing to learn new things

Good communication skills and strong team player

What our preferred candidates have

Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions

Understand and experienced with Cloud platform, eg. Microsoft Azure, AWS, GCP

Business intelligence and reporting tools, eg. Power BI, Tableau, Qlik, etc

Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)

REST/Web API development and management

Knowledge in Statistical software is an advantage

Experience In building machine learning models is a plus"
29-Apr-2022 T11:51,"Principal Data Engineer, Data Engineering",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Principal Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you are responsible for helping to shape the direction of the data engineering domain in GovTech; working with other domain experts, the delivery teams, vendors, and partners to influence technology roadmaps and the adoption of suitable data solutions across government. In addition, you will teach, mentor, and proactively grow fellow data engineers within the organization.

You will be leading multiple teams of data engineers to drive impact at scale, by building whole-of-government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will architect, design and build next-generation central data infrastructure to galvanise digitalisation in the public sector. Taking a more integrated approach to lay the foundation infrastructure and driving pervasive adoption of advanced data technologies throughout the economy and society, you will architect solutions that enable public agencies and sectoral partners to better integrate, exploit and understand their data to improve operations and decision-making, and deliver high-quality digital services to citizens and businesses in Singapore.

Join us on this journey to make a difference for the nation and our future generations, if you are:
• Experienced leading data pipeline building and data wrangling and enjoys optimizing data systems and building them from the ground up as part of multiple cross-functional teams,
• Have strong project management and organizational skills, self-directed and comfortable supporting the data needs across multiple teams, systems and products,
• Excited by the prospect of optimizing or even re-designing central and agencies’ data architecture to support next generation of government products and data initiatives, and
• Recognized expert with proven technical leadership, experienced working in a agile team to build systems from architecture to deployment for cloud and on-prem infrastructure, and
• Have the desire to serve the public good through the use of technology.

What you will be working on:
• Take the driving seat and lead our data engineers in complex technical projects to:
• Expand and optimize our data and data pipeline architecture, as well as optimize data flow systems for cross functional teams in a secured and scalable manner.
• Assemble large, complex data sets that meet functional / non-functional business requirements, transforming data into formats that are easy to consume and analyse.
• Identify, design, and implement internal process improvements, e.g., automating manual processes, optimizing data delivery, re-designing infra for greater scalability.
• Build the infra required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and industry standard Infra-as-Code tools.
• Build analytics tools that utilize the data pipeline to provide actionable insights into key performance metrics and support our next gen of products and data initiatives.
• Work with stakeholders including the Executive, Product, Data and Design teams to support their data and data pipelining use cases, and ensure optimal and secured data delivery architecture is consistent across projects, products and platforms.
• Drive innovative initiatives that uplift data capability across the government sector:
• Develop a strategic sense of key organisations, software and hardware tools, methodologies, and best practices in the data engineering landscape.
• Be the bridge between engineering and product line management and translate market requirements into product definitions and architecture designs.
• Analyse and solve complex data engineering problems, and translate architecture designs into implementations that satisfy the market requirements.
• Educate the team on new technological advances and work alongside them in proof-of-concept research projects and subsequent agile product delivery.
• Provide expert perspective to help shape data-related strategy and initiatives, and be our technical ambassador by writing internal blogs or publishing white papers.

What we are looking for:
• Candidate with 8+ years of experience in a Lead Data Engineer role, who has attained a Bachelor’s degree in Computer Science or other relevant engineering degree.
• In-depth working knowledge across a variety of relational SQL and NoSQL databases, including efficient query authoring, performance optimisation and troubleshooting.
• Experience building and optimizing data, data pipelines, and architectures, with processes to support transformation, data structures, metadata, dependency and workload management.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for further improvement.
• Working knowledge of message queuing, stream processing, and scalable data stores.
• A successful history of processing and extracting value from large datasets from different sources and systems, with strong analytics skills related to working with unstructured data.
• You should also have experience using the following software/tools:
• Big data and stream-processing tools: Hadoop, Spark, Beam, Flink, Kafka, etc.
• Data pipelining and workflow management tools: Luigi, Airflow, etc
• Cloud computing environments and services: AWS, GCP, Azure
• DevSecOps tools: Git, Kubernetes, Docker, Terraform etc
• Programming languages: Python, Java, C++, Scala, etc

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours”"
29-Apr-2022 T11:51,Data Engineer,Thakral One,16 hours ago,$8.5K–$10.5K a month,Full–time,"• Work with Banking team to understand existing SAS code logic written by techno-functional users
• Develop the SAS logic in Big Data environment, using spark, scala and pyspark utilizing client Hadoop ecosystem of the inhouse Data Lake
• Perform unit testing and system integration testing
• Work closely with Business Analysts team to review the test results and obtain sign off
• Deploy the new code in the client Production environment
• Prepare necessary design/operations documentation for future usage
• Perform peers Code quality review and be gatekeeper for quality checks
• 5-8 years of application development experience in Spark, Spark SQL, Scala is a must
• Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc
• Technical proficiency on data mining techniques and performance optimization
• Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL)
• Experience on SAS will be beneficial but not mandatory
• Handling of reporting packages (Tableau, QlikView) is nice to have
• AWS experience is nice to have
• Degree in Computer Science or Engineering is a must
• Good problem diagnosis and creative problem-solving skills
• Passion to learn and master diverse new technologies in the open-source community
• Accuracy and attention to detail
• Team-working, Verbal and Written communication skills

Desired Skills and Experience
Tableau, PySpark, Scala, Big Data, Pipelines, Unit Testing, Hadoop, Application Development, MariaDB, Spark, Data Mining, SQL, Attention to Detail, Python, System Integration Testing, S3"
29-Apr-2022 T11:51,Data Engineer,POWER IT SERVICES,13 hours ago,,Full–time,"Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

· Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

· A successful history of manipulating, processing and extracting value from large datasets.

· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science Statistics, Informatics, Information Systems or another quantitative field.

They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:51,Staff Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,16 hours ago,$8K–$13K a month,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 5 years of experience in Data Engineering role and have good knowledge / working experience in:

• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.

• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.

• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.

• Functional programming languages, e.g. Scala.

• Virtualization and container environment such as Docker and Kubernetes.

•Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:51,Data Engineering Lead,Stemly Pte. Ltd.,16 hours ago,,Full–time,"We are looking for a Senior Data Engineer with a track record of building enterprise grade data platform for cloud products .

This is an exciting opportunity for a technologist to build an innovative SaaS product using state-of-the-art and modern cloud technology stack.

As a Data Engineering Lead, you will be working on all aspects of data, from platform and infra build out to pipeline engineering and writing tooling / services for augmenting and fronting the core platform.

The data platform interfaces with large systems to ingest data (batch, micro-batch) to create statistical / machine learning models in a distributed setting.

You will work closely with data scientists, product managers, legal, compliance and business stakeholders.

You will also evaluate new technologies / frameworks and contribute to various open source big data technologies. You are encouraged to think out of the box and have fun exploring the latest patterns and designs in the fields of software and data engineering.

Your responsibilities
• Architecting and implementing a robust data platform for Stemly's products. Design and develop extremely efficient, reliable and observable data pipelines using technologies such as Airflow, Kubernetes, ElasticSearch, Kafka / KubeMQ, Postgres, ClickHouse etc.
• Design and deliver the next-gen data lifecycle management suite of tools / frameworks , including ingestion and consumption on the top of the data lake to support real-time, API-based and serverless use-cases, along with batch (mini / micro)
• Build and expose metadata catalogue for the Data Lake for easy exploration, profiling as well as lineage requirements
• Work on the development of the data platform iteratively by building quick POCs and converting ideas into real products
• Make data discoverable and easy to use for Data Scientists and Analysts. Enable Data Science teams to test and productize various ML models
• Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.

Support your colleagues by reviewing code and designs
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner

About you
• 8+ years of professional experience in software development in Python and Pandas
• Extensive experience with object-oriented design, coding and testing patterns, including experience with engineering software platforms and data infrastructures
• Must have sound understanding for Big Data components & administration fundamentals
• Hands-on in building a complete data platform using various open source technologies. Must have the knowledge to build metadata, lineage, observability and discoverability for data platform
• Experienced in DevOps best practices like CI / CD, containerization, blue-green deployments, secrets management etc. in the Data ecosystem
• Expert in the principles of distributed computing and data modelling
• Expert in building optimized SQL queries
• Conversant with the latest developments in the areas of Machine Learning models and efficiently supporting the data scientists is a plus
• Experience with Airflow, ClickHouse, Postgres, Kubernetes, GraphDB is a big plus
• Familiarity with Google Cloud Platform (e.g. GCS, Cloud Composer, BigQuery) is a plus
• Outstanding communication and interpersonal skills

About us

We are scientists and engineers with a strong passion for cutting-edge technology and decision science. Our mission is to enable enterprises to reduce time to decision with state-of-the-art forecasting and optimization.

Our algorithms are based on automatic machine learning techniques and allow for the discovery of optimal forecasting models.

We unlock sizable business and financial impact in terms of lower lost demand, decrease in inventory, lower working capital and financing costs.

How to apply

You may already know if you're a fit, but perhaps you're worried about some of the requirements We're looking for smart and passionate Engineers;

if you're the right candidate, we're flexible"
29-Apr-2022 T11:51,Data Engineer,Eastspring Investments (singapore) Limited,16 hours ago,,Full–time,"At Eastspring Investments, we understand that success comes from the talent and commitment of our people. We strive to build a business that you can shape, an inclusive workplace where everyone's ideas are valued and a culture where we can thrive together. Our people stay connected and tuned in to what's happening around us, keeping us ahead of the curve. While focused on the long-term, we look to the future to bring growth, development and benefit to everyone whose lives we touch.

Key Accountabilities

Main responsibilities:

Design and implement our Data and AI central data platform as well as related tools/systems for advanced business analytics and enterprise data governance

Manage data modeling design, writing, and optimizing ETL jobs

Participate in building and enhancing enterprise cloud data warehouse

Deliver and manage in-house and cloud-native data solutions to meet business requirements across firm-wide business units

Assist in creating and monitoring analytics dashboards, for different business functions

Ensure quality, integrity and accuracy of datasets through tracked, secured and auditable controls

Work with stakeholders to assist with data-related technical issues and support their data needs.

Follow and enforce best practices in software development and data engineering

Requirements:

Excellent coding skills with Python and SQL, and solid understanding of object-oriented analysis and design

Working knowledge of common algorithms and data structures, with strong analytical and problem-solving skills

Hands-on experience with Linux and shell scripting

Working experience with containerization (Docker/K8S) and task orchestration tools (Airflow/Luigi, etc.)

Experience with cloud service and tools (AWS/Azure/GCP), as well as cloud data warehouse platforms.

Experience with modern DevOps practices including version control, TDD, CI/CD, etc., for both code and configuration changes.

Basic understanding and experience with ML/AI concepts (e.g. deep learning, deep reinforcement learning, deep bayesian learning), workflows, and toolsets (Jupyter Notebook, etc.), and libraries (Numpy, Pandas, Scikit Learn, PyTorch, etc.) preferably in both cloud-native and desktop deployments.

Experience with traditional RDBMS based systems, including Data Lake, Data Warehouses and Marts, and more modern NoSQL and cloud-native big-data technology stacks such as document-oriented databases, Hadoop, columnar data files (e.g., Parquet), etc.

Familiar with REST APIs, service-oriented architectures (SOA) / microservices, virtualization, and serverless deployment architectures.

Demonstrated ability to understand, work with and deliver robust solutions in more than one programming language, framework, technology stack, runtime environment, etc..

QUALIFICATIONS / EXPERIENCE:

Degree level or higher in Computer Science or another quantitative field

1-5 year(s) technical experience showing increasing sophistication of solutions implemented, and ability to deliver

Fluency in both written and spoken English"
29-Apr-2022 T11:51,Data Engineer - APAC,Tamr,16 hours ago,,Full–time,"Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE
who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being
customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation
This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.
Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion
Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:51,"Data Engineer, Data Engineering",Bank of Singapore,17 hours ago,,Full–time,"At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programmes that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group’s global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today

Bank of Singapore opens doors to new opportunities.
Start your career with Bank of Singapore as a Data Engineer (6 months contract) in our Data Engineering team
Bank of Singapore opens doors to new opportunities. At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programs that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group's global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today
Bank of Singapore has defined an ambitious roadmap for accelerating its data excellence through the implementation of various bank-wide programs. As part of this roadmap, building the foundation of good data governance and management plays a key role in establishing trust in the data that we use.
General Description:
The incumbent will be based in the Data engineering team within the Data Hub team, supporting Bank of Singapore's data sourcing, curation, transformation, and data analytic services to accelerate their data initiatives. He/she will be responsible for expanding and optimizing BOS data pipelines as well as supporting data analytics needs.
Core activities :
• Be responsible for supporting data analytics and dashboarding with respects to the data availability / stability.
• Understanding the data and identifying any potential data issues as well as correcting issues arising from data.
• Define and implement data models suitable for dashboarding
• Power BI dashboarding support

Qualifications
Requirement :
General Knowledge & Experience
• Minimum of 6 years of working experience in data related functions
• Understanding of banking / financial services with exposure to data analytics preferred
Programming & Data
• Strong hands-on skills in SQL & PL/SQL
• Solid background in traditional structured database environments such as Teradata / Oracle
• Knowledge on Power BI dashboard functionality
• Strong skills in Power BI related technologies like power query and DAX
• Exposure to programming languages such as python
Communication & Soft Skills
• Curiosity & a real passion for understanding ""why?""
• Pragmatic ""can do"" approach to finding data based solutions to problems
• Good communication skills with ability to engage across business and technical audiences"
29-Apr-2022 T11:51,Data Engineer,ITCONNECTUS PTE. LTD.,16 hours ago,$5K–$13K a month,Full–time,"Immediate joiners

Requirements
• Technical Data Architect who is strong on Data Warehousing, Big Data, Data Analytics & Data Science for solutioning
• Strong Google Cloud Platform Data Components – BigQuery, BigTable, CloudSQL, Dataproc, Data Flow, Data Fusion, Etc
• Demonstrate extensive skills and success in the implementation of technology projects within a professional environment, with a particular focus on data engineering
• Experienced in delivering Medium to Big Enterprise Data Projects
• GCP Experience including and not limited to MPP systems, Database systems, ETL and ELT systems and Data Flow compute Good to have skills
• A high-level understanding of AI and ML technologies and desire to develop your skills in these areas of emerging technology.
• Should be able to advise the best of breed for the client solutions; Skills Needed
• The Data Engineer coaches the junior data engineering personnel position by bringing them up to speed and help them get better understanding of overall Data ecosystem.
• Prior experience developing, building and deploying on GCP
• Working on Solution deck, IP build, client meetings on requirement gathering"
29-Apr-2022 T11:51,Data Engineer,Asm Technology Singapore Pte Ltd,16 hours ago,,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform. We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets. You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).

Your Responsibilities

Manage and support on-premise and Cloud-based data lake and warehouse systems

Design, build, support and optimize new and existing data structure and ETL processes

Build scalable and efficient data pipelines & services to help analytics teams to process the data

Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results

Liaise with third party tool providers to understand and improve data workflow

Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms

Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification

Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines

At least 2 years' experience in data engineering, automation and integration is preferred

Strong programming and scripting skills in Python and other modern programming languages

Strong data management, schema design and SQL development skills

Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)

Self-motivated and proactive, willing to learn new things

Good communication skills and strong team player

What our preferred candidates have

Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions

Understand and experienced with Cloud platform, eg. Microsoft Azure, AWS, GCP

Business intelligence and reporting tools, eg. Power BI, Tableau, Qlik, etc

Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)

REST/Web API development and management

Knowledge in Statistical software is an advantage

Experience In building machine learning models is a plus"
29-Apr-2022 T11:51,"Principal Data Engineer, Data Engineering",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Principal Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you are responsible for helping to shape the direction of the data engineering domain in GovTech; working with other domain experts, the delivery teams, vendors, and partners to influence technology roadmaps and the adoption of suitable data solutions across government. In addition, you will teach, mentor, and proactively grow fellow data engineers within the organization.

You will be leading multiple teams of data engineers to drive impact at scale, by building whole-of-government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will architect, design and build next-generation central data infrastructure to galvanise digitalisation in the public sector. Taking a more integrated approach to lay the foundation infrastructure and driving pervasive adoption of advanced data technologies throughout the economy and society, you will architect solutions that enable public agencies and sectoral partners to better integrate, exploit and understand their data to improve operations and decision-making, and deliver high-quality digital services to citizens and businesses in Singapore.

Join us on this journey to make a difference for the nation and our future generations, if you are:
• Experienced leading data pipeline building and data wrangling and enjoys optimizing data systems and building them from the ground up as part of multiple cross-functional teams,
• Have strong project management and organizational skills, self-directed and comfortable supporting the data needs across multiple teams, systems and products,
• Excited by the prospect of optimizing or even re-designing central and agencies’ data architecture to support next generation of government products and data initiatives, and
• Recognized expert with proven technical leadership, experienced working in a agile team to build systems from architecture to deployment for cloud and on-prem infrastructure, and
• Have the desire to serve the public good through the use of technology.

What you will be working on:
• Take the driving seat and lead our data engineers in complex technical projects to:
• Expand and optimize our data and data pipeline architecture, as well as optimize data flow systems for cross functional teams in a secured and scalable manner.
• Assemble large, complex data sets that meet functional / non-functional business requirements, transforming data into formats that are easy to consume and analyse.
• Identify, design, and implement internal process improvements, e.g., automating manual processes, optimizing data delivery, re-designing infra for greater scalability.
• Build the infra required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and industry standard Infra-as-Code tools.
• Build analytics tools that utilize the data pipeline to provide actionable insights into key performance metrics and support our next gen of products and data initiatives.
• Work with stakeholders including the Executive, Product, Data and Design teams to support their data and data pipelining use cases, and ensure optimal and secured data delivery architecture is consistent across projects, products and platforms.
• Drive innovative initiatives that uplift data capability across the government sector:
• Develop a strategic sense of key organisations, software and hardware tools, methodologies, and best practices in the data engineering landscape.
• Be the bridge between engineering and product line management and translate market requirements into product definitions and architecture designs.
• Analyse and solve complex data engineering problems, and translate architecture designs into implementations that satisfy the market requirements.
• Educate the team on new technological advances and work alongside them in proof-of-concept research projects and subsequent agile product delivery.
• Provide expert perspective to help shape data-related strategy and initiatives, and be our technical ambassador by writing internal blogs or publishing white papers.

What we are looking for:
• Candidate with 8+ years of experience in a Lead Data Engineer role, who has attained a Bachelor’s degree in Computer Science or other relevant engineering degree.
• In-depth working knowledge across a variety of relational SQL and NoSQL databases, including efficient query authoring, performance optimisation and troubleshooting.
• Experience building and optimizing data, data pipelines, and architectures, with processes to support transformation, data structures, metadata, dependency and workload management.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for further improvement.
• Working knowledge of message queuing, stream processing, and scalable data stores.
• A successful history of processing and extracting value from large datasets from different sources and systems, with strong analytics skills related to working with unstructured data.
• You should also have experience using the following software/tools:
• Big data and stream-processing tools: Hadoop, Spark, Beam, Flink, Kafka, etc.
• Data pipelining and workflow management tools: Luigi, Airflow, etc
• Cloud computing environments and services: AWS, GCP, Azure
• DevSecOps tools: Git, Kubernetes, Docker, Terraform etc
• Programming languages: Python, Java, C++, Scala, etc

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours”"
29-Apr-2022 T11:51,Data Engineer,Thakral One,16 hours ago,$8.5K–$10.5K a month,Full–time,"• Work with Banking team to understand existing SAS code logic written by techno-functional users
• Develop the SAS logic in Big Data environment, using spark, scala and pyspark utilizing client Hadoop ecosystem of the inhouse Data Lake
• Perform unit testing and system integration testing
• Work closely with Business Analysts team to review the test results and obtain sign off
• Deploy the new code in the client Production environment
• Prepare necessary design/operations documentation for future usage
• Perform peers Code quality review and be gatekeeper for quality checks
• 5-8 years of application development experience in Spark, Spark SQL, Scala is a must
• Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc
• Technical proficiency on data mining techniques and performance optimization
• Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL)
• Experience on SAS will be beneficial but not mandatory
• Handling of reporting packages (Tableau, QlikView) is nice to have
• AWS experience is nice to have
• Degree in Computer Science or Engineering is a must
• Good problem diagnosis and creative problem-solving skills
• Passion to learn and master diverse new technologies in the open-source community
• Accuracy and attention to detail
• Team-working, Verbal and Written communication skills

Desired Skills and Experience
Tableau, PySpark, Scala, Big Data, Pipelines, Unit Testing, Hadoop, Application Development, MariaDB, Spark, Data Mining, SQL, Attention to Detail, Python, System Integration Testing, S3"
29-Apr-2022 T11:51,Data Engineer,POWER IT SERVICES,13 hours ago,,Full–time,"Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

· Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

· A successful history of manipulating, processing and extracting value from large datasets.

· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science Statistics, Informatics, Information Systems or another quantitative field.

They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:51,Staff Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,16 hours ago,$8K–$13K a month,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 5 years of experience in Data Engineering role and have good knowledge / working experience in:

• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.

• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.

• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.

• Functional programming languages, e.g. Scala.

• Virtualization and container environment such as Docker and Kubernetes.

•Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:51,Data Engineering Lead,Stemly Pte. Ltd.,16 hours ago,,Full–time,"We are looking for a Senior Data Engineer with a track record of building enterprise grade data platform for cloud products .

This is an exciting opportunity for a technologist to build an innovative SaaS product using state-of-the-art and modern cloud technology stack.

As a Data Engineering Lead, you will be working on all aspects of data, from platform and infra build out to pipeline engineering and writing tooling / services for augmenting and fronting the core platform.

The data platform interfaces with large systems to ingest data (batch, micro-batch) to create statistical / machine learning models in a distributed setting.

You will work closely with data scientists, product managers, legal, compliance and business stakeholders.

You will also evaluate new technologies / frameworks and contribute to various open source big data technologies. You are encouraged to think out of the box and have fun exploring the latest patterns and designs in the fields of software and data engineering.

Your responsibilities
• Architecting and implementing a robust data platform for Stemly's products. Design and develop extremely efficient, reliable and observable data pipelines using technologies such as Airflow, Kubernetes, ElasticSearch, Kafka / KubeMQ, Postgres, ClickHouse etc.
• Design and deliver the next-gen data lifecycle management suite of tools / frameworks , including ingestion and consumption on the top of the data lake to support real-time, API-based and serverless use-cases, along with batch (mini / micro)
• Build and expose metadata catalogue for the Data Lake for easy exploration, profiling as well as lineage requirements
• Work on the development of the data platform iteratively by building quick POCs and converting ideas into real products
• Make data discoverable and easy to use for Data Scientists and Analysts. Enable Data Science teams to test and productize various ML models
• Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.

Support your colleagues by reviewing code and designs
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner

About you
• 8+ years of professional experience in software development in Python and Pandas
• Extensive experience with object-oriented design, coding and testing patterns, including experience with engineering software platforms and data infrastructures
• Must have sound understanding for Big Data components & administration fundamentals
• Hands-on in building a complete data platform using various open source technologies. Must have the knowledge to build metadata, lineage, observability and discoverability for data platform
• Experienced in DevOps best practices like CI / CD, containerization, blue-green deployments, secrets management etc. in the Data ecosystem
• Expert in the principles of distributed computing and data modelling
• Expert in building optimized SQL queries
• Conversant with the latest developments in the areas of Machine Learning models and efficiently supporting the data scientists is a plus
• Experience with Airflow, ClickHouse, Postgres, Kubernetes, GraphDB is a big plus
• Familiarity with Google Cloud Platform (e.g. GCS, Cloud Composer, BigQuery) is a plus
• Outstanding communication and interpersonal skills

About us

We are scientists and engineers with a strong passion for cutting-edge technology and decision science. Our mission is to enable enterprises to reduce time to decision with state-of-the-art forecasting and optimization.

Our algorithms are based on automatic machine learning techniques and allow for the discovery of optimal forecasting models.

We unlock sizable business and financial impact in terms of lower lost demand, decrease in inventory, lower working capital and financing costs.

How to apply

You may already know if you're a fit, but perhaps you're worried about some of the requirements We're looking for smart and passionate Engineers;

if you're the right candidate, we're flexible"
29-Apr-2022 T11:51,Data Engineer,Eastspring Investments (singapore) Limited,16 hours ago,,Full–time,"At Eastspring Investments, we understand that success comes from the talent and commitment of our people. We strive to build a business that you can shape, an inclusive workplace where everyone's ideas are valued and a culture where we can thrive together. Our people stay connected and tuned in to what's happening around us, keeping us ahead of the curve. While focused on the long-term, we look to the future to bring growth, development and benefit to everyone whose lives we touch.

Key Accountabilities

Main responsibilities:

Design and implement our Data and AI central data platform as well as related tools/systems for advanced business analytics and enterprise data governance

Manage data modeling design, writing, and optimizing ETL jobs

Participate in building and enhancing enterprise cloud data warehouse

Deliver and manage in-house and cloud-native data solutions to meet business requirements across firm-wide business units

Assist in creating and monitoring analytics dashboards, for different business functions

Ensure quality, integrity and accuracy of datasets through tracked, secured and auditable controls

Work with stakeholders to assist with data-related technical issues and support their data needs.

Follow and enforce best practices in software development and data engineering

Requirements:

Excellent coding skills with Python and SQL, and solid understanding of object-oriented analysis and design

Working knowledge of common algorithms and data structures, with strong analytical and problem-solving skills

Hands-on experience with Linux and shell scripting

Working experience with containerization (Docker/K8S) and task orchestration tools (Airflow/Luigi, etc.)

Experience with cloud service and tools (AWS/Azure/GCP), as well as cloud data warehouse platforms.

Experience with modern DevOps practices including version control, TDD, CI/CD, etc., for both code and configuration changes.

Basic understanding and experience with ML/AI concepts (e.g. deep learning, deep reinforcement learning, deep bayesian learning), workflows, and toolsets (Jupyter Notebook, etc.), and libraries (Numpy, Pandas, Scikit Learn, PyTorch, etc.) preferably in both cloud-native and desktop deployments.

Experience with traditional RDBMS based systems, including Data Lake, Data Warehouses and Marts, and more modern NoSQL and cloud-native big-data technology stacks such as document-oriented databases, Hadoop, columnar data files (e.g., Parquet), etc.

Familiar with REST APIs, service-oriented architectures (SOA) / microservices, virtualization, and serverless deployment architectures.

Demonstrated ability to understand, work with and deliver robust solutions in more than one programming language, framework, technology stack, runtime environment, etc..

QUALIFICATIONS / EXPERIENCE:

Degree level or higher in Computer Science or another quantitative field

1-5 year(s) technical experience showing increasing sophistication of solutions implemented, and ability to deliver

Fluency in both written and spoken English"
29-Apr-2022 T11:51,Data Engineer - APAC,Tamr,16 hours ago,,Full–time,"Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE
who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being
customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation
This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.
Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion
Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:51,"Data Engineer, Data Engineering",Bank of Singapore,17 hours ago,,Full–time,"At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programmes that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group’s global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today

Bank of Singapore opens doors to new opportunities.
Start your career with Bank of Singapore as a Data Engineer (6 months contract) in our Data Engineering team
Bank of Singapore opens doors to new opportunities. At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programs that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group's global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today
Bank of Singapore has defined an ambitious roadmap for accelerating its data excellence through the implementation of various bank-wide programs. As part of this roadmap, building the foundation of good data governance and management plays a key role in establishing trust in the data that we use.
General Description:
The incumbent will be based in the Data engineering team within the Data Hub team, supporting Bank of Singapore's data sourcing, curation, transformation, and data analytic services to accelerate their data initiatives. He/she will be responsible for expanding and optimizing BOS data pipelines as well as supporting data analytics needs.
Core activities :
• Be responsible for supporting data analytics and dashboarding with respects to the data availability / stability.
• Understanding the data and identifying any potential data issues as well as correcting issues arising from data.
• Define and implement data models suitable for dashboarding
• Power BI dashboarding support

Qualifications
Requirement :
General Knowledge & Experience
• Minimum of 6 years of working experience in data related functions
• Understanding of banking / financial services with exposure to data analytics preferred
Programming & Data
• Strong hands-on skills in SQL & PL/SQL
• Solid background in traditional structured database environments such as Teradata / Oracle
• Knowledge on Power BI dashboard functionality
• Strong skills in Power BI related technologies like power query and DAX
• Exposure to programming languages such as python
Communication & Soft Skills
• Curiosity & a real passion for understanding ""why?""
• Pragmatic ""can do"" approach to finding data based solutions to problems
• Good communication skills with ability to engage across business and technical audiences"
29-Apr-2022 T11:51,Data Engineer,ITCONNECTUS PTE. LTD.,16 hours ago,$5K–$13K a month,Full–time,"Immediate joiners

Requirements
• Technical Data Architect who is strong on Data Warehousing, Big Data, Data Analytics & Data Science for solutioning
• Strong Google Cloud Platform Data Components – BigQuery, BigTable, CloudSQL, Dataproc, Data Flow, Data Fusion, Etc
• Demonstrate extensive skills and success in the implementation of technology projects within a professional environment, with a particular focus on data engineering
• Experienced in delivering Medium to Big Enterprise Data Projects
• GCP Experience including and not limited to MPP systems, Database systems, ETL and ELT systems and Data Flow compute Good to have skills
• A high-level understanding of AI and ML technologies and desire to develop your skills in these areas of emerging technology.
• Should be able to advise the best of breed for the client solutions; Skills Needed
• The Data Engineer coaches the junior data engineering personnel position by bringing them up to speed and help them get better understanding of overall Data ecosystem.
• Prior experience developing, building and deploying on GCP
• Working on Solution deck, IP build, client meetings on requirement gathering"
29-Apr-2022 T11:51,Data Engineer,Manpower Singapore,2 days ago,,Full–time,"Data Engineer

Key Role and Responsibilities:
• Partner with the global and in-market data experts to discover and derive value from connecting external and internal data sources.
• Build (ETL) new and evolve data models and pipelines to power algorithmic based Business Intelligent solutions that addresses business problems requiring descriptive, diagnostic, predictive, and/or prescriptive analytics for pricing, promotion, trade spending, assortment, and sales performance management.
• Translates the algorithms and analytic models into data models as business needs evolves on a going basis after they are put into production.
• Develop a roadmap that scales existing and new data models, to support the portfolio of solutions.
• Expand into Business Intelligence solution development focused on automation and scale up of solutions.

Key Skills Required:

Professional Skills:
R/Python programming languages
MS Power BI
MS Excel
Data Visualisation

General Skills:
Client Management
Project Management
• Diploma or B.S. in a relevant technical field (Operations Research, Computer Science, Statistics, Business Analytics, Econometrics, or Mathematics).
• Overall experience of 2-3 years
• Ability to manipulate and high-volume of data from varying sources
• Expert knowledge of an analysis tool such as Microsoft PowerBI
• Proficiency in R/Python
• Basic track record in working independently with minimal guidance

Interested applicants, please submit your resume to : abelene.kang@manpower.com.sg
Abelene Marianne Kang Mrs Abelene Marianne Rozario R2089914
EA License No: 02C3423 Reg No: 199505951H

Kang Abelene Marianne Mrs Rozario Abelene Marianne

EA License No.: 02C3423 | Personnel Reg No.: R2089914

Please note that your response to this advertisement and communications with us pursuant to this advertisement will constitute informed consent to the collection, use and/or disclosure of personal data by ManpowerGroup Singapore for the purpose of carrying out its business, in compliance with the relevant provisions of the Personal Data Protection Act 2012. To learn more about ManpowerGroup's Global Privacy Policy, please visit https://www.manpower.com.sg/privacy-policy"
29-Apr-2022 T11:51,Data Engineer - User Profile,Shopee,Full–time,,,"Collaborate closely with the product team and provide data solutions to business problems and support business growth and data driven decision makingsDesign, build and maintain batch and realtime streaming user data pipelines using big data platform and technologiesResponsible for user data collection, processing, storage and building the user profile data collectionsAdopt and maintain data engine solutions to support online data services for various business use casesImprove efficiency, scalability, and stability of existing systemsEnsure data quality, consistency and timeliness"
29-Apr-2022 T11:51,"Principal Data Engineer, Data Engineering",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Principal Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you are responsible for helping to shape the direction of the data engineering domain in GovTech; working with other domain experts, the delivery teams, vendors, and partners to influence technology roadmaps and the adoption of suitable data solutions across government. In addition, you will teach, mentor, and proactively grow fellow data engineers within the organization.

You will be leading multiple teams of data engineers to drive impact at scale, by building whole-of-government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will architect, design and build next-generation central data infrastructure to galvanise digitalisation in the public sector. Taking a more integrated approach to lay the foundation infrastructure and driving pervasive adoption of advanced data technologies throughout the economy and society, you will architect solutions that enable public agencies and sectoral partners to better integrate, exploit and understand their data to improve operations and decision-making, and deliver high-quality digital services to citizens and businesses in Singapore.

Join us on this journey to make a difference for the nation and our future generations, if you are:
• Experienced leading data pipeline building and data wrangling and enjoys optimizing data systems and building them from the ground up as part of multiple cross-functional teams,
• Have strong project management and organizational skills, self-directed and comfortable supporting the data needs across multiple teams, systems and products,
• Excited by the prospect of optimizing or even re-designing central and agencies’ data architecture to support next generation of government products and data initiatives, and
• Recognized expert with proven technical leadership, experienced working in a agile team to build systems from architecture to deployment for cloud and on-prem infrastructure, and
• Have the desire to serve the public good through the use of technology.

What you will be working on:
• Take the driving seat and lead our data engineers in complex technical projects to:
• Expand and optimize our data and data pipeline architecture, as well as optimize data flow systems for cross functional teams in a secured and scalable manner.
• Assemble large, complex data sets that meet functional / non-functional business requirements, transforming data into formats that are easy to consume and analyse.
• Identify, design, and implement internal process improvements, e.g., automating manual processes, optimizing data delivery, re-designing infra for greater scalability.
• Build the infra required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and industry standard Infra-as-Code tools.
• Build analytics tools that utilize the data pipeline to provide actionable insights into key performance metrics and support our next gen of products and data initiatives.
• Work with stakeholders including the Executive, Product, Data and Design teams to support their data and data pipelining use cases, and ensure optimal and secured data delivery architecture is consistent across projects, products and platforms.
• Drive innovative initiatives that uplift data capability across the government sector:
• Develop a strategic sense of key organisations, software and hardware tools, methodologies, and best practices in the data engineering landscape.
• Be the bridge between engineering and product line management and translate market requirements into product definitions and architecture designs.
• Analyse and solve complex data engineering problems, and translate architecture designs into implementations that satisfy the market requirements.
• Educate the team on new technological advances and work alongside them in proof-of-concept research projects and subsequent agile product delivery.
• Provide expert perspective to help shape data-related strategy and initiatives, and be our technical ambassador by writing internal blogs or publishing white papers.

What we are looking for:
• Candidate with 8+ years of experience in a Lead Data Engineer role, who has attained a Bachelor’s degree in Computer Science or other relevant engineering degree.
• In-depth working knowledge across a variety of relational SQL and NoSQL databases, including efficient query authoring, performance optimisation and troubleshooting.
• Experience building and optimizing data, data pipelines, and architectures, with processes to support transformation, data structures, metadata, dependency and workload management.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for further improvement.
• Working knowledge of message queuing, stream processing, and scalable data stores.
• A successful history of processing and extracting value from large datasets from different sources and systems, with strong analytics skills related to working with unstructured data.
• You should also have experience using the following software/tools:
• Big data and stream-processing tools: Hadoop, Spark, Beam, Flink, Kafka, etc.
• Data pipelining and workflow management tools: Luigi, Airflow, etc
• Cloud computing environments and services: AWS, GCP, Azure
• DevSecOps tools: Git, Kubernetes, Docker, Terraform etc
• Programming languages: Python, Java, C++, Scala, etc

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours”"
29-Apr-2022 T11:51,"Data Engineer, Data Engineering (6 months Contract)",Bank of Singapore,10 days ago,$8K–$12K a month,Contractor,"Bank of Singapore opens doors to new opportunities.

Start your career with Bank of Singapore as a Data Engineer (6 months contract) in our Data Engineering team!

Bank of Singapore opens doors to new opportunities. At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programs that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group’s global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today!

Bank of Singapore has defined an ambitious roadmap for accelerating its data excellence through the implementation of various bank-wide programs. As part of this roadmap, building the foundation of good data governance and management plays a key role in establishing trust in the data that we use.

General Description

The incumbent will be based in the Data engineering team within the Data Hub team, supporting Bank of Singapore’s data sourcing, curation, transformation, and data analytic services to accelerate their data initiatives. He/she will be responsible for expanding and optimizing BOS data pipelines as well as supporting data analytics needs.

Core Activities
• Be responsible for supporting data analytics and dashboarding with respects to the data availability / stability.
• Understanding the data and identifying any potential data issues as well as correcting issues arising from data.
• Define and implement data models suitable for dashboarding
• Power BI dashboarding support
Requirement

General Knowledge & Experience
• Minimum of 6 years of working experience in data related functions
• Understanding of banking / financial services with exposure to data analytics preferred
Programming & Data
• Strong hands-on skills in SQL & PL/SQL
• Solid background in traditional structured database environments such as Teradata / Oracle
• Knowledge on Power BI dashboard functionality
• Strong skills in Power BI related technologies like power query and DAX
• Exposure to programming languages such as python
Communication & Soft Skills
• Curiosity & a real passion for understanding “why?”
• Pragmatic “can do” approach to finding data based solutions to problems
• Good communication skills with ability to engage across business and technical audiences"
29-Apr-2022 T11:51,Data Engineer,Thakral One Pte. Ltd.,3 days ago,,Full–time,"Job responsibilities: Work with Banking team to understand existing SAS code logic written by techno-functional users Develop the SAS logic in Big Data environment, using spark, scala and pyspark utilizing client Hadoop ecosystem of the inhouse Data Lake Perform unit testing and system integration testing Work closely with Business Analysts team to review the test results and obtain sign off Deploy the new code in the client Production environment Prepare necessary design/operations documentation for future usage Perform peers Code quality review and be gatekeeper for quality checks 5-8 years of application development experience in Spark, Spark SQL, Scala is a must Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc Technical proficiency on data mining techniques and performance optimization Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL) Experience on SAS will be beneficial but not mandatory Handling of reporting packages (Tableau, QlikView) is nice to have AWS experience is nice to have Degree in Computer Science or Engineering is a must Good problem diagnosis and creative problem-solving skills Passion to learn and master diverse new technologies in the open-source community Accuracy and attention to detail Team-working, Verbal and Written communication skills"
29-Apr-2022 T11:51,Data Engineer,ASM TECHNOLOGY SINGAPORE PTE LTD,2 days ago,$4K–$5.5K a month,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform. We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets. You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).

Your Responsibilities
• Manage and support on-premise and Cloud-based data lake and warehouse systems
• Design, build, support and optimize new and existing data structure and ETL processes
• Build scalable and efficient data pipelines & services to help analytics teams to process the data
• Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results
• Liaise with third party tool providers to understand and improve data workflow
• Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms
• Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification
• Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines
• At least 2 years’ experience in data engineering, automation and integration is preferred
• Strong programming and scripting skills in Python and other modern programming languages
• Strong data management, schema design and SQL development skills
• Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)
• Self-motivated and proactive, willing to learn new things
• Good communication skills and strong team player

What our preferred candidates have?
• Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions
• Understand and experienced with Cloud platform, eg. Microsoft Azure, AWS, GCP
• Business intelligence and reporting tools, eg. Power BI, Tableau, Qlik, etc
• Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)
• REST/Web API development and management
• Knowledge in Statistical software is an advantage
• Experience In building machine learning models is a plus"
29-Apr-2022 T11:51,"Data Engineer, MBG",Meta,1 day ago,,Full–time,"Meta is looking for exceptionally talented and experienced engineers to join the MBG Technology team. Our team provides analytics and workflow tools for Meta Business Group ( MBG), partnering with sales, marketing, measurement, support and operations teams.In this role, you’ll work with some of the brightest minds in the industry, work with one of the richest data sets in the world, use cutting edge technology, and get an opportunity to solve some of the most challenging business and engineering problems, at a scale that few companies can match. You will do so by partnering with stakeholders/teams and building scalable, reliable solutions that provide business critical insights and metrics, while ensuring the best uptime and responsiveness.

Data Engineer, MBG Responsibilities:
• Manage data warehouse plans for a business vertical or a group of business verticals
• Build data expertise and own data quality for allocated areas of ownership
• Design, build, optimize, launch and support new and existing data models and analytical solutions
• Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions
• Conduct design and code reviews
• Work with data infrastructure to triage infra issues and drive to resolution
• Manage the delivery of high impact dashboards, tools and data visualizations

Minimum Qualifications:
• 2+ years of experience in the data warehouse space, custom ETL design, implementation and maintenance
• 2+ years of experience in SQL or similar languages, and development experience in at least one language (Python, PHP etc.)
• Experience with data architecture, data modeling, schema design and software development
• Experience in leading data driven projects from definition through interpretation and execution
• Experience with large data sets, Hadoop, and data visualization tools
• Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience working in support of diverse communities"
29-Apr-2022 T11:51,Data Engineer,HCL Technologies,2 days ago,"$7,031–$12,608 a month",Full–time,"We are looking for an experienced Data Engineer to join our growing team of analytics. The candidate will be responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Candidate must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

Responsibilities
· Create and maintain optimal data pipeline.
· Assemble large, complex data sets that meet functional / non-functional business requirements.
· Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing Jobs/code for greater scalability, etc.
· Work with stakeholders including the Product owner, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
· Work with data and analytics experts to strive for greater functionality in our data systems.

Requirements
· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.
·Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.
· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
·Strong analytic skills related to working with structured and unstructured datasets.
· Build processes supporting data transformation, data structures, metadata, dependency and workload management.
·A successful history of manipulating, processing and extracting value from large datasets.
·Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
· Experience supporting and working with cross-functional teams in a dynamic environment.
· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:

· Experience with:
o Big data tools: Hadoop, Spark, Kafka, etc.
o Relational SQL and NoSQL databases, including Postgres and Cassandra.
o Data pipeline and workflow management tools: Airflow, etc.
o AWS cloud services or GCP.
o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:51,Head of Data Engineering,TEKsystems (Allegis Group Singapore Pte Ltd),3 days ago,,Full–time,"Head of Data Engineering

We are looking for a high-caliber data expert and leader working within a team that provides world-class and innovative solutions to support the ever-growing demands in the industry.

This is a permanent opportunity.

What's in it for you:
Exposure to understanding and dealing with the data engineering & analytics requirements from various business units, hands-on analysis of the various data systems, and an opportunity to directly influence and impact the company's data infrastructure to enable data democratization.

The Position:

• 10+ years of experience in managing large-scale data initiatives within the tech or start-up space
• Experience in leading teams that design and build highly scalable data pipelines/data infrastructure in fast paced environments
• Experience in managing teams comprised of other Data Engineers, Data Scientists, Data Analysts and Software Engineers
• Strong understanding of building data models for the target data warehouse
• Clear understanding of distributed computing, especially in databases
• Hands-on in SQL with a deep understanding of query optimization
• It would be great to have experience working on any of the Cloud platforms (GCP, AWS, Azure)
• Exposure to Machine Learning / Artificial intelligence is a plus
• Strong communications skills We regret to inform that only shortlisted candidates will be notified / contacted.

For more information you can email Nicole Sichon in our Singapore office on Nicole.Sichon@teksystems.com quoting Job Reference NicoleSichon531489 or alternatively, apply here to register your interest.

http://jobs.en-sg.teksystems.com/N5oQPt/head-of-data-engineering-itcommunications-unknown-singapore-15264683

Job Reference: Nicole Sichon 531489

EA Registration No.: R1873628, Sichon Andreana Nicole Ong

Allegis Group Singapore Pte Ltd, Company Reg No. 200909448N, EA License No. 10C4544"
29-Apr-2022 T11:51,Senior Data Engineer,Grasshopper Pte. Ltd.,16 hours ago,,Full–time,"OVERVIEW :

We seek a Senior Data Engineer to support our high-frequency trading and quantitative research. At Grasshopper we recognize that effective research and trading strategies are built upon a solid foundation in data collection, storage and service.

Grasshopper trades at a number of exchanges globally, and our success depends on our ability to scale by leveraging the potential of data.

You will, in close collaboration with stakeholders, design and develop highly scalable and fault-tolerant data pipelines in a cloud environment focusing on scalability and performance, as well as the next generation of tools that empower users to generate insights on the data they work with.

RESPONSIBILITIES :
• Work with a team of data engineers across locations, managing project schedules.
• Expand the imagination of data stakeholders on what is possible and educate them in leveraging data they already have.
• Work proactively with traders, researchers and various members outside the data team to meet their needs for timely, accurate and complete data sets.
• Build highly scalable data pipelines to process and analyse billions of messages in real time.
• Set strong technical / architectural / cultural foundations, advocate good design in software development as well as data engineering, and lead by example.

SKILLS & KNOWLEDGE REQUIREMENTS :
• Strong technical leadership qualities, good at working with both people and with code.
• Extensive experience with data modelling and designing / supporting both streaming and batch ETL pipelines.
• Extensive experience in SQL and databases.
• Domain knowledge in finance, especially in buy-side trading, is not required but good to have.
• Proficiency in at least two mainstream programming languages (preferably Python, Java, C++).
• Proficiency in a programming language of a non-OOP paradigm (e.g. functional / logic programming).
• Experience with FP libraries like scalaz / cats / ZIO is a plus.
• Demonstrates good judgment in software design balancing design principles with pragmatics, with an emphasis on simplicity.

See Rich Hickey's Simple Made Easy talk : https : / / www.youtube.com / watchv oytL881p-nQ
• Good to have experience in Google BigQuery.
• Experience working in some cloud services platform (e.g. Google Cloud, AWS, Azure).
• Experience with messaging middleware such as Solace or Kafka.
• Experience with machine learning is good to have.

WHO WE ARE LOOKING FOR :

We are looking for team-players who are creative in their approach to problem solving. They take the initiative to explore different ways to resolve an issue, and systematically find the most efficient and effective way to do it.

They are adept at expressing their ideas and solutions into clean and maintainable code.

THE ENVIRONMENT WE OFFER :

As a growing firm with a tightly knit team, we respect and listen to all our employees. You will get the chance to make an impact by having your voice heard by everyone, including the management.

Our employees enjoy a high level of autonomy at work. We focus on substance, not form - as long as you can perform, you will be recognized and rewarded.

We are also dedicated to supporting our staff and ensuring they develop holistically to maximize their potential in the long-term.

We also provide flexible working arrangement as required and a casual and fun work environment to boot"
29-Apr-2022 T11:51,Data Engineer,Vestiaire Collective,16 days ago,,Full–time,"Vestiaire Collective is the leading global platform for desirable pre- loved fashion. We transform the fashion industry for a more sustainable future, by empowering a community to drive change. Encouraging consumers to join the circular economy as the sustainable alternative to throw-away fashion, the platform is unique due to its highly engaged community, its rare desirable inventory and its authenticity and quality control process. Launched in Paris in October 2009, Vestiairecollective.com has close to 15 million members across 80 countries worldwide with offices in cities Paris, New York, Berlin, Hong Kong, Seoul, Singapore, Shanghai, Ho Chi Minh City and Tokyo.

Our values have built our success and made us who we are as a fast-growing company because we think collective: we work with style, with entrepreneurial spirit and with passion. We currently have a diverse global team of 650 employees representing more than 50 nationalities. Our values are community, activism transparency, dedication and greatness. We are proud to be a BCorp.

We are hiring a Data Engineer and you'll create and innovate the Vestiaire Collective Data Platform in collaboration with our Technical Leads, BI Engineers and Architects

About the role

This is a full-time role based out of our Singapore office reporting to the CTO.

What you'll do
• Co-creating and continuously developing the technical architecture of the Vestiaire Collective Data Platform
• Implementing data ingestion strategies for diverse data sources, such as internal databases, third-party sources, user data trackers
• Managing the Data Platform setup on infrastructure side, including managing the Data cloud account, setting up and managing instances, managing computing and storage capacity within budget constraints
• Guiding Data Warehouse developers with their ETL implementations, pointing to optimal technical solutions to data transformation objectives
• Working closely with Data Scientists to ensure smooth and quick implementation of algorithms in production
• Implementing tools for task scheduling, data quality controls, stability monitoring and alerting
• Continuous learning and staying up-to-date with the latest developments in the data technology space in order to keep the team ahead of the curve

Who you are
• 2+ years of working experience, including work with relational databases, Hadoop, NOSQL and/or cloud infrastructure (e.g. AWS)
• Educational background in Computer Science / Electrical Engineering or other engineering fields
• Solid understanding of database concepts and experience with data processing tools (SQL, Hive, Pig, Spark, etc.)
• Hands-on experience with at least one of the following programming languages: Java, Python, Scala - and curiosity to learn others
• Creative approach toward problem solving, passion for exploring new technologies
• Experience with real-time / stream computing technologies is a plus (e.g. Flink, Storm, Spark Streaming)
• You do not need to be micromanaged; to accomplish team and company goals, you can wear multiple hats and pickup new technologies and languages.

What we offer
• A meaningful job with an impact on the way people consume fashion and promote sustainability
• Flexible work arrangements
• The opportunity to create impact in a high growth environment
• The possibility to work as part of a global diverse team with more than 50 nationalities
• 2 days to help Project - reinforcing your activist journey and volunteer for an association
• Investment in your learning and growth
• Competitive compensation and benefits package
Vestiaire Collective is an equal opportunity employer. We strive to develop an inclusive work environment that reflects the diversity of our fashion activist community"
29-Apr-2022 T11:51,Data Engineer,POWER IT SERVICES,13 hours ago,,Full–time,"Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

· Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

· A successful history of manipulating, processing and extracting value from large datasets.

· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science Statistics, Informatics, Information Systems or another quantitative field.

They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:51,"AVP, Data Engineer - BI Tools",United Overseas Bank,1 day ago,,Full–time,"AVP, Data Engineer - BI Tools

AVP, Data Engineer - BI Tools

Posting Date: 25-Apr-2022

Location: Alexandra, Singapore, SG

Company: United Overseas Bank Limited

About UOB
United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices.
Our history spans more than 80 years. Over this time, we have been guided by our values - Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.

About the Department
The Technology and Operations function is comprised of five teams of specialists with distinct capabilities: business partnership, technology, operations, risk governance and planning support and services. We work closely together to harness the power of technology to support our physical and digital banking services and operations. This includes developing, centralising and standardising technology systems as well as banking operations in Singapore and overseas branches.

Job Responsibilities
You will be responsible for the end-to-end software development and support for all work related to projects, quarterly change requests, L3 production fixes. This includes software product implementation and administration, application design, development, implementation, testing and support. You will be expected to work on Finance & Risk Analytics.
You will also be responsible for quality assurance of the team's delivery in conformance with the Bank-defined software delivery methodology and tools. You will partner with other technology functions to help deliver required technology solutions.

Other responsibilities include:

• Create frameworks, technical features which helps in faster operationalisation of Data models, Analytical models(including AI/ML) and user generated contents (dashboards, reports etc.)

• Effectively partner with citizen data scientists in enabling faster adoption of AL/ML model based systems

• Independently install, customise and integrate software packages and programs

• Carry out POCs involving new data technologies

• Design and develop application frameworks for data integration

• Create technical documents such as solution design, program specifications for target solutions

• Perform design and development of applications which may not be limited to: Software Applications, Data Integration, User Interfaces, Automation

• Maintain and recommend software improvements to ensure a platform centric management of software applications

• Performance tuning

• Work with production support team members to conduct root cause analysis of issues, review new and existing code and/or perform unit testing

• Perform tasks as part of a cross functional development team using agile or other methodologies and utilising project management software

Job Requirements
Functional skillsets
Data Lake, EDW, Data Mart, Data Integration & Visualization
Hands-on experience in implementing large scale data warehouse & analytics platforms in financial services industry with good functional knowledge of products & services offered in Retail bank / Wholesale / Global Markets covering some of the following analytics domains:

• Setting up and running BI tools oriented platform
• Design and develop QlikSense & Microsoft Power BI applications
• Design and develop Applications in SAS, Microsoft-R, Python
• Integration of BI tools with data stores (EDW, data marts )
• Experience in Data Modeling, Data mapping for Data Warehouse and Data Marts solutions
• Experience in financial domain - Retail , Wholesale, Compliance, Digital
• Expertise in design of role based fine grained access control
• Designing cloud ready data solutions, Virtualization
Technical skillsets

• Expertise in installing and configuring of BI/Analytical tools like Qlik, Power BI, SAS, Microsoft R
• Expertise in administration (QMC, powershell scripting), infrastructure design of such tools
• Integration with other data systems like EDW, Data mart, CRM, Digital
• Expertise in building BI semantic layer (Qlik - QVD, PowerBI - Direct Query, SSAS, In Memory SSAS modelling, SSRS, Excel Add In)
• Creating application framework specific to BI tools to speed up user adoption
• Good working experience in security features of BI tools including integration with Active Directory, solutions to protect data in motion and at rest
• Good working experience in fine tuning on line BI queries, BI models, analysis of DAX queries
• Reports/Dashboard/Application development expertise using QLIK, PowerBI, SAS, Microsoft R, Python
2 to 3 technical certifications from enclosed list:

• Cloudera Hadoop distribution - Hive, Impala, Spark, Kudo, Kafka, Flume
• Teradata - Bteq, Query Grid, GCFR, MDM, Data Mover, BAR
• Informatica Data Integration - PC, IDR, BDM, MM, IDQ, EDC
• Data modelling tools (Erwin)
• QlikSense
• Microsoft Power BI - Direct Query, SSAS, SSRS, Excel Add In
• Microsoft - R
• Data science workbenches - Cloudera Workbench, Jupyter, DataRobot, H2O.AI, IBM DSX
• Data Virtualization tool - Denodo, Dremio
• AS400
• Language - SQL, Java, Python, Scala, Pyspark
• Automation / scripting - CtrlM, Shell Scripting, Groovy
Experience for an added advantage:

• CI/CD software, Testing Tools - Jenkins, SonarQube
• Version Control Tool - Aldon+LMe, CA Endeavor
• Deployment Tool kit -Jenkins
• Service or Incident Management (IcM) Tools - Remedy
• Source Code Repository Tool - Bitbucket
• Scheduling Tool - Control-M
• Defect Management Tool - JIRA
• Application Testing tool - QuerySurge
• Cloud certification
• Platforms provided by FICO, Experian, SAS for credit and portfolio management

Be a part of UOB Family
UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.

Apply now and make a difference"
29-Apr-2022 T11:51,Data Engineer,Supernova,16 hours ago,,Full–time,"About Supernova

We are the fastest growing lifestyle & beauty omni-channel ecommerce company. We build iconic beauty and lifestyle brands for a new generation of women worldwide. Independently owned with a global marketing reach, we connect with over 15 million women on social media every week. Headquartered in Singapore, we currently have offices in Berlin, London and LA. Our brands include Sand & Sky and Coco & Eve.

About Supernova's Internal Engine

Together we create Superstars! Our three company pillars are guided by this motto: Our product and brands are of the highest quality and within the top five in their category; our customers are the focus of our actions and their lives improve with our products; our talent and employees grow and develop within Supernova.

This means that investing in hiring, developing and retaining the brightest minds in the world is a top priority. Living our core values of #MakeAnImpact, #TalkAboutIt, #SeekNewOpportunities, #DelightAndSurprise and #MoveForwardTogether creates a unique culture across five different time zones and allows us to realise the enormous potential of this exciting industry. If you're an ambitious, smart, natural collaborator who likes taking risks, influencing, and innovating in a challenging hyper-growth environment, we'd love to talk to you about joining our team.

The role

This role provides tremendous opportunity for the right candidate: to capture the energy and rapid pace of a fast growing start up, while being responsible for a sizeable and growing part of our global footprint.

As Supernova's Data Engineer, you are responsible for the design and implementation of modern, scalable data-centric services and solutions.

You have strong data infrastructure and architecture skills, with a proven track record of handling high data volumes to serve batch as well as streaming needs.

This is a remote position, working as part of a strong and growing Data Team.

Responsibilities:
• Define and build data pipelines from a wide variety of data sources that will enable faster, better, data-informed decision-making within the business.
• Develop custom integrations via REST/SOAP/event-driven architecture.
• Work with stakeholders including the Brand, Product, Ops and Marketing teams to assist with data-related issues and support their data infrastructure needs.
• Build tools for effective maintenance and monitoring of the data infrastructure.
• Contribute to an ongoing effort to improve data reliability, efficiency and quality.
• Performance tuning of data pipelines jobs for optimal end-user experience.

Your Profile:
• Minimum Bachelor's degree in a quantitative field is required e.g. Computer Science, Statistics, Mathematics
• 5+ years of experience in data engineering roles using Python / Java.
• Experience in building the solution architecture, provisioning of infrastructure and delivering data-centric services and applications either in GCP or AWS.
• Strong experience in Event Driven Architecture using Kafka or Pub/Sub etc.
• Extensive knowledge in ELT processing and workflow orchestration using Airflow/ Nifi or similar tools.
• Experience with development ecosystems such as Git, Docker, Jenkins and CI/CD.
• Preferably with a background in Startup / Ecommerce.
• Ability to plan your work and commit to deadlines.
• Ability to work with cross-functional teams to deliver quality results.

This position is available immediately. All applications will be treated confidentially.

Please note that we can only contact successful applicants.

Supernova is an equal opportunity employer. Our values define the working culture and environment we strive to create – fair, diverse, respectful and welcoming of different views. We embrace a culture reflecting a variety of perspectives, insights and backgrounds to drive innovation. We build talented and diverse teams to drive business results and encourage our people to develop to their full potential. We promote harmonious working relationships and encourage a learning and development culture. We welcome applications from talented people coming from all cultures, countries, races, genders, sexual orientations, disabilities, beliefs and generations and are committed to providing a working environment free from harassment, discrimination and retaliation.

See our Applicant Data Protection policy for information on how we collect, use and process your personal data: http://www.supernova.xyz/applicant-data-protection-notice"
29-Apr-2022 T11:51,Data Engineer - APAC,Tamr,16 hours ago,,Full–time,"Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE
who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being
customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation
This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.
Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion
Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:51,Data Engineer,Lilith Games Sg Pte. Ltd.,2 days ago,,Full–time,"Big Data Development Engineer (Advertising Direction) Big Data Development Engineer (Advertising Direction) What you will be doing: 1Responsible for the construction of Big Data Cloud Platform and the maintenance of Advertising Data Warehouse. 2Responsible for the calculation, cleaning and layering of Advertising Data based on Flink, Spark, etc., and store it through Hadoop, Clickhouse, etc. 3Responsible for Advertising Data Analysis and Advertising System Report development. Qualifications & Skills 1Bachelor degree or above, major in computer and other related majors. 2Proficient in Python or Golang coding. 3Proficient with Mysql, Memcache, Redis, Message Queue and other common WEB components. 4Having experience in using one or more of HDFS, Hive, HBase, MongoDB, Kafka, Flink and Spark is preferred. 5Experience in operation and maintenance development is preferred. 6Familiar with Alibaba Cloud and other cloud computing resource deployment and optimization is preferred 7Proficient in coding complex SQL Statements, and have the ability and experience of query optimization. 8Positive and optimistic, strong sense of responsibility, with good team communication and cooperation 大数开工程师（广方） 工作责 1. 负责大数平建，广数建与维护 2. 负责基于FlinkSpark等对广数进行计算清洗分等工作并通过HadoopClickhouse等进行存 3. 负责广数分广系统报表开 任 1.大学本(统招)以上学历，计算机通信等相关业 2.熟练掌Python或Golang代编写 3.熟练使MysqlMemcacheRedis消队列等常WEB件 4.有使HDFS, Hive, HBase, MongoDB, Kafka，Flink, Spark中一项或多项验优先 5.有维开验优先 6.熟阿里等计算部署与优化者优先 7.熟练编写sql语，具备查询优化能力验 8.积，责任心强，工作认真细致，具有良好团队沟通与?作能力"
29-Apr-2022 T11:51,Data Engineer (Analytics),Mediacorp,Full–time,,,"Description

We are looking for a Data Engineer (Web Analytics) to join our Data Architecture team. The role will involve executing the data tracking requirements for Web, Mobile apps, and Smart TV platforms.

Key Responsibilities:
• Manage the tagging of advanced analytics tools across different platforms within the organization.
• Utilize deep understanding of Adobe Analytics, along with other analytics platforms to assist the various decision making teams.
• Execute A/B tests and work on personalization and recommendation systems
• Design, develop and support reporting and analytics applications leveraging data integration tools.
• Work closely with Data Architects & Data Scientists to enforce tracking requirements across different platforms

Requirements
• Degree holder in a technical discipline (Computer Science, IS/IT or related disciplines)
• Minimum 3 years of experience in working with Tag Management Solutions Primarily on Adobe Launch
• Good to know - Google Tag Manager or Tealium/ Ensighten.
• Strong Experience in Tag migration - Adobe Heartbeat implementation, Mobile app Adobe AEP SDK implementation
• Strong Proficiency in JavaScript, specifically as it relates to digital analytics implementation.
• Experience in A/B testing tools such as Adobe Target and/or Optimizely

Mediacorp is committed to creating an inclusive and diverse workplace where talent thrives. Our hiring decisions are made based on merit and fit-to-role. If you have a disability or special need which requires accommodation to participate in the recruitment process, please inform us when you submit your online application. We will be happy to support as necessary.

Thank you for your interest and application to this role. Please note that only short-listed candidates will be contacted"
29-Apr-2022 T11:51,Staff Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,16 hours ago,$8K–$13K a month,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 5 years of experience in Data Engineering role and have good knowledge / working experience in:

• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.

• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.

• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.

• Functional programming languages, e.g. Scala.

• Virtualization and container environment such as Docker and Kubernetes.

•Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:51,Data Engineering Lead,Stemly Pte. Ltd.,16 hours ago,,Full–time,"We are looking for a Senior Data Engineer with a track record of building enterprise grade data platform for cloud products .

This is an exciting opportunity for a technologist to build an innovative SaaS product using state-of-the-art and modern cloud technology stack.

As a Data Engineering Lead, you will be working on all aspects of data, from platform and infra build out to pipeline engineering and writing tooling / services for augmenting and fronting the core platform.

The data platform interfaces with large systems to ingest data (batch, micro-batch) to create statistical / machine learning models in a distributed setting.

You will work closely with data scientists, product managers, legal, compliance and business stakeholders.

You will also evaluate new technologies / frameworks and contribute to various open source big data technologies. You are encouraged to think out of the box and have fun exploring the latest patterns and designs in the fields of software and data engineering.

Your responsibilities
• Architecting and implementing a robust data platform for Stemly's products. Design and develop extremely efficient, reliable and observable data pipelines using technologies such as Airflow, Kubernetes, ElasticSearch, Kafka / KubeMQ, Postgres, ClickHouse etc.
• Design and deliver the next-gen data lifecycle management suite of tools / frameworks , including ingestion and consumption on the top of the data lake to support real-time, API-based and serverless use-cases, along with batch (mini / micro)
• Build and expose metadata catalogue for the Data Lake for easy exploration, profiling as well as lineage requirements
• Work on the development of the data platform iteratively by building quick POCs and converting ideas into real products
• Make data discoverable and easy to use for Data Scientists and Analysts. Enable Data Science teams to test and productize various ML models
• Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.

Support your colleagues by reviewing code and designs
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner

About you
• 8+ years of professional experience in software development in Python and Pandas
• Extensive experience with object-oriented design, coding and testing patterns, including experience with engineering software platforms and data infrastructures
• Must have sound understanding for Big Data components & administration fundamentals
• Hands-on in building a complete data platform using various open source technologies. Must have the knowledge to build metadata, lineage, observability and discoverability for data platform
• Experienced in DevOps best practices like CI / CD, containerization, blue-green deployments, secrets management etc. in the Data ecosystem
• Expert in the principles of distributed computing and data modelling
• Expert in building optimized SQL queries
• Conversant with the latest developments in the areas of Machine Learning models and efficiently supporting the data scientists is a plus
• Experience with Airflow, ClickHouse, Postgres, Kubernetes, GraphDB is a big plus
• Familiarity with Google Cloud Platform (e.g. GCS, Cloud Composer, BigQuery) is a plus
• Outstanding communication and interpersonal skills

About us

We are scientists and engineers with a strong passion for cutting-edge technology and decision science. Our mission is to enable enterprises to reduce time to decision with state-of-the-art forecasting and optimization.

Our algorithms are based on automatic machine learning techniques and allow for the discovery of optimal forecasting models.

We unlock sizable business and financial impact in terms of lower lost demand, decrease in inventory, lower working capital and financing costs.

How to apply

You may already know if you're a fit, but perhaps you're worried about some of the requirements We're looking for smart and passionate Engineers;

if you're the right candidate, we're flexible"
29-Apr-2022 T11:52,Data Engineer Lead,Robert Walters,6 days ago,,Full–time,"An exciting Data Engineer Lead job opportunity has become available at a leading e-commerce company.

About the Data Engineer Lead Role:
This leading e-commerce organisation has a strong presence in Singapore. They are looking to hire a Data Engineer Lead reporting to the head of department.

Key Responsibilities:
• Creating and maintain optimal data pipeline architectures from various channels
• Ensuring optimal accuracy and timeliness of data on real-time basis while implementing monitoring tools to detect data issues
• Proper processing of semi-structured and unstructured datasets
• Working with internal teams to understand requirements and optimise performance

To succeed in this Data Engineer will need to have around five years of experience.

Key Requirements:
• Bachelor’s degree in computer science or equivalent
• Experience in business intelligence tools such as Tableau and PowerBI
• Experience in Python and R
• Able to implement ETL logic using SQL
• Knowledge in Hadoop of spark for data processing
• ELT pipelines management experience along with background in analytics

This is an excellent opportunity to be part of a company with strong footprint within the region.

If you are driven, determined and want to take the next step in your career, this Data Engineer role is right for you. Excellent career progression opportunities await the right person in this exciting role.

Apply today or contact me at +65 6228 5350 to discuss this new opportunity. Alternatively, send your resume to sachet.sethi@robertwalters.com.sg.

Do note that we will only be in touch if your application is shortlisted.

Robert Walters (Singapore) Pte Ltd

ROC No.: 199706961E | EA Licence No.: 03C5451

EA Registration No.: R1439850 Sachet Sethi"
29-Apr-2022 T11:52,Senior Data Engineer,eyos.one,Full–time,,,"We are a dynamic team with great offices in Singapore, London, Bangkok, Jakarta and Sydney, serving our great customers worldwide. We’re expanding fast and are looking for a passionate and driven Senior Data Engineer to join our global tech team in Singapore.

Here's What You Will Be Doing:
• We are looking for a savvy Senior Data Engineer to join our growing team of retail analytics experts and expected to be involved from inception of projects, understand requirements, architect, develop, deploy, and maintain data platform and/or data pipelines, which involves partnering with program and product managers to expand product offering based on business demands.
• Our goal is to drive up retail analytics and adoption of the platform while constantly working towards improving platform performance and scalability, deployment and maintenance require close interaction with various teams.
• Production support for applications is usually required for issues that cannot be resolved by the operations team. Creative and inventive problem-solving skills for reduced turnaround times are highly valued.
• Collaborate with Data Scientists, Data Analysts, and other internal stakeholders to assist with data-related technical issues and support their data pipeline infrastructure and data preparation needs.
• Preparing user documentation to maintain both development and operations continuity is integral to the role.

Here's What You Need To Have/Know:
• BE/BS in Computer Science or equivalent practical experience
• At least 3 - 7 years experience in data analytics environments
• Experience in data warehouses like Snowflake, Google BigQuery and OLTP databases like MySQL, SQL Server, Oracle.
• Hands-on experience in SQL, Python or Scala and Spark
• Hands-on experience with cloud environments (AWS & GCP preferred)
• Experience with dev ops tools like Git, Bitbucket
• Understanding of airflow, dagster or any kind of orchestration tools
• Extensive experience on distributed environment and scalable systems
• Good analytical and problem-solving skills with strong attention to detail
• Excellent communication skills with an ability to work well with others (across APAC and EMEA)
• Knowledge of and experience in automation technologies

Skills that would be a plus
• Basic understanding of containers technologies (like dockers, Kubernetes)
• Exposure to visualization technologies such as Tableau or Sisense
• Prior experience and knowledge on machine learning models deployment processes in AWS or any other cloud systems.
• Exposure to NoSQL platforms

What will you learn on the job?
• FMCG brands with granular market intelligence, targeted marketing automation and data-driven sales optimization, especially in independent trade channel that attracts majority of grocery shopping trips in the Asian markets.
• Data pipeline flow, which converts physical receipts to digital receipts and connect 100% of in-store transactions into any platform they use in real-time.
• Build a platform for data-driven growth in physical retail shops that enables large scale analytics and data science workflows.
• Exposure to massive retail of datasets and deal with some of the most exciting data & analytics challenges"
29-Apr-2022 T11:52,Data Engineer,Morgan McKinley,4 days ago,,Full–time,"Data Engineer

Job Summary
• Singapore
• Permanent
• BBBH811650
• Apr 05, 2022
• Competitive Job Description
Our client is looking for an experienced Data engineer to drive development of information products for data and digital transformation across their group.

Mandatory Skills:
• Master of Business Administrations or master's in quantitative fields (Computer Science, Statistics or similar) with minimum of 5 - 12 years of overall experience.
• Experience programming in Python, Java, SQL, PLSQL.
• Experience with traditional RDBMS based systems and more modern NoSQL technology stacks.
• Expertise building ETL and data pipelines on Databricks.
• Experience working with Big Data technologies such as Hadoop, Cloudera (CDH), Hortonworks (HDP), DataBricks, Spark, Delta, HDFS, HBase, Hive.
• Experience in event streaming with Kafka.
• Experience in ML Model Productionization, Docker.
• Real Time, Batch, Unstructured Data, DW, MDM, Data Marts.
• Proficient in using data visualization tool such as Tableau, Power BI, D3, AmCharts etc.
• Understanding FS industry fundamentals and business problems to find new ways to leverage data.
• Intellectual curiosity to solve data driven problems.
• Independent thoughts and unique ideas on solving business problems.
• Able to collaborate / virtually manage multi-cultural, multi-disciplined, globally dispersed teams.
• Sound understanding of functional areas and analytics problems in areas of Customer, Marketing, Channel, Pricing, Digital, Operations, HR/Corporate analytics.
• String communication skills to be able to explain highly technical problems in simple layman form.
• Ability to articulate the impact of decisions and recommend improvements. Desired Skills:
• Relevant experience in Banking and financial institutions. Job Responsibilities:
• Exceptional data engineering & visualization experience & technical skills. State-of-the-art expertise across, data/information preparation and data insight & visualization using BI (or similar tools).
• Be a data engineering & visualization technical expert. Lead and explain/educate to all levels people in these areas, Coding Databases, Data Integration, Frameworks, Deployment, Architectures and Visualization.
• Contribute to the development of in-house data products. Use your data engineering & visualization expertise plus FS domain & functional knowledge, problem solving skills and independent thinking to create in-house data analytics and data management products.
• Be a team player & an individual contributor. Work with group data office and other business function people as part of larger deliveries, as well as being able to work independently or in small teams to continuously deliver business value.
• Be a trusted partner of our client. Someone that anyone in the company can reach out to for help with creating data engineering & visualization driven business transformation. Those who are keen for the role and would like to discuss the opportunity further, please click ""Apply Now"" or email Alimpan at amukherjee@morganmckinley.com with your updated CV.

Only shortlisted candidates will be responded to, therefore if you do not receive a response within 14 days please accept this as notification that you have not been shortlisted.

Alimpan Mukherjee

Morgan McKinley Pte Ltd

EA Licence No: 11C5502

EA Registration Number: R2198629"
29-Apr-2022 T11:52,Senior Data Engineer,Toptal,4 hours ago,,Full–time,"About The Job

Toptal developers work with speed and efficiency to deliver the highest quality of work. We are looking for someone who is passionate about their client’s business, and ready to work on exciting projects with Fortune 500 companies and Silicon Valley startups, with great rates and zero hassles. If you are looking for a place to advance your career, enhance your skill set, and build connections around the globe, Toptal is right for you.

About Toptal

Toptal is an exclusive network of top freelancers from around the world. Fortune 500 companies and Silicon Valley startups hire Toptal for their most important projects. Toptal is one of the fastest-growing fully remote networks and empowers freelance software developers, designers, finance experts, product managers, and project managers worldwide to grow and excel in their freelance careers.

Toptal clients vary in sizes and industries, from enterprise organizations and big tech companies to Silicon Valley startups and renowned universities. Once you enter the network, our matchers will contact you with project opportunities that fit your expertise and preferences. We have experts in over 120 countries who get to work remotely on projects that meet their career ambitions.

About The Role

As a Data Engineer, your main goal is to be one step ahead of data scientists and analysts. You will support them by providing infrastructure and tools they can use to deliver end-to-end solutions to business problems that can be developed rapidly and maintained easily. This is more than building and maintaining ETL pipelines. We need innovation, creativity, and solutions that will have a significant impact on the client’s velocity.

Requirements
• 3+ years of professional experience in software development
• Working experience with Python and Pandas.
• Familiarity with the basic principles of distributed computing and data modeling.
• Extensive experience with object-oriented design and coding and testing patterns, including experience with engineering software platforms and data infrastructures.
• Working experience with Airflow and Luigi is a big plus.
• Working experience with Scala is a plus.
• Familiarity with Google Cloud Platform (e.g. GCS and BigQuery) is a plus.
• Working experience with Dimensional Modeling and Rails is a plus.
• Outstanding communication and interpersonal skills.
• Full-time availability is a strong advantage

If you’re interested in pursuing an engaging career working on full-time freelance jobs for exclusive clients, take the next step by clicking apply and filling out the short form to get started.

#RemoteJobDataEngineering"
29-Apr-2022 T11:52,Data Engineer,Power It,2 days ago,,Full–time,"· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.
· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.
· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
· Strong analytic skills related to working with structured and unstructured datasets.
· Build processes supporting data transformation, data structures, metadata, dependency and workload management.
· A successful history of manipulating, processing and extracting value from large datasets.
· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
· Experience supporting and working with cross-functional teams in a dynamic environment.
· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics Information Systems or another quantitative field.
They should also have experience using the following software/tools:
· Experience with:
o Big data tools: Hadoop, Spark, Kafka, etc.
o Relational SQL and NoSQL databases, including Postgres and Cassandra.
o Data pipeline and workflow management tools: Airflow, etc.
o AWS cloud services or GCP.
o Stream-processing systems: Spark-Streaming, Flink etc.

Job Type: Permanent

Schedule:
• Monday to Friday"
29-Apr-2022 T11:52,Finance WebApp Data Engineer,Apple,Full–time,,,"Summary
Posted: 27 Mar 2022
Weekly Hours: 40
Role Number:200360362

Apple’s Finance Business Process Reengineering team is seeking a Data Engineer to join our organization. In this role you will help architect, maintain and continually improve data analytics and automation capabilities. Sitting inside a business unit, you must adapt to varied and shifting needs. You will be working with business users, Apple IS&T developers, Web Application developers and data analysts on your team to deliver complete, accurate, well-secured data that enables reporting and analytics across all Finance functions. Comfort with and ability to learn and perform a wide variety of development and engineering tasks combined with knowledge of Finance business processes sets you apart in this role.

Key Qualifications

Key Qualifications
• Experience in building backend infrastructure for scalable data processing and analytics
• Strong SQL and Python skills
• Experience tracking and measuring data quality solid knowledge of database technologies including Snowflake, Teradata, MySQL, MariaDB
• Experience building scalable data pipelines
• Experience with applying data encryption and data security standards
• Preferred experience in tools like Dataiku
• Able to quickly learn new technologies

Description

Description
The Finance Business Process Reengineering (FBPR) organization supports Apple's Finance function worldwide. Finance Data and Technology (FDT) team within the FBPR org enables the Finance organization by providing quality data accessibility, analytics, reporting and automation services. We are looking to expand capabilities in the areas of data privacy, high-performance computing, advanced analytics and general business intelligence.

A Finance WebApp Data Engineer is a technical expert and works tightly with WebApp developers, other data engineers and data analysts on the team to create data integrations, ETL, pipelines, transformation and codebase to drive innovative analytics projects from initial experimentation to production level deployment. They work on critical data engineering problems, building bespoke, reliable, accurate, consistent, and architecturally sound solutions that are aligned with business needs.

The Finance Data Engineer architects, maintains and continually improves data analytics and automation capabilities. The role requires working cross-functionally with business users, Apple IS&T developers, and data analysts to deliver complete, accurate, well-secured data that enables reporting and analytics. This role is required to learn and perform a wide variety of development and engineering tasks, on top of growing their knowledge of Finance business processes to efficiently identify data applicable business questions.

Finance Data Engineers work predominately in Apple’s enterprise data warehouse (EDW), identifying and combining data in an efficient, scalable manner to help answer business questions. When the necessary data is not available on an enterprise system or is generated offline by business users or third parties, the Finance Data Engineer must develop methods to reliably source, validate, and integrate the data into EDW.

The Finance Data Engineer must learn and understand a variety of available IS&T solutions, when and how to use them, and when to develop custom solutions. This paired with a proven record of excellent problem solving and a sharp, open mind will be more important than deep expertise in any one area.

Education & Experience

Education & Experience
Master’s degree or equivalent in Computer Science or related field and two years of experience. Alternatively, a Bachelor’s degree in Computer Science or related field and five years of progressive experience.

Additional Requirements

Additional Requirements
• Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.
• We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation"
29-Apr-2022 T11:52,Data Engineer (Trading),Michael Page,10 days ago,,Full–time,"About Our Client

Our client is a fintech company that is a trusted crypto exchange brand. They are looking for a Data Engineer that has experience in the finance industry.

Job Description
• Collaborate with trading applications and backend developers to advise and fulfill data requirements in terms of consistency, latency and scalability
• Responsible for the implementation and maintenance of relevant database technologies and data quality to meet these data needs
• Collaborate with existing data team to design and implement data synchronization
• Work with development team for the maintenance and enhancement of latency-sensitive market data feed components

The Successful Applicant
• 3 years' working experience in data engineering, database administration, AWS data solutions or relevant field
• Proficient in Python, R, Julia (at least one), SQL, Linux and C++, Java or Rust (at least one)
• Proficient in data pipeline development involving cloud data solutions such as Redshift Snowflake or databases such as SQL, kdb+ or other time series databases
• Experience in market data gateway protocols such as ITCH or socket and web socket programming or real-time messaging frameworks such as Apache Kafka, Aeron, Chronicle is highly desirable

What's on Offer
• Permanent role
• Career Advancement Opportunity
• Opportunity to work on cutting edge technologies"
29-Apr-2022 T11:52,Data Engineer,Glp Pte. Ltd.,16 hours ago,,Full–time,"The Data Engineer will be a key player in the enterprise-wide data transformations projects. You will engage in developing and automating data processing pipelines for data modelling, analysis, and reporting from various data sources system; The primary responsibility of this position is to assist to establish the enterprise data Lake architecture under Microsoft Azure Data factory, Databricks and Synapse and deliver data driven solutions.Job description:· Assist in architecture design, develop, document, and implement end-to-end data pipelines and data driven solutions.· Define roadmap to transform data architecture focusing on scalability, performance and stability for the entire data lifecycle;· Build data flows for data acquisition, aggregation, and modelling, using both batch and streaming paradigms.· Perform data analysis, data profiling, data cleansing, data lineage, data mapping and data transformation.· Develop high-quality code for the core data stack including data integration hub, data warehouse and data pipelines under Azure service.· Execute and deliver best practices in data management and data lifecycle processes, including modular development of data processes, coding and configuration standards, error handling and notification standards, auditing standards, and data archival standards.· Implement security and standards, documenting technical specifications and operating procedures.· Collaborate across developers as part of a SCRUM team ensuring collective team productivity· Provide technical support for any data issues with recommendations, and resolutions.Requirements:· 2 years professional experience as a data engineer, software engineer, data analyst, data scientist, or related role.· Experience with Microsoft Azure Data Integration Stack (Azure Data Lake Gen2, Azure Data Factory, Delta Lake, SSIS, SQL Server, Azure Data Warehouse), Databricks, Spark.· Working experience in Investment or Real Estate industry, preferably with business and functional knowledge.· Expertise building ETL and data pipelines on Databricks using data engineering languages Python and SQL on Azure.· Advanced knowledge and experience working with Python & SQL;· Proven experience with all aspects of the Data Pipeline (Data Sourcing, Transformations, Data Quality, Etc…).· Experience with visual modelling tools including UML· Proficient in using data visualization tool such as Power BI, Workiva and in standard office tools such as Excel.· Familiar with DevOps and Agile methodology"
29-Apr-2022 T11:52,Data Engineer (APAC),TEKsystems (Allegis Group Singapore Pte Ltd),6 days ago,,Full–time,"Data Engineer (APAC)

We are looking for high-calibre Data Engineers to be a part of a growing tech firm!

This is a Permanent opportunity.

The Position:

• Experience in designing and building robust and highly scalable data pipelines
• Strong programming proficiency using Python and SQL
• Strong understanding of building data models and data warehouse technologies.
• Experience with Airflow
• Good to have: BI tools like Tableau or Google Data Studio
• Strong communications skills
What's in it for you?

• Exposure to the latest cutting-edge technologies
• Rewarding work: Opportunity to make real and immediate impact on the business and the industry
• You'll work with a great team. They're exceptional at their jobs, and winning market share from their competitors every day.
• Vibrant international team with a fun, friendly and open start-up company culture We regret to inform that only shortlisted candidates will be notified / contacted.

For more information you can email Krystal in our Singapore office on krystal.fernandez@teksystems.com quoting Job Reference KrystalFernandez531475 or alternatively, apply here to register your interest.

http://jobs.en-sg.teksystems.com/fQHPOu/data-engineer-apac-itcommunications-singapore-singapore-15264412

Job Reference: Krystal Fernandez 531475

EA Registration No.: R21103744, Krystal Anna Fernandez

Allegis Group Singapore Pte Ltd, Company Reg No. 200909448N, EA License No. 10C4544"
29-Apr-2022 T11:52,Data Engineer,REAL ESTATE ANALYTICS PTE. LTD.,4 days ago,$4K–$6.5K a month,Full–time,"Key Responsibilities
• Maintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable and timely delivery of key insights for decision making
• cleaning and normalizing subsets of data of interest as preparatory step before deeper analysis by the data scientists
• Run Modern high performance analytical databases and computation engines
• Design and implement various data health checks to ensure the data quality and consistency across systems
• Design and implement data extraction solution in a distributed system

Skills required
• Experience in handling large data sets and working with structured, unstructured and geographical datasets
• Understanding of databases and best engineering practices - include handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding how to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipeline
• Experience with DevOps and AWS will be an advantage
• Real passion for data, new data technologies, and discovering new and interesting solutions to the company’s data needs
• Excellent communication skills to communicate with the product development engineers to coordinate development of data pipelines, and or any new products features that can be built on top of the results of data analysis"
29-Apr-2022 T11:52,Data Engineer,Tech Mahindra Limited,3 days ago,$60K–$120K a year,Full–time,"What to expect:
• Manage & develop data warehouse/data lake and pipeline plans.
• Develop and uphold best practices with respect to documentation & data protocols
• Design, build and launch new data models in production.
• To also develop new data extraction, transformation and loading processes in production.
• Work in a cross-functional team, Interfacing with engineers, product managers and product designers to understand data needs to define and review technical specifications.
• Build machine learning/reasoning models at scale.

How To Succeed:
• Degree in Computer Science, Information Systems, Computer Engineering, Mathematics, or related disciplines.
• At least 3 years of experience in data engineering, with experience in Kotlin/Scala/ Python, and experiences in Amazon Web Services stack, such as ECS, Kinesis, EMR, and DynamoDB.
• Experienced in building production-grade models using machine learning frameworks such as Tensorflow, Scikit-learn, PySpark and/or others is required.
• Experienced in custom ETL & ELT & Streaming big data design, implementation & maintenance and data warehouse/data lake space.
• Experienced/willing to use Infrastructure-as-code for deploying pipelines.
• Hands-on and deep experience with schema design and dimensional data modelling.
• Ability to write efficient SQL statements.
• Ability to analyse data to identify deliverables, gaps and inconsistencies.
• Excellent communication skills including the ability to identify and communicate data-driven insights.
• Experience in using Hadoop, Spark, HBase, Hive and Pig, is a plus.
• Experience in software engineering, with experience in Kotlin or Java, and Spring Boot is a plus.

What to expect: * Manage & develop data warehouse/data lake and pipeline plans. * Develop and uphold best practices with respect to documentation & data protocols * Design, build and launch n

Skills: Excellent Communication Skills, Machine Learning, Pyspark, Big Data, Pipelines, Kotlin, Hadoop, Software Engineering, Etl, Dynamodb, data engineering , Emr, Sql, Python, Java

Experience: 2.00-5.00 Years"
29-Apr-2022 T11:52,Senior Data Engineer,Singtel Group,27 days ago,,Full–time,"Jobscope
• Develop big data solutions for near real-time stream processing, as well as batch processing on the Big Data platform
• Work with business domain experts, data scientists, and solution designers to identify data relevant for analysis and develop Data solutions
• Fine-tuning of new and existing data pipelines
• Schedule and maintain data pipelines
• Drive optimization, testing and tooling to improve data quality
• Assemble large, complex data sets that meet functional / non-functional business requirements
• Develop APIs to support high throughput data processing, feature engineering, and use cases
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc
• Review and approve high level & detailed design to ensure that the solution delivers to the business needs and align to the data & analytics architecture principles and roadmap
• Understand various data security standards and use secure data security tools to apply and adhere to the required data controls for user access
• Support and contribute to development guidelines and standards for data ingestion
• Adapt and learn new technologies surrounding Data Platform

The Ideal Candidate should possess the following:
• Bachelor’s Degree in IT, Computer Science, Software Engineering, Business Analytics or equivalent.
• Minimum of 6 years of experience with highly distributed, scalable, concurrent and low latency systems working with one
• or more of the following technologies:
• Hadoop distributions Spark
• NoSQL data warehouses such as HBase, Cassandra
• Excellent software & data engineering principles and design patterns
• Proficient in creating and maintaining complex data pipelines end-to-end while maintaining high reliability and security
• Excellent hands-on experience in Scala or Python
• Excellent hands-on experience with SQL and Spark
• Experience with Kafka
• Experience with CI/CD tools and environment
• Experience migrating from on-premise data stores to cloud solutions
• Good working experience with one or more major cloud vendors (ie: Azure,AWS, GCP)
• Experience working in Telco Data Warehouse and / or Data Lake advantageous
• Highly organized, selfmotivated, pro-active, and able to plan
• Ability to analyze and understand complex problems
• Ability to explain technical information in business terms
• Ability to communicate clearly and effectively, both verbally and in writing
• Strong in User Requirements Gathering, Maintenance and Support
• Good experience managing users and vendors
• Experience with Agile Methodology

We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated"
29-Apr-2022 T11:52,Data Engineer,Endowus,5 days ago,,Full–time,"About us
Endowus is Asia's leading fee-only digital wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping all investors grow their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.
Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones. Our mission is clear: help people invest better so they can live easier today, and better tomorrow.

The people you will work with
The team has deep domain knowledge in finance and technology, bringing together decades of experience at Goldman Sachs, Morgan Stanley, UBS, Credit Suisse, The Blackstone Group, AQR, Grab, Dropbox, Lyft, Redmart, Carousell, Bytedance, Grab, Kakao, Alibaba, and more See our leadership team here . We practise inclusion and treasure our diversity in background and experience. A diverse team is our biggest asset and we look for people who share our belief in Endowus' clear mission.

Investors, recognition, licensing
Endowus is backed by global leading strategic and venture capital investors including UBS, Samsung Ventures, EDBI, Prosus Ventures, ZVC, Singtel Innov8, Lightspeed Venture Partners, and SoftBank Ventures Asia.

Endowus has been recognised by the industry with the following awards: Singapore's Rising Star and Fintech Innovation (Asia Asset Management's Best of the Best Awards 2021), LinkedIn Top Start-ups 2021, WealthTech of the Year (Asia FinTech Awards 2021), and the top 15 Singapore's Best Workplaces Award (Great Place to Work).
Endowus is licensed by the Monetary Authority of Singapore (MAS).

About this role; responsibilities & ownership
• We are looking for a Data Engineer who thrives in a fast-paced environment and enjoys driving innovation through rapid prototyping and iterative development.
• Being part of Endowus's Technology team, you will build end-to-end product features that you are confident of delighting user experiences.
• You will need to leverage the entire technology stack to realise these goals and will be part of a team that is constantly tackling difficult questions of scale, architecture, and interaction. Requirements & qualifications

• Bachelors' or above in Computer Science, a related field, or equivalent professional experience
• 3-6 years of experience in designing and implementing key components for highly scalable, distributed data collection and analysis systems built for handling large volumes of data in cloud
• Advanced working SQL knowledge and experience working with relational databases or BigQuery, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimising data pipelines and data sets in Apache Flink or Apache Spark to answer specific business questions.
• Experience in at least one of the JVM languages such as Java, Scala.
• Experience Build processes supporting data transformation, data structures, metadata, dependency and workload management. (experience in Airflow is Plus)
• Experience with distributed databases, such as Cassandra, and the key issues affecting their performance and reliability.
• Hands-on development with key technologies including Scala, Apache Flink and other relevant distributed computing languages, frameworks, and libraries
• Independence and self-reliance while being a proactive team player with excellent communication skills.
• Self-driven, has a strong sense of ownership and able to work and learn independently
• Comfortable with numbers and motivated by steep learning curves Nice to haves
• Experience with message broker systems, such as Apache Kafka.
• Developed mission-critical products, e.g. trading platforms, medical applications
• Basic knowledge of finance and trading
• Preferably some exposure to CQRS / Event Sourcing patterns
• Familiarity with Docker, Kubernetes and Terraform Remote Okay
• We are open to hiring remotely in Asia time zones. Benefits & perks
• Stock options in a fast growing company *
• Employee discount for investing at Endowus
• Flexible working hours and locations so you can live life and your best work - you are trusted to be responsible
• Competitive Staff Benefits; Annual Leave, Medical insurance, Dental *
• Productivity Equipment; Omnidesk Standing Desk & Ergonomic Chairs, Dell UltraSharp USB Type-C Monitor, high-end computer peripherals
• Choice of Apple or PC equipment
• Additional S$250 /HKD 1450 budget for your favourite hardware to boost productivity * Note: * is only applicable to Full-Time employees"
29-Apr-2022 T11:52,(Senior) Data Engineer (APAC),foodpanda Singapore,20 mins,,Full–time,"foodpanda is the largest food and grocery delivery platform in Asia, outside of China. Operating in more than 400 cities across 12 markets, foodpanda continues to expand and grow in its core food delivery business as well as in new verticals like grocery deliveries, with a strong tech infrastructure at its core. From our restaurants-partners, cloud kitchens and cloud grocery stores — foodpanda is just one tap away, delivering everything you need quickly and conveniently to your doorstep.

We are looking for a (Senior) Data Engineer to join our growing data team to help Foodpanda make smart, data-driven business decisions. If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!

What's on your plate:
• Defines and builds data pipelines that will enable faster, better, data-informed decision-making within the business
• Architect, build, and deploy new data models that provide intuitive analytics across the business
• Manage and own the entire data processing system (internally, as well as interfacing with other teams or third party services), provide advice on any necessary infrastructure changes
• Experiment with, select, and implement cutting edge Big Data tools and frameworks required to provide requested capabilities
• Perform all needed data transformation to populate a reporting optimised data warehouse
• Mentoring junior team members through code review and enablement training
• Participate in building machine learning models, deliver analyses and insights to support business decision making

What you bring to the table:
• More than 3 years experience in data analytics / engineering
• Ability to write clean, structured, and high performance SQL and Python code
• Strong experience with big data, Data Warehouse technologies
• Strong oral and written communication skills
• Strong business mindset and ability to grasp business requirements from stakeholders
• Knowledge of Machine Learning is a plus
• Experience in machine learning operations, or setting up environments for data scientists/machine learning engineers would be advantageous

What we can offer you:
• A vibrant and international team with multi-cultural and diverse backgrounds.
• Solving challenges with inspiring colleagues in an all hands-on deck environment.
• Management team that recognizes top performers, welcome our newbies, and shares a love for good food.
• Competitive package, incentives, allowances, food perks, insurance, pension and more"
29-Apr-2022 T11:52,Cloud Data Engineer for IT Data Analytics Team,Garranto Pte. Ltd.,2 days ago,,Full–time,"Type : Full Time / Permanent Role

Location : Singapore

Job Description ï'·
• Act as a subject matter expert in data engineering and GCP data technologies. ï'·
• Closely work with various business teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
• Work with Agile and DevOps techniques and implementation approaches in the delivery. ï'·
• Be required to showcase your GCP Data engineering experience when communicating with business team on their requirements, turning these into technical data solutions. ï'·
• Be required to build and deliver Data solutions using GCP products and offerings.

Qualifications
• Liaise and be part of our extensive GCP community, contributing in the knowledge exchange learning programme of the platform.
• Any Bachelor Degree in Computer Science or related fields
• Minimum 5 years of experience as a data engineer in banking environment.
• Possess analytical skills mental resilience and the ability to think systematically under stressful conditions.
• Highly accountable and takes ownership. Outstanding work ethic, highintegrity, team player, and a lifelong learner.
• Mentor other engineers define our technical culture and help build a fast-growing team.

Skill
• Hands on and deep experience working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, AI Building Blocks, Looker, Cloud Data Fusion, Dataprep, etc.).
• Experience in Spark / Scala / Python / Java / Kafka.
• Experience in MDM, Metadata Management, Data Quality and Data Lineage tools.
• E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.
• Regulatory and Compliance work in Data Management.
• E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
• Experience with SQL and NoSQL modern data stores.
• Work on diversified cloud Platforms consisting of Databases and CICD, Logging, and monitoring tools to provide stable, and reliable DevOps service.
• Hands-on experience with terraform is a plus.
• Build CI / CD pipeline; both design and implementation is an added advantage.

If you are interested please send your CV to HIDDEN TEXT"
29-Apr-2022 T11:52,Data Engineer,TreeDots,3 days ago,,Full–time,"Job description

& requirements

Job Description

TreeDots is currently hiring for a Data Engineer with strong experience in building and maintaining data pipelines.

At TreeDots we drive 3 products, respectively related to social commerce, logistics, and b2b e-commerce. Our vision is to ensure a holistic delivery of agrifood from source to merchants and end-consumers. We believe that environmental issues coming from food waste are possible to solve and those are the main goals of our products.

Duties and Responsibilities

You will be responsible for collecting, managing, and converting raw data into usable information for stakeholders to interpret. You will be responsible for creating the data flows between the data source to the data warehouse and finally to use visualization tools to display prepared data-sets.

Mandatory requirements

● 2-4+ years of experience as a Data Engineer
● Experience with building optimized SQL queries
● Experience with Google Cloud Platform, especially BigQuery and Google Data Studio
● Understanding of the SDLC best practices
● Familiar with API queries and at least 1 programming language
● Familiar with git and Github

Required skills

Git Github SQL"
29-Apr-2022 T11:52,Senior Data Engineer,Kkr Singapore Pte. Ltd.,3 days ago,,Full–time,"Position Summary We are looking for a savvy Data Engineer to be based in Hong Kong / Singapore, to join our growing team of data and analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced cloud-based services, data architecture, data engineering, data pipeline development (ETL), and analytical tools. We are seeking an engineer who enjoys optimizing data systems and building enterprise scale data services. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives. Skill / Experience Required Bachelor's Degree in Computer Science/Engineering or a related discipline. 10+ years Development experience Experience in Python and related open source modules Experience in Python development including Web application frameworks such as Flask / FAST API Experience working with RESTful API Services Strong database skills with a thorough understanding of relational database and understanding of Object Oriented databases Exposure with the AWS Stack / RDS / is preferred Knowledge of open source solutions and trending technologies Good communication and written skills Ability to be self-sufficient and proactive individual contributor Exposure to Private/Public Markets Desirable Understanding of Object Oriented Programming and Design Patterns Knowledge of web standards, security, accessibility, browser compatibility Knowledge of JavaScript, HTML5 and awareness of frameworks such as React.js/Vue.js Experience in a Business Intelligence tool e.g. Tableau and Dremio Exposure to ML / AI and python libraries (pandas, NumPy, SciPy) and concepts"
29-Apr-2022 T11:52,"Data Engineer (5 days, Orchard, $3500-4000)",MACHSPEED HUMAN RESOURCES PTE. LTD.,9 hours ago,,Full–time,"Leading System Integrator company providing IT solutions for network, IT infrastructure, analytics, cyber security and managed services.

Job Responsibilities
• Design, engineer, configure and administer BI project based on given functional and technical requirements
• Collaborate with pre-sales, project and relevant internal teams to deliver the optimal technical solution to client's business problems
• Develop real-time and batch data ingesting and processing pipelines to be used for analysis, machine learning, dashboards, alerts and visualizations
• Review and monitor ETL tasks and performance
• Support testing and deployment
• Provide recommendations and implementation changes to optimize in the customer environment.
• Write and develop custom scripts as needed

Requirements
• Minimum Degree/Diploma in Computer Science, Engineering or IT with at least 2 years of relevant experience
• Hands-on experience in scripting/programming
• Possess knowledge in Networking and Servers(Windows and Linux)
• Possess CEH/ECSA/CISSP/ECSA/CompTIA Security+ certification will be an added advantage
• Working hours: 9am - 6pm, Mondays to Fridays
• Salary range: $3500 - 4000

We thank you for your interest in this position, applicants with relevant experience please send Microsoft word resume with expected salary to

apply@machspeed.com.sg

for faster processing, alternatively, please visit https://www.facebook.com/MachspeedHR for more jobs available, thank you.

You may also call +6563362530 (Look for BingCheng) to find out more

Thank you very much.

Agency License No. 12C6200

EA Registration No: R1437671"
29-Apr-2022 T11:52,Senior Data Engineer (Data Engineering),GVT Government Technology Agency (GovTech),2 days ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Senior Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you will be building Whole-of-Government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will be architecting, designing and building next-generation data infrastructure to galvanise digitalisation in the public sector. You will be given opportunities to lead other engineers to drive impact at scale.

We are looking for enthusiastic and passionate engineers to join us in this journey to make a difference.

What you will be working on:
• Design and build resilient and efficient data pipelines for both batch and real-time streaming data
• Architect and design data infrastructure on cloud using industry standard Infrastructure-as-Code tools
• Execute projects with an Agile mindset
• Build software frameworks to solve data problems at scale
• Collaborate with product managers, software engineers, data analysts and data scientists to build scalable and data-driven platforms and tools
• Be put in the driving seat as an engineering leader

What we are looking for:
• Bachelor’s Degree in Computer Science or have equivalent professional experience
• Have more than 4 years of experience in a technical role
• Experience with data processing tools such as Spark, Beam, Flink
• Experience with the cloud (e.g. AWS, GCP, Azure)
• Experience implementing batch and streaming data pipelines
• Experience writing efficient SQL
• In-depth knowledge of both SQL and NoSQL databases, including performance tuning and troubleshooting
• Familiar with DevOps tools such as Git, Docker, Terraform
• Experience in the public sector is a bonus
• Previous technical leadership experience is a bonus

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round.
• Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:52,Senior Data Engineer,GENPACT CONSULTING (SINGAPORE) PTE. LTD.,2 days ago,$8K–$10K a month,Full–time,"• Work closely with the Product Owners and stake holders to design and build data systems and pipelines to meet the requirements of the proposed solution.

• Play an active role in leading team meetings and workshops with clients.

• Oversee day-to-day Data Engineering team’s operation and performance.

• Interact directly with clients to understand project requirements and deadlines.

• Evaluate business needs and objectives.

• Designing and implementing highly performant data ingestion pipelines from multiple sources using SQL, Python, Apache Spark and Databricks.

• Responsible for deploying codes using Git/Bitbucket as per the CI/CD process.

• Design and Build datasets on Snowflake for faster reporting.

• Design Develop Maintain ETL data pipelines.

• Analyze and organize raw data.

• Research, Diagnose, and Monitor Performance Bottlenecks, etc.

• Ensure standardization of SQL coding practices and adherence to coding standards, change control, and SQL best practices

• Prepare data for prescriptive and predictive modelling.

• Responsible for data integration and data quality

• Enabling data from different sources into ready to consume datasets.

• Performing Data Validation/Exploration tasks.

• Migrating current lake data/ external data into Exasol/Databricks (Lakehouse)

• Helping business stakeholders to visualize and analyze the customized view/data and enable better decisions without any hassle.

• Partnering and coordinating with cross functional stakeholders across timezones"
29-Apr-2022 T11:52,Data Engineering (Internship),Endowus,2 days ago,,Full–time,"About us
Endowus is Asia's leading fee-only digital wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping all investors grow their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.
Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones. Our mission is clear: help people invest better so they can live easier today, and better tomorrow.

The people you will work with
The team has deep domain knowledge in finance and technology, bringing together decades of experience at Goldman Sachs, Morgan Stanley, UBS, Credit Suisse, The Blackstone Group, AQR, Grab, Dropbox, Lyft, Redmart, Carousell, Bytedance, Grab, Kakao, Alibaba, and more See our leadership team here . We practise inclusion and treasure our diversity in background and experience. A diverse team is our biggest asset and we look for people who share our belief in Endowus' clear mission.

Investors, recognition, licensing
Endowus is backed by global leading strategic and venture capital investors including UBS, Samsung Ventures, EDBI, Prosus Ventures, ZVC, Singtel Innov8, Lightspeed Venture Partners, and SoftBank Ventures Asia.

Endowus has been recognised by the industry with the following awards: Singapore's Rising Star and Fintech Innovation (Asia Asset Management's Best of the Best Awards 2021), LinkedIn Top Start-ups 2021, WealthTech of the Year (Asia FinTech Awards 2021), and the top 15 Singapore's Best Workplaces Award (Great Place to Work).
Endowus is licensed by the Monetary Authority of Singapore (MAS).

About Data Engineering Intern Role
• We are looking for a Data Engineering Intern who thrives in a fast-paced environment and enjoys driving innovation through rapid prototyping and iterative development.
• Being part of Endowus's Data Engineering team, you will shadow a senior colleague and support in building end-to-end product features that the team is working on.
• You will need to leverage the entire technology stack to realise these goals and will be part of a team that is constantly tackling difficult questions of scale, architecture, and interaction.
• You will experience real-life day to day operations as a valued member of the team Requirements:
• Pursuing Bachelor Degree in Computer Science, a related field, or equivalent professional experience
• Able to commit to a 10 weeks summer internship.
• Able to work independently while being a proactive team player with excellent communication skills.
• Some fluency with hands-on software development and strong understanding of SDLC activities.
• Ability to analyse data (e.g. using SQL or Excel)
• Knowledge of CI/CD practises is a plus.
• Knowledge in at least one of the JVM languages such as Java, Scala is a plus
• Motivated, positive attitude, responsible and proactive.
• Passionate in learning and working with new technologies. Nice to haves
• Basic knowledge of finance and trading Benefits & perks
• Stock options in a fast growing company *
• Employee discount for investing at Endowus
• Flexible working hours and locations so you can live life and your best work - you are trusted to be responsible
• Competitive Staff Benefits; Annual Leave, Medical insurance, Dental *
• Productivity Equipment; Omnidesk Standing Desk & Ergonomic Chairs, Dell UltraSharp USB Type-C Monitor, high-end computer peripherals
• Choice of Apple or PC equipment
• Additional S$250 /HKD 1450 budget for your favourite hardware to boost productivity * Note: * is only applicable to Full-Time employees"
29-Apr-2022 T11:52,Senior Data Engineer,Selby Jennings,23 days ago,,Full–time,"A global multi-manager platform with pods of internal and partner portfolio managers, are searching for a Senior Data Engineer to join their Singapore office. This firm work across quantitative, fundamental equity and tactical trading strategies with over 30+ years of experience globally. They focus on investing in their own proprietary technologies so are able to be compete to be one of the best in their field.

The Senior Data Engineer will be responsible for...
• Maintaining, enhancing and optimising the data platform
• Creating new data processing pipelines
• Improving data processes including integrating new data formats, analytical tools etc
• Collaborate with Portfolio Managers, Data Scientists and Data Ops for data sets

The Senior Data Engineer should have the following...
• Strong knowledge and experience with python scripting
• Experience with Financial Data and Big Data
• Experience building, deploying and running on cloud (AWS Preferred)
• Mentoring junior engineers

For further information about this position please apply for this Senior Data Engineer in Singapore"
29-Apr-2022 T11:52,Data Engineer – SQL,AiDA Technologies,Full–time,,,"[vc_row css_animation="""" row_type=""row"" use_row_as_full_screen_section=""no"" type=""full_width"" angled_section=""no"" text_align=""left"" background_image_as_pattern=""without_pattern""][vc_column][vc_column_text]
Job Title: Data Engineer

Industry: Computer Software, Financial Services, Information Technology & Services
Employment Type: Full-time
Job Location: Singapore
Seniority Level: Entry level
Job Functions: Information Technology

Job summary

We are looking for a Data Engineer who will work on the data transformation and system deployment. The primary focus will be on understanding data science solutions of a particular problem, implement data transformations required, as web APIs . You will also work with a team of highly motivated members and integrate the solution in client environments.

Responsibilities and duties
• Writing SQL queries
• Linking different tables into one table based on a provided schema
• Implementing data transformation (including ETL) for machine learning modelling
• System integration and testing

Qualifications and skills (requirement)
• Diploma, B.Eng or BS
• Hand-on experience working with SQL-based technologies on traditional RDBMS, such as Oracle, Postgresql or MySQL
• Proficiency in Python programming or Java

Qualifications and skills (good to have)
• Database System Design
• Basic Knowledge on Machine Learning
• Experience with AWS services including S3, Redshift, EMR and RDS.
• DevOps tools such as Docker
• UI development

Benefits and perks
• An exciting learning opportunity at a fast paced startup with open and friendly colleagues
• Competitive salary based on skills & experience with excellent insurance and other benefits.
• Opportunity to work with best data scientists in the world,including Kaggle Grand Master
• Deliver high impact solutions for prestigious clients across ASEAN, India, and Hong Kong.

[/vc_column_text][/vc_column][/vc_row"
29-Apr-2022 T11:52,Data Engineer,Persolkelly Singapore Pte. Ltd.,3 days ago,,Full–time,"Responsibilities
• Design and implement relevant data models in the form of data marts stored in Operational Data Stores, Data Warehouses or Big Data platforms
• Build data pipelines to bring information from source systems, harmonise and cleanse data to support analytics initiatives for core business metrics and performance trends.
• Perform data profiling to understand data quality and advise practical measures to address such data issues through data transformation and data loading
• Dive into company data to identify sources and features that will drive business objectives.
• Work closely with project manager and technical leads to provide regular status reporting and support them to refine issues / problem statements and propose / evaluate relevant analytics solutions

Requirements :
• Prior experience building large scale enterprise data pipelines using commercial and / or open source data management tools from vendors such as Informatica, Talend, Microsoft, IBM or Oracle
• Strong knowledge of data manipulation languages such as SQL necessary to build and maintain complex queries and data pipelines
• Practical appreciation of data quality metrics and remediation strategies
• Data modelling and architecting skills including strong foundation in data warehousing concepts, data normalisation, and dimensional data modelling such as OLAP
• Undergraduate or graduate degree in Computer science or equivalent

To Apply :

Interested candidates, who wish to apply for the above position; please send in your resume to HIDDEN TEXT or click the ' Apply Now ' below and ' ATTN : BVIN '

We regret that only shortlisted applicants would be notified.

B Vidita Nantini REG No : R22105644

PERSOLKELLY SINGAPORE PTE LTD EA License No : 01C4394

By sending us your personal data and curriculum vitae (CV), you are deemed to consent to PERSOLKELLY Singapore Pte Ltd and its affiliates collecting, using and disclosing my personal data for the purposes set out in the Privacy Policy which is available at www.

persolkelly.com.sg I also acknowledge that I have read, understood, and agree to the said Privacy Policy"
29-Apr-2022 T11:52,Data Engineer,Huxley,1 day ago,,Full–time,"Huxley is currently working with a digital assets company that is rapidly expanding in the region. This is an exciting opportunity for highly motivated individuals keen to work in a busy environment and contribute with ideas.

We are currently looking to fill 2 Data Engineer positions focusing on data modelling and risk related projects:

Data Engineer should have the following:
• 3+ year of experience developing machine models
• Proficient in either Python, Java and/or Scala
• Master's Degree with working experience OR Phd
• Understanding of deep learning is a plus

Data Engineer will be responsible for:
• Building Models reliable on machine learning, graph mining and other data driven technologies. Identify potential threats in KYC, Payments, Credit, Reward etc
• Maintain the Models to meet business expectations.
• Work on data that analyses customer feedback and satisfaction analysis by leveraging on machine learning techniques.
• Collaborate effectively with various stakeholders

Please note that this opportunity is open for Singaporeans only.

If you are interested in the above-mentioned Data Engineer role located in Singapore, do contact me and attach your latest CV. I will be glad to provide additional information and guide you through the next steps where necessary.

Consultant Details:

Anastasija Horoscuka

EA License No.: R2093146

Huxley is a trading division of SThree Pte Limited

(Registration Number: 200720126E | SThree Pte Limited Licence Number 16S8216 | Huxley Licence Number 53132076J)

As per Ministry of Manpower (MoM) requirements, if you're suitable for any roles that we will be putting you forward for, we will need to request for your identification number. Please be assured that this will not be disclosed with our client or any other parties other than MoM.

Award winner of:

Recruitment Agency of the Year by Asia Recruitment Awards 2019 | Best Client Services by Asia Recruitment Awards 2017 | Best Overseas Operation by Global Recruiters 2017 | Highly Commended for Best Large Recruitment Business 2017 | Commended for Best In-House Training by Global Recruiters 2017"
29-Apr-2022 T11:52,"Data Engineer, MBG",Meta,2 days ago,,Full–time,"Meta is looking for exceptionally talented and experienced engineers to join the MBG Technology team. Our team provides analytics and workflow tools for Meta Business Group ( MBG), partnering with sales, marketing, measurement, support and operations teams.In this role, you’ll work with some of the brightest minds in the industry, work with one of the richest data sets in the world, use cutting edge technology, and get an opportunity to solve some of the most challenging business and engineering problems, at a scale that few companies can match. You will do so by partnering with stakeholders/teams and building scalable, reliable solutions that provide business critical insights and metrics, while ensuring the best uptime and responsiveness.

Data Engineer, MBG Responsibilities:
• Manage data warehouse plans for a business vertical or a group of business verticals
• Build data expertise and own data quality for allocated areas of ownership
• Design, build, optimize, launch and support new and existing data models and analytical solutions
• Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions
• Conduct design and code reviews
• Work with data infrastructure to triage infra issues and drive to resolution
• Manage the delivery of high impact dashboards, tools and data visualizations

Minimum Qualifications:
• 2+ years of experience in the data warehouse space, custom ETL design, implementation and maintenance
• 2+ years of experience in SQL or similar languages, and development experience in at least one language (Python, PHP etc.)
• Experience with data architecture, data modeling, schema design and software development
• Experience in leading data driven projects from definition through interpretation and execution
• Experience with large data sets, Hadoop, and data visualization tools
• Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience working in support of diverse communities"
29-Apr-2022 T11:52,Data Engineer,KS CONSULTING PTE. LTD.,24 days ago,$5.5K–$8K a month,Full–time,"Our client, a leading financial services company is looking to hire a Data Engineer with excellent communication skills and experience in data modelling and optimizing ETL jobs.

Responsibilities:
• Manage data modeling design, writing, and optimizing ETL jobs
• Participate in building and enhancing enterprise cloud data warehouse
• Deliver and manage in-house and cloud-native data solutions to meet business requirements across firm-wide business units
• Design and implement Data and AI central data platform as well as related tools/systems for advanced business analytics and enterprise data governance
• Assist in creating and monitoring analytics dashboards, for different business functions
• Ensure quality, integrity and accuracy of datasets through tracked, secured and auditable controls

Requirements:
• Hands-on experience with Linux and shell scripting
• Working experience with containerization (Docker/K8S) and task orchestration tools (Airflow/Luigi, etc.)
• Experience with cloud service and tools (AWS/Azure/GCP), as well as cloud data warehouse platforms.
• Experience with modern DevOps practices including version control, TDD, CI/CD, etc., for both code and configuration changes.
• Excellent coding skills with Python and SQL, and solid understanding of object-oriented analysis and design
• Working knowledge of common algorithms and data structures, with strong analytical and problem-solving skills
• Basic understanding and experience with ML/AI concepts (e.g. deep learning, deep reinforcement learning, deep bayesian learning), workflows, and toolsets (Jupyter Notebook, etc.), and libraries (Numpy, Pandas, Scikit Learn, PyTorch, etc.) preferably in both cloud-native and desktop deployments.
• Experience with traditional RDBMS based systems, including Data Lake, Data Warehouses and Marts, and more modern NoSQL and cloud-native big-data

If you or anyone within your network is keen to discuss it further then please share your resue with manisha@kstalentsolutions.com"
29-Apr-2022 T11:52,"Manager, Product Development (Data Engineer)",MasterCard,Full–time,,,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion (https://www.mastercard.us/en-us/vision/who-we-are/diversity-inclusion.html) for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Manager, Product Development (Data Engineer)

Scope

Create and maintain optimal data pipeline architecture

Participate in development of data and analytic infrastructure for product development

Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.

Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.

Keep up to date with latest open source tools for data engineering

Expert knowledge in ML lifecycle

Qualifications

At least 4 years of experience as data engineer, building and deploying data pipeline

High proficiency in using Python/Scala, Spark(tuning jobs), Hadoop platforms to build Big Data products & platforms

Experience with data pipeline and workflow management tools: NIFI, Airflow.

Experienced with Agile frameworks, e.g., Scrum.

Experience with open source tools to build data pipeline, deploying and monitoring the algorithm

Comfortable in developing shell scripts for automation

Comfortable in creating CI/CD pipeline for testing, deploying and monitoring algorithm

Motivation, flexibility, self-direction, and desire to thrive on small project teams

Experience with the visualization tools like tableau, looker

COVID-19 Considerations

We value the safety of each member of our community because we know we’re all in this together. In many locations, which may change over time, we’ve implemented a virtual hiring process and continue to interview candidates by video or phone. In addition, in some locations, only individuals who have been fully vaccinated will be permitted inside Mastercard offices until further notice.

In the US, Mastercard is a government contractor, which may legally require most Mastercard employees to be vaccinated unless a verified approved medical or religious exemption is granted. Further, we are currently making every effort towards having employees return to work in the office 2 days per week, if that makes sense for their team. Everyone must be vaccinated to enter Mastercard offices at this time. Therefore, we expect all candidates to be vaccinated or to be approved for a medical or religious accommodation prior to commencing work at Mastercard.

Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.

Requisition ID: R-155294"
29-Apr-2022 T11:52,Data Engineer -Python ( 1 year extendable contract),Michael Page,$60K–$78K a year,,Contractor,"About Our Client

Our client is a renowned name in the Banking industry and is currently seeking for data engineers to design,develop and implement data applications

Job Description
• Solid hands on experience in writing Python scripts for data processing and analytics use cases.
• Should have experience on building batch and streaming data pipelines.
• Extensive experience in implementing scalable Data pipeline using Python or R/Scala, SQL.
• Exposure to core banking system, good understanding of Private banking product's life cycle
• Participate in reviews and meetings and provide updates on project progress
• Take responsibility for ensuring that risks and issues are identified and managed closely and drive all stakeholders to deliver on time and to the required quality standards
• Ensure production stability and timely communication to stake holders

The Successful Applicant
• Minimum 3 years of experience in Designing and developing Data applications using python and other programming languages.
• Bachelor's degree in information technology or computer science;
• Experience building APIs using python libraries.
• Experience working with structured and unstructured data, SQL scripting, Kafka streaming.
• Good to have an understanding of Kafka messaging system.
• Experience working in a Private Banking environment is preferred.
• Experience working on building data pipelines for Data and analytics aplications or a Data Warehouse system

What's on Offer

You will be part of an organisation who sees value in investing in their employees. Stability in your career is a key for them.The remuneration for this role will be competitive and in line with the market"
29-Apr-2022 T11:52,"Senior Data Engineer, Global Business Intelligence",Apple,16 days ago,,Full–time,"Summary
Posted: Apr 14, 2022

Role Number:200366968

At Apple, new ideas have a way of becoming extraordinary products, services and customer experiences. Bring passion and dedication to your job and there's no telling what you could accomplish!

The people here at Apple don’t just create products — they create the kind of wonder that’s revolutionised entire industries. It’s the diversity of those people and their ideas that inspires the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it!

The Global Business Intelligence team provides data services, analytics, reporting and data science solutions to Apple’s business groups including Retail, AppleCare, Operations, Finance, Sales, Marketing, Apple Media Products and external partners. We lead enterprise data stores with petabytes of data, process billions of rows of data in batch and real-time to provide solutions to our users helping them gain insight and to make informed business decisions.

We are looking for a Senior Data Engineer to join our team. You will engage directly with key business partners to understand the business strategies and solution needs, drive and lead technical design discussions with development teams. You should be passionate about data and its nuances, and enjoy getting hands-on with technical solution design and development work. In Apple, we leverage a diverse technology stack such as Teradata, PostgreSQL, Snowflake, SingleStore, HANA, Spark, Cassandra and beyond. Designing, developing and scaling solutions using these technologies are a core part of our daily job.

Key Qualifications

Key Qualifications
• Experience in designing and building dimensional data models to improve accessibility, efficiency and quality of data.
• Hands on database development experience with Relational or MPP/distributed systems such as Snowflake/ Teradata/ SingleStore/ Hadoop
• Programming experience in building end to end data pipelines with SQL, Python and/or Scala
• Proficient in writing Advanced SQL with expertise in performance tuning
• Experience working with data at scale
• Ability to communicate effectively, both written and verbal, with technical and non- technical teams
• Strong understanding of development processes and agile methodologies
• Experience with data science and machine learning tools as well as cloud technologies is a plus

Description

Description
Design and build data structures on platforms such as Snowflake, and highly scalable data pipelines using technologies such Spark and Kafka to provide efficient reporting and analytics capability,

Build data and reporting solutions (in platforms such as SingleStore) that are highly optimised for fast data access

Translate complex business requirements into scalable technical solutions meeting data warehousing design standards. Solid understanding of analytics needs and proactiveness to build generic solutions to improve overall efficiency.

Strong understanding of analytics needs and a proactive approach, focusing on reusable solutions

Lead/work with many global teams, communicate effectively, both written and verbal, with technical and non-technical multi-functional teams.

Keep up-to-date on the newest technology solutions in market to generate innovative ideas to solve business challenges.

We seek a self starter, forward-thinking person with strong leadership capabilities.

Education & Experience

Education & Experience

Additional Requirements

Additional Requirements
• Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.
• We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation"
29-Apr-2022 T11:52,Big Data Engineer,Salt,22 days ago,$8.5K–$9.5K a month,Contractor,"SALT is hiring Big Data Engineer for a global technology client on a two-year convertable contract role.

You will be responsible for
• Responsible for the monitoring and uptime of all production and Non-Production Big Data Stack.
• Strive to improve the stability, security, efficiency, scalability, and availability of production systems by applying software engineering practices and by implementing monitors and alerts.
• Estimate Bigdata stack capacities; develop methods for monitoring capacity and usage.
• Lead efforts to develop and improve procedures for automated monitoring and proactive intervention, reducing any need downtime.
• Provide and drive architecture and the design of Big Data solutions
• Design and implement high performance systems supporting large volume real time and batched data processes
• Propose and design data architecture solutions for scalability, high availability, fault tolerance, and elasticity for various application needs.
• Develop ETL pipelines with robust monitoring and alarming
• Develop data models that are optimized for business usability and understanding
• Develop and optimize data tables using best practices for partitioning, compression, parallelization, etc.
• Optimize SQL and ETL solutions to solve various reporting requirements.

You should be coming from
• Overall 8+ years of relevant industry experience
• Good knowledge of big data technology landscape and concepts related to distributed storage/computing.
• Experience with big data frameworks (e.g., Hadoop, Spark) and distributions (Cloudera, Hortonworks, MapR).
• Experience with batch & ETL jobs to ingest and process data from multiple data sources.
• Experience with NoSQL databases (e.g., Cassandra, MongoDB, Neo4J, Elasticsearch, InfluxDB)
• Experience with querying tools (e.g., Hive, Spark SQL, Impala).
• Experience in real-time stream processing, using solutions such as Kafka, AWS Kinesis, Flume, and/or Spark Streaming.
• Experience in DevOps and DataOps principles (e.g., Infrastructure as code, automating different parts of the data pipeline).
• Monitor and evaluate Big Data stack performance and adjust configuration as appropriate
• Participate in disaster recovery design, testing and execution

If this sounds like your ideal next move, I want to talk to you! Reach out to me at jvenkataraman@welovesalt.com or apply via the posting.

CEI No: R1659595 / Licence No: 07C3147

Salt is acting as an Employment Business in relation to this vacancy.

Job Information
Job Reference: JO-2204-253832
Salary: Negotiable
Salary per: month
Job Duration:
Job Start Date: 01/06/2022
Job Industries: Technology
Job Locations: Singapore
Job Types: Contract

Apply for this Job

Name *

Please enter your full name.

Email *

Enter a valid email address.

Upload a CV *

Upload your CV to accompany your application for this job.

Please tick this box to consent to us using your data. How we use your data is outlined in our privacy policy *

Fields marked with * are required"
29-Apr-2022 T11:52,Azure Data Engineer,Avanade,18 mins,,Full–time,"As a Data Engineering Consultant at Avanade, you will use modern data engineering techniques to build Azure based data pipelines and platforms to support Avanade's customers in making better business decisions.

In this role, you will be engaged in the Data Engineering role family inside the Analytics Talent Community. You will design and build data pipelines, data platform, dashboarding and reporting solutions, data consumption APIs and a variety of other solutions and tools for end users. You will be a critical point within the data supply chain, while ensuring that all stakeholders have access to and can work with the latest data.

In line with an always increasing market demand, Avanade is looking to expand its Analytics team with experienced Data Engineer (but not only, see other job postings) to work on state-of-the-art data platform on the Azure cloud.

""In addition to the opportunity to work on great projects with the latest technologies, Avanade is distinguished above all by its informal culture and a strong us-feeling. Everyone is willing to help each other, is approachable and frequently there are evenings or days organized for training, knowledge sharing or team bonding. With a good mix of working hard on fun and challenging projects and attending fun events organized for colleagues, Avanade is a Top Employer for a reason."" - Robin - Data Engineer at Avanade.

You make the difference because:
• You like to work with the latest tools & techniques in the field of data to help the largest companies in Belgium (or other countries, if you consider it);
• Microsoft Azure Data & Analytics PaaS & SaaS Services such as Databricks (Spark), HD Insight, Data Factory, Data Lake, CosmosDB, SQL and DevOps have no/little secrets for you, or you have a similar technology stack expertise that can be transposed;
• In addition, you have experience with data modeling/warehousing techniques and data integration;
• You also have advanced knowledge of at least one programming languages (Python, Scala or Java);
• CI/CD, VCS, Parallel Processing (MPP) and NoSQL are not abstract concepts to you;
• You have at least 3 years of relevant work experience, or can justify a similar expertise;
• You have a Bachelor’s or Master’s degree in Computer Science, Mathematics, Statistics or another relevant field;
• You speak English, preferably also Dutch or French.

Why should you choose Avanade?
• At Avanade we continuously invest in the development of your skills. We help you to keep up to speed with the latest in technology and business.
• Besides an exciting international environment, a unique chance to participate in challenging and state-of-the-art projects and interaction with inspiring colleagues who have a professional attitude and attach considerable importance to teamwork; we stand for:
• A competitive salary with additional benefits, such as a company car, phone, insurances.
• A very strong training & development focus, with an internal University and ample opportunity to learn and grow.
• A personal career plan, controlled by you and guided by Avanade.

Do you feel the match?

Do you recognize yourself in this description and would you like to get to know us? Then apply immediately by sending in your CV via our website. Applying by e-mail is not possible. If we see a match, we will call you quickly for an introduction and you will have 2 job interviews. Do you have any questions or want to know why we love working at Avanade? Then contact our recruiter Eline Rubertus"
29-Apr-2022 T11:52,Staff Data Engineer,Twitter,3 days ago,,Full–time,"Who We Are :

Twitter users generate many terabytes of data every day; Twitter engineers run hundreds of experiments; Twitter Data Engineers build data pipelines and data processes that calculate metrics and scale increasingly sophisticated models of users and content.

The Data Science team at Twitter is at the intersection of all this data and strives to make it actionable to all business units around Twitter.

Data Engineers work alongside Data Scientists analyze this data via observational analyses, trend analyses, modeling, and new measurement strategies.

We also implement metrics to track the impact of new product experiments and more generally find ways to make very large scale data approachable to guide our decisions.

What Youll Do :

Twitter has very large and complex datasets. As a Twitter Data Engineer you will build datasets and make them accessible to our partner teams by writing great production code to simplify the complexity.

Your work will enable Product Managers and other decision-makers across the company to bring together insights and inform our product and strategy.

In every decision that you influence, you will see the product improve and be more valuable to Twitter users.

We are trying to improve Twitter. To improve something, we need to be able to measure it. As a Data Engineer you will enable better measurements and ensure measurement accuracy so that we know where we are doing well and where we want to improve.

As such, you will :

Design, develop, and launch extremely efficient and reliable data pipelines to move data and to provide intuitive analytics to our partner teams.

Make Twitter-scale data more discoverable and easy to use for Data Scientists and Analysts across the company.

Collaborate with other engineers and Data Scientists to discover the best solutions.

Support your colleagues by reviewing code and designs.

Diagnose and solve issues in our existing data pipelines and envision and build their successors.

Who You Are :

You want to be part of a community of the most talented, forward-thinking Data Scientists and Engineers in the industry.

You are a strong Scala or Java developer. You demonstrate clear and concise communication and data-driven decision-making.

You are passionate about learning or growing your expertise in some or all of the following :

Qualifications

B.S. and / or M.S. in Computer Science or a related technical field, or equivalent experience

8+ years of experience in either data infrastructure or backend systems

Strong understanding of SQL

Broad knowledge of the data infrastructure ecosystem

Experience with Hadoop or other MapReduce-based architectures

Experience working with large data volumes

Good understanding of one or more of the following : Scala, C++, or Java

Experience with any of the following is a plus :

Scalding

Full Stack Development

Company Description

Twitter serves the public conversation because conversation is a force for good in the world. The opportunity to help the world connect, debate, learn, and solve problems is what draws us to careers at Twitter, and its what keeps us here.

Additional Information

We are committed to an inclusive and diverse Twitter. Twitter is an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, age, disability, veteran, genetic information, marital status or any other legally protected status.

San Francisco applicants : Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records"
29-Apr-2022 T11:52,Data Engineer,Raffles Medical Group,1 day ago,,Full–time,"Data Engineer, Raffles Medical Group

Raffles Medical Group (RMG) is a leading integrated regional healthcare provider, listed on the Singapore Stock Exchange (SGX:BSL). Our regional presence spans 14 cities with medical facilities in Singapore, China, Japan, Vietnam and Cambodia.

At RMG, you’ll be part of a team who is focused on delivering high quality healthcare experiences to our broad spectrum of clients. We strive for an environment where we inspire one another, encourage you to achieve your best, and importantly, share success together. We create opportunities to develop your skills and chart your career with us.

We are looking for a Data Engineer to build data infrastructure to power the insights needed for evidence-based decision-making and enhance service-delivery. You will be architecting, designing and building next-generation data infrastructure to drive digital transformation in the heath sector.

Key Responsibilities/Duties

Design and build resilient and efficient data pipelines for both batch and real-time streaming data

Architect and design data infrastructure on cloud using industry standard Infrastructure-as-Code tools

Execute projects with an Agile mindset

Build software frameworks to solve data problems at scale

Collaborate with product managers, software engineers, data analysts and data scientists to build scalable and data-driven platforms and tools

Obsess over cyber security by identifying threat & risk, ensuring all the components, from a platform, frameworks to the applications are fully secure and are compliant to infosec policies

Develop metrics to measure and monitor client and server API health and utilization

Requirements

Degree in Computer Science or have equivalent professional experience

At least 3 years of relevant experience in developing scalable, secured, fault tolerant, resilient & mission-critical data platforms

Experience with data processing tools such as Spark, Beam, Flink, Airflow, Kafka, dbt

Experience with the cloud (e.g. AWS, GCP, Azure)

Experience implementing batch and streaming data pipelines

Experience writing efficient SQL

Proficiency in at least one of the programming languages Java, Scala, or Python along with a fair understanding of runtime complexities.

In-depth knowledge of both SQL and NoSQL databases, including performance tuning and troubleshooting

In-depth knowledge of data modelling, data warehouse, and data lake

Familiar with DevOps tools such as Git, Docker, Terraform

Familiar with MLOps tools such as MLFlow, Kubeflow

Passionate about working in healthcare and raising the bar on healthcare solutions in Singapore and the region

Thank you for your interest. We regret that only shortlisted applicants will be notified"
29-Apr-2022 T11:52,Data Engineer / Software Engineer - Data,Zoom,4 days ago,,Full–time,"Work Styles at Zoom

In most cases, you will have the opportunity to choose your preferred working location from the following options when you join Zoom: in-person, hybrid or remote. Visit this page for more information about Zoom's Workstyles.

About Us

Zoomies help people stay connected so they can get more done together. We set out to build the best video product for the enterprise, and today help people communicate better with products like Zoom Contact Center, Zoom Phone, Zoom Events, Zoom Apps, Zoom Rooms, and Zoom Webinar.

We’re problem-solvers, working at a fast pace to design solutions with our customers and users in mind. Here, you’ll work across teams to deliver impactful projects that are changing the way people communicate and enjoy opportunities to advance your career in a diverse, inclusive environment.

You will be part of a team whose focus is to solve cutting edge AI / MLproblems and deploy models that constantly advance the state-of-the-art. You will be working on designing the data infrastructure and solve other interesting bottlenecks that are challenging at Zoom's scale.

Responsibilities :

Design and implementation of data infrastructure for Machine Learning (ML) projects for our Research and Development Org

Assembling large, complex data sets and make the data more discoverable and easy to be used by the AI / ML team to support various Machine Learning initiatives. This includes unifying pre-processing work flows for data for different AI tasks.

Build and manage data sets, also by identifying and using suitable big data technologies, such as Apache Hadoop, DVC, Apache Drill,

Write tools to transform raw data sources into easily accessible models by coding across several languages such as Python, and SQL

Deploy, develop and maintain tools for data collection, data crawling and data annotation, e.g. as web applications using Django and Angular or similar technology.

Along with looking at external sources of data, build data expertise and own data quality for the pipelines you create.

Requirements :

Bachelor’s or higher degree in Computer Science or related fields

Experience in building and optimising data pipelines
• Docker
• Experience with AWS cloud services, e.g. S3
• Experience working with large data volumes
• Broad knowledge of the data infrastructure ecosystem
• Knowledge of technologies like DVC or Hadoop
• Good understanding of one or more of the following : Python, JavaScript, C++, Django or Angular

Please note : This job opportunity is based out of Karlsruhe, Germany

We believe that the unique contributions of all Zoomies is the driver of our success. To make sure that our products and culture continue to incorporate everyone's perspectives and experience we never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. Zoom is proud to be an equal opportunity workplace and is an affirmative action employer. All your information will be kept confidential according to EEO guidelines.

We welcome people of different backgrounds, experiences, abilities and perspectives including qualified applicants with arrest and conviction records and any qualified applicants requiring reasonable accommodations in accordance with the law. If you need any assistance or accommodations due to a medical condition, or if you need assistance accessing our website or completing the application process, please let us know by emailing us at careers@zoom.us.

Zoom requires all U.S. employees who will work in person at a Zoom office, attend in-person Zoom meetings or have in-person customer meetings to be fully vaccinated. Zoom will consider requests for reasonable accommodations for religious or medical reasons as required under applicable law.

At Zoom, we care about our employees, their families, and their well-being. As part of our award-winning workplace culture and commitment to delivering happiness, our benefits program offers a variety of perks, benefits, and options to help employees maintain their physical, mental, emotional, and financial health; support work-life balance; and contribute to their community in meaningful ways. To view our benefits, click here.

Explore Zoom:
• Hear from our leadership team
• Browse Awards and Employee Reviews on Comparably
• Visit our Blog
• Zoom with us!
• Find us on social at the links below and on Instagram
• View more jobs, sign up for job alerts and join our talent community. Visit the Zoom careers site.

#LI-Remote"
29-Apr-2022 T11:52,Software/Data Engineer#TeSA #CLT,Aida Technologies Pte. Ltd.,3 days ago,,Full–time,"Training Programme This Company-Led Training (CLT) programme is a 9 to 12 months on the job training with Aida Technologies Pte Ltd to equip fresh professionals with industry skills to become an Artifical Intelligence / Machine learning (AI/ML) Engineer. Each trainee will be trained and mentored in one or more of the following areas: AI and ML applications Data Management process Machine learning, deep learning techniques and tools Delpu AI/ML as a data product and/or data service Minimum Entry Requirements Singapore citizen Diploma/Degree in Computer Science or Engineering. Fresh and Mid-level professionals are welcome Application Process Interested individuals can apply directly with Aida Technologies Pte Ltd. Please email your CV to [Confidential Information]."
29-Apr-2022 T11:52,Data Engineer - HFT - Singapore- Leading Quant Fund | London,Oxford Knight,3 days ago,,Full–time,"Data Engineer - HFT - Singapore

Summary

The positive feel of a start-up with the benefits that come with a more established player, this leading HFT firm is looking for a dynamic software engineer to join one of their most successful quant trading teams. Collaborating extensively with traders and other technologists, you'll design, write and maintain a complex Python infrastructure. This role is as front office as developers come within the firm without being a Quant.

The ideal candidate will be passionate about: data, development, and keen to learn about automated electronic trading. You will be expected to solve difficult technical problems in a fast-paced and energetic environment, with a focus on processes that are robust, scalable and fault-tolerant. Most problems require high-availability, high-throughput and low-latency solutions.

The trading team see technology as a key component of their continued success and candidates will be exposed to cool, cutting-edge technologies.

Financial experience is not essential; just a willingness to learn, and learn quickly.
• **Interviews will be remote; ideally start dates will be in-house, but they're flexible.***

Requirements

• Excellent problem-solving skills, ability to make the right engineering decisions to achieve maintainability, extensibility and debuggability
• Solid knowledge of Python and Linux. (Some C++ preferable)
• Self-directed and able to take ownership of several projects at once
• Knowledge of SQL, Redis, InfluxDB. is a bonus
• Bachelor's degree (or higher) in Computer Science or Computer Engineering (or equivalent)
Benefits

• Competitive base salary + bonus; they will pay leading market rate / are flexible for the right candidate
• They're willing to be flexible with WFH
• Collaborative and rewarding work environment and culture
• Free breakfast , lunch and dinner

Contact
If this sounds like you, or you would like to know more, please get in touch.

Andy Stirling-Martin
andy@oxfordknight.co.uk
07453 28768
linkedin.com/in/andrew-stirling-martin-7664a946"
29-Apr-2022 T11:52,"Director, Data Engineering",Singtel Group,19 days ago,,Full–time,"Singtel, Asia’s leading communications technology group, provides an extensive range of telecommunications and digital services to millions of consumers and businesses across Asia, Australia, Africa and the USA. With over 140 years of innovation behind us, we continue to push boundaries in our networks and services, to enrich lives and transform businesses.

Our core values – Customer Focus, Challenger Spirit, Teamwork, Integrity, and Personal Excellence – shape the way we work. We are passionate about making a difference and have an open and inclusive culture where everyone is empowered to do their best. Our diverse business means you will enjoy unique opportunities and rewarding experiences to learn and grow your career in a dynamic industry.

Join us and experience what it’s like to be with an Employer of Choice*. Together, let’s create a brighter digital future for all. *Awarded at the HR Fest Awards 2020

About the role

As part of our expansion and increasing demands for Big Data projects within Singtel Group, we are looking for a Big Data Director to:
• Define and govern Big Data strategy, architecture and roadmap
• Drive, lead and align new capabilities in the area of Big Data and Data Integration across Singtel Group
• Lead and manage Big Data development and operations teams
• Track and monitor vendor’s SLA performance
• Plan and manage annual budget for both capital expenditures and operating expenses
• Develop multi-year IT investment roadmaps that link to the Group IT strategy
• Define standards and guidelines for development and operations
• Build, maintain and improve strong relationship with business leaders, Group IT domains (departments) and IT service providers to deliver value via data
• Represent Group IT at assigned project and / or program steering committees
• Present and pitch at relevant senior leadership levels and /or executive steering committees
• Drive Group IT and / or domain initiatives to the team
• Promote a DevOps culture through building relationships with development & operations and driving enhancements to the end-to-end release process
• Proactively advise the senior management team on the emerging technologies and digital trends that are most relevant to the company's goals and evolving needs
• Hire, develop, evaluate, reward, and retain a highly-qualified team

Responsibilities
• Develop, define and govern Singtel Big Data and data management architecture, guidelines and roadmap with stakeholders
• Build up new capabilities for Big Data to support business initiatives
• Overall responsible for delivery and operations for Big Data functions
• Manage multiple Big Data delivery / project teams comprise of internal IT professionals and several IT service providers to ensure that projects / enhancements are delivered within the agreed scope, budget and schedule
• Lead the application support teams comprise of internal IT professionals and IT service providers in managing, monitoring and optimizing of Big Data Platforms to ensure these applications meet the agreed SLA agreed with business
• Troubleshoot Big Data platforms and software, including performance-tunes of BI applications
• Budgeting, tracking and forecasting capital & operational expenditure
• Interface with product vendors to keep abreast of new technologies, pricing and customer applicability. Participate in vendor evaluations
• Lead and establish development guidelines, standards and best practices
• Implement the most appropriate and effective IT organizational design to support and engage with the business
• Work with business stakeholders to develop and analyze big data needs
• Manage and drive outsourced vendors to achieve defined delivery and operations objectives
• Manage resource capacity and staffing
• Provide functional leadership to assigned staff and deliver input to staff performance and development
• Identify capability for the team and provide guidance, training, and problem-solving assistance to team members
• Present and engage senior leadership levels and /or executive steering committees on Big Data initiatives, projects and operations
• Drive Group IT initiatives to the team
• Develop and execute an analytics program that will allow company business leaders to make data-based decisions

The ideal candidate should possess
• Bachelor's degree in Business Management, IT, Computer Science or equivalent.
• At least 12 years of experience in the area of big data, data warehousing, BI & reporting and / or data management, including at least 8 years of experience in managing BI or big data development and operations team
• Experience in managing and driving outsourced vendors to delivery and operations objectives
• Hands-on experience in handling incident, problem, configuration, capacity and availability management
• Successfully implemented large-scale data warehouse / data lake
• Expert in building and optimizing ‘big data’ data pipelines
• Experience in data management, data architecture and design
• Strong technical knowledge of data integrations, ETL and data warehouse data modelling
• Strong knowledge of BI reporting tools
• Strong knowledge of SQL
• Good understanding of Telco data models
• Experience to do cost estimation and working with external vendors
• Experience with DevOps tools and environment
• Experience in building an enterprise level data analytics capability.
• Experience in leading complex, major change initiatives; skills in change management
• Customer-service oriented
• Strong background in operational and capital finances, and IT budget development

We believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers. We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative"
29-Apr-2022 T11:52,[Internship 2022] Data Engineer Intern (APAC),foodpanda Singapore,20 mins,,Full–time,"This is a full-time internship opportunity starting in May 2022

At foodpanda, we're on a mission to redefine how tech, food, people and culture are connected. Operating in more than 400 cities across 12 locations worldwide, foodpanda continues to expand and grow in its core food delivery business as well as in new verticals like grocery deliveries, with a strong tech infrastructure at its core. From our restaurants-partners, cloud kitchens and cloud grocery stores — foodpanda is just one tap away, delivering everything you need quickly and conveniently to your doorstep.

We are looking for undergraduates with a passion for data and insights to join our APAC Regional teams in Singapore! You will be empowered to find the most effective way of using data to help foodpanda make smart, data-driven business decisions.

This is an amazing opportunity for individuals who would like to learn, thrive and hone their analytical skills. If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!

What's on the menu for you:
• Defines and builds data pipelines that will enable faster, better, data-informed decision-making within the business
• Architect, build, and deploy new data models that provide intuitive analytics across the business
• Manage and own the entire data processing system (internally, as well as interfacing with other teams or third party services), provide advice on any necessary infrastructure changes
• Experiment with, select, and implement cutting edge Big Data tools and frameworks required to provide requested capabilities
• Perform all needed data transformation to populate a reporting optimised data warehouse
• Participate in building machine learning models, deliver analyses and insights to support business decision making

What you bring to the table:
• Penultimate or final year undergraduate students pursuing bachelor’s degree in Computer Science, Engineering, Data Analytics, Mathematics, or related discipline
• Ability to write clean, structured, and high performance SQL code
• Strong oral and written communication skills
• Strong business mindset and ability to grasp business requirements from stakeholders
• Champion of data and visualization with strong presentation and story-telling skills
• Knowledge and experience with BI tools (Tableau, Data Studio), big data, Data Warehouse technologies, Python or R.
• Knowledge of Machine Learning, Big Data, Data Pipelines, or setting up environments for data scientists/machine learning engineers is a plus.

What we Offer:
• A dynamic and challenging work environment.
• A company committed to developing you personally and professionally.
• A great working atmosphere with regular company and team events.
• A vibrant and international team committed to diversity and inclusion.
• Responsibility from day one in a fast growing and global company.
• Other benefits include free food and learning and development opportunities"
29-Apr-2022 T11:52,Data Engineer,Quesscorp Singapore Pte Ltd,11 days ago,,Full–time,"Deliver big data solution based on premise Hadoop or cloud-based systems like AWS.

• Design ingestion layer for structured & unstructured data (text, voice, xml etc) & implement insurance specific data model for business & analytics use.
• Deliver ELT solution including data extraction, transformation, cleansing, data integration and data management.
• Implement batch & near real time data ingestion pipelines based on reference architecture like Lambda.
• Build advanced data sources for self-service analytics systems
• Operationalize analytics models for production usage with big data workflows, proper security & access control.
• Provide development and architecture support for internal analytics tools & optimize execution performance for complex data pipelines.
• Bachelor’s Degree in Computer Science / IT
• 3+ years of experience in Big Data Engineering using tools like Spark, Hadoop, Hive, etc.
• 3+ years of experience with Software Engineering using Python / development using modern Relational and NoSQL databases like MySQL, PostgreSQL, and MongoDB
• Worked on Talend (Big Data) and/or Informatica Data Engineering 10.4 platform using modules DEI, DES, PowerExchange and PowerCenter
• Experience in developing medium to large scale Data Integration projects, including Real-Time and APIs based initiatives.
• Good knowledge on spark coding and shell scripting
• Hand on experience with Kafka for publishing and consuming Data"
29-Apr-2022 T11:52,Data Engineer,SOL-X PTE. LTD.,4 days ago,$6K–$9K a month,Full–time,"This role is for a highly skilled data engineer with deep knowledge in data systems, will be able to work independently with minimal supervision and will have a divide and conquer mindset when attacking data flows. This role is responsible for expanding and optimizing our data and data pipelines architecture as well as optimizing data flows and collections for analytics. This role will support data scientists, products and engineering team on the data initiatives and will ensure optimal data delivery throughout multiple ongoing projects.

Responsibilities:

· Create and maintain optimal data pipeline architecture.

· Assemble complex data sets to meet specific requirements.

· Creating data systems that ingest data from various sources.

· Implement flows with distributed systems and cloud architecture.

· Write efficient, well documented, and highly readable code.

· Schedule/automate data pipelines and monitor their performance.

· Write ad-hoc queries in order to perform analysis

· Interact with the teams to gather requirements and explain his work

Qualifications & Experience:

· Bachelor’s degree in engineering majors / computer science or similar relevant field

· 5+ years of experience in Software Engineering / BI / Data Warehouse design and development using modern

Relational and NoSQL databases like MySQL, PostgreSQL, and MongoDB

· 3+ years of experience in building SQL scripts and automating DWH pipelines.

· Good knowledge and experience with Python for data manipulation

· Experience with Unix and cloud solutions such as Azure Blob, AWS EC2, AWS EMR

· Data Modelling and Data Science experience building predictive models

· Experience with data automation/data orchestration tools such as Prefect is a plus"
29-Apr-2022 T11:52,Senior/Principal Big Data Engineer,ORACLE CORPORATION SINGAPORE PTE LTD,6 days ago,,Full–time,"Applicants are required to read, write, and speak the following languages: English

The Java Platform Group is looking for an experienced, passionate, and highly motivated Big Data Engineer. Our team is responsible for delivering the Java platform that is used by millions of developers. We are looking for an engineer with a thorough working experience of both the Java Platform and the implementation of the cloud native services. We are hiring to further expand our cloud service to more Oracle commercial regions and we are continuously investing to grow the service's capabilities and footprint across the globe.

As a Big Data Engineer with the Java Platform Group, you will be a leading contributor in Big Data Analytics of Oracle’s latest Cloud Services Technologies. You will take an active role in the definition and evolution of standard practices and procedures. Additionally, you will be responsible for scaling our existing infrastructure, incorporating new data sources, building robust data pipelines for production level cloud ********** you have a passion Big Data processing this is the place where you can make a difference.

Responsibilities

Develop ETL pipelines with robust monitoring and alarming

Develop data models that are optimized for business usability and understanding

Develop and optimize data tables using best practices for partitioning, compression, parallelization, etc.

Develop and maintain metadata, data catalog and documentation in regard to the data flow of the system.

Ensure the handling of the data is in compliant to security and privacy requirements of the organization.

Optimize SQL and ETL solutions to solve various reporting requirements.

Maintain and continuously improve Java Management Services operations inline with the service's SLOs

Ensure JMS performance, uptime and scale, maintaining high standards of code quality

Work with agile development methodology, adhering to best practices established by Oracle and project team and pursuing continued learning opportunities

Knowledge sharing and provide mentorship to junior engineers.

Perform in-depth data analysis of incoming JVM telemetry data

Provide technical consulting and mentoring to team members on optimization techniques, new algorithm design,

Implementation and conformance to architecture standards

You will be responsible for architecting and developing data models

Help deploy and productionize Machine Learning models.

Skills we require:

10+ years experience with designing, implementing, testing and operating cloud native in an Agile and DevOps setup

3+ years of industry experience in Data Engineering with experience manipulating, and extracting data from large datasets.

3+ years of Data Warehousing experience preferably with Oracle or related technology displaying strength in SQL, python/pyspark scripting, data modeling and ETL development.

2+ years working in a role as Lead / Principal Engineer or Architect

Broad knowledge of different types of data storage engines - (non)relational, row/column oriented dbs. e.g. Oracle, Postgres, MySQL, QB/redshift, Elastic, Hive, HBase

Advanced query language (SQL) knowledge

Experience with stream processing engines (Kafka, Kinesis)

Experience in using a Java framework in development preferable using Dropwizard

Experience deep working experience with one or more major cloud vendors (OCI, Azure, AWS, GCP)

Strong analytical skills, 2+ years’ experience with Python and an interest in Machine Learning

Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Design, develop, troubleshoot and debug software programs for databases, applications, tools, networks etc.

As a member of the software engineering division, you will take an active role in the definition and evolution of standard practices and procedures. You will be responsible for defining and developing software for tasks associated with the developing, designing and debugging of software applications or operating systems.

Work is non-routine and very complex, involving the application of advanced technical/business skills in area of specialization. Leading contributor individually and as a team member, providing direction and mentoring to others. BS or MS degree or equivalent experience relevant to functional area. 7 years of software engineering or related experience.

: Product Development

: Regular Employee Hire

: Oracle

All over the world, people's lives are better because of Oracle. Want to make a difference? Join our company of change-makers.

From Oracle to culinary school and back again. Bonnie Carlson Kaypaghian uses the skills she learned to create recipes for her daughter’s Type 1 Diabetes and has written a cookbook to share with the world. #LifeatOracle

The Java Platform Group is looking for an experienced, passionate, and highly motivated Big Data Engineer. Our team is responsible for delivering the Java platform that is used by millions of developers. We are looking for an engineer with a thorough working experience of both the Java Platform and the implementation of the cloud native services. We are hiring to further expand our cloud service to more Oracle commercial regions and we are continuously investing to grow the service's capabilities and footprint across the globe.

As a Big Data Engineer with the Java Platform Group, you will be a leading contributor in Big Data Analytics of Oracle’s latest Cloud Services Technologies. You will take an active role in the definition and evolution of standard practices and procedures. Additionally, you will be responsible for scaling our existing infrastructure, incorporating new data sources, building robust data pipelines for production level cloud ********** you have a passion Big Data processing this is the place where you can make a difference.

Responsibilities

Develop ETL pipelines with robust monitoring and alarming

Develop data models that are optimized for business usability and understanding

Develop and optimize data tables using best practices for partitioning, compression, parallelization, etc.

Develop and maintain metadata, data catalog and documentation in regard to the data flow of the system.

Ensure the handling of the data is in compliant to security and privacy requirements of the organization.

Optimize SQL and ETL solutions to solve various reporting requirements.

Maintain and continuously improve Java Management Services operations inline with the service's SLOs

Ensure JMS performance, uptime and scale, maintaining high standards of code quality

Work with agile development methodology, adhering to best practices established by Oracle and project team and pursuing continued learning opportunities

Knowledge sharing and provide mentorship to junior engineers.

Perform in-depth data analysis of incoming JVM telemetry data

Provide technical consulting and mentoring to team members on optimization techniques, new algorithm design,

Implementation and conformance to architecture standards

You will be responsible for architecting and developing data models

Help deploy and productionize Machine Learning models.

Skills we require:

10+ years experience with designing, implementing, testing and operating cloud native in an Agile and DevOps setup

3+ years of industry experience in Data Engineering with experience manipulating, and extracting data from large datasets.

3+ years of Data Warehousing experience preferably with Oracle or related technology displaying strength in SQL, python/pyspark scripting, data modeling and ETL development.

2+ years working in a role as Lead / Principal Engineer or Architect

Broad knowledge of different types of data storage engines - (non)relational, row/column oriented dbs. e.g. Oracle, Postgres, MySQL, QB/redshift, Elastic, Hive, HBase

Advanced query language (SQL) knowledge

Experience with stream processing engines (Kafka, Kinesis)

Experience in using a Java framework in development preferable using Dropwizard

Experience deep working experience with one or more major cloud vendors (OCI, Azure, AWS, GCP)

Strong analytical skills, 2+ years’ experience with Python and an interest in Machine Learning

Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy

Oracle is the world’s leading provider of business software. But you probably already knew that. With a presence in over 175 countries, we are one of the biggest technology companies on the planet. What you might not know is that we are leading a cloud revolution. We’re using emerging technologies like AI, machine learning, and blockchain to solve critical real-world problems. From advancing energy efficiency to reimagining online commerce, the work we do is not only transforming the world of business—it’s helping governments, powering nonprofits, and giving billions of people the tools they need to outpace change and make a difference. Mission: To help people see data in new ways, discover insights, unlock endless possibilities.
Future Creators We are creators, free-thinkers, and basically a bunch of problem-solvers who are eternally asking ‘what if"
29-Apr-2022 T11:52,"Data Engineer, Capability Development (DART)",GVT Government Technology Agency (GovTech),2 days ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Data Engineer in GovTech’s Data Science & Artificial Intelligence Division, you will be involved in architecting, managing and running advanced analytics and data platforms. For this role, you will be part of the Data Analytics Readiness Team (DART) within GovTech, where you will help drive GovTech’s Data Transformation efforts. You will assist to establish and implement effective operational workflows, data pipelines, procedures and best practices in accordance with government infrastructure and security policies to ensure that the platforms are monitored, secure, available and reliable.

We are looking for an independent and motivated engineer who has experience in both small and large-scale analytics platform projects, and in building and maintaining the infrastructure.

What you will be working on:
• Design, Architect, Deploy, and maintain solutions on Microsoft Azure or AWS using different Cloud & Big Data Technologies to provide secure and governed access to data for business users.
• Manage the full life-cycle of a data warehouse/lakehouse solutions from requirement gathering and analysis to platform selection, design of the architecture, and deployment.
• Collaborate with data stewards, data analysts and data scientists to build data pipelines from enterprise systems such as Workday to collect, clean, harmonise, merge and consolidate data sources for data warehouse/lakehouse.

Where we are looking for:
• Diploma/Degree in Computer Science or Information Technology or related disciplines
• Experience with the cloud (e.g. AWS, GCP, Azure)
• Hands-on experience in implementing Data Lake/Data Warehouse with technologies like – Databricks, Azure Synapse Analytics, SQL Database, AWS Lake formation.
• Programming & debugging skills in Python, Scala or R.
• Proficient in SQL.
• Knowledge of both SQL and NoSQL databases.
• Comfortable with DevOps tools like AWS Cloud Formation/Terraform, Docker and Git for CI/CD development.
• Prior experience with data engineering tools and frameworks like Airflow, Kafka, Hadoop, Spark, Kubernetes.
• Familiar in building REST services is good to have.

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:52,Senior Data Engineer,Denodo,3 days ago,,Full–time,"Denodo is looking for a technical, passionate, hands-on data advocate, customer-oriented, and willing to take Denodo learning and adoption to the next level to join our Global Office team, as part of the Customer Success organization.

Your mission: ensure Denodo customers, partners, and any user of the Denodo Platform have available all the material they need to be successful in their Data Management solutions.
In this role you will combine high technical expertise, data ecosystem knowledge and customer-facing skills to interact with data, customers (internal and external), and other stakeholders to get inspired with new ideas, and put them into practice to facilitate Denodo adoption.

Duties and responsibilities:
• Obtain a strong knowledge of the Denodo Platform, be able to deliver a superb technical pitch, including overview of our key and advanced features and benefits, differentiation, and competitive positioning.
• Constantly learn new things and maintain an overview of modern technologies.
• Ensure Denodo users are well-equipped with the resources and training needed to effectively perform with the Denodo Platform in an environment of fast-paced change
• Follow up on the release calendar of updates and versions to identify opportunities to create new material.
• Interact with internal and external stakeholders to provide updates on the roadmap and get feedback.
• Interact with the local managers to coordinate the development and maintenance of technical resources.
• Develop white papers, presentations, training materials or documentation on related topics.
• Define metrics, analyze issues, data and trends to make data-driven decisions that let improve the quality of the technical resources and new ways of working.
• Ensure that upper level management is aware of issues regarding processes, protocols or educational materials

Qualifications

Desired skills and experience
• Self-motivated, proactive and solution oriented. A passion for improving Denodo user experience.
• Excellent organizational skills being able to manage change effectively.
• Excellent verbal and written communication skills to be able to interact with technical and business counterparts
• Strong analytical and problem solving abilities.
• Active listener.
• Lots of curiosity. You never stop learning new things.
• Creativity. We love to be surprised with innovative solutions.
• Be a team worker with positive attitude
• Willingness to occasionally travel.
• BS or higher degree in Computer Science or Information Systems.
• 3+ years of demonstrated experience in a similar role.
• Experience with the data landscape.
• Fluent in English ( min. C1 Level) (only for non English speaking countries)

Employment Practices

Denodo is an equal opportunity employer and prohibits discrimination and harassment of any kind. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by applicable law. Denodo will provide reasonable accommodation to employees who have protected disabilities in accordance with applicable law.

We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee"
29-Apr-2022 T11:52,(Senior) Data Engineer (Information Management Systems),Centre for Strategic Infocomm Technologies,Full–time,,,"As a data engineer in the Information Management Systems team, you will develop critical pipelines to convert and serve large volumes of varied unstructured data into usable datasets for other services and end-users. Together with your driven and multi-talented product team, you have the autonomy to determine the best way to deliver value and the technologies used.

As CSIT is an agency under the Ministry of Defence (Singapore), only Singapore Citizens will be considered."
29-Apr-2022 T11:52,Principal Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,9 hours ago,,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 10-12 years of experience in Data Engineering role and have good knowledge / working experience in:
• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.
• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.
• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.
• Functional programming languages, e.g. Scala.
• Virtualization and container environment such as Docker and Kubernetes.
• Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:52,Data Engineer - Search,Shopee,Full–time,,,"Design, build and maintain the ingestion system to support various types of data (e.g. User behavior, RDS, NoSQL DB and others) to be ingested to the data warehouse more timely and accuratelyTranslate data requirements into scalable technical data service with low latency and high concurrencyDesign, build and maintain the batch or real-time data pipeline in production using Hadoop big data technologyAnalyse and improve efficiency, scalability, and stability of the systemDefine and manage service level agreement and data quality for all data sets in allocated areas of ownership"
29-Apr-2022 T11:52,Data Engineer,ITCONNECTUS PTE. LTD.,4 days ago,$6K–$8K a month,Full–time,"Key responsibilities:
• Single point of contact and liaison of all company teams
• Provide environment/integration support for SCP platforms and tools
• Takeover from Transformation Partner and DigiTech for Operate Support
• Liaise with Business/System Owner, DigiTech and Transformation Partner on requests and issues
• Support and follow-up with Project team on requests and issues

Requirements:
• Must have 5 Years of good experience
• Degree or diploma in IT/Computer Science/Information Systems or equivalent
• 3+ years of experience relevant data engineering preferred.
• Experience with data pipeline and ETL development preferred
• Proficient in Pyspark, Python and SQL
• Experience with Java, C#, JavaScript/TypeScript, open-source, Docker and Terraform technologies.
• Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing and operations
• Fast learner and a team player
• Knowledge in AWS/Azure/GCP"
29-Apr-2022 T11:52,"Data Engineer, FinTech",foodpanda,4 days ago,"$5,923–$11,769 a month",Full–time,"We are looking for a Data Engineer, to join our growing team that who will work closely with our fintech squads based in Singapore. You will be part of an international team of highly talented and motivated people.

This role reports directly to our Director of Data & Product Analytics.

What's On Your Plate
• Build and execute SQL queries & database schema as required by the business
• Work with software engineers and architects to design the target data architecture as per business requirements
• Architect, build and deploy data models with adequate documentation and validation
• Implement data processing and pipelining queries that will enable data-informed decision-making within the business
• Enable data consumers such as analytics, data products, machine learning etc. by actively seeking opportunities to automate data pipelines
• Continuously experiment state of the art technologies and proactively seek opportunities to improve the data ecosystem
• Actively document the codes models and data dictionaries that are being developed.
What You Bring To The Table
• 2 - 4 years of relevant experience in data analytics / engineering, preferably on a cloud setup
• Strong knowledge on SQL, data structures and database schemas
• Experience in data pipelining tools such as Airflow
• Good working knowledge of Python / Java or any other programming language
• Knowledgeable in big data, Data Warehouse technologies
• Knowledgeable in agile tool sets such as Jira, Confluence etc.
• Preferred qualifications:
• Experience in Google Cloud Platform
• Working knowledge on NoSQL datastores such as MongoDB, DynamoDB etc.

What We Can Offer You
• A vibrant and international team with multicultural and diverse backgrounds.
• Solving challenges with inspiring colleagues in an all hands-on deck environment.
• Management team that recognizes top performers, welcomes our newbies, and shares a love for good food.
• Competitive package, incentives, allowances, food perks, insurance, pension and more"
29-Apr-2022 T11:52,Data Engineer,TOTAL EBIZ SOLUTIONS,Full–time,,,"• Ability to build both batch and streaming data pipeline in a big data environment from the upstream systems in collaboration with technology and other data teams.
• Develop an in-depth understanding of the underlying data, data model, and business domain by investigating, reviewing, and analyzing data from structured & unstructured sources. Leverage this knowledge to design and build common/ master data models.
• Independently, collaborate with cross-functional stakeholders – other department teams and other technology teams to understand their data needs, formulate and complete end-to-end analysis that includes business understanding, data gathering, data discovery, building and designing master/ common data models using cloud-based data pipelines.
• Ability to maintain and optimize the data pipelines. Monitor the health of production pipelines frequently to ensure accuracy and stability.
• Diagnose, analyze and provide solutions to issues in scripts, reports, tools, data, etc.
• Strictly adhere to the controls and standards for meeting the enterprise technology data quality requirements, by using various in-house data quality solutions.
• Identify, design, and implement improvements, e.g. automating manual processes, optimizing data delivery, re-designing architecture for greater scalability, etc.
• Train/ mentor junior data engineers and the Lazada business associates and enable Lazada personnel on building and understanding various data querying and data-discovery skills.

Job Requirements
• Strong education background in Computer Science/Information Systems (preferred).
• Must have Big Data/SQL skills.
• Strong understanding of database & data warehouse design/administration, hands-on experience with MySQL, Hadoop, Hive SQL, Flink / Spark technology, and Python. Knowledge of API programming and NoSQL databases will be an added advantage
• More than 5 years of experience in end-to-end data warehousing, data modeling, and as well as distributed processing systems. Previous experience of working in cloud-based environments, especially Alicloud would be an added advantage.
• Good understanding of data quality frameworks and data governance.
• Previous experience in implementing them at an enterprise level would be an added advantage.
• Experience in enablement and other enterprise-wide training activities.
• Strong analytical/problem-solving skill balance with good written and oral communication skills.
• A good understanding of accounting concepts, finance processes, and financial performance metrics is a plus.
• Tenacity to develop ideas independently and thrive in a fast-paced start-up environment.
• A self-motivated, driven, flexible, quick learning, high achieving, “can do” mentality"
29-Apr-2022 T11:52,Data Engineer (SQL/Python),Scientec Consulting Pte. Ltd.,4 days ago,,Full–time,"Responsibilities
• Responsible for databases, data warehouses and large-scale data processing systems
• Develop, construct, test and maintain analytical data warehouse and deploy and continuously improve ETL / ELT tasks
• Design, develop, review and optimize the core database store procedures in batch job scripts and java scripts.
• PL / SQL and SQL Tuning and optimization

Requirements
• Degree in IT or equivalent
• Experience data architecture, data warehousing, data processing, data modelling and ETL / ELT
• Experience in database development (Oracle SQL / PLSQL)
• Expert in using Python

To apply, please send your updated resume to HIDDEN TEXT

By submitting any application or resume to us, you will be deemed to have agreed & consented to us collecting, using, retaining & disclosing your personal information to prospective employers for their consideration.

If you wish to withdraw your consent or correct any of your personal data, please drop us an email at HIDDEN TEXT to let us know.

Note : Any resumes of job applications sent to this mailbox will not be attended as it is solely for the purpose of personal data protection related matters.)

We will contact you if your skills and experience are suitable for the role, or if there is a similar opportunity that is available presently or in the future.

Resquid Quitaleg Airene (R1656137)

ScienTec Consulting Pte Ltd - 11C5781"
29-Apr-2022 T11:52,Data Engineer - Quantitative Trading Firm/Market Maker,Aptitude Asia,3 days ago,,Full–time,"Our client, a leading high frequency trading firm, currently looking for a Data Engineer to join the team in Singapore. The team is to provide high-quality research and reference data for the firm to utilize, accessing financial data and analytical services which support the trading, research, and operational functions.

Responsibilities
• Responsible for building, maintaining and improving the acquisition and monitoring data platform
• Responsible for delivery and the construction of derived sets of data, analysis and cleaning
• Develop and implement tools to provide autonomy around the platforms data life-cycle
• Maintain an inventory of the data estate
• Implement the control workflow for representing data events
• Work with the investment team, quant research, operations to ensure their data-related requirements are captured and delivered
• Liaise with external vendors confidently and assist in vendor audit and compliance reporting

Requirements
• Master of Mathematics, Computer Science, Physics or Engineering and related field; strong candidates with Bachelor of Science degrees will also be considered.
• At least 3 years exepereinces for data analyst role or relevant data experience in either a financial institution or data vendor
• Strong background in financial markets and associated data sets i.e. market data, reference data
• Must have a working understanding of data engineering practices
• Strong numerical and analytical skills and problem-solving ability
• Skilled in the use and understanding of modern database technologies including relational, document, and temporal column data stores.
• Skilled in the use of a programming language such as Python for data engineering

To apply, please send your updated CV to donna@aptitudeasia.com. Thanks"
29-Apr-2022 T11:52,Data Engineer,LILITH GAMES SG PTE. LTD.,1 day ago,,Full–time,"Big Data Development Engineer (Data Center)

What You Will Be Doing
• Plan, design, develop and implement core data system in the business field
• Develop and maintain real-time/ offline data processing tasks
• Build industry-leading distributed systems such as storage and computing to provide a reliable infrastructure for massive data and large-scale business systems
• Develop various data services, such as data developing platform, quality monitoring system,

Qualifications & Skills
• BSc in Computer Science or related major.
• At least 3 years of big data platform R&D.
• Familiar with common algorithms and data structures, and be proficient in Java / Python / Scala / shell language (at least two)
• Familiar with deployment and implementation of docker and k8s related containers.
• Experienced working with big data technologies such as Hadoop, Flink, Hive, Spark, etc.
• Enthusiastic about learning new technologies and persistent in pursuit of high concurrency and distributed architecture design.
• Have good team communication and collaboration skills.

大数据开发工程师（数据中台方向）

工作职责：
• 参与业务领域核心数据体系的规划设计以及开发落地
• 参与实时/离线数据加工任务的开发与维护
• 参与大数据基础架构的开发与迭代
• 参与各类数据服务开发，包含但不限于：数据平台开发，数据质量监控，BI等

岗位要求：
• 本科及以上学历，3年以上大数据平台研发经验。
• 具有扎实的编程功底，熟悉常用的算法和数据结构，精通Java/Python/Scala/Shell语言（至少两种）
• 熟悉Hadoop等大数据框架，有Flink、Spark等相关实时流计算框架经验优先
• 熟悉docker、k8s相关容器的部署实施
• 对新技术保持好奇心，对高并发和分布式架构有执着的追求"
29-Apr-2022 T11:52,Data Engineer - Data Warehouse,Shopee,Full–time,,,"The Engineering and Technology team is at the core of the Shopee platform development. The team is made up of a group of passionate engineers from all over the world, striving to build the best systems with the most suitable technologies. Our engineers do not merely solve problems at hand; We build foundations for a long-lasting future. We don't limit ourselves on what we can or can't do; we take matters into our own hands even if it means drilling down to the bottom layer of the computing platform. Shopee's hyper-growing business scale has transformed most ""innocent"" problems into huge technical challenges, and there is no better place to experience it first-hand if you love technologies as much as we do. Browse our Engineering and Technology team openings to see how you can make an impact with us.

Job Description:
• Design and manage Data warehouse plan for a product, such as: design data model, define data metric, data governance and so on
• Collaborate with Business Intelligence Business and Product Management teams
• Use data to solve problems, identify needs and opportunities
• Design, build and maintain the batch or real time data pipeline in production using Hadoop big data technology
• Define and manage service level agreement, data quality for all data sets in allocated areas of ownership

Requirements:
• Bachelor's Degree in Computer Science or a related technical field.
• 2~10+ years working experience with programming languages,such as Java,Scala,Python
• Ability to write efficient SQL statements
• Familiar with Hadoop, Spark and Flink data processing, experience of TB data processing experience is a plus
• Familiar with data warehouse modeling, such as dimensional data modeling and schema design
• Understand data mining or machine learning
• Excellent communication skills including the ability to identify and communicate data driven insights
• Strong logical thinking and analysis abilities
• Passionate, self-motivated, and takes ownership"
29-Apr-2022 T11:52,Head of Data Engineering,Apersona Pte. Ltd.,5 days ago,,Full–time,"What you will do
• Place client value and human experience at the centre of everything we do
• Use data engineering to support material impact and drive disruptive transformation across public and private sectors
• Maintain public trust ensuring fairness, ethics, accountability, and transparency
• Build a world-class team with expert capabilities in data engineering
• Create a culture of excellence and lead with confidence, charisma, context, and humility working effectively at all levels
• Lead delivery of data engineering solutions through incubation, proofs-of-concept, to commercialisation and deployment
• Support development of go-to-market plans for both AI & data, understand strategic opportunities, develop trusted partnerships, and deliver social progress
• Educate, enable, and coach teams on data engineering in Temus, clients and in the broader community
• Adopt a cloud first strategy to enhance agility and elasticity partnering with vendors to support specific public sector needs

Suitable candidates
• Bachelor’s Degree in Statistics/Computer Science/Data Analytics or related quantitative field
• Minimum 10 years of relevant experience in consulting/ data science related functions with with at least 3 years of leadership / people management experience
• Data Development: Deep expertise in developing and managing software for data processing: Python, Java, SQL, KSQL, Scala, Spark, Flink, Beam, AWS Glue, Google Dataflow, etc.
• Data Platforms: Deep expertise in building and managing data platforms: HBase, MongoDB, Cassandra, Redis, PostgreSQL, Oracle, MySQL, Kafka, Kinesis, DynamoDB, Redshift, Cloud SQL, Cloud Spanner, Cloud Bigtable, Firestore, BigQuery, Azure SQL, Cosmos DB, Stream Analytics, Synapse Analytics, DeltaLake, Elasticsearch, Snowflake etc.
• Data Management & Governance: Strong skills in data management: accuracy, integrity, latency, classification, security, lineage, metadata, etc.
• Communication: Strong ability to communicate with a broad range of clients, colleagues, and partners across a variety of contexts and formats.
• Partnership: Strong ability to develop and maintain relationships amongst clients, colleagues, and partners
• Leadership: Demonstrated capability to lead, inspire, coach and mentor team members and colleagues.

Job Types: Full-time, Permanent

Benefits:
• Health insurance
• Parental leave
• Professional development

Schedule:
• Monday to Friday

Supplemental Pay:
• Performance bonus"
29-Apr-2022 T11:52,Market Data Engineer - 12 Months Contract,Adecco Personnel Pte. Ltd.,8 days ago,,Full–time,"This role will not only act as the first line of support for Market Data infrastructure across Asia Pacific regions including applications, servers and connectivity, but also be heavily involved in testing and implementation of real-time market data infrastructure and in-house tools. It requires a wide range of skills and expertise which are critical in the delivery of reliable, timely and cost effective market data products, solutions and services that enable our business lines and their electronic trading systems to make highly informed and competitive decisions.

Key Responsibilities
• Support and maintain market data infrastructures/platforms and expeditiously apply troubleshooting/problem-solving skills to minimize business productivity loss, customer impact, and financial/reputational risk.
• Coordinate with and escalate to market data vendors and exchanges in the region for their data content/technical issues and changes.
• Manage vendor and exchange driven changes, technical activities and relationships.
• Plan and implement production changes after trading hours, and conduct testing to verify the changes are successful.
• Act as the SME of market data distributed by regional exchanges and major vendors, perform in-depth analysis over data content and behavior to effectively support low latency electronic trading applications.
• Design, implement and constantly improve monitoring/capacity/performance/data analysis tools to ensure comprehensive coverage over data accuracy and reliability & infrastructure availability and stability.

Key Requirements
• Overview of financial and banking industry and strong data knowledge for Asia Equity markets
• Strong scripting skills for software/system management and automation (Perl, Python, bash scripting)
• Understanding and experience with market data delivery platforms like TREP/BBG/Direct exchange feeds
• Administration and support experience with server Operating Systems like Red Hat Linux and Windows environment
• IT Service Management knowledge and mindset including Change Management, Incident Management, Problem Management.

Strong analytical skills and logical thinking"
29-Apr-2022 T11:52,Sr Data Engineer Data Engineer,Carousell Group,3 days ago,,Full–time,"We are now looking for Data Engineers/ Sr. Data Engineers to join our Data team.You will: Build, maintain and scale efficient data infrastructure, ETL and reporting pipelinesWork with data scientists and machine learning engineers in getting ML and deep learning models to production readyInvestigate and research data quality and integrity from data sourcesDevelop and maintain analytics platform, business intelligence and experimentation toolsDevelop and maintain scalable platform for tracking business intelligence, build for reliability and redundancyManage data collection and organize the models, and also forecast the future needsCoach and mentor junior data engineers to be more effective individual contributors You have: User obsession and empathy.Drive and resourcefulness to persevere and overcome obstacles achieving challenging goals.Focus on impact and results. You work on the right things and get them done.High integrity and ability to positively collaborate with othersFluent in Python or similar programming languageExperience in data engineering tools like Hadoop, Spark, BigQuery, Airflow, etc.Experience in deploying and scaling ML models. Plus point for deploying and scaling deep learning models.Data-driven and passionate about solving problems through dataInquisitive and curious to delve deep into data to investigate trends or anomaliesDetail oriented and be able to work efficiently in a fast-paced team environmentKeen on data technologies and picking up new skills and tools along the wayStrong critical thinking and ability to frame issues in a logical manner P.S. The job title will be given based on working experience and interview performance"
29-Apr-2022 T11:52,Senior Data Engineer – Cyber Security R&D,NCS Group,4 days ago,$7.5K–$15K a month,Full–time,"• Design and build efficient, scalable and resilient ETL pipelines for both batch and real-time streaming data
• Architect and design data infrastructure on cloud using industry standard Infrastructure-as-Code tools
• Administer and maintain big data infrastructure, including performance tuning and troubleshooting
• Collaborate with data scientists and software engineers to build data-driven security platforms and services
• Assemble and process large and structured/unstructured datasets that meet cyber security business requirements.
• Create impactful demonstrations to showcase cyber analytics capabilities
• Design and build services with a focus on business value and usability
• Contribute to cross-functional technical discussion

Requirements:
• Degree in Computer Science, Computer Engineering, Information Technology, or equivalent
• At least 5-7 years of experience in data modelling and designing ETL pipelines
• Proficient in cloud technologies and services provided by AWS Azure, and GCP
• Familiar with DevOps tools, such as Git, Docker, Terraform
• Proficient in machine learning and data visualisation tools
• Proficient in SQL and other scripting languages, such as Phython and Bash
• In-depth knowledge of both SQL and NoSQL databases, including performance tuning and troubleshooting
• Strong sense of responsibility and accountability
• Excellent communication skills, both written and verbal
• Ability to work in a fast-paced, culturally diverse environment"
29-Apr-2022 T11:53,Data Engineer,ITCONNECTUS PTE. LTD.,16 hours ago,$5K–$13K a month,Full–time,"Immediate joiners

Requirements
• Technical Data Architect who is strong on Data Warehousing, Big Data, Data Analytics & Data Science for solutioning
• Strong Google Cloud Platform Data Components – BigQuery, BigTable, CloudSQL, Dataproc, Data Flow, Data Fusion, Etc
• Demonstrate extensive skills and success in the implementation of technology projects within a professional environment, with a particular focus on data engineering
• Experienced in delivering Medium to Big Enterprise Data Projects
• GCP Experience including and not limited to MPP systems, Database systems, ETL and ELT systems and Data Flow compute Good to have skills
• A high-level understanding of AI and ML technologies and desire to develop your skills in these areas of emerging technology.
• Should be able to advise the best of breed for the client solutions; Skills Needed
• The Data Engineer coaches the junior data engineering personnel position by bringing them up to speed and help them get better understanding of overall Data ecosystem.
• Prior experience developing, building and deploying on GCP
• Working on Solution deck, IP build, client meetings on requirement gathering"
29-Apr-2022 T11:53,Data Engineer,Manpower Singapore,2 days ago,,Full–time,"Data Engineer

Key Role and Responsibilities:
• Partner with the global and in-market data experts to discover and derive value from connecting external and internal data sources.
• Build (ETL) new and evolve data models and pipelines to power algorithmic based Business Intelligent solutions that addresses business problems requiring descriptive, diagnostic, predictive, and/or prescriptive analytics for pricing, promotion, trade spending, assortment, and sales performance management.
• Translates the algorithms and analytic models into data models as business needs evolves on a going basis after they are put into production.
• Develop a roadmap that scales existing and new data models, to support the portfolio of solutions.
• Expand into Business Intelligence solution development focused on automation and scale up of solutions.

Key Skills Required:

Professional Skills:
R/Python programming languages
MS Power BI
MS Excel
Data Visualisation

General Skills:
Client Management
Project Management
• Diploma or B.S. in a relevant technical field (Operations Research, Computer Science, Statistics, Business Analytics, Econometrics, or Mathematics).
• Overall experience of 2-3 years
• Ability to manipulate and high-volume of data from varying sources
• Expert knowledge of an analysis tool such as Microsoft PowerBI
• Proficiency in R/Python
• Basic track record in working independently with minimal guidance

Interested applicants, please submit your resume to : abelene.kang@manpower.com.sg
Abelene Marianne Kang Mrs Abelene Marianne Rozario R2089914
EA License No: 02C3423 Reg No: 199505951H

Kang Abelene Marianne Mrs Rozario Abelene Marianne

EA License No.: 02C3423 | Personnel Reg No.: R2089914

Please note that your response to this advertisement and communications with us pursuant to this advertisement will constitute informed consent to the collection, use and/or disclosure of personal data by ManpowerGroup Singapore for the purpose of carrying out its business, in compliance with the relevant provisions of the Personal Data Protection Act 2012. To learn more about ManpowerGroup's Global Privacy Policy, please visit https://www.manpower.com.sg/privacy-policy"
29-Apr-2022 T11:53,Data Engineer - User Profile,Shopee,Full–time,,,"Collaborate closely with the product team and provide data solutions to business problems and support business growth and data driven decision makingsDesign, build and maintain batch and realtime streaming user data pipelines using big data platform and technologiesResponsible for user data collection, processing, storage and building the user profile data collectionsAdopt and maintain data engine solutions to support online data services for various business use casesImprove efficiency, scalability, and stability of existing systemsEnsure data quality, consistency and timeliness"
29-Apr-2022 T11:53,"Principal Data Engineer, Data Engineering",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Principal Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you are responsible for helping to shape the direction of the data engineering domain in GovTech; working with other domain experts, the delivery teams, vendors, and partners to influence technology roadmaps and the adoption of suitable data solutions across government. In addition, you will teach, mentor, and proactively grow fellow data engineers within the organization.

You will be leading multiple teams of data engineers to drive impact at scale, by building whole-of-government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will architect, design and build next-generation central data infrastructure to galvanise digitalisation in the public sector. Taking a more integrated approach to lay the foundation infrastructure and driving pervasive adoption of advanced data technologies throughout the economy and society, you will architect solutions that enable public agencies and sectoral partners to better integrate, exploit and understand their data to improve operations and decision-making, and deliver high-quality digital services to citizens and businesses in Singapore.

Join us on this journey to make a difference for the nation and our future generations, if you are:
• Experienced leading data pipeline building and data wrangling and enjoys optimizing data systems and building them from the ground up as part of multiple cross-functional teams,
• Have strong project management and organizational skills, self-directed and comfortable supporting the data needs across multiple teams, systems and products,
• Excited by the prospect of optimizing or even re-designing central and agencies’ data architecture to support next generation of government products and data initiatives, and
• Recognized expert with proven technical leadership, experienced working in a agile team to build systems from architecture to deployment for cloud and on-prem infrastructure, and
• Have the desire to serve the public good through the use of technology.

What you will be working on:
• Take the driving seat and lead our data engineers in complex technical projects to:
• Expand and optimize our data and data pipeline architecture, as well as optimize data flow systems for cross functional teams in a secured and scalable manner.
• Assemble large, complex data sets that meet functional / non-functional business requirements, transforming data into formats that are easy to consume and analyse.
• Identify, design, and implement internal process improvements, e.g., automating manual processes, optimizing data delivery, re-designing infra for greater scalability.
• Build the infra required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and industry standard Infra-as-Code tools.
• Build analytics tools that utilize the data pipeline to provide actionable insights into key performance metrics and support our next gen of products and data initiatives.
• Work with stakeholders including the Executive, Product, Data and Design teams to support their data and data pipelining use cases, and ensure optimal and secured data delivery architecture is consistent across projects, products and platforms.
• Drive innovative initiatives that uplift data capability across the government sector:
• Develop a strategic sense of key organisations, software and hardware tools, methodologies, and best practices in the data engineering landscape.
• Be the bridge between engineering and product line management and translate market requirements into product definitions and architecture designs.
• Analyse and solve complex data engineering problems, and translate architecture designs into implementations that satisfy the market requirements.
• Educate the team on new technological advances and work alongside them in proof-of-concept research projects and subsequent agile product delivery.
• Provide expert perspective to help shape data-related strategy and initiatives, and be our technical ambassador by writing internal blogs or publishing white papers.

What we are looking for:
• Candidate with 8+ years of experience in a Lead Data Engineer role, who has attained a Bachelor’s degree in Computer Science or other relevant engineering degree.
• In-depth working knowledge across a variety of relational SQL and NoSQL databases, including efficient query authoring, performance optimisation and troubleshooting.
• Experience building and optimizing data, data pipelines, and architectures, with processes to support transformation, data structures, metadata, dependency and workload management.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for further improvement.
• Working knowledge of message queuing, stream processing, and scalable data stores.
• A successful history of processing and extracting value from large datasets from different sources and systems, with strong analytics skills related to working with unstructured data.
• You should also have experience using the following software/tools:
• Big data and stream-processing tools: Hadoop, Spark, Beam, Flink, Kafka, etc.
• Data pipelining and workflow management tools: Luigi, Airflow, etc
• Cloud computing environments and services: AWS, GCP, Azure
• DevSecOps tools: Git, Kubernetes, Docker, Terraform etc
• Programming languages: Python, Java, C++, Scala, etc

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours”"
29-Apr-2022 T11:53,"Data Engineer, Data Engineering (6 months Contract)",Bank of Singapore,10 days ago,$8K–$12K a month,Contractor,"Bank of Singapore opens doors to new opportunities.

Start your career with Bank of Singapore as a Data Engineer (6 months contract) in our Data Engineering team!

Bank of Singapore opens doors to new opportunities. At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programs that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group’s global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today!

Bank of Singapore has defined an ambitious roadmap for accelerating its data excellence through the implementation of various bank-wide programs. As part of this roadmap, building the foundation of good data governance and management plays a key role in establishing trust in the data that we use.

General Description

The incumbent will be based in the Data engineering team within the Data Hub team, supporting Bank of Singapore’s data sourcing, curation, transformation, and data analytic services to accelerate their data initiatives. He/she will be responsible for expanding and optimizing BOS data pipelines as well as supporting data analytics needs.

Core Activities
• Be responsible for supporting data analytics and dashboarding with respects to the data availability / stability.
• Understanding the data and identifying any potential data issues as well as correcting issues arising from data.
• Define and implement data models suitable for dashboarding
• Power BI dashboarding support
Requirement

General Knowledge & Experience
• Minimum of 6 years of working experience in data related functions
• Understanding of banking / financial services with exposure to data analytics preferred
Programming & Data
• Strong hands-on skills in SQL & PL/SQL
• Solid background in traditional structured database environments such as Teradata / Oracle
• Knowledge on Power BI dashboard functionality
• Strong skills in Power BI related technologies like power query and DAX
• Exposure to programming languages such as python
Communication & Soft Skills
• Curiosity & a real passion for understanding “why?”
• Pragmatic “can do” approach to finding data based solutions to problems
• Good communication skills with ability to engage across business and technical audiences"
29-Apr-2022 T11:53,Data Engineer,ASM TECHNOLOGY SINGAPORE PTE LTD,2 days ago,$4K–$5.5K a month,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform. We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets. You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).

Your Responsibilities
• Manage and support on-premise and Cloud-based data lake and warehouse systems
• Design, build, support and optimize new and existing data structure and ETL processes
• Build scalable and efficient data pipelines & services to help analytics teams to process the data
• Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results
• Liaise with third party tool providers to understand and improve data workflow
• Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms
• Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification
• Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines
• At least 2 years’ experience in data engineering, automation and integration is preferred
• Strong programming and scripting skills in Python and other modern programming languages
• Strong data management, schema design and SQL development skills
• Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)
• Self-motivated and proactive, willing to learn new things
• Good communication skills and strong team player

What our preferred candidates have?
• Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions
• Understand and experienced with Cloud platform, eg. Microsoft Azure, AWS, GCP
• Business intelligence and reporting tools, eg. Power BI, Tableau, Qlik, etc
• Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)
• REST/Web API development and management
• Knowledge in Statistical software is an advantage
• Experience In building machine learning models is a plus"
29-Apr-2022 T11:53,Data Engineer,Thakral One Pte. Ltd.,3 days ago,,Full–time,"Job responsibilities: Work with Banking team to understand existing SAS code logic written by techno-functional users Develop the SAS logic in Big Data environment, using spark, scala and pyspark utilizing client Hadoop ecosystem of the inhouse Data Lake Perform unit testing and system integration testing Work closely with Business Analysts team to review the test results and obtain sign off Deploy the new code in the client Production environment Prepare necessary design/operations documentation for future usage Perform peers Code quality review and be gatekeeper for quality checks 5-8 years of application development experience in Spark, Spark SQL, Scala is a must Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc Technical proficiency on data mining techniques and performance optimization Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL) Experience on SAS will be beneficial but not mandatory Handling of reporting packages (Tableau, QlikView) is nice to have AWS experience is nice to have Degree in Computer Science or Engineering is a must Good problem diagnosis and creative problem-solving skills Passion to learn and master diverse new technologies in the open-source community Accuracy and attention to detail Team-working, Verbal and Written communication skills"
29-Apr-2022 T11:53,"Data Engineer, MBG",Meta,1 day ago,,Full–time,"Meta is looking for exceptionally talented and experienced engineers to join the MBG Technology team. Our team provides analytics and workflow tools for Meta Business Group ( MBG), partnering with sales, marketing, measurement, support and operations teams.In this role, you’ll work with some of the brightest minds in the industry, work with one of the richest data sets in the world, use cutting edge technology, and get an opportunity to solve some of the most challenging business and engineering problems, at a scale that few companies can match. You will do so by partnering with stakeholders/teams and building scalable, reliable solutions that provide business critical insights and metrics, while ensuring the best uptime and responsiveness.

Data Engineer, MBG Responsibilities:
• Manage data warehouse plans for a business vertical or a group of business verticals
• Build data expertise and own data quality for allocated areas of ownership
• Design, build, optimize, launch and support new and existing data models and analytical solutions
• Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions
• Conduct design and code reviews
• Work with data infrastructure to triage infra issues and drive to resolution
• Manage the delivery of high impact dashboards, tools and data visualizations

Minimum Qualifications:
• 2+ years of experience in the data warehouse space, custom ETL design, implementation and maintenance
• 2+ years of experience in SQL or similar languages, and development experience in at least one language (Python, PHP etc.)
• Experience with data architecture, data modeling, schema design and software development
• Experience in leading data driven projects from definition through interpretation and execution
• Experience with large data sets, Hadoop, and data visualization tools
• Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience working in support of diverse communities"
29-Apr-2022 T11:53,Data Engineer,HCL Technologies,2 days ago,"$7,031–$12,608 a month",Full–time,"We are looking for an experienced Data Engineer to join our growing team of analytics. The candidate will be responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Candidate must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

Responsibilities
· Create and maintain optimal data pipeline.
· Assemble large, complex data sets that meet functional / non-functional business requirements.
· Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing Jobs/code for greater scalability, etc.
· Work with stakeholders including the Product owner, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
· Work with data and analytics experts to strive for greater functionality in our data systems.

Requirements
· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.
·Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.
· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
·Strong analytic skills related to working with structured and unstructured datasets.
· Build processes supporting data transformation, data structures, metadata, dependency and workload management.
·A successful history of manipulating, processing and extracting value from large datasets.
·Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
· Experience supporting and working with cross-functional teams in a dynamic environment.
· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:

· Experience with:
o Big data tools: Hadoop, Spark, Kafka, etc.
o Relational SQL and NoSQL databases, including Postgres and Cassandra.
o Data pipeline and workflow management tools: Airflow, etc.
o AWS cloud services or GCP.
o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:53,Head of Data Engineering,TEKsystems (Allegis Group Singapore Pte Ltd),3 days ago,,Full–time,"Head of Data Engineering

We are looking for a high-caliber data expert and leader working within a team that provides world-class and innovative solutions to support the ever-growing demands in the industry.

This is a permanent opportunity.

What's in it for you:
Exposure to understanding and dealing with the data engineering & analytics requirements from various business units, hands-on analysis of the various data systems, and an opportunity to directly influence and impact the company's data infrastructure to enable data democratization.

The Position:

• 10+ years of experience in managing large-scale data initiatives within the tech or start-up space
• Experience in leading teams that design and build highly scalable data pipelines/data infrastructure in fast paced environments
• Experience in managing teams comprised of other Data Engineers, Data Scientists, Data Analysts and Software Engineers
• Strong understanding of building data models for the target data warehouse
• Clear understanding of distributed computing, especially in databases
• Hands-on in SQL with a deep understanding of query optimization
• It would be great to have experience working on any of the Cloud platforms (GCP, AWS, Azure)
• Exposure to Machine Learning / Artificial intelligence is a plus
• Strong communications skills We regret to inform that only shortlisted candidates will be notified / contacted.

For more information you can email Nicole Sichon in our Singapore office on Nicole.Sichon@teksystems.com quoting Job Reference NicoleSichon531489 or alternatively, apply here to register your interest.

http://jobs.en-sg.teksystems.com/N5oQPt/head-of-data-engineering-itcommunications-unknown-singapore-15264683

Job Reference: Nicole Sichon 531489

EA Registration No.: R1873628, Sichon Andreana Nicole Ong

Allegis Group Singapore Pte Ltd, Company Reg No. 200909448N, EA License No. 10C4544"
29-Apr-2022 T11:53,Senior Data Engineer,Grasshopper Pte. Ltd.,16 hours ago,,Full–time,"OVERVIEW :

We seek a Senior Data Engineer to support our high-frequency trading and quantitative research. At Grasshopper we recognize that effective research and trading strategies are built upon a solid foundation in data collection, storage and service.

Grasshopper trades at a number of exchanges globally, and our success depends on our ability to scale by leveraging the potential of data.

You will, in close collaboration with stakeholders, design and develop highly scalable and fault-tolerant data pipelines in a cloud environment focusing on scalability and performance, as well as the next generation of tools that empower users to generate insights on the data they work with.

RESPONSIBILITIES :
• Work with a team of data engineers across locations, managing project schedules.
• Expand the imagination of data stakeholders on what is possible and educate them in leveraging data they already have.
• Work proactively with traders, researchers and various members outside the data team to meet their needs for timely, accurate and complete data sets.
• Build highly scalable data pipelines to process and analyse billions of messages in real time.
• Set strong technical / architectural / cultural foundations, advocate good design in software development as well as data engineering, and lead by example.

SKILLS & KNOWLEDGE REQUIREMENTS :
• Strong technical leadership qualities, good at working with both people and with code.
• Extensive experience with data modelling and designing / supporting both streaming and batch ETL pipelines.
• Extensive experience in SQL and databases.
• Domain knowledge in finance, especially in buy-side trading, is not required but good to have.
• Proficiency in at least two mainstream programming languages (preferably Python, Java, C++).
• Proficiency in a programming language of a non-OOP paradigm (e.g. functional / logic programming).
• Experience with FP libraries like scalaz / cats / ZIO is a plus.
• Demonstrates good judgment in software design balancing design principles with pragmatics, with an emphasis on simplicity.

See Rich Hickey's Simple Made Easy talk : https : / / www.youtube.com / watchv oytL881p-nQ
• Good to have experience in Google BigQuery.
• Experience working in some cloud services platform (e.g. Google Cloud, AWS, Azure).
• Experience with messaging middleware such as Solace or Kafka.
• Experience with machine learning is good to have.

WHO WE ARE LOOKING FOR :

We are looking for team-players who are creative in their approach to problem solving. They take the initiative to explore different ways to resolve an issue, and systematically find the most efficient and effective way to do it.

They are adept at expressing their ideas and solutions into clean and maintainable code.

THE ENVIRONMENT WE OFFER :

As a growing firm with a tightly knit team, we respect and listen to all our employees. You will get the chance to make an impact by having your voice heard by everyone, including the management.

Our employees enjoy a high level of autonomy at work. We focus on substance, not form - as long as you can perform, you will be recognized and rewarded.

We are also dedicated to supporting our staff and ensuring they develop holistically to maximize their potential in the long-term.

We also provide flexible working arrangement as required and a casual and fun work environment to boot"
29-Apr-2022 T11:53,Data Engineer,Vestiaire Collective,16 days ago,,Full–time,"Vestiaire Collective is the leading global platform for desirable pre- loved fashion. We transform the fashion industry for a more sustainable future, by empowering a community to drive change. Encouraging consumers to join the circular economy as the sustainable alternative to throw-away fashion, the platform is unique due to its highly engaged community, its rare desirable inventory and its authenticity and quality control process. Launched in Paris in October 2009, Vestiairecollective.com has close to 15 million members across 80 countries worldwide with offices in cities Paris, New York, Berlin, Hong Kong, Seoul, Singapore, Shanghai, Ho Chi Minh City and Tokyo.

Our values have built our success and made us who we are as a fast-growing company because we think collective: we work with style, with entrepreneurial spirit and with passion. We currently have a diverse global team of 650 employees representing more than 50 nationalities. Our values are community, activism transparency, dedication and greatness. We are proud to be a BCorp.

We are hiring a Data Engineer and you'll create and innovate the Vestiaire Collective Data Platform in collaboration with our Technical Leads, BI Engineers and Architects

About the role

This is a full-time role based out of our Singapore office reporting to the CTO.

What you'll do
• Co-creating and continuously developing the technical architecture of the Vestiaire Collective Data Platform
• Implementing data ingestion strategies for diverse data sources, such as internal databases, third-party sources, user data trackers
• Managing the Data Platform setup on infrastructure side, including managing the Data cloud account, setting up and managing instances, managing computing and storage capacity within budget constraints
• Guiding Data Warehouse developers with their ETL implementations, pointing to optimal technical solutions to data transformation objectives
• Working closely with Data Scientists to ensure smooth and quick implementation of algorithms in production
• Implementing tools for task scheduling, data quality controls, stability monitoring and alerting
• Continuous learning and staying up-to-date with the latest developments in the data technology space in order to keep the team ahead of the curve

Who you are
• 2+ years of working experience, including work with relational databases, Hadoop, NOSQL and/or cloud infrastructure (e.g. AWS)
• Educational background in Computer Science / Electrical Engineering or other engineering fields
• Solid understanding of database concepts and experience with data processing tools (SQL, Hive, Pig, Spark, etc.)
• Hands-on experience with at least one of the following programming languages: Java, Python, Scala - and curiosity to learn others
• Creative approach toward problem solving, passion for exploring new technologies
• Experience with real-time / stream computing technologies is a plus (e.g. Flink, Storm, Spark Streaming)
• You do not need to be micromanaged; to accomplish team and company goals, you can wear multiple hats and pickup new technologies and languages.

What we offer
• A meaningful job with an impact on the way people consume fashion and promote sustainability
• Flexible work arrangements
• The opportunity to create impact in a high growth environment
• The possibility to work as part of a global diverse team with more than 50 nationalities
• 2 days to help Project - reinforcing your activist journey and volunteer for an association
• Investment in your learning and growth
• Competitive compensation and benefits package
Vestiaire Collective is an equal opportunity employer. We strive to develop an inclusive work environment that reflects the diversity of our fashion activist community"
29-Apr-2022 T11:53,Data Engineer,POWER IT SERVICES,13 hours ago,,Full–time,"Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

· Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

· A successful history of manipulating, processing and extracting value from large datasets.

· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science Statistics, Informatics, Information Systems or another quantitative field.

They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:53,"AVP, Data Engineer - BI Tools",United Overseas Bank,1 day ago,,Full–time,"AVP, Data Engineer - BI Tools

AVP, Data Engineer - BI Tools

Posting Date: 25-Apr-2022

Location: Alexandra, Singapore, SG

Company: United Overseas Bank Limited

About UOB
United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices.
Our history spans more than 80 years. Over this time, we have been guided by our values - Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.

About the Department
The Technology and Operations function is comprised of five teams of specialists with distinct capabilities: business partnership, technology, operations, risk governance and planning support and services. We work closely together to harness the power of technology to support our physical and digital banking services and operations. This includes developing, centralising and standardising technology systems as well as banking operations in Singapore and overseas branches.

Job Responsibilities
You will be responsible for the end-to-end software development and support for all work related to projects, quarterly change requests, L3 production fixes. This includes software product implementation and administration, application design, development, implementation, testing and support. You will be expected to work on Finance & Risk Analytics.
You will also be responsible for quality assurance of the team's delivery in conformance with the Bank-defined software delivery methodology and tools. You will partner with other technology functions to help deliver required technology solutions.

Other responsibilities include:

• Create frameworks, technical features which helps in faster operationalisation of Data models, Analytical models(including AI/ML) and user generated contents (dashboards, reports etc.)

• Effectively partner with citizen data scientists in enabling faster adoption of AL/ML model based systems

• Independently install, customise and integrate software packages and programs

• Carry out POCs involving new data technologies

• Design and develop application frameworks for data integration

• Create technical documents such as solution design, program specifications for target solutions

• Perform design and development of applications which may not be limited to: Software Applications, Data Integration, User Interfaces, Automation

• Maintain and recommend software improvements to ensure a platform centric management of software applications

• Performance tuning

• Work with production support team members to conduct root cause analysis of issues, review new and existing code and/or perform unit testing

• Perform tasks as part of a cross functional development team using agile or other methodologies and utilising project management software

Job Requirements
Functional skillsets
Data Lake, EDW, Data Mart, Data Integration & Visualization
Hands-on experience in implementing large scale data warehouse & analytics platforms in financial services industry with good functional knowledge of products & services offered in Retail bank / Wholesale / Global Markets covering some of the following analytics domains:

• Setting up and running BI tools oriented platform
• Design and develop QlikSense & Microsoft Power BI applications
• Design and develop Applications in SAS, Microsoft-R, Python
• Integration of BI tools with data stores (EDW, data marts )
• Experience in Data Modeling, Data mapping for Data Warehouse and Data Marts solutions
• Experience in financial domain - Retail , Wholesale, Compliance, Digital
• Expertise in design of role based fine grained access control
• Designing cloud ready data solutions, Virtualization
Technical skillsets

• Expertise in installing and configuring of BI/Analytical tools like Qlik, Power BI, SAS, Microsoft R
• Expertise in administration (QMC, powershell scripting), infrastructure design of such tools
• Integration with other data systems like EDW, Data mart, CRM, Digital
• Expertise in building BI semantic layer (Qlik - QVD, PowerBI - Direct Query, SSAS, In Memory SSAS modelling, SSRS, Excel Add In)
• Creating application framework specific to BI tools to speed up user adoption
• Good working experience in security features of BI tools including integration with Active Directory, solutions to protect data in motion and at rest
• Good working experience in fine tuning on line BI queries, BI models, analysis of DAX queries
• Reports/Dashboard/Application development expertise using QLIK, PowerBI, SAS, Microsoft R, Python
2 to 3 technical certifications from enclosed list:

• Cloudera Hadoop distribution - Hive, Impala, Spark, Kudo, Kafka, Flume
• Teradata - Bteq, Query Grid, GCFR, MDM, Data Mover, BAR
• Informatica Data Integration - PC, IDR, BDM, MM, IDQ, EDC
• Data modelling tools (Erwin)
• QlikSense
• Microsoft Power BI - Direct Query, SSAS, SSRS, Excel Add In
• Microsoft - R
• Data science workbenches - Cloudera Workbench, Jupyter, DataRobot, H2O.AI, IBM DSX
• Data Virtualization tool - Denodo, Dremio
• AS400
• Language - SQL, Java, Python, Scala, Pyspark
• Automation / scripting - CtrlM, Shell Scripting, Groovy
Experience for an added advantage:

• CI/CD software, Testing Tools - Jenkins, SonarQube
• Version Control Tool - Aldon+LMe, CA Endeavor
• Deployment Tool kit -Jenkins
• Service or Incident Management (IcM) Tools - Remedy
• Source Code Repository Tool - Bitbucket
• Scheduling Tool - Control-M
• Defect Management Tool - JIRA
• Application Testing tool - QuerySurge
• Cloud certification
• Platforms provided by FICO, Experian, SAS for credit and portfolio management

Be a part of UOB Family
UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.

Apply now and make a difference"
29-Apr-2022 T11:53,Data Engineer,Supernova,16 hours ago,,Full–time,"About Supernova

We are the fastest growing lifestyle & beauty omni-channel ecommerce company. We build iconic beauty and lifestyle brands for a new generation of women worldwide. Independently owned with a global marketing reach, we connect with over 15 million women on social media every week. Headquartered in Singapore, we currently have offices in Berlin, London and LA. Our brands include Sand & Sky and Coco & Eve.

About Supernova's Internal Engine

Together we create Superstars! Our three company pillars are guided by this motto: Our product and brands are of the highest quality and within the top five in their category; our customers are the focus of our actions and their lives improve with our products; our talent and employees grow and develop within Supernova.

This means that investing in hiring, developing and retaining the brightest minds in the world is a top priority. Living our core values of #MakeAnImpact, #TalkAboutIt, #SeekNewOpportunities, #DelightAndSurprise and #MoveForwardTogether creates a unique culture across five different time zones and allows us to realise the enormous potential of this exciting industry. If you're an ambitious, smart, natural collaborator who likes taking risks, influencing, and innovating in a challenging hyper-growth environment, we'd love to talk to you about joining our team.

The role

This role provides tremendous opportunity for the right candidate: to capture the energy and rapid pace of a fast growing start up, while being responsible for a sizeable and growing part of our global footprint.

As Supernova's Data Engineer, you are responsible for the design and implementation of modern, scalable data-centric services and solutions.

You have strong data infrastructure and architecture skills, with a proven track record of handling high data volumes to serve batch as well as streaming needs.

This is a remote position, working as part of a strong and growing Data Team.

Responsibilities:
• Define and build data pipelines from a wide variety of data sources that will enable faster, better, data-informed decision-making within the business.
• Develop custom integrations via REST/SOAP/event-driven architecture.
• Work with stakeholders including the Brand, Product, Ops and Marketing teams to assist with data-related issues and support their data infrastructure needs.
• Build tools for effective maintenance and monitoring of the data infrastructure.
• Contribute to an ongoing effort to improve data reliability, efficiency and quality.
• Performance tuning of data pipelines jobs for optimal end-user experience.

Your Profile:
• Minimum Bachelor's degree in a quantitative field is required e.g. Computer Science, Statistics, Mathematics
• 5+ years of experience in data engineering roles using Python / Java.
• Experience in building the solution architecture, provisioning of infrastructure and delivering data-centric services and applications either in GCP or AWS.
• Strong experience in Event Driven Architecture using Kafka or Pub/Sub etc.
• Extensive knowledge in ELT processing and workflow orchestration using Airflow/ Nifi or similar tools.
• Experience with development ecosystems such as Git, Docker, Jenkins and CI/CD.
• Preferably with a background in Startup / Ecommerce.
• Ability to plan your work and commit to deadlines.
• Ability to work with cross-functional teams to deliver quality results.

This position is available immediately. All applications will be treated confidentially.

Please note that we can only contact successful applicants.

Supernova is an equal opportunity employer. Our values define the working culture and environment we strive to create – fair, diverse, respectful and welcoming of different views. We embrace a culture reflecting a variety of perspectives, insights and backgrounds to drive innovation. We build talented and diverse teams to drive business results and encourage our people to develop to their full potential. We promote harmonious working relationships and encourage a learning and development culture. We welcome applications from talented people coming from all cultures, countries, races, genders, sexual orientations, disabilities, beliefs and generations and are committed to providing a working environment free from harassment, discrimination and retaliation.

See our Applicant Data Protection policy for information on how we collect, use and process your personal data: http://www.supernova.xyz/applicant-data-protection-notice"
29-Apr-2022 T11:53,Data Engineer (Analytics),Mediacorp,Full–time,,,"Description

We are looking for a Data Engineer (Web Analytics) to join our Data Architecture team. The role will involve executing the data tracking requirements for Web, Mobile apps, and Smart TV platforms.

Key Responsibilities:
• Manage the tagging of advanced analytics tools across different platforms within the organization.
• Utilize deep understanding of Adobe Analytics, along with other analytics platforms to assist the various decision making teams.
• Execute A/B tests and work on personalization and recommendation systems
• Design, develop and support reporting and analytics applications leveraging data integration tools.
• Work closely with Data Architects & Data Scientists to enforce tracking requirements across different platforms

Requirements
• Degree holder in a technical discipline (Computer Science, IS/IT or related disciplines)
• Minimum 3 years of experience in working with Tag Management Solutions Primarily on Adobe Launch
• Good to know - Google Tag Manager or Tealium/ Ensighten.
• Strong Experience in Tag migration - Adobe Heartbeat implementation, Mobile app Adobe AEP SDK implementation
• Strong Proficiency in JavaScript, specifically as it relates to digital analytics implementation.
• Experience in A/B testing tools such as Adobe Target and/or Optimizely

Mediacorp is committed to creating an inclusive and diverse workplace where talent thrives. Our hiring decisions are made based on merit and fit-to-role. If you have a disability or special need which requires accommodation to participate in the recruitment process, please inform us when you submit your online application. We will be happy to support as necessary.

Thank you for your interest and application to this role. Please note that only short-listed candidates will be contacted"
29-Apr-2022 T11:53,Data Engineer,REAL ESTATE ANALYTICS PTE. LTD.,4 days ago,$4K–$6.5K a month,Full–time,"Key Responsibilities
• Maintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable and timely delivery of key insights for decision making
• cleaning and normalizing subsets of data of interest as preparatory step before deeper analysis by the data scientists
• Run Modern high performance analytical databases and computation engines
• Design and implement various data health checks to ensure the data quality and consistency across systems
• Design and implement data extraction solution in a distributed system

Skills required
• Experience in handling large data sets and working with structured, unstructured and geographical datasets
• Understanding of databases and best engineering practices - include handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding how to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipeline
• Experience with DevOps and AWS will be an advantage
• Real passion for data, new data technologies, and discovering new and interesting solutions to the company’s data needs
• Excellent communication skills to communicate with the product development engineers to coordinate development of data pipelines, and or any new products features that can be built on top of the results of data analysis"
29-Apr-2022 T11:53,Data Engineer,Lilith Games Sg Pte. Ltd.,2 days ago,,Full–time,"Big Data Development Engineer (Advertising Direction) Big Data Development Engineer (Advertising Direction) What you will be doing: 1Responsible for the construction of Big Data Cloud Platform and the maintenance of Advertising Data Warehouse. 2Responsible for the calculation, cleaning and layering of Advertising Data based on Flink, Spark, etc., and store it through Hadoop, Clickhouse, etc. 3Responsible for Advertising Data Analysis and Advertising System Report development. Qualifications & Skills 1Bachelor degree or above, major in computer and other related majors. 2Proficient in Python or Golang coding. 3Proficient with Mysql, Memcache, Redis, Message Queue and other common WEB components. 4Having experience in using one or more of HDFS, Hive, HBase, MongoDB, Kafka, Flink and Spark is preferred. 5Experience in operation and maintenance development is preferred. 6Familiar with Alibaba Cloud and other cloud computing resource deployment and optimization is preferred 7Proficient in coding complex SQL Statements, and have the ability and experience of query optimization. 8Positive and optimistic, strong sense of responsibility, with good team communication and cooperation 大数开工程师（广方） 工作责 1. 负责大数平建，广数建与维护 2. 负责基于FlinkSpark等对广数进行计算清洗分等工作并通过HadoopClickhouse等进行存 3. 负责广数分广系统报表开 任 1.大学本(统招)以上学历，计算机通信等相关业 2.熟练掌Python或Golang代编写 3.熟练使MysqlMemcacheRedis消队列等常WEB件 4.有使HDFS, Hive, HBase, MongoDB, Kafka，Flink, Spark中一项或多项验优先 5.有维开验优先 6.熟阿里等计算部署与优化者优先 7.熟练编写sql语，具备查询优化能力验 8.积，责任心强，工作认真细致，具有良好团队沟通与?作能力"
29-Apr-2022 T11:53,Data Engineering Lead,Stemly Pte. Ltd.,16 hours ago,,Full–time,"We are looking for a Senior Data Engineer with a track record of building enterprise grade data platform for cloud products .

This is an exciting opportunity for a technologist to build an innovative SaaS product using state-of-the-art and modern cloud technology stack.

As a Data Engineering Lead, you will be working on all aspects of data, from platform and infra build out to pipeline engineering and writing tooling / services for augmenting and fronting the core platform.

The data platform interfaces with large systems to ingest data (batch, micro-batch) to create statistical / machine learning models in a distributed setting.

You will work closely with data scientists, product managers, legal, compliance and business stakeholders.

You will also evaluate new technologies / frameworks and contribute to various open source big data technologies. You are encouraged to think out of the box and have fun exploring the latest patterns and designs in the fields of software and data engineering.

Your responsibilities
• Architecting and implementing a robust data platform for Stemly's products. Design and develop extremely efficient, reliable and observable data pipelines using technologies such as Airflow, Kubernetes, ElasticSearch, Kafka / KubeMQ, Postgres, ClickHouse etc.
• Design and deliver the next-gen data lifecycle management suite of tools / frameworks , including ingestion and consumption on the top of the data lake to support real-time, API-based and serverless use-cases, along with batch (mini / micro)
• Build and expose metadata catalogue for the Data Lake for easy exploration, profiling as well as lineage requirements
• Work on the development of the data platform iteratively by building quick POCs and converting ideas into real products
• Make data discoverable and easy to use for Data Scientists and Analysts. Enable Data Science teams to test and productize various ML models
• Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.

Support your colleagues by reviewing code and designs
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner

About you
• 8+ years of professional experience in software development in Python and Pandas
• Extensive experience with object-oriented design, coding and testing patterns, including experience with engineering software platforms and data infrastructures
• Must have sound understanding for Big Data components & administration fundamentals
• Hands-on in building a complete data platform using various open source technologies. Must have the knowledge to build metadata, lineage, observability and discoverability for data platform
• Experienced in DevOps best practices like CI / CD, containerization, blue-green deployments, secrets management etc. in the Data ecosystem
• Expert in the principles of distributed computing and data modelling
• Expert in building optimized SQL queries
• Conversant with the latest developments in the areas of Machine Learning models and efficiently supporting the data scientists is a plus
• Experience with Airflow, ClickHouse, Postgres, Kubernetes, GraphDB is a big plus
• Familiarity with Google Cloud Platform (e.g. GCS, Cloud Composer, BigQuery) is a plus
• Outstanding communication and interpersonal skills

About us

We are scientists and engineers with a strong passion for cutting-edge technology and decision science. Our mission is to enable enterprises to reduce time to decision with state-of-the-art forecasting and optimization.

Our algorithms are based on automatic machine learning techniques and allow for the discovery of optimal forecasting models.

We unlock sizable business and financial impact in terms of lower lost demand, decrease in inventory, lower working capital and financing costs.

How to apply

You may already know if you're a fit, but perhaps you're worried about some of the requirements We're looking for smart and passionate Engineers;

if you're the right candidate, we're flexible"
29-Apr-2022 T11:53,Data Engineer - APAC,Tamr,16 hours ago,,Full–time,"Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE
who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being
customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation
This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.
Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion
Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:53,Senior Data Engineer,eyos.one,Full–time,,,"We are a dynamic team with great offices in Singapore, London, Bangkok, Jakarta and Sydney, serving our great customers worldwide. We’re expanding fast and are looking for a passionate and driven Senior Data Engineer to join our global tech team in Singapore.

Here's What You Will Be Doing:
• We are looking for a savvy Senior Data Engineer to join our growing team of retail analytics experts and expected to be involved from inception of projects, understand requirements, architect, develop, deploy, and maintain data platform and/or data pipelines, which involves partnering with program and product managers to expand product offering based on business demands.
• Our goal is to drive up retail analytics and adoption of the platform while constantly working towards improving platform performance and scalability, deployment and maintenance require close interaction with various teams.
• Production support for applications is usually required for issues that cannot be resolved by the operations team. Creative and inventive problem-solving skills for reduced turnaround times are highly valued.
• Collaborate with Data Scientists, Data Analysts, and other internal stakeholders to assist with data-related technical issues and support their data pipeline infrastructure and data preparation needs.
• Preparing user documentation to maintain both development and operations continuity is integral to the role.

Here's What You Need To Have/Know:
• BE/BS in Computer Science or equivalent practical experience
• At least 3 - 7 years experience in data analytics environments
• Experience in data warehouses like Snowflake, Google BigQuery and OLTP databases like MySQL, SQL Server, Oracle.
• Hands-on experience in SQL, Python or Scala and Spark
• Hands-on experience with cloud environments (AWS & GCP preferred)
• Experience with dev ops tools like Git, Bitbucket
• Understanding of airflow, dagster or any kind of orchestration tools
• Extensive experience on distributed environment and scalable systems
• Good analytical and problem-solving skills with strong attention to detail
• Excellent communication skills with an ability to work well with others (across APAC and EMEA)
• Knowledge of and experience in automation technologies

Skills that would be a plus
• Basic understanding of containers technologies (like dockers, Kubernetes)
• Exposure to visualization technologies such as Tableau or Sisense
• Prior experience and knowledge on machine learning models deployment processes in AWS or any other cloud systems.
• Exposure to NoSQL platforms

What will you learn on the job?
• FMCG brands with granular market intelligence, targeted marketing automation and data-driven sales optimization, especially in independent trade channel that attracts majority of grocery shopping trips in the Asian markets.
• Data pipeline flow, which converts physical receipts to digital receipts and connect 100% of in-store transactions into any platform they use in real-time.
• Build a platform for data-driven growth in physical retail shops that enables large scale analytics and data science workflows.
• Exposure to massive retail of datasets and deal with some of the most exciting data & analytics challenges"
29-Apr-2022 T11:53,Data Engineer Lead,Robert Walters,6 days ago,,Full–time,"An exciting Data Engineer Lead job opportunity has become available at a leading e-commerce company.

About the Data Engineer Lead Role:
This leading e-commerce organisation has a strong presence in Singapore. They are looking to hire a Data Engineer Lead reporting to the head of department.

Key Responsibilities:
• Creating and maintain optimal data pipeline architectures from various channels
• Ensuring optimal accuracy and timeliness of data on real-time basis while implementing monitoring tools to detect data issues
• Proper processing of semi-structured and unstructured datasets
• Working with internal teams to understand requirements and optimise performance

To succeed in this Data Engineer will need to have around five years of experience.

Key Requirements:
• Bachelor’s degree in computer science or equivalent
• Experience in business intelligence tools such as Tableau and PowerBI
• Experience in Python and R
• Able to implement ETL logic using SQL
• Knowledge in Hadoop of spark for data processing
• ELT pipelines management experience along with background in analytics

This is an excellent opportunity to be part of a company with strong footprint within the region.

If you are driven, determined and want to take the next step in your career, this Data Engineer role is right for you. Excellent career progression opportunities await the right person in this exciting role.

Apply today or contact me at +65 6228 5350 to discuss this new opportunity. Alternatively, send your resume to sachet.sethi@robertwalters.com.sg.

Do note that we will only be in touch if your application is shortlisted.

Robert Walters (Singapore) Pte Ltd

ROC No.: 199706961E | EA Licence No.: 03C5451

EA Registration No.: R1439850 Sachet Sethi"
29-Apr-2022 T11:53,Data Engineer,Morgan McKinley,4 days ago,,Full–time,"Data Engineer

Job Summary
• Singapore
• Permanent
• BBBH811650
• Apr 05, 2022
• Competitive Job Description
Our client is looking for an experienced Data engineer to drive development of information products for data and digital transformation across their group.

Mandatory Skills:
• Master of Business Administrations or master's in quantitative fields (Computer Science, Statistics or similar) with minimum of 5 - 12 years of overall experience.
• Experience programming in Python, Java, SQL, PLSQL.
• Experience with traditional RDBMS based systems and more modern NoSQL technology stacks.
• Expertise building ETL and data pipelines on Databricks.
• Experience working with Big Data technologies such as Hadoop, Cloudera (CDH), Hortonworks (HDP), DataBricks, Spark, Delta, HDFS, HBase, Hive.
• Experience in event streaming with Kafka.
• Experience in ML Model Productionization, Docker.
• Real Time, Batch, Unstructured Data, DW, MDM, Data Marts.
• Proficient in using data visualization tool such as Tableau, Power BI, D3, AmCharts etc.
• Understanding FS industry fundamentals and business problems to find new ways to leverage data.
• Intellectual curiosity to solve data driven problems.
• Independent thoughts and unique ideas on solving business problems.
• Able to collaborate / virtually manage multi-cultural, multi-disciplined, globally dispersed teams.
• Sound understanding of functional areas and analytics problems in areas of Customer, Marketing, Channel, Pricing, Digital, Operations, HR/Corporate analytics.
• String communication skills to be able to explain highly technical problems in simple layman form.
• Ability to articulate the impact of decisions and recommend improvements. Desired Skills:
• Relevant experience in Banking and financial institutions. Job Responsibilities:
• Exceptional data engineering & visualization experience & technical skills. State-of-the-art expertise across, data/information preparation and data insight & visualization using BI (or similar tools).
• Be a data engineering & visualization technical expert. Lead and explain/educate to all levels people in these areas, Coding Databases, Data Integration, Frameworks, Deployment, Architectures and Visualization.
• Contribute to the development of in-house data products. Use your data engineering & visualization expertise plus FS domain & functional knowledge, problem solving skills and independent thinking to create in-house data analytics and data management products.
• Be a team player & an individual contributor. Work with group data office and other business function people as part of larger deliveries, as well as being able to work independently or in small teams to continuously deliver business value.
• Be a trusted partner of our client. Someone that anyone in the company can reach out to for help with creating data engineering & visualization driven business transformation. Those who are keen for the role and would like to discuss the opportunity further, please click ""Apply Now"" or email Alimpan at amukherjee@morganmckinley.com with your updated CV.

Only shortlisted candidates will be responded to, therefore if you do not receive a response within 14 days please accept this as notification that you have not been shortlisted.

Alimpan Mukherjee

Morgan McKinley Pte Ltd

EA Licence No: 11C5502

EA Registration Number: R2198629"
29-Apr-2022 T11:53,Senior Data Engineer,Toptal,4 hours ago,,Full–time,"About The Job

Toptal developers work with speed and efficiency to deliver the highest quality of work. We are looking for someone who is passionate about their client’s business, and ready to work on exciting projects with Fortune 500 companies and Silicon Valley startups, with great rates and zero hassles. If you are looking for a place to advance your career, enhance your skill set, and build connections around the globe, Toptal is right for you.

About Toptal

Toptal is an exclusive network of top freelancers from around the world. Fortune 500 companies and Silicon Valley startups hire Toptal for their most important projects. Toptal is one of the fastest-growing fully remote networks and empowers freelance software developers, designers, finance experts, product managers, and project managers worldwide to grow and excel in their freelance careers.

Toptal clients vary in sizes and industries, from enterprise organizations and big tech companies to Silicon Valley startups and renowned universities. Once you enter the network, our matchers will contact you with project opportunities that fit your expertise and preferences. We have experts in over 120 countries who get to work remotely on projects that meet their career ambitions.

About The Role

As a Data Engineer, your main goal is to be one step ahead of data scientists and analysts. You will support them by providing infrastructure and tools they can use to deliver end-to-end solutions to business problems that can be developed rapidly and maintained easily. This is more than building and maintaining ETL pipelines. We need innovation, creativity, and solutions that will have a significant impact on the client’s velocity.

Requirements
• 3+ years of professional experience in software development
• Working experience with Python and Pandas.
• Familiarity with the basic principles of distributed computing and data modeling.
• Extensive experience with object-oriented design and coding and testing patterns, including experience with engineering software platforms and data infrastructures.
• Working experience with Airflow and Luigi is a big plus.
• Working experience with Scala is a plus.
• Familiarity with Google Cloud Platform (e.g. GCS and BigQuery) is a plus.
• Working experience with Dimensional Modeling and Rails is a plus.
• Outstanding communication and interpersonal skills.
• Full-time availability is a strong advantage

If you’re interested in pursuing an engaging career working on full-time freelance jobs for exclusive clients, take the next step by clicking apply and filling out the short form to get started.

#RemoteJobDataEngineering"
29-Apr-2022 T11:53,Data Engineer,Power It,2 days ago,,Full–time,"· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.
· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.
· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
· Strong analytic skills related to working with structured and unstructured datasets.
· Build processes supporting data transformation, data structures, metadata, dependency and workload management.
· A successful history of manipulating, processing and extracting value from large datasets.
· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
· Experience supporting and working with cross-functional teams in a dynamic environment.
· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics Information Systems or another quantitative field.
They should also have experience using the following software/tools:
· Experience with:
o Big data tools: Hadoop, Spark, Kafka, etc.
o Relational SQL and NoSQL databases, including Postgres and Cassandra.
o Data pipeline and workflow management tools: Airflow, etc.
o AWS cloud services or GCP.
o Stream-processing systems: Spark-Streaming, Flink etc.

Job Type: Permanent

Schedule:
• Monday to Friday"
29-Apr-2022 T11:53,Finance WebApp Data Engineer,Apple,Full–time,,,"Summary
Posted: 27 Mar 2022
Weekly Hours: 40
Role Number:200360362

Apple’s Finance Business Process Reengineering team is seeking a Data Engineer to join our organization. In this role you will help architect, maintain and continually improve data analytics and automation capabilities. Sitting inside a business unit, you must adapt to varied and shifting needs. You will be working with business users, Apple IS&T developers, Web Application developers and data analysts on your team to deliver complete, accurate, well-secured data that enables reporting and analytics across all Finance functions. Comfort with and ability to learn and perform a wide variety of development and engineering tasks combined with knowledge of Finance business processes sets you apart in this role.

Key Qualifications

Key Qualifications
• Experience in building backend infrastructure for scalable data processing and analytics
• Strong SQL and Python skills
• Experience tracking and measuring data quality solid knowledge of database technologies including Snowflake, Teradata, MySQL, MariaDB
• Experience building scalable data pipelines
• Experience with applying data encryption and data security standards
• Preferred experience in tools like Dataiku
• Able to quickly learn new technologies

Description

Description
The Finance Business Process Reengineering (FBPR) organization supports Apple's Finance function worldwide. Finance Data and Technology (FDT) team within the FBPR org enables the Finance organization by providing quality data accessibility, analytics, reporting and automation services. We are looking to expand capabilities in the areas of data privacy, high-performance computing, advanced analytics and general business intelligence.

A Finance WebApp Data Engineer is a technical expert and works tightly with WebApp developers, other data engineers and data analysts on the team to create data integrations, ETL, pipelines, transformation and codebase to drive innovative analytics projects from initial experimentation to production level deployment. They work on critical data engineering problems, building bespoke, reliable, accurate, consistent, and architecturally sound solutions that are aligned with business needs.

The Finance Data Engineer architects, maintains and continually improves data analytics and automation capabilities. The role requires working cross-functionally with business users, Apple IS&T developers, and data analysts to deliver complete, accurate, well-secured data that enables reporting and analytics. This role is required to learn and perform a wide variety of development and engineering tasks, on top of growing their knowledge of Finance business processes to efficiently identify data applicable business questions.

Finance Data Engineers work predominately in Apple’s enterprise data warehouse (EDW), identifying and combining data in an efficient, scalable manner to help answer business questions. When the necessary data is not available on an enterprise system or is generated offline by business users or third parties, the Finance Data Engineer must develop methods to reliably source, validate, and integrate the data into EDW.

The Finance Data Engineer must learn and understand a variety of available IS&T solutions, when and how to use them, and when to develop custom solutions. This paired with a proven record of excellent problem solving and a sharp, open mind will be more important than deep expertise in any one area.

Education & Experience

Education & Experience
Master’s degree or equivalent in Computer Science or related field and two years of experience. Alternatively, a Bachelor’s degree in Computer Science or related field and five years of progressive experience.

Additional Requirements

Additional Requirements
• Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.
• We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation"
29-Apr-2022 T11:53,Data Engineer,Glp Pte. Ltd.,16 hours ago,,Full–time,"The Data Engineer will be a key player in the enterprise-wide data transformations projects. You will engage in developing and automating data processing pipelines for data modelling, analysis, and reporting from various data sources system; The primary responsibility of this position is to assist to establish the enterprise data Lake architecture under Microsoft Azure Data factory, Databricks and Synapse and deliver data driven solutions.Job description:· Assist in architecture design, develop, document, and implement end-to-end data pipelines and data driven solutions.· Define roadmap to transform data architecture focusing on scalability, performance and stability for the entire data lifecycle;· Build data flows for data acquisition, aggregation, and modelling, using both batch and streaming paradigms.· Perform data analysis, data profiling, data cleansing, data lineage, data mapping and data transformation.· Develop high-quality code for the core data stack including data integration hub, data warehouse and data pipelines under Azure service.· Execute and deliver best practices in data management and data lifecycle processes, including modular development of data processes, coding and configuration standards, error handling and notification standards, auditing standards, and data archival standards.· Implement security and standards, documenting technical specifications and operating procedures.· Collaborate across developers as part of a SCRUM team ensuring collective team productivity· Provide technical support for any data issues with recommendations, and resolutions.Requirements:· 2 years professional experience as a data engineer, software engineer, data analyst, data scientist, or related role.· Experience with Microsoft Azure Data Integration Stack (Azure Data Lake Gen2, Azure Data Factory, Delta Lake, SSIS, SQL Server, Azure Data Warehouse), Databricks, Spark.· Working experience in Investment or Real Estate industry, preferably with business and functional knowledge.· Expertise building ETL and data pipelines on Databricks using data engineering languages Python and SQL on Azure.· Advanced knowledge and experience working with Python & SQL;· Proven experience with all aspects of the Data Pipeline (Data Sourcing, Transformations, Data Quality, Etc…).· Experience with visual modelling tools including UML· Proficient in using data visualization tool such as Power BI, Workiva and in standard office tools such as Excel.· Familiar with DevOps and Agile methodology"
29-Apr-2022 T11:53,Data Engineer (Trading),Michael Page,10 days ago,,Full–time,"About Our Client

Our client is a fintech company that is a trusted crypto exchange brand. They are looking for a Data Engineer that has experience in the finance industry.

Job Description
• Collaborate with trading applications and backend developers to advise and fulfill data requirements in terms of consistency, latency and scalability
• Responsible for the implementation and maintenance of relevant database technologies and data quality to meet these data needs
• Collaborate with existing data team to design and implement data synchronization
• Work with development team for the maintenance and enhancement of latency-sensitive market data feed components

The Successful Applicant
• 3 years' working experience in data engineering, database administration, AWS data solutions or relevant field
• Proficient in Python, R, Julia (at least one), SQL, Linux and C++, Java or Rust (at least one)
• Proficient in data pipeline development involving cloud data solutions such as Redshift Snowflake or databases such as SQL, kdb+ or other time series databases
• Experience in market data gateway protocols such as ITCH or socket and web socket programming or real-time messaging frameworks such as Apache Kafka, Aeron, Chronicle is highly desirable

What's on Offer
• Permanent role
• Career Advancement Opportunity
• Opportunity to work on cutting edge technologies"
29-Apr-2022 T11:53,Data Engineer (APAC),TEKsystems (Allegis Group Singapore Pte Ltd),6 days ago,,Full–time,"Data Engineer (APAC)

We are looking for high-calibre Data Engineers to be a part of a growing tech firm!

This is a Permanent opportunity.

The Position:

• Experience in designing and building robust and highly scalable data pipelines
• Strong programming proficiency using Python and SQL
• Strong understanding of building data models and data warehouse technologies.
• Experience with Airflow
• Good to have: BI tools like Tableau or Google Data Studio
• Strong communications skills
What's in it for you?

• Exposure to the latest cutting-edge technologies
• Rewarding work: Opportunity to make real and immediate impact on the business and the industry
• You'll work with a great team. They're exceptional at their jobs, and winning market share from their competitors every day.
• Vibrant international team with a fun, friendly and open start-up company culture We regret to inform that only shortlisted candidates will be notified / contacted.

For more information you can email Krystal in our Singapore office on krystal.fernandez@teksystems.com quoting Job Reference KrystalFernandez531475 or alternatively, apply here to register your interest.

http://jobs.en-sg.teksystems.com/fQHPOu/data-engineer-apac-itcommunications-singapore-singapore-15264412

Job Reference: Krystal Fernandez 531475

EA Registration No.: R21103744, Krystal Anna Fernandez

Allegis Group Singapore Pte Ltd, Company Reg No. 200909448N, EA License No. 10C4544"
29-Apr-2022 T11:53,Staff Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,16 hours ago,$8K–$13K a month,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 5 years of experience in Data Engineering role and have good knowledge / working experience in:

• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.

• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.

• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.

• Functional programming languages, e.g. Scala.

• Virtualization and container environment such as Docker and Kubernetes.

•Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:53,Data Engineer,Tech Mahindra Limited,3 days ago,$60K–$120K a year,Full–time,"What to expect:
• Manage & develop data warehouse/data lake and pipeline plans.
• Develop and uphold best practices with respect to documentation & data protocols
• Design, build and launch new data models in production.
• To also develop new data extraction, transformation and loading processes in production.
• Work in a cross-functional team, Interfacing with engineers, product managers and product designers to understand data needs to define and review technical specifications.
• Build machine learning/reasoning models at scale.

How To Succeed:
• Degree in Computer Science, Information Systems, Computer Engineering, Mathematics, or related disciplines.
• At least 3 years of experience in data engineering, with experience in Kotlin/Scala/ Python, and experiences in Amazon Web Services stack, such as ECS, Kinesis, EMR, and DynamoDB.
• Experienced in building production-grade models using machine learning frameworks such as Tensorflow, Scikit-learn, PySpark and/or others is required.
• Experienced in custom ETL & ELT & Streaming big data design, implementation & maintenance and data warehouse/data lake space.
• Experienced/willing to use Infrastructure-as-code for deploying pipelines.
• Hands-on and deep experience with schema design and dimensional data modelling.
• Ability to write efficient SQL statements.
• Ability to analyse data to identify deliverables, gaps and inconsistencies.
• Excellent communication skills including the ability to identify and communicate data-driven insights.
• Experience in using Hadoop, Spark, HBase, Hive and Pig, is a plus.
• Experience in software engineering, with experience in Kotlin or Java, and Spring Boot is a plus.

What to expect: * Manage & develop data warehouse/data lake and pipeline plans. * Develop and uphold best practices with respect to documentation & data protocols * Design, build and launch n

Skills: Excellent Communication Skills, Machine Learning, Pyspark, Big Data, Pipelines, Kotlin, Hadoop, Software Engineering, Etl, Dynamodb, data engineering , Emr, Sql, Python, Java

Experience: 2.00-5.00 Years"
29-Apr-2022 T11:53,Senior Data Engineer,Singtel Group,27 days ago,,Full–time,"Jobscope
• Develop big data solutions for near real-time stream processing, as well as batch processing on the Big Data platform
• Work with business domain experts, data scientists, and solution designers to identify data relevant for analysis and develop Data solutions
• Fine-tuning of new and existing data pipelines
• Schedule and maintain data pipelines
• Drive optimization, testing and tooling to improve data quality
• Assemble large, complex data sets that meet functional / non-functional business requirements
• Develop APIs to support high throughput data processing, feature engineering, and use cases
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc
• Review and approve high level & detailed design to ensure that the solution delivers to the business needs and align to the data & analytics architecture principles and roadmap
• Understand various data security standards and use secure data security tools to apply and adhere to the required data controls for user access
• Support and contribute to development guidelines and standards for data ingestion
• Adapt and learn new technologies surrounding Data Platform

The Ideal Candidate should possess the following:
• Bachelor’s Degree in IT, Computer Science, Software Engineering, Business Analytics or equivalent.
• Minimum of 6 years of experience with highly distributed, scalable, concurrent and low latency systems working with one
• or more of the following technologies:
• Hadoop distributions Spark
• NoSQL data warehouses such as HBase, Cassandra
• Excellent software & data engineering principles and design patterns
• Proficient in creating and maintaining complex data pipelines end-to-end while maintaining high reliability and security
• Excellent hands-on experience in Scala or Python
• Excellent hands-on experience with SQL and Spark
• Experience with Kafka
• Experience with CI/CD tools and environment
• Experience migrating from on-premise data stores to cloud solutions
• Good working experience with one or more major cloud vendors (ie: Azure,AWS, GCP)
• Experience working in Telco Data Warehouse and / or Data Lake advantageous
• Highly organized, selfmotivated, pro-active, and able to plan
• Ability to analyze and understand complex problems
• Ability to explain technical information in business terms
• Ability to communicate clearly and effectively, both verbally and in writing
• Strong in User Requirements Gathering, Maintenance and Support
• Good experience managing users and vendors
• Experience with Agile Methodology

We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated"
29-Apr-2022 T11:53,Data Engineer,Endowus,5 days ago,,Full–time,"About us
Endowus is Asia's leading fee-only digital wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping all investors grow their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.
Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones. Our mission is clear: help people invest better so they can live easier today, and better tomorrow.

The people you will work with
The team has deep domain knowledge in finance and technology, bringing together decades of experience at Goldman Sachs, Morgan Stanley, UBS, Credit Suisse, The Blackstone Group, AQR, Grab, Dropbox, Lyft, Redmart, Carousell, Bytedance, Grab, Kakao, Alibaba, and more See our leadership team here . We practise inclusion and treasure our diversity in background and experience. A diverse team is our biggest asset and we look for people who share our belief in Endowus' clear mission.

Investors, recognition, licensing
Endowus is backed by global leading strategic and venture capital investors including UBS, Samsung Ventures, EDBI, Prosus Ventures, ZVC, Singtel Innov8, Lightspeed Venture Partners, and SoftBank Ventures Asia.

Endowus has been recognised by the industry with the following awards: Singapore's Rising Star and Fintech Innovation (Asia Asset Management's Best of the Best Awards 2021), LinkedIn Top Start-ups 2021, WealthTech of the Year (Asia FinTech Awards 2021), and the top 15 Singapore's Best Workplaces Award (Great Place to Work).
Endowus is licensed by the Monetary Authority of Singapore (MAS).

About this role; responsibilities & ownership
• We are looking for a Data Engineer who thrives in a fast-paced environment and enjoys driving innovation through rapid prototyping and iterative development.
• Being part of Endowus's Technology team, you will build end-to-end product features that you are confident of delighting user experiences.
• You will need to leverage the entire technology stack to realise these goals and will be part of a team that is constantly tackling difficult questions of scale, architecture, and interaction. Requirements & qualifications

• Bachelors' or above in Computer Science, a related field, or equivalent professional experience
• 3-6 years of experience in designing and implementing key components for highly scalable, distributed data collection and analysis systems built for handling large volumes of data in cloud
• Advanced working SQL knowledge and experience working with relational databases or BigQuery, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimising data pipelines and data sets in Apache Flink or Apache Spark to answer specific business questions.
• Experience in at least one of the JVM languages such as Java, Scala.
• Experience Build processes supporting data transformation, data structures, metadata, dependency and workload management. (experience in Airflow is Plus)
• Experience with distributed databases, such as Cassandra, and the key issues affecting their performance and reliability.
• Hands-on development with key technologies including Scala, Apache Flink and other relevant distributed computing languages, frameworks, and libraries
• Independence and self-reliance while being a proactive team player with excellent communication skills.
• Self-driven, has a strong sense of ownership and able to work and learn independently
• Comfortable with numbers and motivated by steep learning curves Nice to haves
• Experience with message broker systems, such as Apache Kafka.
• Developed mission-critical products, e.g. trading platforms, medical applications
• Basic knowledge of finance and trading
• Preferably some exposure to CQRS / Event Sourcing patterns
• Familiarity with Docker, Kubernetes and Terraform Remote Okay
• We are open to hiring remotely in Asia time zones. Benefits & perks
• Stock options in a fast growing company *
• Employee discount for investing at Endowus
• Flexible working hours and locations so you can live life and your best work - you are trusted to be responsible
• Competitive Staff Benefits; Annual Leave, Medical insurance, Dental *
• Productivity Equipment; Omnidesk Standing Desk & Ergonomic Chairs, Dell UltraSharp USB Type-C Monitor, high-end computer peripherals
• Choice of Apple or PC equipment
• Additional S$250 /HKD 1450 budget for your favourite hardware to boost productivity * Note: * is only applicable to Full-Time employees"
29-Apr-2022 T11:53,(Senior) Data Engineer (APAC),foodpanda Singapore,20 mins,,Full–time,"foodpanda is the largest food and grocery delivery platform in Asia, outside of China. Operating in more than 400 cities across 12 markets, foodpanda continues to expand and grow in its core food delivery business as well as in new verticals like grocery deliveries, with a strong tech infrastructure at its core. From our restaurants-partners, cloud kitchens and cloud grocery stores — foodpanda is just one tap away, delivering everything you need quickly and conveniently to your doorstep.

We are looking for a (Senior) Data Engineer to join our growing data team to help Foodpanda make smart, data-driven business decisions. If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!

What's on your plate:
• Defines and builds data pipelines that will enable faster, better, data-informed decision-making within the business
• Architect, build, and deploy new data models that provide intuitive analytics across the business
• Manage and own the entire data processing system (internally, as well as interfacing with other teams or third party services), provide advice on any necessary infrastructure changes
• Experiment with, select, and implement cutting edge Big Data tools and frameworks required to provide requested capabilities
• Perform all needed data transformation to populate a reporting optimised data warehouse
• Mentoring junior team members through code review and enablement training
• Participate in building machine learning models, deliver analyses and insights to support business decision making

What you bring to the table:
• More than 3 years experience in data analytics / engineering
• Ability to write clean, structured, and high performance SQL and Python code
• Strong experience with big data, Data Warehouse technologies
• Strong oral and written communication skills
• Strong business mindset and ability to grasp business requirements from stakeholders
• Knowledge of Machine Learning is a plus
• Experience in machine learning operations, or setting up environments for data scientists/machine learning engineers would be advantageous

What we can offer you:
• A vibrant and international team with multi-cultural and diverse backgrounds.
• Solving challenges with inspiring colleagues in an all hands-on deck environment.
• Management team that recognizes top performers, welcome our newbies, and shares a love for good food.
• Competitive package, incentives, allowances, food perks, insurance, pension and more"
29-Apr-2022 T11:53,Cloud Data Engineer for IT Data Analytics Team,Garranto Pte. Ltd.,2 days ago,,Full–time,"Type : Full Time / Permanent Role

Location : Singapore

Job Description ï'·
• Act as a subject matter expert in data engineering and GCP data technologies. ï'·
• Closely work with various business teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
• Work with Agile and DevOps techniques and implementation approaches in the delivery. ï'·
• Be required to showcase your GCP Data engineering experience when communicating with business team on their requirements, turning these into technical data solutions. ï'·
• Be required to build and deliver Data solutions using GCP products and offerings.

Qualifications
• Liaise and be part of our extensive GCP community, contributing in the knowledge exchange learning programme of the platform.
• Any Bachelor Degree in Computer Science or related fields
• Minimum 5 years of experience as a data engineer in banking environment.
• Possess analytical skills mental resilience and the ability to think systematically under stressful conditions.
• Highly accountable and takes ownership. Outstanding work ethic, highintegrity, team player, and a lifelong learner.
• Mentor other engineers define our technical culture and help build a fast-growing team.

Skill
• Hands on and deep experience working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, AI Building Blocks, Looker, Cloud Data Fusion, Dataprep, etc.).
• Experience in Spark / Scala / Python / Java / Kafka.
• Experience in MDM, Metadata Management, Data Quality and Data Lineage tools.
• E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.
• Regulatory and Compliance work in Data Management.
• E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
• Experience with SQL and NoSQL modern data stores.
• Work on diversified cloud Platforms consisting of Databases and CICD, Logging, and monitoring tools to provide stable, and reliable DevOps service.
• Hands-on experience with terraform is a plus.
• Build CI / CD pipeline; both design and implementation is an added advantage.

If you are interested please send your CV to HIDDEN TEXT"
29-Apr-2022 T11:53,Data Engineer,TreeDots,3 days ago,,Full–time,"Job description

& requirements

Job Description

TreeDots is currently hiring for a Data Engineer with strong experience in building and maintaining data pipelines.

At TreeDots we drive 3 products, respectively related to social commerce, logistics, and b2b e-commerce. Our vision is to ensure a holistic delivery of agrifood from source to merchants and end-consumers. We believe that environmental issues coming from food waste are possible to solve and those are the main goals of our products.

Duties and Responsibilities

You will be responsible for collecting, managing, and converting raw data into usable information for stakeholders to interpret. You will be responsible for creating the data flows between the data source to the data warehouse and finally to use visualization tools to display prepared data-sets.

Mandatory requirements

● 2-4+ years of experience as a Data Engineer
● Experience with building optimized SQL queries
● Experience with Google Cloud Platform, especially BigQuery and Google Data Studio
● Understanding of the SDLC best practices
● Familiar with API queries and at least 1 programming language
● Familiar with git and Github

Required skills

Git Github SQL"
29-Apr-2022 T11:53,"Data Engineer (5 days, Orchard, $3500-4000)",MACHSPEED HUMAN RESOURCES PTE. LTD.,9 hours ago,,Full–time,"Leading System Integrator company providing IT solutions for network, IT infrastructure, analytics, cyber security and managed services.

Job Responsibilities
• Design, engineer, configure and administer BI project based on given functional and technical requirements
• Collaborate with pre-sales, project and relevant internal teams to deliver the optimal technical solution to client's business problems
• Develop real-time and batch data ingesting and processing pipelines to be used for analysis, machine learning, dashboards, alerts and visualizations
• Review and monitor ETL tasks and performance
• Support testing and deployment
• Provide recommendations and implementation changes to optimize in the customer environment.
• Write and develop custom scripts as needed

Requirements
• Minimum Degree/Diploma in Computer Science, Engineering or IT with at least 2 years of relevant experience
• Hands-on experience in scripting/programming
• Possess knowledge in Networking and Servers(Windows and Linux)
• Possess CEH/ECSA/CISSP/ECSA/CompTIA Security+ certification will be an added advantage
• Working hours: 9am - 6pm, Mondays to Fridays
• Salary range: $3500 - 4000

We thank you for your interest in this position, applicants with relevant experience please send Microsoft word resume with expected salary to

apply@machspeed.com.sg

for faster processing, alternatively, please visit https://www.facebook.com/MachspeedHR for more jobs available, thank you.

You may also call +6563362530 (Look for BingCheng) to find out more

Thank you very much.

Agency License No. 12C6200

EA Registration No: R1437671"
29-Apr-2022 T11:53,Senior Data Engineer,Kkr Singapore Pte. Ltd.,3 days ago,,Full–time,"Position Summary We are looking for a savvy Data Engineer to be based in Hong Kong / Singapore, to join our growing team of data and analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced cloud-based services, data architecture, data engineering, data pipeline development (ETL), and analytical tools. We are seeking an engineer who enjoys optimizing data systems and building enterprise scale data services. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives. Skill / Experience Required Bachelor's Degree in Computer Science/Engineering or a related discipline. 10+ years Development experience Experience in Python and related open source modules Experience in Python development including Web application frameworks such as Flask / FAST API Experience working with RESTful API Services Strong database skills with a thorough understanding of relational database and understanding of Object Oriented databases Exposure with the AWS Stack / RDS / is preferred Knowledge of open source solutions and trending technologies Good communication and written skills Ability to be self-sufficient and proactive individual contributor Exposure to Private/Public Markets Desirable Understanding of Object Oriented Programming and Design Patterns Knowledge of web standards, security, accessibility, browser compatibility Knowledge of JavaScript, HTML5 and awareness of frameworks such as React.js/Vue.js Experience in a Business Intelligence tool e.g. Tableau and Dremio Exposure to ML / AI and python libraries (pandas, NumPy, SciPy) and concepts"
29-Apr-2022 T11:53,Senior Data Engineer (Data Engineering),GVT Government Technology Agency (GovTech),2 days ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Senior Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you will be building Whole-of-Government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will be architecting, designing and building next-generation data infrastructure to galvanise digitalisation in the public sector. You will be given opportunities to lead other engineers to drive impact at scale.

We are looking for enthusiastic and passionate engineers to join us in this journey to make a difference.

What you will be working on:
• Design and build resilient and efficient data pipelines for both batch and real-time streaming data
• Architect and design data infrastructure on cloud using industry standard Infrastructure-as-Code tools
• Execute projects with an Agile mindset
• Build software frameworks to solve data problems at scale
• Collaborate with product managers, software engineers, data analysts and data scientists to build scalable and data-driven platforms and tools
• Be put in the driving seat as an engineering leader

What we are looking for:
• Bachelor’s Degree in Computer Science or have equivalent professional experience
• Have more than 4 years of experience in a technical role
• Experience with data processing tools such as Spark, Beam, Flink
• Experience with the cloud (e.g. AWS, GCP, Azure)
• Experience implementing batch and streaming data pipelines
• Experience writing efficient SQL
• In-depth knowledge of both SQL and NoSQL databases, including performance tuning and troubleshooting
• Familiar with DevOps tools such as Git, Docker, Terraform
• Experience in the public sector is a bonus
• Previous technical leadership experience is a bonus

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round.
• Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:53,Senior Data Engineer,GENPACT CONSULTING (SINGAPORE) PTE. LTD.,2 days ago,$8K–$10K a month,Full–time,"• Work closely with the Product Owners and stake holders to design and build data systems and pipelines to meet the requirements of the proposed solution.

• Play an active role in leading team meetings and workshops with clients.

• Oversee day-to-day Data Engineering team’s operation and performance.

• Interact directly with clients to understand project requirements and deadlines.

• Evaluate business needs and objectives.

• Designing and implementing highly performant data ingestion pipelines from multiple sources using SQL, Python, Apache Spark and Databricks.

• Responsible for deploying codes using Git/Bitbucket as per the CI/CD process.

• Design and Build datasets on Snowflake for faster reporting.

• Design Develop Maintain ETL data pipelines.

• Analyze and organize raw data.

• Research, Diagnose, and Monitor Performance Bottlenecks, etc.

• Ensure standardization of SQL coding practices and adherence to coding standards, change control, and SQL best practices

• Prepare data for prescriptive and predictive modelling.

• Responsible for data integration and data quality

• Enabling data from different sources into ready to consume datasets.

• Performing Data Validation/Exploration tasks.

• Migrating current lake data/ external data into Exasol/Databricks (Lakehouse)

• Helping business stakeholders to visualize and analyze the customized view/data and enable better decisions without any hassle.

• Partnering and coordinating with cross functional stakeholders across timezones"
29-Apr-2022 T11:53,Data Engineering (Internship),Endowus,2 days ago,,Full–time,"About us
Endowus is Asia's leading fee-only digital wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping all investors grow their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.
Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones. Our mission is clear: help people invest better so they can live easier today, and better tomorrow.

The people you will work with
The team has deep domain knowledge in finance and technology, bringing together decades of experience at Goldman Sachs, Morgan Stanley, UBS, Credit Suisse, The Blackstone Group, AQR, Grab, Dropbox, Lyft, Redmart, Carousell, Bytedance, Grab, Kakao, Alibaba, and more See our leadership team here . We practise inclusion and treasure our diversity in background and experience. A diverse team is our biggest asset and we look for people who share our belief in Endowus' clear mission.

Investors, recognition, licensing
Endowus is backed by global leading strategic and venture capital investors including UBS, Samsung Ventures, EDBI, Prosus Ventures, ZVC, Singtel Innov8, Lightspeed Venture Partners, and SoftBank Ventures Asia.

Endowus has been recognised by the industry with the following awards: Singapore's Rising Star and Fintech Innovation (Asia Asset Management's Best of the Best Awards 2021), LinkedIn Top Start-ups 2021, WealthTech of the Year (Asia FinTech Awards 2021), and the top 15 Singapore's Best Workplaces Award (Great Place to Work).
Endowus is licensed by the Monetary Authority of Singapore (MAS).

About Data Engineering Intern Role
• We are looking for a Data Engineering Intern who thrives in a fast-paced environment and enjoys driving innovation through rapid prototyping and iterative development.
• Being part of Endowus's Data Engineering team, you will shadow a senior colleague and support in building end-to-end product features that the team is working on.
• You will need to leverage the entire technology stack to realise these goals and will be part of a team that is constantly tackling difficult questions of scale, architecture, and interaction.
• You will experience real-life day to day operations as a valued member of the team Requirements:
• Pursuing Bachelor Degree in Computer Science, a related field, or equivalent professional experience
• Able to commit to a 10 weeks summer internship.
• Able to work independently while being a proactive team player with excellent communication skills.
• Some fluency with hands-on software development and strong understanding of SDLC activities.
• Ability to analyse data (e.g. using SQL or Excel)
• Knowledge of CI/CD practises is a plus.
• Knowledge in at least one of the JVM languages such as Java, Scala is a plus
• Motivated, positive attitude, responsible and proactive.
• Passionate in learning and working with new technologies. Nice to haves
• Basic knowledge of finance and trading Benefits & perks
• Stock options in a fast growing company *
• Employee discount for investing at Endowus
• Flexible working hours and locations so you can live life and your best work - you are trusted to be responsible
• Competitive Staff Benefits; Annual Leave, Medical insurance, Dental *
• Productivity Equipment; Omnidesk Standing Desk & Ergonomic Chairs, Dell UltraSharp USB Type-C Monitor, high-end computer peripherals
• Choice of Apple or PC equipment
• Additional S$250 /HKD 1450 budget for your favourite hardware to boost productivity * Note: * is only applicable to Full-Time employees"
29-Apr-2022 T11:53,Senior Data Engineer,Selby Jennings,23 days ago,,Full–time,"A global multi-manager platform with pods of internal and partner portfolio managers, are searching for a Senior Data Engineer to join their Singapore office. This firm work across quantitative, fundamental equity and tactical trading strategies with over 30+ years of experience globally. They focus on investing in their own proprietary technologies so are able to be compete to be one of the best in their field.

The Senior Data Engineer will be responsible for...
• Maintaining, enhancing and optimising the data platform
• Creating new data processing pipelines
• Improving data processes including integrating new data formats, analytical tools etc
• Collaborate with Portfolio Managers, Data Scientists and Data Ops for data sets

The Senior Data Engineer should have the following...
• Strong knowledge and experience with python scripting
• Experience with Financial Data and Big Data
• Experience building, deploying and running on cloud (AWS Preferred)
• Mentoring junior engineers

For further information about this position please apply for this Senior Data Engineer in Singapore"
29-Apr-2022 T11:53,Data Engineer – SQL,AiDA Technologies,Full–time,,,"[vc_row css_animation="""" row_type=""row"" use_row_as_full_screen_section=""no"" type=""full_width"" angled_section=""no"" text_align=""left"" background_image_as_pattern=""without_pattern""][vc_column][vc_column_text]
Job Title: Data Engineer

Industry: Computer Software, Financial Services, Information Technology & Services
Employment Type: Full-time
Job Location: Singapore
Seniority Level: Entry level
Job Functions: Information Technology

Job summary

We are looking for a Data Engineer who will work on the data transformation and system deployment. The primary focus will be on understanding data science solutions of a particular problem, implement data transformations required, as web APIs . You will also work with a team of highly motivated members and integrate the solution in client environments.

Responsibilities and duties
• Writing SQL queries
• Linking different tables into one table based on a provided schema
• Implementing data transformation (including ETL) for machine learning modelling
• System integration and testing

Qualifications and skills (requirement)
• Diploma, B.Eng or BS
• Hand-on experience working with SQL-based technologies on traditional RDBMS, such as Oracle, Postgresql or MySQL
• Proficiency in Python programming or Java

Qualifications and skills (good to have)
• Database System Design
• Basic Knowledge on Machine Learning
• Experience with AWS services including S3, Redshift, EMR and RDS.
• DevOps tools such as Docker
• UI development

Benefits and perks
• An exciting learning opportunity at a fast paced startup with open and friendly colleagues
• Competitive salary based on skills & experience with excellent insurance and other benefits.
• Opportunity to work with best data scientists in the world,including Kaggle Grand Master
• Deliver high impact solutions for prestigious clients across ASEAN, India, and Hong Kong.

[/vc_column_text][/vc_column][/vc_row"
29-Apr-2022 T11:53,Data Engineer,Persolkelly Singapore Pte. Ltd.,3 days ago,,Full–time,"Responsibilities
• Design and implement relevant data models in the form of data marts stored in Operational Data Stores, Data Warehouses or Big Data platforms
• Build data pipelines to bring information from source systems, harmonise and cleanse data to support analytics initiatives for core business metrics and performance trends.
• Perform data profiling to understand data quality and advise practical measures to address such data issues through data transformation and data loading
• Dive into company data to identify sources and features that will drive business objectives.
• Work closely with project manager and technical leads to provide regular status reporting and support them to refine issues / problem statements and propose / evaluate relevant analytics solutions

Requirements :
• Prior experience building large scale enterprise data pipelines using commercial and / or open source data management tools from vendors such as Informatica, Talend, Microsoft, IBM or Oracle
• Strong knowledge of data manipulation languages such as SQL necessary to build and maintain complex queries and data pipelines
• Practical appreciation of data quality metrics and remediation strategies
• Data modelling and architecting skills including strong foundation in data warehousing concepts, data normalisation, and dimensional data modelling such as OLAP
• Undergraduate or graduate degree in Computer science or equivalent

To Apply :

Interested candidates, who wish to apply for the above position; please send in your resume to HIDDEN TEXT or click the ' Apply Now ' below and ' ATTN : BVIN '

We regret that only shortlisted applicants would be notified.

B Vidita Nantini REG No : R22105644

PERSOLKELLY SINGAPORE PTE LTD EA License No : 01C4394

By sending us your personal data and curriculum vitae (CV), you are deemed to consent to PERSOLKELLY Singapore Pte Ltd and its affiliates collecting, using and disclosing my personal data for the purposes set out in the Privacy Policy which is available at www.

persolkelly.com.sg I also acknowledge that I have read, understood, and agree to the said Privacy Policy"
29-Apr-2022 T11:53,Data Engineer,Huxley,1 day ago,,Full–time,"Huxley is currently working with a digital assets company that is rapidly expanding in the region. This is an exciting opportunity for highly motivated individuals keen to work in a busy environment and contribute with ideas.

We are currently looking to fill 2 Data Engineer positions focusing on data modelling and risk related projects:

Data Engineer should have the following:
• 3+ year of experience developing machine models
• Proficient in either Python, Java and/or Scala
• Master's Degree with working experience OR Phd
• Understanding of deep learning is a plus

Data Engineer will be responsible for:
• Building Models reliable on machine learning, graph mining and other data driven technologies. Identify potential threats in KYC, Payments, Credit, Reward etc
• Maintain the Models to meet business expectations.
• Work on data that analyses customer feedback and satisfaction analysis by leveraging on machine learning techniques.
• Collaborate effectively with various stakeholders

Please note that this opportunity is open for Singaporeans only.

If you are interested in the above-mentioned Data Engineer role located in Singapore, do contact me and attach your latest CV. I will be glad to provide additional information and guide you through the next steps where necessary.

Consultant Details:

Anastasija Horoscuka

EA License No.: R2093146

Huxley is a trading division of SThree Pte Limited

(Registration Number: 200720126E | SThree Pte Limited Licence Number 16S8216 | Huxley Licence Number 53132076J)

As per Ministry of Manpower (MoM) requirements, if you're suitable for any roles that we will be putting you forward for, we will need to request for your identification number. Please be assured that this will not be disclosed with our client or any other parties other than MoM.

Award winner of:

Recruitment Agency of the Year by Asia Recruitment Awards 2019 | Best Client Services by Asia Recruitment Awards 2017 | Best Overseas Operation by Global Recruiters 2017 | Highly Commended for Best Large Recruitment Business 2017 | Commended for Best In-House Training by Global Recruiters 2017"
29-Apr-2022 T11:53,"Data Engineer, MBG",Meta,2 days ago,,Full–time,"Meta is looking for exceptionally talented and experienced engineers to join the MBG Technology team. Our team provides analytics and workflow tools for Meta Business Group ( MBG), partnering with sales, marketing, measurement, support and operations teams.In this role, you’ll work with some of the brightest minds in the industry, work with one of the richest data sets in the world, use cutting edge technology, and get an opportunity to solve some of the most challenging business and engineering problems, at a scale that few companies can match. You will do so by partnering with stakeholders/teams and building scalable, reliable solutions that provide business critical insights and metrics, while ensuring the best uptime and responsiveness.

Data Engineer, MBG Responsibilities:
• Manage data warehouse plans for a business vertical or a group of business verticals
• Build data expertise and own data quality for allocated areas of ownership
• Design, build, optimize, launch and support new and existing data models and analytical solutions
• Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions
• Conduct design and code reviews
• Work with data infrastructure to triage infra issues and drive to resolution
• Manage the delivery of high impact dashboards, tools and data visualizations

Minimum Qualifications:
• 2+ years of experience in the data warehouse space, custom ETL design, implementation and maintenance
• 2+ years of experience in SQL or similar languages, and development experience in at least one language (Python, PHP etc.)
• Experience with data architecture, data modeling, schema design and software development
• Experience in leading data driven projects from definition through interpretation and execution
• Experience with large data sets, Hadoop, and data visualization tools
• Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience working in support of diverse communities"
29-Apr-2022 T11:53,Data Engineer,KS CONSULTING PTE. LTD.,24 days ago,$5.5K–$8K a month,Full–time,"Our client, a leading financial services company is looking to hire a Data Engineer with excellent communication skills and experience in data modelling and optimizing ETL jobs.

Responsibilities:
• Manage data modeling design, writing, and optimizing ETL jobs
• Participate in building and enhancing enterprise cloud data warehouse
• Deliver and manage in-house and cloud-native data solutions to meet business requirements across firm-wide business units
• Design and implement Data and AI central data platform as well as related tools/systems for advanced business analytics and enterprise data governance
• Assist in creating and monitoring analytics dashboards, for different business functions
• Ensure quality, integrity and accuracy of datasets through tracked, secured and auditable controls

Requirements:
• Hands-on experience with Linux and shell scripting
• Working experience with containerization (Docker/K8S) and task orchestration tools (Airflow/Luigi, etc.)
• Experience with cloud service and tools (AWS/Azure/GCP), as well as cloud data warehouse platforms.
• Experience with modern DevOps practices including version control, TDD, CI/CD, etc., for both code and configuration changes.
• Excellent coding skills with Python and SQL, and solid understanding of object-oriented analysis and design
• Working knowledge of common algorithms and data structures, with strong analytical and problem-solving skills
• Basic understanding and experience with ML/AI concepts (e.g. deep learning, deep reinforcement learning, deep bayesian learning), workflows, and toolsets (Jupyter Notebook, etc.), and libraries (Numpy, Pandas, Scikit Learn, PyTorch, etc.) preferably in both cloud-native and desktop deployments.
• Experience with traditional RDBMS based systems, including Data Lake, Data Warehouses and Marts, and more modern NoSQL and cloud-native big-data

If you or anyone within your network is keen to discuss it further then please share your resue with manisha@kstalentsolutions.com"
29-Apr-2022 T11:53,"Manager, Product Development (Data Engineer)",MasterCard,Full–time,,,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion (https://www.mastercard.us/en-us/vision/who-we-are/diversity-inclusion.html) for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Manager, Product Development (Data Engineer)

Scope

Create and maintain optimal data pipeline architecture

Participate in development of data and analytic infrastructure for product development

Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.

Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.

Keep up to date with latest open source tools for data engineering

Expert knowledge in ML lifecycle

Qualifications

At least 4 years of experience as data engineer, building and deploying data pipeline

High proficiency in using Python/Scala, Spark(tuning jobs), Hadoop platforms to build Big Data products & platforms

Experience with data pipeline and workflow management tools: NIFI, Airflow.

Experienced with Agile frameworks, e.g., Scrum.

Experience with open source tools to build data pipeline, deploying and monitoring the algorithm

Comfortable in developing shell scripts for automation

Comfortable in creating CI/CD pipeline for testing, deploying and monitoring algorithm

Motivation, flexibility, self-direction, and desire to thrive on small project teams

Experience with the visualization tools like tableau, looker

COVID-19 Considerations

We value the safety of each member of our community because we know we’re all in this together. In many locations, which may change over time, we’ve implemented a virtual hiring process and continue to interview candidates by video or phone. In addition, in some locations, only individuals who have been fully vaccinated will be permitted inside Mastercard offices until further notice.

In the US, Mastercard is a government contractor, which may legally require most Mastercard employees to be vaccinated unless a verified approved medical or religious exemption is granted. Further, we are currently making every effort towards having employees return to work in the office 2 days per week, if that makes sense for their team. Everyone must be vaccinated to enter Mastercard offices at this time. Therefore, we expect all candidates to be vaccinated or to be approved for a medical or religious accommodation prior to commencing work at Mastercard.

Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.

Requisition ID: R-155294"
29-Apr-2022 T11:53,Data Engineer -Python ( 1 year extendable contract),Michael Page,$60K–$78K a year,,Contractor,"About Our Client

Our client is a renowned name in the Banking industry and is currently seeking for data engineers to design,develop and implement data applications

Job Description
• Solid hands on experience in writing Python scripts for data processing and analytics use cases.
• Should have experience on building batch and streaming data pipelines.
• Extensive experience in implementing scalable Data pipeline using Python or R/Scala, SQL.
• Exposure to core banking system, good understanding of Private banking product's life cycle
• Participate in reviews and meetings and provide updates on project progress
• Take responsibility for ensuring that risks and issues are identified and managed closely and drive all stakeholders to deliver on time and to the required quality standards
• Ensure production stability and timely communication to stake holders

The Successful Applicant
• Minimum 3 years of experience in Designing and developing Data applications using python and other programming languages.
• Bachelor's degree in information technology or computer science;
• Experience building APIs using python libraries.
• Experience working with structured and unstructured data, SQL scripting, Kafka streaming.
• Good to have an understanding of Kafka messaging system.
• Experience working in a Private Banking environment is preferred.
• Experience working on building data pipelines for Data and analytics aplications or a Data Warehouse system

What's on Offer

You will be part of an organisation who sees value in investing in their employees. Stability in your career is a key for them.The remuneration for this role will be competitive and in line with the market"
29-Apr-2022 T11:53,Big Data Engineer,Salt,22 days ago,$8.5K–$9.5K a month,Contractor,"SALT is hiring Big Data Engineer for a global technology client on a two-year convertable contract role.

You will be responsible for
• Responsible for the monitoring and uptime of all production and Non-Production Big Data Stack.
• Strive to improve the stability, security, efficiency, scalability, and availability of production systems by applying software engineering practices and by implementing monitors and alerts.
• Estimate Bigdata stack capacities; develop methods for monitoring capacity and usage.
• Lead efforts to develop and improve procedures for automated monitoring and proactive intervention, reducing any need downtime.
• Provide and drive architecture and the design of Big Data solutions
• Design and implement high performance systems supporting large volume real time and batched data processes
• Propose and design data architecture solutions for scalability, high availability, fault tolerance, and elasticity for various application needs.
• Develop ETL pipelines with robust monitoring and alarming
• Develop data models that are optimized for business usability and understanding
• Develop and optimize data tables using best practices for partitioning, compression, parallelization, etc.
• Optimize SQL and ETL solutions to solve various reporting requirements.

You should be coming from
• Overall 8+ years of relevant industry experience
• Good knowledge of big data technology landscape and concepts related to distributed storage/computing.
• Experience with big data frameworks (e.g., Hadoop, Spark) and distributions (Cloudera, Hortonworks, MapR).
• Experience with batch & ETL jobs to ingest and process data from multiple data sources.
• Experience with NoSQL databases (e.g., Cassandra, MongoDB, Neo4J, Elasticsearch, InfluxDB)
• Experience with querying tools (e.g., Hive, Spark SQL, Impala).
• Experience in real-time stream processing, using solutions such as Kafka, AWS Kinesis, Flume, and/or Spark Streaming.
• Experience in DevOps and DataOps principles (e.g., Infrastructure as code, automating different parts of the data pipeline).
• Monitor and evaluate Big Data stack performance and adjust configuration as appropriate
• Participate in disaster recovery design, testing and execution

If this sounds like your ideal next move, I want to talk to you! Reach out to me at jvenkataraman@welovesalt.com or apply via the posting.

CEI No: R1659595 / Licence No: 07C3147

Salt is acting as an Employment Business in relation to this vacancy.

Job Information
Job Reference: JO-2204-253832
Salary: Negotiable
Salary per: month
Job Duration:
Job Start Date: 01/06/2022
Job Industries: Technology
Job Locations: Singapore
Job Types: Contract

Apply for this Job

Name *

Please enter your full name.

Email *

Enter a valid email address.

Upload a CV *

Upload your CV to accompany your application for this job.

Please tick this box to consent to us using your data. How we use your data is outlined in our privacy policy *

Fields marked with * are required"
29-Apr-2022 T11:53,Azure Data Engineer,Avanade,18 mins,,Full–time,"As a Data Engineering Consultant at Avanade, you will use modern data engineering techniques to build Azure based data pipelines and platforms to support Avanade's customers in making better business decisions.

In this role, you will be engaged in the Data Engineering role family inside the Analytics Talent Community. You will design and build data pipelines, data platform, dashboarding and reporting solutions, data consumption APIs and a variety of other solutions and tools for end users. You will be a critical point within the data supply chain, while ensuring that all stakeholders have access to and can work with the latest data.

In line with an always increasing market demand, Avanade is looking to expand its Analytics team with experienced Data Engineer (but not only, see other job postings) to work on state-of-the-art data platform on the Azure cloud.

""In addition to the opportunity to work on great projects with the latest technologies, Avanade is distinguished above all by its informal culture and a strong us-feeling. Everyone is willing to help each other, is approachable and frequently there are evenings or days organized for training, knowledge sharing or team bonding. With a good mix of working hard on fun and challenging projects and attending fun events organized for colleagues, Avanade is a Top Employer for a reason."" - Robin - Data Engineer at Avanade.

You make the difference because:
• You like to work with the latest tools & techniques in the field of data to help the largest companies in Belgium (or other countries, if you consider it);
• Microsoft Azure Data & Analytics PaaS & SaaS Services such as Databricks (Spark), HD Insight, Data Factory, Data Lake, CosmosDB, SQL and DevOps have no/little secrets for you, or you have a similar technology stack expertise that can be transposed;
• In addition, you have experience with data modeling/warehousing techniques and data integration;
• You also have advanced knowledge of at least one programming languages (Python, Scala or Java);
• CI/CD, VCS, Parallel Processing (MPP) and NoSQL are not abstract concepts to you;
• You have at least 3 years of relevant work experience, or can justify a similar expertise;
• You have a Bachelor’s or Master’s degree in Computer Science, Mathematics, Statistics or another relevant field;
• You speak English, preferably also Dutch or French.

Why should you choose Avanade?
• At Avanade we continuously invest in the development of your skills. We help you to keep up to speed with the latest in technology and business.
• Besides an exciting international environment, a unique chance to participate in challenging and state-of-the-art projects and interaction with inspiring colleagues who have a professional attitude and attach considerable importance to teamwork; we stand for:
• A competitive salary with additional benefits, such as a company car, phone, insurances.
• A very strong training & development focus, with an internal University and ample opportunity to learn and grow.
• A personal career plan, controlled by you and guided by Avanade.

Do you feel the match?

Do you recognize yourself in this description and would you like to get to know us? Then apply immediately by sending in your CV via our website. Applying by e-mail is not possible. If we see a match, we will call you quickly for an introduction and you will have 2 job interviews. Do you have any questions or want to know why we love working at Avanade? Then contact our recruiter Eline Rubertus"
29-Apr-2022 T11:53,Staff Data Engineer,Twitter,3 days ago,,Full–time,"Who We Are :

Twitter users generate many terabytes of data every day; Twitter engineers run hundreds of experiments; Twitter Data Engineers build data pipelines and data processes that calculate metrics and scale increasingly sophisticated models of users and content.

The Data Science team at Twitter is at the intersection of all this data and strives to make it actionable to all business units around Twitter.

Data Engineers work alongside Data Scientists analyze this data via observational analyses, trend analyses, modeling, and new measurement strategies.

We also implement metrics to track the impact of new product experiments and more generally find ways to make very large scale data approachable to guide our decisions.

What Youll Do :

Twitter has very large and complex datasets. As a Twitter Data Engineer you will build datasets and make them accessible to our partner teams by writing great production code to simplify the complexity.

Your work will enable Product Managers and other decision-makers across the company to bring together insights and inform our product and strategy.

In every decision that you influence, you will see the product improve and be more valuable to Twitter users.

We are trying to improve Twitter. To improve something, we need to be able to measure it. As a Data Engineer you will enable better measurements and ensure measurement accuracy so that we know where we are doing well and where we want to improve.

As such, you will :

Design, develop, and launch extremely efficient and reliable data pipelines to move data and to provide intuitive analytics to our partner teams.

Make Twitter-scale data more discoverable and easy to use for Data Scientists and Analysts across the company.

Collaborate with other engineers and Data Scientists to discover the best solutions.

Support your colleagues by reviewing code and designs.

Diagnose and solve issues in our existing data pipelines and envision and build their successors.

Who You Are :

You want to be part of a community of the most talented, forward-thinking Data Scientists and Engineers in the industry.

You are a strong Scala or Java developer. You demonstrate clear and concise communication and data-driven decision-making.

You are passionate about learning or growing your expertise in some or all of the following :

Qualifications

B.S. and / or M.S. in Computer Science or a related technical field, or equivalent experience

8+ years of experience in either data infrastructure or backend systems

Strong understanding of SQL

Broad knowledge of the data infrastructure ecosystem

Experience with Hadoop or other MapReduce-based architectures

Experience working with large data volumes

Good understanding of one or more of the following : Scala, C++, or Java

Experience with any of the following is a plus :

Scalding

Full Stack Development

Company Description

Twitter serves the public conversation because conversation is a force for good in the world. The opportunity to help the world connect, debate, learn, and solve problems is what draws us to careers at Twitter, and its what keeps us here.

Additional Information

We are committed to an inclusive and diverse Twitter. Twitter is an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, age, disability, veteran, genetic information, marital status or any other legally protected status.

San Francisco applicants : Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records"
29-Apr-2022 T11:53,"Senior Data Engineer, Global Business Intelligence",Apple,16 days ago,,Full–time,"Summary
Posted: Apr 14, 2022

Role Number:200366968

At Apple, new ideas have a way of becoming extraordinary products, services and customer experiences. Bring passion and dedication to your job and there's no telling what you could accomplish!

The people here at Apple don’t just create products — they create the kind of wonder that’s revolutionised entire industries. It’s the diversity of those people and their ideas that inspires the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it!

The Global Business Intelligence team provides data services, analytics, reporting and data science solutions to Apple’s business groups including Retail, AppleCare, Operations, Finance, Sales, Marketing, Apple Media Products and external partners. We lead enterprise data stores with petabytes of data, process billions of rows of data in batch and real-time to provide solutions to our users helping them gain insight and to make informed business decisions.

We are looking for a Senior Data Engineer to join our team. You will engage directly with key business partners to understand the business strategies and solution needs, drive and lead technical design discussions with development teams. You should be passionate about data and its nuances, and enjoy getting hands-on with technical solution design and development work. In Apple, we leverage a diverse technology stack such as Teradata, PostgreSQL, Snowflake, SingleStore, HANA, Spark, Cassandra and beyond. Designing, developing and scaling solutions using these technologies are a core part of our daily job.

Key Qualifications

Key Qualifications
• Experience in designing and building dimensional data models to improve accessibility, efficiency and quality of data.
• Hands on database development experience with Relational or MPP/distributed systems such as Snowflake/ Teradata/ SingleStore/ Hadoop
• Programming experience in building end to end data pipelines with SQL, Python and/or Scala
• Proficient in writing Advanced SQL with expertise in performance tuning
• Experience working with data at scale
• Ability to communicate effectively, both written and verbal, with technical and non- technical teams
• Strong understanding of development processes and agile methodologies
• Experience with data science and machine learning tools as well as cloud technologies is a plus

Description

Description
Design and build data structures on platforms such as Snowflake, and highly scalable data pipelines using technologies such Spark and Kafka to provide efficient reporting and analytics capability,

Build data and reporting solutions (in platforms such as SingleStore) that are highly optimised for fast data access

Translate complex business requirements into scalable technical solutions meeting data warehousing design standards. Solid understanding of analytics needs and proactiveness to build generic solutions to improve overall efficiency.

Strong understanding of analytics needs and a proactive approach, focusing on reusable solutions

Lead/work with many global teams, communicate effectively, both written and verbal, with technical and non-technical multi-functional teams.

Keep up-to-date on the newest technology solutions in market to generate innovative ideas to solve business challenges.

We seek a self starter, forward-thinking person with strong leadership capabilities.

Education & Experience

Education & Experience

Additional Requirements

Additional Requirements
• Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.
• We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation"
29-Apr-2022 T11:53,Data Engineer,Raffles Medical Group,1 day ago,,Full–time,"Data Engineer, Raffles Medical Group

Raffles Medical Group (RMG) is a leading integrated regional healthcare provider, listed on the Singapore Stock Exchange (SGX:BSL). Our regional presence spans 14 cities with medical facilities in Singapore, China, Japan, Vietnam and Cambodia.

At RMG, you’ll be part of a team who is focused on delivering high quality healthcare experiences to our broad spectrum of clients. We strive for an environment where we inspire one another, encourage you to achieve your best, and importantly, share success together. We create opportunities to develop your skills and chart your career with us.

We are looking for a Data Engineer to build data infrastructure to power the insights needed for evidence-based decision-making and enhance service-delivery. You will be architecting, designing and building next-generation data infrastructure to drive digital transformation in the heath sector.

Key Responsibilities/Duties

Design and build resilient and efficient data pipelines for both batch and real-time streaming data

Architect and design data infrastructure on cloud using industry standard Infrastructure-as-Code tools

Execute projects with an Agile mindset

Build software frameworks to solve data problems at scale

Collaborate with product managers, software engineers, data analysts and data scientists to build scalable and data-driven platforms and tools

Obsess over cyber security by identifying threat & risk, ensuring all the components, from a platform, frameworks to the applications are fully secure and are compliant to infosec policies

Develop metrics to measure and monitor client and server API health and utilization

Requirements

Degree in Computer Science or have equivalent professional experience

At least 3 years of relevant experience in developing scalable, secured, fault tolerant, resilient & mission-critical data platforms

Experience with data processing tools such as Spark, Beam, Flink, Airflow, Kafka, dbt

Experience with the cloud (e.g. AWS, GCP, Azure)

Experience implementing batch and streaming data pipelines

Experience writing efficient SQL

Proficiency in at least one of the programming languages Java, Scala, or Python along with a fair understanding of runtime complexities.

In-depth knowledge of both SQL and NoSQL databases, including performance tuning and troubleshooting

In-depth knowledge of data modelling, data warehouse, and data lake

Familiar with DevOps tools such as Git, Docker, Terraform

Familiar with MLOps tools such as MLFlow, Kubeflow

Passionate about working in healthcare and raising the bar on healthcare solutions in Singapore and the region

Thank you for your interest. We regret that only shortlisted applicants will be notified"
29-Apr-2022 T11:53,Data Engineer / Software Engineer - Data,Zoom,4 days ago,,Full–time,"Work Styles at Zoom

In most cases, you will have the opportunity to choose your preferred working location from the following options when you join Zoom: in-person, hybrid or remote. Visit this page for more information about Zoom's Workstyles.

About Us

Zoomies help people stay connected so they can get more done together. We set out to build the best video product for the enterprise, and today help people communicate better with products like Zoom Contact Center, Zoom Phone, Zoom Events, Zoom Apps, Zoom Rooms, and Zoom Webinar.

We’re problem-solvers, working at a fast pace to design solutions with our customers and users in mind. Here, you’ll work across teams to deliver impactful projects that are changing the way people communicate and enjoy opportunities to advance your career in a diverse, inclusive environment.

You will be part of a team whose focus is to solve cutting edge AI / MLproblems and deploy models that constantly advance the state-of-the-art. You will be working on designing the data infrastructure and solve other interesting bottlenecks that are challenging at Zoom's scale.

Responsibilities :

Design and implementation of data infrastructure for Machine Learning (ML) projects for our Research and Development Org

Assembling large, complex data sets and make the data more discoverable and easy to be used by the AI / ML team to support various Machine Learning initiatives. This includes unifying pre-processing work flows for data for different AI tasks.

Build and manage data sets, also by identifying and using suitable big data technologies, such as Apache Hadoop, DVC, Apache Drill,

Write tools to transform raw data sources into easily accessible models by coding across several languages such as Python, and SQL

Deploy, develop and maintain tools for data collection, data crawling and data annotation, e.g. as web applications using Django and Angular or similar technology.

Along with looking at external sources of data, build data expertise and own data quality for the pipelines you create.

Requirements :

Bachelor’s or higher degree in Computer Science or related fields

Experience in building and optimising data pipelines
• Docker
• Experience with AWS cloud services, e.g. S3
• Experience working with large data volumes
• Broad knowledge of the data infrastructure ecosystem
• Knowledge of technologies like DVC or Hadoop
• Good understanding of one or more of the following : Python, JavaScript, C++, Django or Angular

Please note : This job opportunity is based out of Karlsruhe, Germany

We believe that the unique contributions of all Zoomies is the driver of our success. To make sure that our products and culture continue to incorporate everyone's perspectives and experience we never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. Zoom is proud to be an equal opportunity workplace and is an affirmative action employer. All your information will be kept confidential according to EEO guidelines.

We welcome people of different backgrounds, experiences, abilities and perspectives including qualified applicants with arrest and conviction records and any qualified applicants requiring reasonable accommodations in accordance with the law. If you need any assistance or accommodations due to a medical condition, or if you need assistance accessing our website or completing the application process, please let us know by emailing us at careers@zoom.us.

Zoom requires all U.S. employees who will work in person at a Zoom office, attend in-person Zoom meetings or have in-person customer meetings to be fully vaccinated. Zoom will consider requests for reasonable accommodations for religious or medical reasons as required under applicable law.

At Zoom, we care about our employees, their families, and their well-being. As part of our award-winning workplace culture and commitment to delivering happiness, our benefits program offers a variety of perks, benefits, and options to help employees maintain their physical, mental, emotional, and financial health; support work-life balance; and contribute to their community in meaningful ways. To view our benefits, click here.

Explore Zoom:
• Hear from our leadership team
• Browse Awards and Employee Reviews on Comparably
• Visit our Blog
• Zoom with us!
• Find us on social at the links below and on Instagram
• View more jobs, sign up for job alerts and join our talent community. Visit the Zoom careers site.

#LI-Remote"
29-Apr-2022 T11:53,Software/Data Engineer#TeSA #CLT,Aida Technologies Pte. Ltd.,3 days ago,,Full–time,"Training Programme This Company-Led Training (CLT) programme is a 9 to 12 months on the job training with Aida Technologies Pte Ltd to equip fresh professionals with industry skills to become an Artifical Intelligence / Machine learning (AI/ML) Engineer. Each trainee will be trained and mentored in one or more of the following areas: AI and ML applications Data Management process Machine learning, deep learning techniques and tools Delpu AI/ML as a data product and/or data service Minimum Entry Requirements Singapore citizen Diploma/Degree in Computer Science or Engineering. Fresh and Mid-level professionals are welcome Application Process Interested individuals can apply directly with Aida Technologies Pte Ltd. Please email your CV to [Confidential Information]."
29-Apr-2022 T11:53,Data Engineer - HFT - Singapore- Leading Quant Fund | London,Oxford Knight,3 days ago,,Full–time,"Data Engineer - HFT - Singapore

Summary

The positive feel of a start-up with the benefits that come with a more established player, this leading HFT firm is looking for a dynamic software engineer to join one of their most successful quant trading teams. Collaborating extensively with traders and other technologists, you'll design, write and maintain a complex Python infrastructure. This role is as front office as developers come within the firm without being a Quant.

The ideal candidate will be passionate about: data, development, and keen to learn about automated electronic trading. You will be expected to solve difficult technical problems in a fast-paced and energetic environment, with a focus on processes that are robust, scalable and fault-tolerant. Most problems require high-availability, high-throughput and low-latency solutions.

The trading team see technology as a key component of their continued success and candidates will be exposed to cool, cutting-edge technologies.

Financial experience is not essential; just a willingness to learn, and learn quickly.
• **Interviews will be remote; ideally start dates will be in-house, but they're flexible.***

Requirements

• Excellent problem-solving skills, ability to make the right engineering decisions to achieve maintainability, extensibility and debuggability
• Solid knowledge of Python and Linux. (Some C++ preferable)
• Self-directed and able to take ownership of several projects at once
• Knowledge of SQL, Redis, InfluxDB. is a bonus
• Bachelor's degree (or higher) in Computer Science or Computer Engineering (or equivalent)
Benefits

• Competitive base salary + bonus; they will pay leading market rate / are flexible for the right candidate
• They're willing to be flexible with WFH
• Collaborative and rewarding work environment and culture
• Free breakfast , lunch and dinner

Contact
If this sounds like you, or you would like to know more, please get in touch.

Andy Stirling-Martin
andy@oxfordknight.co.uk
07453 28768
linkedin.com/in/andrew-stirling-martin-7664a946"
29-Apr-2022 T11:53,"Director, Data Engineering",Singtel Group,19 days ago,,Full–time,"Singtel, Asia’s leading communications technology group, provides an extensive range of telecommunications and digital services to millions of consumers and businesses across Asia, Australia, Africa and the USA. With over 140 years of innovation behind us, we continue to push boundaries in our networks and services, to enrich lives and transform businesses.

Our core values – Customer Focus, Challenger Spirit, Teamwork, Integrity, and Personal Excellence – shape the way we work. We are passionate about making a difference and have an open and inclusive culture where everyone is empowered to do their best. Our diverse business means you will enjoy unique opportunities and rewarding experiences to learn and grow your career in a dynamic industry.

Join us and experience what it’s like to be with an Employer of Choice*. Together, let’s create a brighter digital future for all. *Awarded at the HR Fest Awards 2020

About the role

As part of our expansion and increasing demands for Big Data projects within Singtel Group, we are looking for a Big Data Director to:
• Define and govern Big Data strategy, architecture and roadmap
• Drive, lead and align new capabilities in the area of Big Data and Data Integration across Singtel Group
• Lead and manage Big Data development and operations teams
• Track and monitor vendor’s SLA performance
• Plan and manage annual budget for both capital expenditures and operating expenses
• Develop multi-year IT investment roadmaps that link to the Group IT strategy
• Define standards and guidelines for development and operations
• Build, maintain and improve strong relationship with business leaders, Group IT domains (departments) and IT service providers to deliver value via data
• Represent Group IT at assigned project and / or program steering committees
• Present and pitch at relevant senior leadership levels and /or executive steering committees
• Drive Group IT and / or domain initiatives to the team
• Promote a DevOps culture through building relationships with development & operations and driving enhancements to the end-to-end release process
• Proactively advise the senior management team on the emerging technologies and digital trends that are most relevant to the company's goals and evolving needs
• Hire, develop, evaluate, reward, and retain a highly-qualified team

Responsibilities
• Develop, define and govern Singtel Big Data and data management architecture, guidelines and roadmap with stakeholders
• Build up new capabilities for Big Data to support business initiatives
• Overall responsible for delivery and operations for Big Data functions
• Manage multiple Big Data delivery / project teams comprise of internal IT professionals and several IT service providers to ensure that projects / enhancements are delivered within the agreed scope, budget and schedule
• Lead the application support teams comprise of internal IT professionals and IT service providers in managing, monitoring and optimizing of Big Data Platforms to ensure these applications meet the agreed SLA agreed with business
• Troubleshoot Big Data platforms and software, including performance-tunes of BI applications
• Budgeting, tracking and forecasting capital & operational expenditure
• Interface with product vendors to keep abreast of new technologies, pricing and customer applicability. Participate in vendor evaluations
• Lead and establish development guidelines, standards and best practices
• Implement the most appropriate and effective IT organizational design to support and engage with the business
• Work with business stakeholders to develop and analyze big data needs
• Manage and drive outsourced vendors to achieve defined delivery and operations objectives
• Manage resource capacity and staffing
• Provide functional leadership to assigned staff and deliver input to staff performance and development
• Identify capability for the team and provide guidance, training, and problem-solving assistance to team members
• Present and engage senior leadership levels and /or executive steering committees on Big Data initiatives, projects and operations
• Drive Group IT initiatives to the team
• Develop and execute an analytics program that will allow company business leaders to make data-based decisions

The ideal candidate should possess
• Bachelor's degree in Business Management, IT, Computer Science or equivalent.
• At least 12 years of experience in the area of big data, data warehousing, BI & reporting and / or data management, including at least 8 years of experience in managing BI or big data development and operations team
• Experience in managing and driving outsourced vendors to delivery and operations objectives
• Hands-on experience in handling incident, problem, configuration, capacity and availability management
• Successfully implemented large-scale data warehouse / data lake
• Expert in building and optimizing ‘big data’ data pipelines
• Experience in data management, data architecture and design
• Strong technical knowledge of data integrations, ETL and data warehouse data modelling
• Strong knowledge of BI reporting tools
• Strong knowledge of SQL
• Good understanding of Telco data models
• Experience to do cost estimation and working with external vendors
• Experience with DevOps tools and environment
• Experience in building an enterprise level data analytics capability.
• Experience in leading complex, major change initiatives; skills in change management
• Customer-service oriented
• Strong background in operational and capital finances, and IT budget development

We believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers. We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative"
29-Apr-2022 T11:53,[Internship 2022] Data Engineer Intern (APAC),foodpanda Singapore,20 mins,,Full–time,"This is a full-time internship opportunity starting in May 2022

At foodpanda, we're on a mission to redefine how tech, food, people and culture are connected. Operating in more than 400 cities across 12 locations worldwide, foodpanda continues to expand and grow in its core food delivery business as well as in new verticals like grocery deliveries, with a strong tech infrastructure at its core. From our restaurants-partners, cloud kitchens and cloud grocery stores — foodpanda is just one tap away, delivering everything you need quickly and conveniently to your doorstep.

We are looking for undergraduates with a passion for data and insights to join our APAC Regional teams in Singapore! You will be empowered to find the most effective way of using data to help foodpanda make smart, data-driven business decisions.

This is an amazing opportunity for individuals who would like to learn, thrive and hone their analytical skills. If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!

What's on the menu for you:
• Defines and builds data pipelines that will enable faster, better, data-informed decision-making within the business
• Architect, build, and deploy new data models that provide intuitive analytics across the business
• Manage and own the entire data processing system (internally, as well as interfacing with other teams or third party services), provide advice on any necessary infrastructure changes
• Experiment with, select, and implement cutting edge Big Data tools and frameworks required to provide requested capabilities
• Perform all needed data transformation to populate a reporting optimised data warehouse
• Participate in building machine learning models, deliver analyses and insights to support business decision making

What you bring to the table:
• Penultimate or final year undergraduate students pursuing bachelor’s degree in Computer Science, Engineering, Data Analytics, Mathematics, or related discipline
• Ability to write clean, structured, and high performance SQL code
• Strong oral and written communication skills
• Strong business mindset and ability to grasp business requirements from stakeholders
• Champion of data and visualization with strong presentation and story-telling skills
• Knowledge and experience with BI tools (Tableau, Data Studio), big data, Data Warehouse technologies, Python or R.
• Knowledge of Machine Learning, Big Data, Data Pipelines, or setting up environments for data scientists/machine learning engineers is a plus.

What we Offer:
• A dynamic and challenging work environment.
• A company committed to developing you personally and professionally.
• A great working atmosphere with regular company and team events.
• A vibrant and international team committed to diversity and inclusion.
• Responsibility from day one in a fast growing and global company.
• Other benefits include free food and learning and development opportunities"
29-Apr-2022 T11:53,Data Engineer,Quesscorp Singapore Pte Ltd,11 days ago,,Full–time,"Deliver big data solution based on premise Hadoop or cloud-based systems like AWS.

• Design ingestion layer for structured & unstructured data (text, voice, xml etc) & implement insurance specific data model for business & analytics use.
• Deliver ELT solution including data extraction, transformation, cleansing, data integration and data management.
• Implement batch & near real time data ingestion pipelines based on reference architecture like Lambda.
• Build advanced data sources for self-service analytics systems
• Operationalize analytics models for production usage with big data workflows, proper security & access control.
• Provide development and architecture support for internal analytics tools & optimize execution performance for complex data pipelines.
• Bachelor’s Degree in Computer Science / IT
• 3+ years of experience in Big Data Engineering using tools like Spark, Hadoop, Hive, etc.
• 3+ years of experience with Software Engineering using Python / development using modern Relational and NoSQL databases like MySQL, PostgreSQL, and MongoDB
• Worked on Talend (Big Data) and/or Informatica Data Engineering 10.4 platform using modules DEI, DES, PowerExchange and PowerCenter
• Experience in developing medium to large scale Data Integration projects, including Real-Time and APIs based initiatives.
• Good knowledge on spark coding and shell scripting
• Hand on experience with Kafka for publishing and consuming Data"
29-Apr-2022 T11:53,Data Engineer,SOL-X PTE. LTD.,4 days ago,$6K–$9K a month,Full–time,"This role is for a highly skilled data engineer with deep knowledge in data systems, will be able to work independently with minimal supervision and will have a divide and conquer mindset when attacking data flows. This role is responsible for expanding and optimizing our data and data pipelines architecture as well as optimizing data flows and collections for analytics. This role will support data scientists, products and engineering team on the data initiatives and will ensure optimal data delivery throughout multiple ongoing projects.

Responsibilities:

· Create and maintain optimal data pipeline architecture.

· Assemble complex data sets to meet specific requirements.

· Creating data systems that ingest data from various sources.

· Implement flows with distributed systems and cloud architecture.

· Write efficient, well documented, and highly readable code.

· Schedule/automate data pipelines and monitor their performance.

· Write ad-hoc queries in order to perform analysis

· Interact with the teams to gather requirements and explain his work

Qualifications & Experience:

· Bachelor’s degree in engineering majors / computer science or similar relevant field

· 5+ years of experience in Software Engineering / BI / Data Warehouse design and development using modern

Relational and NoSQL databases like MySQL, PostgreSQL, and MongoDB

· 3+ years of experience in building SQL scripts and automating DWH pipelines.

· Good knowledge and experience with Python for data manipulation

· Experience with Unix and cloud solutions such as Azure Blob, AWS EC2, AWS EMR

· Data Modelling and Data Science experience building predictive models

· Experience with data automation/data orchestration tools such as Prefect is a plus"
29-Apr-2022 T11:53,Senior/Principal Big Data Engineer,ORACLE CORPORATION SINGAPORE PTE LTD,6 days ago,,Full–time,"Applicants are required to read, write, and speak the following languages: English

The Java Platform Group is looking for an experienced, passionate, and highly motivated Big Data Engineer. Our team is responsible for delivering the Java platform that is used by millions of developers. We are looking for an engineer with a thorough working experience of both the Java Platform and the implementation of the cloud native services. We are hiring to further expand our cloud service to more Oracle commercial regions and we are continuously investing to grow the service's capabilities and footprint across the globe.

As a Big Data Engineer with the Java Platform Group, you will be a leading contributor in Big Data Analytics of Oracle’s latest Cloud Services Technologies. You will take an active role in the definition and evolution of standard practices and procedures. Additionally, you will be responsible for scaling our existing infrastructure, incorporating new data sources, building robust data pipelines for production level cloud ********** you have a passion Big Data processing this is the place where you can make a difference.

Responsibilities

Develop ETL pipelines with robust monitoring and alarming

Develop data models that are optimized for business usability and understanding

Develop and optimize data tables using best practices for partitioning, compression, parallelization, etc.

Develop and maintain metadata, data catalog and documentation in regard to the data flow of the system.

Ensure the handling of the data is in compliant to security and privacy requirements of the organization.

Optimize SQL and ETL solutions to solve various reporting requirements.

Maintain and continuously improve Java Management Services operations inline with the service's SLOs

Ensure JMS performance, uptime and scale, maintaining high standards of code quality

Work with agile development methodology, adhering to best practices established by Oracle and project team and pursuing continued learning opportunities

Knowledge sharing and provide mentorship to junior engineers.

Perform in-depth data analysis of incoming JVM telemetry data

Provide technical consulting and mentoring to team members on optimization techniques, new algorithm design,

Implementation and conformance to architecture standards

You will be responsible for architecting and developing data models

Help deploy and productionize Machine Learning models.

Skills we require:

10+ years experience with designing, implementing, testing and operating cloud native in an Agile and DevOps setup

3+ years of industry experience in Data Engineering with experience manipulating, and extracting data from large datasets.

3+ years of Data Warehousing experience preferably with Oracle or related technology displaying strength in SQL, python/pyspark scripting, data modeling and ETL development.

2+ years working in a role as Lead / Principal Engineer or Architect

Broad knowledge of different types of data storage engines - (non)relational, row/column oriented dbs. e.g. Oracle, Postgres, MySQL, QB/redshift, Elastic, Hive, HBase

Advanced query language (SQL) knowledge

Experience with stream processing engines (Kafka, Kinesis)

Experience in using a Java framework in development preferable using Dropwizard

Experience deep working experience with one or more major cloud vendors (OCI, Azure, AWS, GCP)

Strong analytical skills, 2+ years’ experience with Python and an interest in Machine Learning

Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Design, develop, troubleshoot and debug software programs for databases, applications, tools, networks etc.

As a member of the software engineering division, you will take an active role in the definition and evolution of standard practices and procedures. You will be responsible for defining and developing software for tasks associated with the developing, designing and debugging of software applications or operating systems.

Work is non-routine and very complex, involving the application of advanced technical/business skills in area of specialization. Leading contributor individually and as a team member, providing direction and mentoring to others. BS or MS degree or equivalent experience relevant to functional area. 7 years of software engineering or related experience.

: Product Development

: Regular Employee Hire

: Oracle

All over the world, people's lives are better because of Oracle. Want to make a difference? Join our company of change-makers.

From Oracle to culinary school and back again. Bonnie Carlson Kaypaghian uses the skills she learned to create recipes for her daughter’s Type 1 Diabetes and has written a cookbook to share with the world. #LifeatOracle

The Java Platform Group is looking for an experienced, passionate, and highly motivated Big Data Engineer. Our team is responsible for delivering the Java platform that is used by millions of developers. We are looking for an engineer with a thorough working experience of both the Java Platform and the implementation of the cloud native services. We are hiring to further expand our cloud service to more Oracle commercial regions and we are continuously investing to grow the service's capabilities and footprint across the globe.

As a Big Data Engineer with the Java Platform Group, you will be a leading contributor in Big Data Analytics of Oracle’s latest Cloud Services Technologies. You will take an active role in the definition and evolution of standard practices and procedures. Additionally, you will be responsible for scaling our existing infrastructure, incorporating new data sources, building robust data pipelines for production level cloud ********** you have a passion Big Data processing this is the place where you can make a difference.

Responsibilities

Develop ETL pipelines with robust monitoring and alarming

Develop data models that are optimized for business usability and understanding

Develop and optimize data tables using best practices for partitioning, compression, parallelization, etc.

Develop and maintain metadata, data catalog and documentation in regard to the data flow of the system.

Ensure the handling of the data is in compliant to security and privacy requirements of the organization.

Optimize SQL and ETL solutions to solve various reporting requirements.

Maintain and continuously improve Java Management Services operations inline with the service's SLOs

Ensure JMS performance, uptime and scale, maintaining high standards of code quality

Work with agile development methodology, adhering to best practices established by Oracle and project team and pursuing continued learning opportunities

Knowledge sharing and provide mentorship to junior engineers.

Perform in-depth data analysis of incoming JVM telemetry data

Provide technical consulting and mentoring to team members on optimization techniques, new algorithm design,

Implementation and conformance to architecture standards

You will be responsible for architecting and developing data models

Help deploy and productionize Machine Learning models.

Skills we require:

10+ years experience with designing, implementing, testing and operating cloud native in an Agile and DevOps setup

3+ years of industry experience in Data Engineering with experience manipulating, and extracting data from large datasets.

3+ years of Data Warehousing experience preferably with Oracle or related technology displaying strength in SQL, python/pyspark scripting, data modeling and ETL development.

2+ years working in a role as Lead / Principal Engineer or Architect

Broad knowledge of different types of data storage engines - (non)relational, row/column oriented dbs. e.g. Oracle, Postgres, MySQL, QB/redshift, Elastic, Hive, HBase

Advanced query language (SQL) knowledge

Experience with stream processing engines (Kafka, Kinesis)

Experience in using a Java framework in development preferable using Dropwizard

Experience deep working experience with one or more major cloud vendors (OCI, Azure, AWS, GCP)

Strong analytical skills, 2+ years’ experience with Python and an interest in Machine Learning

Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy

Oracle is the world’s leading provider of business software. But you probably already knew that. With a presence in over 175 countries, we are one of the biggest technology companies on the planet. What you might not know is that we are leading a cloud revolution. We’re using emerging technologies like AI, machine learning, and blockchain to solve critical real-world problems. From advancing energy efficiency to reimagining online commerce, the work we do is not only transforming the world of business—it’s helping governments, powering nonprofits, and giving billions of people the tools they need to outpace change and make a difference. Mission: To help people see data in new ways, discover insights, unlock endless possibilities.
Future Creators We are creators, free-thinkers, and basically a bunch of problem-solvers who are eternally asking ‘what if"
29-Apr-2022 T11:53,"Data Engineer, Capability Development (DART)",GVT Government Technology Agency (GovTech),2 days ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Data Engineer in GovTech’s Data Science & Artificial Intelligence Division, you will be involved in architecting, managing and running advanced analytics and data platforms. For this role, you will be part of the Data Analytics Readiness Team (DART) within GovTech, where you will help drive GovTech’s Data Transformation efforts. You will assist to establish and implement effective operational workflows, data pipelines, procedures and best practices in accordance with government infrastructure and security policies to ensure that the platforms are monitored, secure, available and reliable.

We are looking for an independent and motivated engineer who has experience in both small and large-scale analytics platform projects, and in building and maintaining the infrastructure.

What you will be working on:
• Design, Architect, Deploy, and maintain solutions on Microsoft Azure or AWS using different Cloud & Big Data Technologies to provide secure and governed access to data for business users.
• Manage the full life-cycle of a data warehouse/lakehouse solutions from requirement gathering and analysis to platform selection, design of the architecture, and deployment.
• Collaborate with data stewards, data analysts and data scientists to build data pipelines from enterprise systems such as Workday to collect, clean, harmonise, merge and consolidate data sources for data warehouse/lakehouse.

Where we are looking for:
• Diploma/Degree in Computer Science or Information Technology or related disciplines
• Experience with the cloud (e.g. AWS, GCP, Azure)
• Hands-on experience in implementing Data Lake/Data Warehouse with technologies like – Databricks, Azure Synapse Analytics, SQL Database, AWS Lake formation.
• Programming & debugging skills in Python, Scala or R.
• Proficient in SQL.
• Knowledge of both SQL and NoSQL databases.
• Comfortable with DevOps tools like AWS Cloud Formation/Terraform, Docker and Git for CI/CD development.
• Prior experience with data engineering tools and frameworks like Airflow, Kafka, Hadoop, Spark, Kubernetes.
• Familiar in building REST services is good to have.

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:53,Senior Data Engineer,Denodo,3 days ago,,Full–time,"Denodo is looking for a technical, passionate, hands-on data advocate, customer-oriented, and willing to take Denodo learning and adoption to the next level to join our Global Office team, as part of the Customer Success organization.

Your mission: ensure Denodo customers, partners, and any user of the Denodo Platform have available all the material they need to be successful in their Data Management solutions.
In this role you will combine high technical expertise, data ecosystem knowledge and customer-facing skills to interact with data, customers (internal and external), and other stakeholders to get inspired with new ideas, and put them into practice to facilitate Denodo adoption.

Duties and responsibilities:
• Obtain a strong knowledge of the Denodo Platform, be able to deliver a superb technical pitch, including overview of our key and advanced features and benefits, differentiation, and competitive positioning.
• Constantly learn new things and maintain an overview of modern technologies.
• Ensure Denodo users are well-equipped with the resources and training needed to effectively perform with the Denodo Platform in an environment of fast-paced change
• Follow up on the release calendar of updates and versions to identify opportunities to create new material.
• Interact with internal and external stakeholders to provide updates on the roadmap and get feedback.
• Interact with the local managers to coordinate the development and maintenance of technical resources.
• Develop white papers, presentations, training materials or documentation on related topics.
• Define metrics, analyze issues, data and trends to make data-driven decisions that let improve the quality of the technical resources and new ways of working.
• Ensure that upper level management is aware of issues regarding processes, protocols or educational materials

Qualifications

Desired skills and experience
• Self-motivated, proactive and solution oriented. A passion for improving Denodo user experience.
• Excellent organizational skills being able to manage change effectively.
• Excellent verbal and written communication skills to be able to interact with technical and business counterparts
• Strong analytical and problem solving abilities.
• Active listener.
• Lots of curiosity. You never stop learning new things.
• Creativity. We love to be surprised with innovative solutions.
• Be a team worker with positive attitude
• Willingness to occasionally travel.
• BS or higher degree in Computer Science or Information Systems.
• 3+ years of demonstrated experience in a similar role.
• Experience with the data landscape.
• Fluent in English ( min. C1 Level) (only for non English speaking countries)

Employment Practices

Denodo is an equal opportunity employer and prohibits discrimination and harassment of any kind. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by applicable law. Denodo will provide reasonable accommodation to employees who have protected disabilities in accordance with applicable law.

We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee"
29-Apr-2022 T11:53,(Senior) Data Engineer (Information Management Systems),Centre for Strategic Infocomm Technologies,Full–time,,,"As a data engineer in the Information Management Systems team, you will develop critical pipelines to convert and serve large volumes of varied unstructured data into usable datasets for other services and end-users. Together with your driven and multi-talented product team, you have the autonomy to determine the best way to deliver value and the technologies used.

As CSIT is an agency under the Ministry of Defence (Singapore), only Singapore Citizens will be considered."
29-Apr-2022 T11:53,Principal Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,9 hours ago,,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 10-12 years of experience in Data Engineering role and have good knowledge / working experience in:
• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.
• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.
• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.
• Functional programming languages, e.g. Scala.
• Virtualization and container environment such as Docker and Kubernetes.
• Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:53,Data Engineer - Search,Shopee,Full–time,,,"Design, build and maintain the ingestion system to support various types of data (e.g. User behavior, RDS, NoSQL DB and others) to be ingested to the data warehouse more timely and accuratelyTranslate data requirements into scalable technical data service with low latency and high concurrencyDesign, build and maintain the batch or real-time data pipeline in production using Hadoop big data technologyAnalyse and improve efficiency, scalability, and stability of the systemDefine and manage service level agreement and data quality for all data sets in allocated areas of ownership"
29-Apr-2022 T11:53,"Data Engineer, FinTech",foodpanda,4 days ago,"$5,923–$11,769 a month",Full–time,"We are looking for a Data Engineer, to join our growing team that who will work closely with our fintech squads based in Singapore. You will be part of an international team of highly talented and motivated people.

This role reports directly to our Director of Data & Product Analytics.

What's On Your Plate
• Build and execute SQL queries & database schema as required by the business
• Work with software engineers and architects to design the target data architecture as per business requirements
• Architect, build and deploy data models with adequate documentation and validation
• Implement data processing and pipelining queries that will enable data-informed decision-making within the business
• Enable data consumers such as analytics, data products, machine learning etc. by actively seeking opportunities to automate data pipelines
• Continuously experiment state of the art technologies and proactively seek opportunities to improve the data ecosystem
• Actively document the codes models and data dictionaries that are being developed.
What You Bring To The Table
• 2 - 4 years of relevant experience in data analytics / engineering, preferably on a cloud setup
• Strong knowledge on SQL, data structures and database schemas
• Experience in data pipelining tools such as Airflow
• Good working knowledge of Python / Java or any other programming language
• Knowledgeable in big data, Data Warehouse technologies
• Knowledgeable in agile tool sets such as Jira, Confluence etc.
• Preferred qualifications:
• Experience in Google Cloud Platform
• Working knowledge on NoSQL datastores such as MongoDB, DynamoDB etc.

What We Can Offer You
• A vibrant and international team with multicultural and diverse backgrounds.
• Solving challenges with inspiring colleagues in an all hands-on deck environment.
• Management team that recognizes top performers, welcomes our newbies, and shares a love for good food.
• Competitive package, incentives, allowances, food perks, insurance, pension and more"
29-Apr-2022 T11:53,Data Engineer,ITCONNECTUS PTE. LTD.,4 days ago,$6K–$8K a month,Full–time,"Key responsibilities:
• Single point of contact and liaison of all company teams
• Provide environment/integration support for SCP platforms and tools
• Takeover from Transformation Partner and DigiTech for Operate Support
• Liaise with Business/System Owner, DigiTech and Transformation Partner on requests and issues
• Support and follow-up with Project team on requests and issues

Requirements:
• Must have 5 Years of good experience
• Degree or diploma in IT/Computer Science/Information Systems or equivalent
• 3+ years of experience relevant data engineering preferred.
• Experience with data pipeline and ETL development preferred
• Proficient in Pyspark, Python and SQL
• Experience with Java, C#, JavaScript/TypeScript, open-source, Docker and Terraform technologies.
• Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing and operations
• Fast learner and a team player
• Knowledge in AWS/Azure/GCP"
29-Apr-2022 T11:53,Data Engineer,TOTAL EBIZ SOLUTIONS,Full–time,,,"• Ability to build both batch and streaming data pipeline in a big data environment from the upstream systems in collaboration with technology and other data teams.
• Develop an in-depth understanding of the underlying data, data model, and business domain by investigating, reviewing, and analyzing data from structured & unstructured sources. Leverage this knowledge to design and build common/ master data models.
• Independently, collaborate with cross-functional stakeholders – other department teams and other technology teams to understand their data needs, formulate and complete end-to-end analysis that includes business understanding, data gathering, data discovery, building and designing master/ common data models using cloud-based data pipelines.
• Ability to maintain and optimize the data pipelines. Monitor the health of production pipelines frequently to ensure accuracy and stability.
• Diagnose, analyze and provide solutions to issues in scripts, reports, tools, data, etc.
• Strictly adhere to the controls and standards for meeting the enterprise technology data quality requirements, by using various in-house data quality solutions.
• Identify, design, and implement improvements, e.g. automating manual processes, optimizing data delivery, re-designing architecture for greater scalability, etc.
• Train/ mentor junior data engineers and the Lazada business associates and enable Lazada personnel on building and understanding various data querying and data-discovery skills.

Job Requirements
• Strong education background in Computer Science/Information Systems (preferred).
• Must have Big Data/SQL skills.
• Strong understanding of database & data warehouse design/administration, hands-on experience with MySQL, Hadoop, Hive SQL, Flink / Spark technology, and Python. Knowledge of API programming and NoSQL databases will be an added advantage
• More than 5 years of experience in end-to-end data warehousing, data modeling, and as well as distributed processing systems. Previous experience of working in cloud-based environments, especially Alicloud would be an added advantage.
• Good understanding of data quality frameworks and data governance.
• Previous experience in implementing them at an enterprise level would be an added advantage.
• Experience in enablement and other enterprise-wide training activities.
• Strong analytical/problem-solving skill balance with good written and oral communication skills.
• A good understanding of accounting concepts, finance processes, and financial performance metrics is a plus.
• Tenacity to develop ideas independently and thrive in a fast-paced start-up environment.
• A self-motivated, driven, flexible, quick learning, high achieving, “can do” mentality"
29-Apr-2022 T11:53,Data Engineer (SQL/Python),Scientec Consulting Pte. Ltd.,4 days ago,,Full–time,"Responsibilities
• Responsible for databases, data warehouses and large-scale data processing systems
• Develop, construct, test and maintain analytical data warehouse and deploy and continuously improve ETL / ELT tasks
• Design, develop, review and optimize the core database store procedures in batch job scripts and java scripts.
• PL / SQL and SQL Tuning and optimization

Requirements
• Degree in IT or equivalent
• Experience data architecture, data warehousing, data processing, data modelling and ETL / ELT
• Experience in database development (Oracle SQL / PLSQL)
• Expert in using Python

To apply, please send your updated resume to HIDDEN TEXT

By submitting any application or resume to us, you will be deemed to have agreed & consented to us collecting, using, retaining & disclosing your personal information to prospective employers for their consideration.

If you wish to withdraw your consent or correct any of your personal data, please drop us an email at HIDDEN TEXT to let us know.

Note : Any resumes of job applications sent to this mailbox will not be attended as it is solely for the purpose of personal data protection related matters.)

We will contact you if your skills and experience are suitable for the role, or if there is a similar opportunity that is available presently or in the future.

Resquid Quitaleg Airene (R1656137)

ScienTec Consulting Pte Ltd - 11C5781"
29-Apr-2022 T11:53,Data Engineer,LILITH GAMES SG PTE. LTD.,1 day ago,,Full–time,"Big Data Development Engineer (Data Center)

What You Will Be Doing
• Plan, design, develop and implement core data system in the business field
• Develop and maintain real-time/ offline data processing tasks
• Build industry-leading distributed systems such as storage and computing to provide a reliable infrastructure for massive data and large-scale business systems
• Develop various data services, such as data developing platform, quality monitoring system,

Qualifications & Skills
• BSc in Computer Science or related major.
• At least 3 years of big data platform R&D.
• Familiar with common algorithms and data structures, and be proficient in Java / Python / Scala / shell language (at least two)
• Familiar with deployment and implementation of docker and k8s related containers.
• Experienced working with big data technologies such as Hadoop, Flink, Hive, Spark, etc.
• Enthusiastic about learning new technologies and persistent in pursuit of high concurrency and distributed architecture design.
• Have good team communication and collaboration skills.

大数据开发工程师（数据中台方向）

工作职责：
• 参与业务领域核心数据体系的规划设计以及开发落地
• 参与实时/离线数据加工任务的开发与维护
• 参与大数据基础架构的开发与迭代
• 参与各类数据服务开发，包含但不限于：数据平台开发，数据质量监控，BI等

岗位要求：
• 本科及以上学历，3年以上大数据平台研发经验。
• 具有扎实的编程功底，熟悉常用的算法和数据结构，精通Java/Python/Scala/Shell语言（至少两种）
• 熟悉Hadoop等大数据框架，有Flink、Spark等相关实时流计算框架经验优先
• 熟悉docker、k8s相关容器的部署实施
• 对新技术保持好奇心，对高并发和分布式架构有执着的追求"
29-Apr-2022 T11:53,Data Engineer - Quantitative Trading Firm/Market Maker,Aptitude Asia,3 days ago,,Full–time,"Our client, a leading high frequency trading firm, currently looking for a Data Engineer to join the team in Singapore. The team is to provide high-quality research and reference data for the firm to utilize, accessing financial data and analytical services which support the trading, research, and operational functions.

Responsibilities
• Responsible for building, maintaining and improving the acquisition and monitoring data platform
• Responsible for delivery and the construction of derived sets of data, analysis and cleaning
• Develop and implement tools to provide autonomy around the platforms data life-cycle
• Maintain an inventory of the data estate
• Implement the control workflow for representing data events
• Work with the investment team, quant research, operations to ensure their data-related requirements are captured and delivered
• Liaise with external vendors confidently and assist in vendor audit and compliance reporting

Requirements
• Master of Mathematics, Computer Science, Physics or Engineering and related field; strong candidates with Bachelor of Science degrees will also be considered.
• At least 3 years exepereinces for data analyst role or relevant data experience in either a financial institution or data vendor
• Strong background in financial markets and associated data sets i.e. market data, reference data
• Must have a working understanding of data engineering practices
• Strong numerical and analytical skills and problem-solving ability
• Skilled in the use and understanding of modern database technologies including relational, document, and temporal column data stores.
• Skilled in the use of a programming language such as Python for data engineering

To apply, please send your updated CV to donna@aptitudeasia.com. Thanks"
29-Apr-2022 T11:53,Data Engineer - Data Warehouse,Shopee,Full–time,,,"The Engineering and Technology team is at the core of the Shopee platform development. The team is made up of a group of passionate engineers from all over the world, striving to build the best systems with the most suitable technologies. Our engineers do not merely solve problems at hand; We build foundations for a long-lasting future. We don't limit ourselves on what we can or can't do; we take matters into our own hands even if it means drilling down to the bottom layer of the computing platform. Shopee's hyper-growing business scale has transformed most ""innocent"" problems into huge technical challenges, and there is no better place to experience it first-hand if you love technologies as much as we do. Browse our Engineering and Technology team openings to see how you can make an impact with us.

Job Description:
• Design and manage Data warehouse plan for a product, such as: design data model, define data metric, data governance and so on
• Collaborate with Business Intelligence Business and Product Management teams
• Use data to solve problems, identify needs and opportunities
• Design, build and maintain the batch or real time data pipeline in production using Hadoop big data technology
• Define and manage service level agreement, data quality for all data sets in allocated areas of ownership

Requirements:
• Bachelor's Degree in Computer Science or a related technical field.
• 2~10+ years working experience with programming languages,such as Java,Scala,Python
• Ability to write efficient SQL statements
• Familiar with Hadoop, Spark and Flink data processing, experience of TB data processing experience is a plus
• Familiar with data warehouse modeling, such as dimensional data modeling and schema design
• Understand data mining or machine learning
• Excellent communication skills including the ability to identify and communicate data driven insights
• Strong logical thinking and analysis abilities
• Passionate, self-motivated, and takes ownership"
29-Apr-2022 T11:53,Head of Data Engineering,Apersona Pte. Ltd.,5 days ago,,Full–time,"What you will do
• Place client value and human experience at the centre of everything we do
• Use data engineering to support material impact and drive disruptive transformation across public and private sectors
• Maintain public trust ensuring fairness, ethics, accountability, and transparency
• Build a world-class team with expert capabilities in data engineering
• Create a culture of excellence and lead with confidence, charisma, context, and humility working effectively at all levels
• Lead delivery of data engineering solutions through incubation, proofs-of-concept, to commercialisation and deployment
• Support development of go-to-market plans for both AI & data, understand strategic opportunities, develop trusted partnerships, and deliver social progress
• Educate, enable, and coach teams on data engineering in Temus, clients and in the broader community
• Adopt a cloud first strategy to enhance agility and elasticity partnering with vendors to support specific public sector needs

Suitable candidates
• Bachelor’s Degree in Statistics/Computer Science/Data Analytics or related quantitative field
• Minimum 10 years of relevant experience in consulting/ data science related functions with with at least 3 years of leadership / people management experience
• Data Development: Deep expertise in developing and managing software for data processing: Python, Java, SQL, KSQL, Scala, Spark, Flink, Beam, AWS Glue, Google Dataflow, etc.
• Data Platforms: Deep expertise in building and managing data platforms: HBase, MongoDB, Cassandra, Redis, PostgreSQL, Oracle, MySQL, Kafka, Kinesis, DynamoDB, Redshift, Cloud SQL, Cloud Spanner, Cloud Bigtable, Firestore, BigQuery, Azure SQL, Cosmos DB, Stream Analytics, Synapse Analytics, DeltaLake, Elasticsearch, Snowflake etc.
• Data Management & Governance: Strong skills in data management: accuracy, integrity, latency, classification, security, lineage, metadata, etc.
• Communication: Strong ability to communicate with a broad range of clients, colleagues, and partners across a variety of contexts and formats.
• Partnership: Strong ability to develop and maintain relationships amongst clients, colleagues, and partners
• Leadership: Demonstrated capability to lead, inspire, coach and mentor team members and colleagues.

Job Types: Full-time, Permanent

Benefits:
• Health insurance
• Parental leave
• Professional development

Schedule:
• Monday to Friday

Supplemental Pay:
• Performance bonus"
29-Apr-2022 T11:53,Market Data Engineer - 12 Months Contract,Adecco Personnel Pte. Ltd.,8 days ago,,Full–time,"This role will not only act as the first line of support for Market Data infrastructure across Asia Pacific regions including applications, servers and connectivity, but also be heavily involved in testing and implementation of real-time market data infrastructure and in-house tools. It requires a wide range of skills and expertise which are critical in the delivery of reliable, timely and cost effective market data products, solutions and services that enable our business lines and their electronic trading systems to make highly informed and competitive decisions.

Key Responsibilities
• Support and maintain market data infrastructures/platforms and expeditiously apply troubleshooting/problem-solving skills to minimize business productivity loss, customer impact, and financial/reputational risk.
• Coordinate with and escalate to market data vendors and exchanges in the region for their data content/technical issues and changes.
• Manage vendor and exchange driven changes, technical activities and relationships.
• Plan and implement production changes after trading hours, and conduct testing to verify the changes are successful.
• Act as the SME of market data distributed by regional exchanges and major vendors, perform in-depth analysis over data content and behavior to effectively support low latency electronic trading applications.
• Design, implement and constantly improve monitoring/capacity/performance/data analysis tools to ensure comprehensive coverage over data accuracy and reliability & infrastructure availability and stability.

Key Requirements
• Overview of financial and banking industry and strong data knowledge for Asia Equity markets
• Strong scripting skills for software/system management and automation (Perl, Python, bash scripting)
• Understanding and experience with market data delivery platforms like TREP/BBG/Direct exchange feeds
• Administration and support experience with server Operating Systems like Red Hat Linux and Windows environment
• IT Service Management knowledge and mindset including Change Management, Incident Management, Problem Management.

Strong analytical skills and logical thinking"
29-Apr-2022 T11:53,Sr Data Engineer Data Engineer,Carousell Group,3 days ago,,Full–time,"We are now looking for Data Engineers/ Sr. Data Engineers to join our Data team.You will: Build, maintain and scale efficient data infrastructure, ETL and reporting pipelinesWork with data scientists and machine learning engineers in getting ML and deep learning models to production readyInvestigate and research data quality and integrity from data sourcesDevelop and maintain analytics platform, business intelligence and experimentation toolsDevelop and maintain scalable platform for tracking business intelligence, build for reliability and redundancyManage data collection and organize the models, and also forecast the future needsCoach and mentor junior data engineers to be more effective individual contributors You have: User obsession and empathy.Drive and resourcefulness to persevere and overcome obstacles achieving challenging goals.Focus on impact and results. You work on the right things and get them done.High integrity and ability to positively collaborate with othersFluent in Python or similar programming languageExperience in data engineering tools like Hadoop, Spark, BigQuery, Airflow, etc.Experience in deploying and scaling ML models. Plus point for deploying and scaling deep learning models.Data-driven and passionate about solving problems through dataInquisitive and curious to delve deep into data to investigate trends or anomaliesDetail oriented and be able to work efficiently in a fast-paced team environmentKeen on data technologies and picking up new skills and tools along the wayStrong critical thinking and ability to frame issues in a logical manner P.S. The job title will be given based on working experience and interview performance"
29-Apr-2022 T11:53,Senior Data Engineer – Cyber Security R&D,NCS Group,4 days ago,$7.5K–$15K a month,Full–time,"• Design and build efficient, scalable and resilient ETL pipelines for both batch and real-time streaming data
• Architect and design data infrastructure on cloud using industry standard Infrastructure-as-Code tools
• Administer and maintain big data infrastructure, including performance tuning and troubleshooting
• Collaborate with data scientists and software engineers to build data-driven security platforms and services
• Assemble and process large and structured/unstructured datasets that meet cyber security business requirements.
• Create impactful demonstrations to showcase cyber analytics capabilities
• Design and build services with a focus on business value and usability
• Contribute to cross-functional technical discussion

Requirements:
• Degree in Computer Science, Computer Engineering, Information Technology, or equivalent
• At least 5-7 years of experience in data modelling and designing ETL pipelines
• Proficient in cloud technologies and services provided by AWS Azure, and GCP
• Familiar with DevOps tools, such as Git, Docker, Terraform
• Proficient in machine learning and data visualisation tools
• Proficient in SQL and other scripting languages, such as Phython and Bash
• In-depth knowledge of both SQL and NoSQL databases, including performance tuning and troubleshooting
• Strong sense of responsibility and accountability
• Excellent communication skills, both written and verbal
• Ability to work in a fast-paced, culturally diverse environment"
29-Apr-2022 T11:53,Data Engineer Intern,Centre for Strategic Infocomm Technologies,,,,"Duration: 2-4 Months, > 4 Months
Internship Type: Undergraduate; Pre-University

Raw data flows at high speed and high volume across enterprise systems. The multiplicity of file format and erroneous file content often make integration and maintenance of systems a challenging task. Data Engineers need to find trends in this complex environment and develop algorithms to make raw streaming data useful to the organisation.

In your application, please submit the following documents in a single PDF file for 'Resume/CV':
1. Resume
2. All Education Certificates to-date (from secondary to tertiary education)
- GCE 'O'/'N' Levels, NITEC
- Prelim/GCE 'A' Levels, IB/Diploma
- Latest university transcript from Year 1 (for current undergraduates)"
29-Apr-2022 T11:53,Data Engineer,LILITH GAMES SG PTE. LTD.,1 day ago,,Full–time,"Big Data Development Engineer (Advertising Direction)

Big Data Development Engineer (Advertising Direction)

What You Will Be Doing

1、Responsible for the construction of Big Data Cloud Platform and the maintenance of Advertising Data Warehouse.

2、Responsible for the calculation, cleaning and layering of Advertising Data based on Flink, Spark, etc., and store it through Hadoop, Clickhouse, etc.

3、Responsible for Advertising Data Analysis and Advertising System Report development.

Qualifications & Skills

1、Bachelor degree or above, major in computer and other related majors.

2、Proficient in Python or Golang coding.

3、Proficient with Mysql, Memcache, Redis, Message Queue and other common WEB components.

4、Having experience in using one or more of HDFS, Hive, HBase, MongoDB, Kafka, Flink and Spark is preferred.

5、Experience In Operation And Maintenance Development Is Preferred.

6、Familiar with Alibaba Cloud and other cloud computing resource deployment and optimization is preferred。

7、Proficient in coding complex SQL Statements, and have the ability and experience of query optimization.

8、Positive and optimistic, strong sense of responsibility, with good team communication and cooperation

大数据开发工程师（广告方向）

工作职责
• 负责大数据云平台搭建，广告数据仓库的搭建与维护。
• 负责基于Flink、Spark等对广告数据的进行计算、清洗、分层等工作。并通过Hadoop、Clickhouse等进行存储。
• 负责广告数据分析及广告系统报表的开发。

任职要求
• 大学本科(统招)及以上学历，计算机、通信等相关专业。
• 熟练掌握Python或Golang代码编写。
• 熟练使用Mysql、Memcache、Redis、消息队列等常用WEB组件。
• 有使用HDFS, Hive, HBase, MongoDB, Kafka，Flink, Spark中的一项或多项的经验优先。
• 有运维开发经验优先。
• 熟悉阿里云等云计算资源部署与优化者优先
• 熟练编写复杂的sql语句，具备查询优化的能力及经验。
• 积极乐观，责任心强，工作认真细致，具有良好的团队沟通与协作能力。"
29-Apr-2022 T11:53,"Data Engineer, Digital Innovation with Singapore Tech Giant",PeopleSearch,6 days ago,,Full–time,"Digital services company delivering innovative Products&Solutions is looking for a Data Engineer to join Digital Transformation center. Responsibilities: • Design, implement innovative solutions using statistical models, ML, other data mining techniques • Work closely with Data scientists defining, building enterprise data exchange platforms. • Handle challenges of real-world data analytics • Design, build, support and optimize new and existing data models and ETL processes. • Develop and support the data pipeline to integrate new data from various data sources • Research and experiment with new advances in analytical methods, algorithms and tools to deepen and develop new capabilities:deep learning, optimization.

Key requirements:
• Over 3 years of Python scripting, use of NumPy, Pandas and/ or scikit-learn
• Nice to have experience in Data Science, Analytics
• Data visualization tools expertince with Tableau, Power BI or packages in Python or Javascript
• Experience with RDBMS such as MySQL, PostgreSQL, MongoDB, MS SQL Server

Stack you will be exposed to (Good to have as an advantage) :
• Experience with deployment and maintenance of models in production, e.g. APIs with capped response time, performance monitoring, automatic retraining and tracking data/ model lineage
• Experience with cloud platforms (AWS, GCP, Azure)
• Experience with Docker, Kubernetes and other supporting container platforms/ tools
• Experience in data crawling including popular social APIs and web scraping
• Experience with application development, particularly full stack web development
• Experience with Hadoop"
29-Apr-2022 T11:53,Data Engineer/Scientist,PATSNAP PTE. LTD.,3 days ago,$4.5K–$8K a month,Full–time,"Company Introduction

PatSnap is a software company helping R&D leaders maximise the value of innovation intelligence within their R&D workflow and strategic planning. As the global leaders in connected innovation intelligence, Patsnap use AI-powered and machine learning technology to comb through billions of datasets, and help innovators connect the dots.

Job description

We are looking for a Data Engineer to join the revolution to help us improve various business outcomes and drive innovation. You will join a multidisciplinary team helping to shape our Product development. This is an excellent opportunity to take advantage of emerging trends and technologies to a real-world difference.

Responsibilities
• Study and transform various kind of structured and unstructured data
• Big data analytics
• Create data pipeline for regular data updates
• Analysing unstructured data using basic NLP techniques and extract important fields
• Store data in databases (nosql and graphdb)
• Develop NLP systems according to requirements
• Perform statistical analysis of results and refine models
• Remain updated in the rapidly changing field of machine learning
• Deploy models and create APIs

Requirements
• Proven experience as Data Engineer or similar role
• Experience with big data analytics libraries such as pyspark is must
• Experience with AWS and database technologies is plus
• Strong communication skills
• An analytical mind with problem-solving abilities
• Degree in Computer Science, Mathematics, Computational Linguistics or similar field
• Domain knowledge of Material sciences and/or chemistry is big plus"
29-Apr-2022 T11:53,Staff Data Engineer,Micron,5 days ago,,Full–time,"Our vision is to transform how the world uses information to enrich life for all.

Join an inclusive team passionate about one thing: using their expertise in the relentless pursuit of innovation for customers and partners. The solutions we build help make everything from virtual reality experiences to breakthroughs in neural networks possible. We do it all while committing to integrity, sustainability, and giving back to our communities. Because doing so can fuel the very innovation we are pursuing.

JR21781 Staff Data Engineer

Broad knowledge and experience in:
• Understanding of Big Data Engineering/processing, Business Intelligence and Advanced analytics
• Developing ETL/ELT processes
• Knowledge in databases and Data warehouse modeling
• Knowledge in Cloud based data engineering and Machine Learning Models
• Knowledge in building APIs for application integration
• Experience with various frameworks and processes, such as Agile
• Determine transformation requirements and develop processes to bring structured and unstructured data from the source to a new physical Data Model
• Work with Data Scientists to implement strategies for cleaning and preparing data for analysis, to develop data imputation algorithms, and optimize performance of big data and machine learning systems

Above average skills in:
• Big Data Engineering and Processing using Hadoop stack (Hadoop, Hive, HDFS, Spark and HBase etc.)
• Develop ETL/ELT processing using Apache Ni-Fi
• Strong background on SQL and databases
• Programming Skills in Python or Scala
• Data Analysis and Validation skills

Demonstrated ability to:
• Work in a dynamic, fast-paced, work environment
• Self-motivated with the ability to work under minimal direction
• To adapt to new technologies and learn quickly
• A passion for data and information with strong analytical, problem solving, and organizational skills
• Work in multi-functional groups, with diverse interests and requirements, to a common objective

Communicate very well with distributed teams (written, verbal and presentation

About Micron Technology, Inc.

We are an industry leader in innovative memory and storage solutions transforming how the world uses information to enrich life for all . With a relentless focus on our customers, technology leadership, and manufacturing and operational excellence, Micron delivers a rich portfolio of high-performance DRAM, NAND, and NOR memory and storage products through our Micron® and Crucial® brands. Every day, the innovations that our people create fuel the data economy, enabling advances in artificial intelligence and 5G applications that unleash opportunities — from the data center to the intelligent edge and across the client and mobile user experience.

To learn more, please visit micron.com/careers

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.

Please note that in order to assist in providing a safe and healthy workplace for all Micron team members, new employment offers for jobs based in India, Malaysia, Singapore, and the U.S., are contingent upon the applicant’s provision of a copy of their COVID-19 vaccination document to Micron on a confidential basis prior to their scheduled start date confirming that they have completed the COVID-19 vaccination process, subject to any written request for medical or religious accommodation and to the extent permitted by applicable law.

To request assistance with the application process and/or for reasonable accommodations, please contact hrsupport_in@micron.com

Micron Prohibits the use of child labor and complies with all applicable laws, rules, regulations, and other international and industry labor standards.

Micron does not charge candidates any recruitment fees or unlawfully collect any other payment from candidates as consideration for their employment with Micron"
29-Apr-2022 T11:53,Data Engineer,Klook,6 days ago,,Full–time,"At Klook, we love creating moments of joy. Our platform connects people around the world with experiences that bring a smile to their faces, at a touch of a button. We are a global team of diverse Klookers who push boundaries every day, learn fast with feedback and take ownership to drive the change we want to see. Together, we help each other make the world a more joyful place. Up for the challenge Join us today! About this role Our Data Engineer is pivotal in working on data ETL pipelines and data integrations as well as other big data systems. This role reports into our Data team lead and will be based in our Singapore office. Our ideal candidate should be familiar with and passionate about big data technologies, strong communication skills, quick learner, and has attention to detail. About Technology & Engineering In a fast-growing industry like ours, we can't afford to stand still. At Technology & Engineering, we constantly test and improve our products to create the best experience in the travel and leisure industry. The team hires curious and analytical people who are always to push boundaries and have real impact. What you'll do Design and develop solutions to store and process data on cloud platforms Design and develop ETL processes to ingest data into data warehouse, and ensure data accuracy and stability Participate in building big data platform that processes data at scale Work on data streaming systems and frameworks to ensure timely delivery of accurate data Work on cloud-native data infrastructure, optimize data performance, implement data monitoring and alert tools, and manage various data workloads Create scripts and workflows to automate repeated data processing tasks and simplify complex task flows Assisting in quality control of quantitative and qualitative research projects Participate in peer code reviews and produce high quality documentation What you'll need BS/MS or higher degree in mathematics, computer science or other technical/quantitative discipline Ideally 2 years experience in data development and engineering. Fresh graduates may be considered for the role. Familiarity with Hadoop, Hive, HBase, Flink, Hudi, Airflow, Kafka, etc Exposure to big data solutions from Amazon AWS/ Google Cloud, such as Redshift, EMR, BigQuery, Dataflow, Composer, etc. Proficient in SQL, and familiar with Java and Python A spirit of collaboration and transparent communication High personal code/development standards (peer testing, unit testing, documentation, etc) Fully vaccinated from COVID-19 Klook is proud to be an equal opportunity employer. We hire talented and passionate people of all backgrounds. We believe that a joyful workplace is an inclusive workplace, one where employees from all walks of life have an equal opportunity to thrive. We're dedicated to creating a welcoming and supportive culture where everyone belongs. Klook does not accept unsolicited resumes from any temporary staffing agency, placement service or professional recruiter (Agency). Klook will not be responsible for, and will not pay, any fees, commissions or other payments related to such unsolicited resumes. An Agency must obtain advance written approval from Klook's Talent Acquisition Team to submit resumes, and then only in conjunction with a valid fully-executed agreement for service and in response to a specific job opening for which the Agency has been requested to submit resumes for. Klook will not be responsible for, and will not pay, any fees, commissions or other payments to any Agency that does not have such agreement in place or does not comply with the foregoing"
29-Apr-2022 T11:53,Data Engineering Team Lead,Endowus,3 days ago,,Full–time,"About us
Endowus is Asia's leading fee-only digital wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping all investors grow their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.
Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones. Our mission is clear: help people invest better so they can live easier today, and better tomorrow.

The people you will work with
The team has deep domain knowledge in finance and technology, bringing together decades of experience at Goldman Sachs, Morgan Stanley, UBS, Credit Suisse, The Blackstone Group, AQR, Grab, Dropbox, Lyft, Redmart, Carousell, Bytedance, Grab, Kakao, Alibaba, and more See our leadership team here . We practise inclusion and treasure our diversity in background and experience. A diverse team is our biggest asset and we look for people who share our belief in Endowus' clear mission.

Investors, recognition, licensing
Endowus is backed by global leading strategic and venture capital investors including UBS, Samsung Ventures, EDBI, Prosus Ventures, ZVC, Singtel Innov8, Lightspeed Venture Partners, and SoftBank Ventures Asia.

Endowus has been recognised by the industry with the following awards: Singapore's Rising Star and Fintech Innovation (Asia Asset Management's Best of the Best Awards 2021), LinkedIn Top Start-ups 2021, WealthTech of the Year (Asia FinTech Awards 2021), and the top 15 Singapore's Best Workplaces Award (Great Place to Work).
Endowus is licensed by the Monetary Authority of Singapore (MAS).

About this team
The Data Engineering Team builds and operates the scalable data platform that powers data analytics and business intelligence for better decision making at Endowus. Working with colleagues in Data Analytics, Growth, Marketing, and Operations teams, the data engineering team creates data solutions that provide them with performant, near real time access to internal & third party data and insights.
We are a full stack team that builds our systems using cloud native patterns and operates them with high standards of engineering & operational excellence.
About this role; responsibilities & ownership
• Own the technical design, delivery, reliability & security of our core data platform.
• Lead, mentor and grow a small talented team of data engineers.
• Work closely with the Product team, other Engineering teams, and stakeholders in Data Analytics, Growth, Marketing, Operations, Compliance & IT Risk to define and prioritise our data platform roadmap.
• Empower the Data Engineering team to achieve high levels of technical quality & reliability. Requirements & qualifications
• Bachelors' or above in Computer Science, a related field, or equivalent professional experience.
• At least 6 years experience in designing and implementing highly scalable, distributed data collection, aggregation, and analysis systems built for handling large volumes of data in the cloud.
• At least 2 years experience as a tech lead facing business users directly while directly or indirectly managing the performance & delivery of the team
• Strong team and project leadership with a history of leading technical initiatives
• Significant hands-on experience building and optimising data pipelines for data collection, transformation, aggregation in Apache Flink or Apache Spark, using dependency and workflow management tools such as Airflow, operating in a public cloud environment like AWS, GCP or Azure.
• Advanced SQL knowledge and strong experience working with relational and non-relational databases.
• Experience integrating BI tools such as Tableau, Mode, Looker, etc.
• Experience integrating data sources using REST and streaming protocols, especially using Kafka.
• Strong grasp of at least one of the JVM languages such as Java, Scala.
• Experience with building systems & processes to handle data quality, data privacy, and data sovereignty requirements.
• Experience with agile processes, testing, CI/CD, and production error/metrics monitoring.
• Self-driven with a strong sense of ownership.
• Comfortable with numbers and motivated by steep learning curves.
• Has a strong product sense and is empathetic to customers' experiences of using the product. Nice to haves
• Domain experience in a B2C context is a strong plus.
• Knowledge of finance, wealth, and trading domain.
• Some exposure to CQRS / Event Sourcing patterns.
• Experience with AWS or GCP, Cassandra, Kafka, Kubernetes, Terraform. Benefits & perks
• Stock options in a fast growing company *
• Employee discount for investing at Endowus
• Flexible working hours and locations so you can live life and your best work - you are trusted to be responsible
• Competitive Staff Benefits; Annual Leave, Medical insurance, Dental *
• Productivity Equipment; Omnidesk Standing Desk & Ergonomic Chairs, Dell UltraSharp USB Type-C Monitor, high-end computer peripherals
• Choice of Apple or PC equipment
• Additional S$250 /HKD 1450 budget for your favourite hardware to boost productivity * Note: * is only applicable to Full-Time employees"
29-Apr-2022 T11:53,Data Engineer,PSA Singapore,4 days ago,,Full–time,"You will work in the Data Engineering team in Infocomm Technology & Data Division to:-

• Design, Develop, Test, Deploy and Maintain data pipelines (ETL) on the Enterprise Data Warehouse and Big Data Platform
• Design and Develop the API /Web Services framework for curation of new datasets whether internal or external (Internet), and to interface with other systems (both internal and external)
• Explore and source new data sets to address emerging business use case needs
• Maintain the data quality and keep improving the data efficiency

Requirements:

• Possess a good Bachelor’s degree in Computer Science or Computer Engineering; a Specialization in Software Engineering will be advantageous.
• Those with 2-3 years of related work experience will be preferred.
• Good grasp of Software Engineering principles such as Requirements Gathering (both functional and non-functional), Modular & Re- usable Design.
• Proficient in ETL using programming language /tools such as Python and/or SSIS and/or Informatica Power Centre
• Able to develop data applications including integration with ICT systems, build APIs and web applications via .Java and/or Python
• Familiarity with MS SQL, PostgreSQL or Oracle is preferred.
• There will an added advantage for any of the following:-
o Proficient in Data Modelling and Data Mining.
o Experience in designing and building scalable database schema for applications.
o Understanding of Object-Oriented Design.
o Knowledge of or prior work experience on Big Data platforms such as Hadoop or using Spark.
o Experience in the cloud environment setup using Microsoft Azure"
29-Apr-2022 T11:53,Data Engineering Tech Lead,Rakuten Asia,3 days ago,$9K–$14K a month,Full–time,"The Global Data Supervisory Department (aka GDSD) oversees the development and operation of our data platform in Rakuten group. We provide data products and platforms for Rakuten’s line of businesses, as well as technology solutions.

GDSD services are important to Rakuten as it has big contribution to Rakuten's profit and being one of the key drivers for growing various platform businesses of Rakuten Group including our e-commerce platform business.

We are looking for Technical Lead – Data Engineering, who is passionate to deal with complex technical challenges and who are interested to work closely in the fast pacing business environment.

Key Responsibilities:
• Collaborate with stakeholders, business development team, architect, project manager and data engineers to understand business requirement & questions, processes, and related data, and convert the information into measurable technical design & deliverables and achieving tangible business goals
• Work with the leadership to set the standards for data engineering practices within team and support across other disciplines
• Lead decision-making process for selection of architecture design & solutions, use the right analytical libraries, tools & technologies, frameworks and google cloud services in delivering sustainable and scalable ETL pipelines and data platforms
• Help and mentor Data Engineering team produce high-quality code following agile methodology, advocating code quality, enhancing automation testing coverage, and deployment approaches
• Good oral and written presentation skills, produce useful documentation, organizing workshops, and brownbag sessions to promote product adoption & utilization, provide the essential support to the stakeholders for the products user journey, and a good story-telling skills
• Develop, ship, and monitor data platforms & cloud infrastructure using best CI/CD (DevOps), DataOps and monitoring tools & optimization best practices
• Passionate about emerging technologies and early adoption
• Help us to shape the next generation of our products

Essential Competencies :
• Graduate degree educated in computer science or a relevant subject.
• 8+ years of experience in developing & delivering Advance Analytics solutions, Data Warehousing, Big Data Platforms, ETL /ELT data pipelines (both batch and streaming), and data modeling
• 5+ years of solid experience in architecting, developing, monitoring, and optimizing data platform and pipelines using public cloud services (preferably google cloud platform or aws big data services)
• 5+ years of solid experience in Big Data Technologies: Apache spark, Apache Airflow, Apache Beam, Kafka, HDFS, HIVE, and/or big data services in any public cloud (preferably gcp BigQuery, Dataflow, PubSub, DataProc, gcs, Cloud Composer, Cloud Catalog)
• 5+ years of solid experience in architecting, monitoring, and optimizing cloud infrastructure usage for data platforms in public cloud environment and strong experience & hands-on knowledge in cloud IAM, infrastructure security, monitoring, IAAS (infrastructure As A Code, Terraform preferred), networking, cloud cost reduction techniques (preferably google cloud platform or aws)
• 8+ years of solid experience in writing, reviewing, and optimizing code using SQL, python, java
• 2+ years of experience in any data visualization tool (preferably google data studio)
• 2+ years of experience in docker, Kubernetes, and Container registry & CI/CD for containers (preferably google cloud services, GKE, Cloud registry and Apache-spark on Kubernetes setup)
• Hands-on experience in using JSON, YAML, parquet & snappy, git, github-actions, Terraform, JIRA ( Epic, Stories, Reports etc.)
• Experience in architecting and building APIs (REST preferably), Serverless (functions), NoSQL DB and event-driven platforms is advantage

Rakuten is an equal opportunities employer and welcomes applications regardless of sex, marital status, ethnic origin, sexual orientation, religious belief or age"
29-Apr-2022 T11:53,"Consultant, Appl Architect",Singtel Group,18 days ago,,Full–time,"Data Engineer (Database Administrator)

The Data Engineer (Database Administrator) is responsible to perform database Administration activities such as database setup, defining/maintaining database objects, backup and recovery. He/She is also heavily involved in database design and review, application SQL tuning and maintaining documentation meeting customer’s expectation. As the experience in this position grows, there will be opportunities to expand to database consultancies areas such as performance tuning, data governance and data policies.

Key Responsibilities :

- Understand the basic concepts of database management

- Understand and follow user work processes

- Understand the requirements from the application team for defining objects

- Perform simple database tasks such as defining objects, conduct backup and recovery.

- Perform SQL tuning and provide suggestions to application teams

- Maintain documentation related to database environment

- Work with application team and setup database environment based on customer policy and application requirements

- Trouble-shoot technical problems faced by the project team

- Assist and respond promptly to incident, investigate & provide temporary &/or permanent resolution of incidents escalated. Provide timely status updates to relevant parties.

- Adhere to strict security practices for compliance

- Manage customer expectation in terms of schedule

- Recommend changes to existing procedures and policies

- Writing scripts/programs for service monitoring and health check of all systems

- Day-to-day monitoring, backup, deployment and maintenance of all databases

- Depending on project requirement, there is a need to support 24x7 requirement. DBA is expected to come back to office and work.

The successful candidate shouls possess:

- Minimum 3 years in software development industry or server infrastructure Project

- Application development experience with relational databases such as SQL server Databases.

- Should have participated in development projects. Participated in data modelling and review meeting.

- Hands on experience in trouble-shoot technical problems faced by the project team.

- Certification in SQL server or other database technologies is value add.

- Experience in setting DB replication and Replication management environment is value add.

- Involved Application performance Tuning and database Performance management is value add.

- Experience with Cloud DB Administration is a plus

- Additionally, working knowledge of NOSQL DB such as Mongo, MariaDB, MySQL is value add.

We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated"
29-Apr-2022 T11:53,Software Engineer - Data Platform,Twitter,4 days ago,,Full–time,"Who We Are

As engineers on the Data Platform team, our mission is to build the fastest, most reliable, and largest-scale data processing technologies in the world - able to cope with ever-increasing volumes of data in real time - and then apply them to the companys most critical and fundamental data problems.

As a member of the team, you will be the source of truth for Twitters most fundamental data - such as Tweet content, engagement data, and user relationships - along with core metrics such as daily and monthly active users.

You will surface these datasets in real time to mission-critical products and business applications throughout the company.

You will empower dozens of engineering teams, hundreds of co-workers, and millions of users to dream of new insights and new possibilities.

What You'll Do

If this sounds like a team you want to be a part of, fantastic! We are looking for experienced engineers who love writing code, data engineering, understanding our customers, and collaborating with teammates to ship useful software.

Sample projects weve built :
• Real-time aggregations of interactions on tweets at 5M / sec scale;
• Unify our batch processing pipelines that count and validate user activity;
• Use hidden Markov modelling to categorize users tweeting states.

Who You Are

You take satisfaction in building resilient, performant, and thoroughly tested distributed systems that can power the most business-critical applications.

You get stuff done and thrive in a small group environment. You have a strong sense of ownership and a curiosity to understand how things work, even if they take you outside your area of expertise.

You welcome feedback on are constantly looking for ways to improve yourself.

Qualifications

On our team, we need people who :
• Have 1+ years of relevant professional experience;
• Have backend development experience with a solid foundation in data pipelines, distributed systems, large-scale data processing;
• Have proficiency with Scala, Java, C / C++ or Python;
• Show deep understanding in at least one data processing framework including Hadoop, Spark, Flink, KafkaStreams, or Dataflow;
• Enjoy working with our internal customers and having empathy for their problems;

Embrace a growth mindset and want to improve ourselves, the team, our processes, and the products we work on.

Additionally it would be nice if you had :
• Success in developing in a hybrid-cloud environment;
• Experience with Lambda architectures and different ways of implementing them;
• Working knowledge of ETL and a query language;
• Experience with on-call responsibilities.

Company Description

Twitter is whats happening and what people are talking about right now. For us, life's not about a job, it's about purpose.

We believe real change starts with conversation. Here, your voice matters. Come as you are and together we'll do what's right (not what's easy) to serve the public conversation.

Additional Information

All your information will be kept confidential according to EEO guidelines.

Twitter is committed to inclusion.

We are committed to an inclusive and diverse Twitter. Twitter is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status.

San Francisco applicants : Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records"
29-Apr-2022 T11:53,Senior Data Engineer,Lynx Analytics,24 mins,,Full–time,"COMPANY OVERVIEW

Founded in 2010, Lynx Analytics is a predictive analytics company run by world-class quantitative marketing scientists and industry-experienced data scientists. Our focus is to become a leading analytics solution provider in our chosen fields of expertise (telecom, retail, life sciences, and financial services) while advancing graph analytics technology.

Lynx is headquartered in Singapore with operations in Hong Kong, Germany, USA, Hungary, South Africa, Indonesia, and several other Southeast Asian countries. We work with some of the world’s largest companies and are constantly looking to expand our knowledge base and geographical footprint. Lynx Analytics’ technology is deployed with various Clients across Asia and has significant growth potential.

We have a diverse and inclusive global team comprising Professors, PhDs, MSc’s, and MBAs from Ivy Leagues, INSEAD and NUS with a broad spectrum of experience in start-ups and blue-chip companies (Google, SAP, Vodafone GE, Morgan Stanley, Barclays, HSBC to name but a few). It is the combination of our industry insight and experience, scalable proprietary technology, and highly qualified people that drives our compelling value proposition.

We are looking for ambitious, innovative, empathetic and relentless team players to explore the career opportunities that we offer as we continue to scale our operations.

As our Senior Data Engineer, you will work on automating and productizing advanced big data transformation and analytics pipelines. You would be working with standard big data technologies (Hadoop, Spark, etc) as well as our proprietary big graph analysis framework.

KEY RESPONSIBILITIES

A Senior Data Engineer’s responsibility is to implement and deploy data analysis pipelines at various clients of Lynx Analytics. This includes participating in and supervising the activities below:
• Understand deeply the business problem that we are trying to solve by our analytical solution
• Through continuous consultations with employees of our client, discover the client's existing data sources that are relevant to the problem we try to solve. This includes discussions with client IT, data owners, future business users, etc.
• Working together with the IT teams of the client, define the technical architecture for the analytical solution that we are to deploy for the client.
• Implement the data ingestion subsystem: this is the system responsible for moving all the necessary data sources to a single location where the actual analysis will happen.
• Implement the data analysis pipelines.
• Integrate the results into business UIs developed by Lynx or pre-existing client software systems

REQUIREMENTS
• Relevant tertiary qualification, preferably at Masters level or above, in Engineering or another relevant discipline with strong academic results
• Strong programming skills
• Solid knowledge of Python and Java (or better yet, Scala)
• Good understanding of the Linux OS including basic sysadmin and shell scripting abilities
• SQL
• Experience in project delivery in a B2B setting
• Good problem-solving skills
• Fluency in English
• Willingness to travel

DESIRABLE
• Experience in Big Data
• A minimum of 6 years of experience in Data Science or Analytics
• Industry experience in working for a big enterprise (like our clients)

WHAT WE OFFER
• Opportunity to work on creating innovative, leading-edge data science pipelines using our state of the art, in-house built big graph tool
• Work closely with the developers of the (big graph) tool you will be building upon
• Be a member of a very strong team with mathematicians, ex-Googlers, Ivy League professors, MBA alumni and telecommunications industry experts
• Startup atmosphere
• Competitive salary
• Equity incentives for employees
• Opportunity to travel (Southeast Asia, US, and Europe)
• Flexible working hours, family-friendly workplace"
29-Apr-2022 T11:53,Data Engineer - APAC,Tamr,22 hours ago,,Full–time,"Company Description

Tamr is the enterprise data mastering company trusted by large enterprises like Blackstone, the US Air Force, Toyota, and GSK. The company’s patented software platform uses machine learning supplemented with human feedback to master and prepare data across myriad silos to deliver previously unavailable business-changing insights. With a co-founding team led by Andy Palmer (founding CEO of Vertica) and Mike Stonebraker (Turing Award winner) and backed by top-tier investors such as NEA and GV, Tamr is transforming how companies get value from their data.

Job Description

Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE

who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being

customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation

This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.

Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion

Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:53,Data Engineer,MANN+HUMMEL VENTURES PTE. LTD.,4 days ago,$8K–$9K a month,Full–time,"Your challenge
• Working cross-functionally with business managers/product managers/engineers and data scientists to gather requirements and to understand their business processes
• Design, develop, deploy, manage scalable cloud infrastructures that solve business problems
• Designing, implementing, and managing optimal data analytics pipelines from end-to-end
• Making strategic data architecture recommendations
• Implementing data science frameworks to enable organization-wide experiments, enable data science research and development and enable deployment of production solutions
• Implementing systems that enable delivery of insights to business units and customers
• Apply dev-ops processes to deliver and maintain production level systems
• Apply quality processes to ensure the requirements are met
• Implement and manage security in accordance with industry standards
• Document all aspects of design, implement, test and release

Your profile

As a successful applicant, you would have a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field with the ability to manage stakeholders and communicate well. You will have strong experience in cloud technologies (AWS/Azure or similar) with at least 5 years of experience in a Data Engineer role.
• A history of working with large scale reliable data systems
• Proficient with cloud technologies and application of native services
• Experience with big data tools and delivery of big data solutions
• Experience in working with different types of data stores including SQL, NoSQL, warehouses, lakes, etc
• Experience working in Hadoop ecosystem and Spark is a plus
• Experience with data ingestion and data integration tools and frameworks, data pipeline and workflow management, common data science tools such as R, python, Big data technologies, Tableau
• Proven ability to deliver high profile activities to tight timescales
• Proven ability to apply analytical and creative thought
• Ability and desire to learn and pick up new tools and technologies for problem solving, enhancing analysis results and accuracy, and optimizing workflow efficiency
• Independent and possess creative problem solving skills to address business problems from different perspectives
• Ability to distil and communicate results to all organisational levels
• Practice a lean agile scrum process to continuously deliver value to customers
• Proven success in contributing to a team-oriented environment
• Ability to interact with global teams and manage the related cultural challenges
• Be able to interact with development teams to determine project requirements

Please indicate in your resume, the position you are applying.

Are you full of ideas? Are you keen to take on responsibility and really achieve something? Then our doors are open to you. This is a company that lives out its values, gives people the freedom to use their own initiative, offers many development opportunities and many exciting projects – all of which awaits you here"
29-Apr-2022 T11:53,Data Engineer (C# .Net Engineer) -Only Local,Capgemini Singapore Pte Ltd,8 days ago,,Full–time,"Hi Greeting!! .Net Developer Experience:5+ Years Location: Singapore Roles: 4+ years of experience as a C#/.NET Developer Backend Developer in C# or .NET (Engineer background is not a must and client is not interested in front end experience such as ASP.NET/Web API) Good understanding and have experience in entity framework (perhaps also Linq) Expecting the candidate can present / articulate the related technical concepts and how-to implement to show the capabilities to perform hands-on job Can work independently on the tasks assigned with limited support Excellent knowledge and hands-on skills in C#. NET WinForms, Multithreading, TPL, LINQ etc Experience of working on third party libraries like Sync fusion, Infragistics or DevExpress UI libraries and Log4Net Unit testing, integration testing, behaviour testing, end to end testing with JUnit, Mockito, Cucumber, etc. Experience using relational databases and SQL (such as PostgreSQL or Oracle) Nice to have Rest API, Microservice knowledge of at least one specific asset class / line of financial instruments would be valued"
29-Apr-2022 T11:53,"Financial Services Advisory, Data Engineer",KPMG,3 days ago,,Full–time,"As KPMG embarks on a rapid growth strategy to become the preferred data driven strategy execution firm in South East Asia, we established the propositions in data driven transformation, and cloud & data center-of-excellence for all the different line of businesses to serve our clients. The ideal candidate should have experience in digital and data driven transformation while working for cloud providers, consulting firm or financial services organisations.

This role involves:
• Work closely with clients and stakeholders to solve enterprise problems like Database Migrations, Cloud Native and Serverless Applications, Big Data and Analytics Solutions
• Translate business requirements & end to end designs into technical implementations and responsible for building batch and real-time data warehouse
• Build robust batch and streaming data pipelines for production-grade data products /platforms, ensuring scalability and reliability
• Create web services or APIs to connect and integrate analytical stacks to application layers
• Build and maintain both cloud and on-premise data infrastructure
• Manage data modelling design, writing, and optimizing ETL jobs
• Data mining analysis and processing ability, participate in data access, inspection, optimisation and other processing processes
⠀
The ideal candidate should possess:
• Bachelors or Masters in Computer Science, Computer Engineering, Information Systems, or similar IT related courses or Exceptional candidates with Diploma / Specialist Diploma in Informatics / Analytics / Machine Learning / Data Analytics
• 1 – 6 years of experience in ETL, data pipeline building, and data warehousing
• Proficient in designing efficient and robust data integration workflows using at least one of Informatica, Datastage, SSIS, Ab Initio, Talend, etc.
• Exposure to Database technologies such as Oracle, DB2, Ms SQL, MySQL, Postgress, Informix, etc.
• Solid knowledge in data processing, like Hadoop, Mapreduce, Hive, Storm, Spark, Kylin, Scribe, Kafka, HBase, Canal, Sqoop etc
• Proficient in SQL, R and Python
• Advanced understanding of database principles, security, and administration
• Familiar with SQL, dimensional modelling, data warehousing, and data integration
• Familiar with system architecture design and analysis
• Competent in software engineering and agile development
• Able to implement and maintain components of big data technologies for both exploratory and production data science platforms
• Strong client and stakeholder management abilities coupled with excellent communication, written, analytical, organisational and problem-solving skills"
29-Apr-2022 T11:53,Data Engineer 3 - AWS & Python (Contractual),The Economist,1 day ago,,Full–time,"The Economist Intelligence Unit (EIU) is a world leader in global business intelligence. We help businesses, the financial sector and governments to understand how the world is changing and how that creates opportunities to be seized and risks to be managed.

At our heart is a 50 year forward look, a global forecast of the majority of the world’s economies, we seek to analyse the future and deliver that insight through multiple channels and insights, allowing our clients to take better trading, investment and policy decisions.

We’re changing, embedding alternate data sources such as GPS and satellite data into our forecasting, products will increasingly be tailored to individual clients, driven by some of the most innovative data in the market. A highly collaborative team of Product Managers, Customer Experience and Product Engineering is being created with a focus on creating business and customer value driven by real time analytics alongside our traditional products.

What will you experience

At Economist Intelligence Unit (EIU) we believe having the right work-life balance is super important; striking balance between your personal and professional life is critical to wellbeing and happiness. We offer flexible working and have recently shifted to a 'remote first' working policy with a minimum expectation of coming to the office two days a month, however you can come in more often if you wish to.

How you will contribute:
• Build data pipelines: Architecting, creating and maintaining data pipelines and ETL processes in AWS via Python, Glue and Lambda
• Support and Transition: Support and optimise our current desktop data tool set and Excel analysis pipeline to a transformative Cloud scale Big Data Architecture environment.
• Work in an agile environment: within a collaborative agile product team using Kanban
• Collaborate across departments: Work in close relationship with data science teams and with business (economists/data) analysts in refining their data requirements for various initiatives and data consumption requirements.
• Educate and train: Required to train colleagues such as data scientists, analysts, and stakeholders in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases.
• Participate in ensuring compliance and governance during data use: To ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives.
• Become a data and analytics evangelist: This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals.

To succeed in this role it would be an advantage if you possess:
• Experience with programing in Python, and Lambda functions
• Knowledge of building bespoke ETL solutions, and extracting data using Data APIs
• MS SQL Server (data modelling, T-SQL, and SSIS) for managing business data and reporting
• Prior experience in design and developing microservice architecture
• Ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management.
• A combination of IT skills, data governance skills, analytics skills and economics knowledge
• An advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (postgraduation diploma or related) or a related quantitative field or equivalent work experience.
• Experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms"
29-Apr-2022 T11:53,Big Data Engineer,PERSOLKELLY SINGAPORE PTE. LTD.,4 days ago,$7K–$13K a month,Full–time,"Responsibilities
• Develop data processing pipelines for ingestion, modelling, analysis, mining and reporting with Enterprise Big Data Lake
• Responsible for the code writing of the core module of the system
• Develop POC and build data pipeline architecture using of the overall technical framework of the software
• Work closely with teams ensure timely delivery of assignments

Requirements:
• Experience building large scale enterprise data pipelines using commercial and/or open source Big Data platforms from vendors such as Hortonworks/Cloudera, MapR, for Hadoop based platforms or NoSQL platforms such as Cassandra, HBase, DataStax, Couchbase, Elastic Search, Neo4j etc
• Hands on experience in Spark, Scala, Impala, Hive SQL, Apache Nifi necessary to build and maintain complex queries, streaming and real-time data pipelines
• Data modelling and architecting skills including strong foundation in data warehousing concepts, data normalisation, and dimensional data modelling such as OLAP

To Apply:

Interested candidates, who wish to apply for the above position; please send in your resume to TOS3@persolkelly.com or click the ""Apply Now"" below and ""ATTN: BVIN""

We regret that only shortlisted applicants would be notified.

B Vidita Nantini | REG No : R22105644

PERSOLKELLY SINGAPORE PTE LTD | EA License No : 01C4394

By sending us your personal data and curriculum vitae (CV), you are deemed to consent to PERSOLKELLY Singapore Pte Ltd and its affiliates collecting, using and disclosing my personal data for the purposes set out in the Privacy Policy which is available at www.persolkelly.com.sg I also acknowledge that I have read, understood, and agree to the said Privacy Policy"
29-Apr-2022 T11:53,Algorithm Engineer - Customer Lifetime Value (CLV),Shopee,Full–time,,,"Responsible for user CLV prediction, establishing real-time prediction model, and responsible for basic algorithm and strategy research of CLV system in multiple scenarios.Responsible for the feature engineering of algorithms, continue to expand business tags, mine user portraits, and continuously improve prediction accuracy.Build and optimise the algorithm architecture, improve forecasting efficiency, and develop data products and decision-making tools based on business scenarios."
29-Apr-2022 T11:53,"Associate Data Engineer, Data & Analytics - Technology Consulting",EY,6 days ago,,Full–time,"At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all.

The opportunity

EY DnA is the data and advanced analytics capability within EY Asia-Pacific, with over 500 specialist employees working across multiple industry sectors.

We implement information-driven strategies, data platforms and advanced data analytics solution systems that help grow, optimize and protect client organizations. We go beyond strategy and provide end to end design, build and implementation of real life data environments and have some of the best architects, project managers, business analysts, data scientists, big data engineers, developers and consultants in the region.

We are looking for a Data Engineer within the DnA team in our Singapore office. This role is offered on a flexible full time basis.

Your key responsibilities
• Strong analytical and problem-solving skills
• Strong drive to excel professionally, and to guide and motivate others
• Advanced written and verbal communication skills
• Dedicated, innovative, resourceful, analytical and able to work under pressure
• Foster an efficient, innovative and team-oriented work environment

Skills and attributes for success
• Experience in ETL, Data Engineering, Scripting.
• Knowledge and experience in end-to-end project delivery, either traditional SDLC or agile delivery methodologies (or hybrid approaches)
• Experience in a delivery role on Business Intelligence, Data Warehousing, Big Data or analytics projects
• Exceptional communication, documentation and presentation skills and stakeholder management experiences
• Experience in business intelligence, data warehousing/platform, and data strategy projects

To qualify for the role, you must have
• At least 2 years’ experience as a Data Engineer with experience in development and maintenance support
• Minimum 2yrs experience in dealing large data sets in Greenplum / Hadoop / Oracle / DB2
• Experience in shell /batch scripting
• Data Migration experience in AWS, Azure, Hadoop, GCP environments
• Experience in development and maintenance of Data processing pipelines
• Experience developing machine learning workflows
• Work closely with business analysts to create data components
• Application packaging and deployment experience across DEV to PROD environments

Ideally, you’ll also have
• Dashboarding experience with Tableau / Power BI
• Experience in engaging with both technical and non-technical stakeholders
• Consulting experience and background, including engaging directly with clients
• Degree in Computer Science or IT or Business Analytics

What we look for
• Highly motivated individuals with excellent problem-solving skills and the ability to prioritize shifting workloads in a rapidly changing industry.
• An effective communicator, you’ll be a confident leader equipped with strong people management skills and a genuine passion to make things happen in a dynamic organization.

What we offer

EY offers a competitive remuneration package commensurate with your work experience where you’ll be rewarded for your individual and team performance. We are committed to being an inclusive employer and are happy to consider flexible working arrangements, where this may be needed, guided by our FWA Policy.

Plus, we offer:
• Continuous learning: You’ll develop the mindset and skills to navigate whatever comes next.
• Success as defined by you: We’ll provide the tools and flexibility, so you can make a meaningful impact, your way.
• Transformative leadership: We’ll give you the insights, coaching and confidence to be the leader the world needs.
• Diverse and inclusive culture: You’ll be embraced for who you are and empowered to use your voice to help others find theirs.

If you can demonstrate that you meet the criteria above, please contact us as soon as possible.

The exceptional EY experience. It’s yours to build.

Apply now"
29-Apr-2022 T11:53,Senior Data Engineer,Aon,23 days ago,,Full–time,"We're hiring! Aon's Center for Innovation & Analytics (ACIA) is currently recruiting a Senior Data Engineer to join our Health Analytics team in Singapore. About ACIA Aon's Centers for Innovation and Analytics are at the heart of delivering Aon's Data & Analytic Services team's mission to: Accelerate the rate of innovation through digital solutions to help better respond to clients evolving needs Provide foundational data and analytics capabilities in one place for 50,000 Aon colleagues and our global clients who use our risk and people solutions Established in 2012, there are over 100 colleagues in Singapore's Centre today including actuaries, software developers, data scientists, financial analysts and accountants. We are expanding rapidly and looking for dedicated individuals who can leverage emerging technologies and collaborate across Aon's solution lines to help clients and colleagues make better, data-driven decisions today and tomorrow. Responsibilities: Be part of a dynamic team focused on healthcare data and innovation initiatives Collaborate with a team of experienced actuaries, data scientists, developers and business experts to deliver on Aon's data, analytics and reporting capabilities Lead the design, build and maintenance of a scalable data pipeline architecture from ingestion to transformation to storage to consumption across various applications Focus on an efficient approach to design, allowing for agile and robust development to meet ever-changing business needs Contribute to the design of our data lake and the development of all data assets and downstream warehouses/marts Partner with data scientists and analysts to ensure efficient and accurate flow of data into downstream analytics tools and reporting solutions Implement automation wherever possible to streamline the process, minimizing the need for manual intervention Assist in database performance tuning and data lifecycle management Develop and integrate an automated data validation and quality control mechanism throughout the pipeline Problem solve complex issues and work to create elegant, simplified solutions Maintain up-to-date technical and operational documentation Requirements: BS or MS degree in Computer Science, Information Technology or equivalent 5+ years in-depth experience in working with a relational database system (eg. MSSQL, PostgreSQL) Highly knowledgeable in ETL/ELT processes, both design and implementation Some experience in a programming language (eg.Python ,C#) Experience building data pipelines that ingests various types of data sources into a database or data lake and populating a structured warehouse or data mart Fluent in both complex SQL query performance tuning and database performance tuning Experience in Apache Spark/Databricks, Python/Pandas, R/Tidyverse and large file processing and analytics data pipelines Knowledge of data security and privacy practices and regulations, including data access controls and de-identification and protection of sensitive data Proven ability to convey technical information successfully to a wide variety of stakeholders Quick and eager student of new technologies as and when they are needed Knowledge of Agile Scrum and Continuous Delivery practices is desirable Experience in BI tools (e.g. Tableau, Power BI) is an advantage Experience working with medical and prescription drug claims data is an advantage How to Apply Your opportunity to empower results could start right here. Make your mark and apply online today with a brief covering letter and your resume, sharing relevant achievements for this position. Please upload your resume in PDF format. We Offer You A competitive total rewards package, continuing education & training, and tremendous potential with a growing worldwide organization. Our Colleague Experience Every day, our colleagues make a difference, work with the best, own their potential, and value one another. Together, we share this one purpose: to empower economic and human possibility around the world. This unifying goal is at the heart of our identity, and it lives in everything we do. To learn more about our colleague experience, visit Aon Colleague Experience. Aon is an equal opportunities employer. We are committed to creating a winning and inclusive culture where everyone feels valued and has opportunities for growth and development. 2504705"
29-Apr-2022 T11:53,Lead Data Engineer,Evolution Recruitment Solutions Pte. Ltd.,7 days ago,,Full–time,"We're Achilles Systems. Be ready to Innovate.

Achilles Systems is a pioneering Business-to-Government (B2G) and Financial Software-as-a-Solution (SaaS) holding company.

We build technology solutions and applications to promote the growth of Southeast Asia's economy.

Since 2015 OnlinePajak, an Achilles Systems company, has created a new technology category by revolutionizing the way companies and individuals process taxes, invoices and payrolls in Indonesia.

With more than 10% of the Indonesian economy using the OnlinePajak platform, the company benefits from a highly valuable partner ecosystem extending across multiple industries and sectors.

Backed by Sequoia Capital, Tencent, Warburg Pincus, VISA, Altos Ventures, Alpha JWC Ventures, Endeavor Catalyst and Global Innovation fund, OnlinePajak is headquartered in Indonesia with presence in Sydney and Singapore.

Work with innovative teams across multiple functions and topics

We are looking for an experienced Lead Data Engineer to join our team. The successful candidate will have strong technical skills and mentor our team of data engineers to grow to be best in class.

You will use various methods to transform and improve our data engineering systems.

To succeed in this position you should have strong programming skills and the ability to combine data from different sources.

Data engineer skills also include familiarity with data engineering related devops and infrastructure, and some knowledge of analytical methods or machine learning methods.

Responsibilities :
• Lead and mentor our data engineers to grow as a high performing team
• Evaluate business needs and objectives
• Work closely with our data science and insights teams
• Build out and advance our data systems and ETL pipeline
• Build algorithms and prototypes
• Maintain the health and growth of our Data infrastructure
• Combine raw information from different sources
• Explore ways to enhance data quality and reliability
• Identify opportunities for data acquisition
• Develop analytical tools and programs
• Collaborate with data scientists and architects on several projects

Qualifications :
• Bachelor's degree in computer science or management information systems and / or 5+ years equivalent work experience.
• Previous experience as a data engineering lead or in a similar role
• Strong programming abilities in one or two languages (such as Scala, Java,Kotlin, Python, Rust, Go, C#, F#)
• You have proven experience with ETL tools and their design.
• Strong SQL ability.
• A strong understanding of functional and distributed systems.
• Experience with no-SQL or graph databases is valued.
• Interested in, or even better, familiar with machine learning is valued but not required.
• A degree in computer science, data science or statistics is valued but not required"
29-Apr-2022 T11:53,"Data Engineer, Capability Development (DART)",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Data Engineer in GovTech’s Data Science & Artificial Intelligence Division, you will be involved in architecting, managing and running advanced analytics and data platforms. For this role, you will be part of the Data Analytics Readiness Team (DART) within GovTech, where you will help drive GovTech’s Data Transformation efforts. You will assist to establish and implement effective operational workflows, data pipelines, procedures and best practices in accordance with government infrastructure and security policies to ensure that the platforms are monitored, secure, available and reliable.

We are looking for an independent and motivated engineer who has experience in both small and large-scale analytics platform projects, and in building and maintaining the infrastructure.

What you will be working on:
• Design, Architect, Deploy, and maintain solutions on Microsoft Azure or AWS using different Cloud & Big Data Technologies to provide secure and governed access to data for business users.
• Manage the full life-cycle of a data warehouse/lakehouse solutions from requirement gathering and analysis to platform selection, design of the architecture, and deployment.
• Collaborate with data stewards, data analysts and data scientists to build data pipelines from enterprise systems such as Workday to collect, clean, harmonise, merge and consolidate data sources for data warehouse/lakehouse.

Where we are looking for:
• Diploma/Degree in Computer Science or Information Technology or related disciplines
• Experience with the cloud (e.g. AWS, GCP, Azure)
• Hands-on experience in implementing Data Lake/Data Warehouse with technologies like – Databricks, Azure Synapse Analytics, SQL Database, AWS Lake formation.
• Programming & debugging skills in Python, Scala or R.
• Proficient in SQL.
• Knowledge of both SQL and NoSQL databases.
• Comfortable with DevOps tools like AWS Cloud Formation/Terraform, Docker and Git for CI/CD development.
• Prior experience with data engineering tools and frameworks like Airflow, Kafka, Hadoop, Spark, Kubernetes.
• Familiar in building REST services is good to have.

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:53,Lead Data Engineer,Cake DeFi,7 days ago,"$8,667–$15,111 a month",Full–time,"Cake DeFi’s Business Intelligence and Data Engineering team is on a mission to build a robust, scalable, and resilient Data Platform by leveraging efficient tools and technologies, for data producers and consumers to identify everyday opportunities through data-driven innovation.

The team is entrusted to engineer a next generation data platform, which is a once in a lifetime opportunity to develop a modern data platform from scratch. If building world class tools and technologies which can scale to handle large data volumes excites you, then we’d love for you to be a part of our exciting journey. This team will never stop learning, innovating, and expanding so that we can build the latest and best tools and technologies for Cake’s continued success.

About the role

Data Engineers in Cake get to work in a challenging, fast paced and ever-changing environment that will push you to grow and learn. As a Data Engineer, you'll work very closely with the Product/Business and Engineering team to build efficient tools and technologies to innovate on how data is effectively used at Cake. You would empower users to solve/address complex problems using data in a truly self-serve mode.

What you’ll do:
• Design and define data architecture framework, standards and principles, including modelling, metadata and security
• Recommend solutions to improve new and existing data platforms including migration
• Build, deploy and manage big data tools with solid devops functions. Be able to manage CI/CD pipelines
• Streamline data access and security to enable BI analysts and engineers to easily access data whenever they need to
• Developing automation framework using programming languages such as python/scala and automate the data workflows such as ingestion, aggregation, ETL processing etc
• Maintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable and timely delivery of key insights for decision making
• Run Modern high performance analytical databases, with Solid understanding of distributed computing, and be able to build out scalable and reliable ETL pipelines and processes to ingest data from a variety of data sources with high performance analytical databases and computation engines like Spark, Presto and others

What you’ll need:
• A degree or higher in Computer Science, Software Engineering, Information Technology or other related technical disciplines
• 8+ years of experience working in related areas with deep technical abilities in data management products/technologies
• Proficiency in SQL and deep understanding of DWH architecture and data/table formats such as parquet/json/csv in distributed data processing and storage systems
• Good experience in handling large data sets and working with structured, unstructured data, prior experience with Redshift or other MPP systems is a must
• Good experience in building nifty data pipelines using tools such as Glue, AWS Data pipeline or similar
• Knowledgeable on cloud systems like AWS, Azure
• Good experience with programming languages like Python, Scala, Java and scripting languages such as bash
• Excellent communication skills, to act as an effective liaison between platform engineering, DevOps and BI teams

Good to have:
• Deep understanding on databases and best engineering practices - include handling and logging errors, monitoring the system, building fault-tolerant pipelines, understanding how to scale, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipeline

Why work with Cake:
• Do something with purpose; Be a part of the future that will shape on how people are dealing with their finances in Crypto and Blockchain
• Fast moving, challenging and unique business problems
• International, diverse work environment and flat hierarchy
• Great career development opportunities in a growing company
• Competitive salary
• Flexible working hours, unlimited discretionary leave, casual work attire"
29-Apr-2022 T11:53,DATA ENGINEER (2),3I INFOTECH ASIA PACIFIC PTE. LTD.,5 days ago,$3.5K–$8K a month,Contractor,"DATA ENGINEER

Job Description

A ) Understand data management priorities, data assets, data policies and guidelines.

b) Collaborate with a team to design, architect and manage enterprise data architecture, data pipelines and data products.

c) Integrate and collate data sources with data systems, with compliance to data security and organisational governance standards.

d) Build ingestion pipelines to collect, clean, harmonise, merge and consolidate data sources.

e) Support the design and development of APIs to expose data to systems via secure means.

f) Work with data steward, product managers, software engineers, data analysts and data scientists to build scalable data repositories that facilitate insights discovery by officers.

Candidate Specifications

A )Experience in Data Warehouse operations, with good understanding of data warehousing concepts with good understanding of data warehousing concepts

like (Data Models, SCDs, ETL / ELT, and RBACs). Experience with building data pipelines and writing ETL / ELT scripts for data processing

Technical Skillset and Experience

b) Proficiency in SQL scripting for databases like AWS Aurora, Snowflake, and MSSQL.

c) Familiarity in Python programming.

d) Proficiency with data pipelining and scheduling tools like AWS Glue and Microsoft SSIS.

e) Familiarity with DevOPs and CICD tools such as Git, Docker, Terraform is a plus.

f) Able to administrate tools like Tableau Server.

g) Experience with Amazon Cloud services.

3i Infotech Asia Pacific Pte Ltd

Future is of what you make of it today! We are continuously growing and are looking for that persona that wants to grow along. I am looking for a Data Analyst to lead the transformation journey with 3i Infotech Asia Pacific Pte Ltd. If you are that someone and available to join in short notice and looking to bring the change around, pls inbox your profile to

zarina.bevi@3i-infotech.com

Best Regards

Zarina

Hr

Asia Pacific Region

Contact +65 94576290"
29-Apr-2022 T11:53,Data Engineer,HCL Technologies,1 day ago,,Full–time,"We are looking for an experienced Data Engineer to join our growing team of analytics. The candidate will be responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Candidate must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

Responsibilities

· Create and maintain optimal data pipeline.

· Assemble large, complex data sets that meet functional / non-functional business requirements.

· Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing Jobs/code for greater scalability, etc.

· Work with stakeholders including the Product owner, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

· Work with data and analytics experts to strive for greater functionality in our data systems.

Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

·Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

·Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

·A successful history of manipulating, processing and extracting value from large datasets.

·Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:53,Data Engineer,Endowus,5 days ago,,Full–time,"Data Engineering Team | Singapore / Remote

About us

Endowus is Asia’s leading fee-only digital wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping all investors grow their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.

Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones. Our mission is clear: help people invest better so they can live easier today, and better tomorrow.

The people you will work with

The team has deep domain knowledge in finance and technology, bringing together decades of experience at Goldman Sachs, Morgan Stanley, UBS, Credit Suisse, The Blackstone Group, AQR, Grab, Dropbox, Lyft, Redmart Carousell, Bytedance, Grab, Kakao, Alibaba, and more. See our leadership team here. We practise inclusion and treasure our diversity in background and experience. A diverse team is our biggest asset and we look for people who share our belief in Endowus' clear mission.‍

Investors, recognition, licensing

Endowus is backed by global leading strategic and venture capital investors including UBS, Samsung Ventures, EDBI, Prosus Ventures, ZVC, Singtel Innov8, Lightspeed Venture Partners, and SoftBank Ventures Asia.

Endowus has been recognised by the industry with the following awards: Singapore’s Rising Star and Fintech Innovation (Asia Asset Management’s Best of the Best Awards 2021), LinkedIn Top Start-ups 2021, WealthTech of the Year (Asia FinTech Awards 2021), and the top 15 Singapore’s Best Workplaces Award (Great Place to Work).

Endowus is licensed by the Monetary Authority of Singapore (MAS).

About this role; responsibilities & ownership
• We are looking for a Data Engineer who thrives in a fast-paced environment and enjoys driving innovation through rapid prototyping and iterative development.
• Being part of Endowus's Technology team, you will build end-to-end product features that you are confident of delighting user experiences.
• You will need to leverage the entire technology stack to realise these goals and will be part of a team that is constantly tackling difficult questions of scale, architecture, and interaction.

Requirements & qualifications
• Bachelors' or above in Computer Science, a related field, or equivalent professional experience
• 3-6 years of experience in designing and implementing key components for highly scalable, distributed data collection and analysis systems built for handling large volumes of data in cloud
• Advanced working SQL knowledge and experience working with relational databases or BigQuery, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimising data pipelines and data sets in Apache Flink or Apache Spark to answer specific business questions.
• Experience in at least one of the JVM languages such as Java, Scala.
• Experience Build processes supporting data transformation, data structures, metadata, dependency and workload management. (experience in Airflow is Plus)
• Experience with distributed databases, such as Cassandra, and the key issues affecting their performance and reliability.
• Hands-on development with key technologies including Scala, Apache Flink and other relevant distributed computing languages, frameworks, and libraries
• Independence and self-reliance while being a proactive team player with excellent communication skills.
• Self-driven, has a strong sense of ownership and able to work and learn independently
• Comfortable with numbers and motivated by steep learning curves

Nice to haves
• Experience with message broker systems, such as Apache Kafka.
• Developed mission-critical products, e.g. trading platforms, medical applications
• Basic knowledge of finance and trading
• Preferably some exposure to CQRS / Event Sourcing patterns
• Familiarity with Docker, Kubernetes and Terraform

Remote Okay
• We are open to hiring remotely in Asia time zones.

Benefits & perks
• Stock options in a fast growing company *
• Employee discount for investing at Endowus
• Flexible working hours and locations so you can live life and your best work - you are trusted to be responsible
• Competitive Staff Benefits; Annual Leave, Medical insurance, Dental *
• Productivity Equipment; Omnidesk Standing Desk & Ergonomic Chairs, Dell UltraSharp USB Type-C Monitor, high-end computer peripherals
• Choice of Apple or PC equipment
• Additional S$250 /HKD 1450 budget for your favourite hardware to boost productivity *

Note: * is only applicable to Full-Time employees

‍

How to apply

Everyone can be an Endowus client. If you are not yet a client, please create an account to understand our services and technology, and can see if our mission is something that aligns with your personal beliefs. We value your feedback on our product and will ask you for your insights throughout the interview process.

To apply email to

Engineering.Careers@Endowus.com
with the following details:
• Email subject: [Role you are applying for], [Your name]
• Attach your CV
• Content portfolio — Original content creation (English and Traditional Chinese), translation (English to Traditional Chinese), any marketing/social media samples
• Cover letter in body of your email
• Desired salary/package
• Notice period (if any)
• Current location
• Visa assistance for role location? (Y/N"
29-Apr-2022 T11:53,Data Engineer (Python/Snowflake),Keyrus Singapore Pte. Ltd.,19 days ago,,Full–time,"Keyrus is an international consulting firm, specializing in the integration of data intelligence and Digital solutions. With over 3000 employees spread across 20 countries, Keyrus continues to deliver on such projects to a wide range of clients from various industries including but not limited to Banking/Finance, Healthcare/pharmaceuticals, FMCG, Oil & Gas, and more. As part of Keyrus solution delivery, we are also in a position to recruit and place technical consultants to complement on existing client projects with their expertise. As such, we seek innovative and agile people to support ambitious and forthcoming technological challenges. The team in Singapore is currently looking for a Data Engineer with strong expertise in handling building pipelines, in order to support our client activities. The high level job scope and skills required are: Responsibilities - Work closely with our client Data Engineering teams to transition their data warehouse onto Snowflake, including leading and conducting a comprehensive UAT for each market - Collaborate with global Engineering team to integrate new data sources or introduce new features or procedures - Develop, maintain, and optimize data pipelines and assemble datasets in the data lake (Snowflake) that enable analysts to build comprehensive analyses easily and effectively - Develop, maintain, and optimize ETL flows and calculated fields in BI platform (Domo) to ensure the right balance of flexibility and performance based on the dashboard use case - Manage data, scripts, and documentation in line with business requirements and compliance rules - Create and maintain alerts that notify the relevant teams of any data issues, as well as to business users if the data is not ready for consumption - Perform root cause analysis on data and processes to identify opportunities for improvement - Build dashboards, perform analyses, and present findings to solve business questions raised by different teams across the organization - Analyze business trends and proactively suggest new ways to better understand the different markets - Adhere to technology standards and consistency Requirements - Bachelor's degree or higher in any field; computer science, analytics, engineering, or mathematics degrees are a plus - Advanced professional expertise in SQL and Python programming - Minimum 4 years work experience in building data pipelines, transforming, and cleaning data - Hands-on experience managing relational databases; experience with Snowflake, AWS, and Informatica is a huge plus - Strong project management and organizational skills - Strong analytical and problem-solving skills - Clear and effective verbal and written communication skills - Strong business acumen and ability to grasp business needs - Strong attention to detail - Self-starter, curious, and eager to try or learn new skills"
29-Apr-2022 T11:53,Staff Data Engineer (Data Product),TECHKNOWLEDGEY PTE. LTD.,5 days ago,$8K–$12K a month,Full–time,"• Development and programming functions to ensure that projects are delivered on time and within budget with good code quality.
• Work with architects, systems analysts, project managers, QA, and other developers to successfully implement business requirements while applying the latest available tools and technology.
• Responsible for the architecture, design, development, implementation of data-based software applications. This includes working with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations.
• Interact with business units to define requirements/modifications and use case to utilize in designing appropriate solutions.
• Provide recommendation on scope and scale of effort required to develop solution.
• Design, develop, document, and implement new programs and subprograms, as well as
• enhancements, modifications, and corrections to existing software - Develop testing and debugging routines.
• Create documentation and procedures for installation and maintenance.
• Build and maintain relationship with global and virtual teams and third parties on software development or support issues.
• Identify opportunities for further enhancements and refinements to standards, best practices, and development methodologies.
• Work directly with Architects, System Analysts, Dev leads, and QA team leads to manage the technical aspects of a development pipeline.

Requirements
• Extensive experience in architecting and developing real-time applications that are fault tolerant, scalable and can handle high volumes.
• Experience in best practices for API development and design patterns.
• Experience in all phases of software development life cycle including project management, functional requirements definition, technical design, development, testing, quality assurance, system certification, systems implementation, and system validation.
• Consistently able to assess and evaluate problems in a production environment and manage risk to the service when recommending change.
• Strong secure coding practices.
• Good Knowledge on Hadoop framework and related Big Data Technologies (HDFS, Map
• Reduce, Spark, HBase, Kafka).
• Strong knowledge in Java or Scala or Python.
• Strong knowledge of database concepts, systems architecture, and data structures is a must.
• Java/J2EE/Angular, Spring Cloud, Microservices and strong knowledge on API development is desired.
• Experience working in an Agile and Test-Driven Development environment.
• Process oriented with strong analytical and problem-solving skills.
• Work independently and mentor others in the team and with minimal supervision.
• Ability to juggle multiple projects and change direction mid-course based on business drivers.
• Ability to work independently in a high throughput environment.
• Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style"
29-Apr-2022 T11:53,Data Engineer,SPH Media,"$6,000–$8,250 a month",,Full–time,"● Work with a mix of cloud infrastructure and open-source technologies to build, scale, and maintain first-party data workloads.
● Implement and manage both streaming and batch data pipelines.
● Drive audience and behavioural analytics, as well as key business metrics with associated reporting.
● Collaborate with both product engineering and data science teams.
● Ensure sound data governance from collection to storage and activation of a broad range of data products.
● Support the business’s goal of democratisation of data.

You may be a good fit if:

● You have at least 3 years of experience in cloud-based data engineering.
● You have built cloud-based scalable and reliable data pipelines at significant scale.
● You have experience in data-related languages and frameworks such as Spark, AirFlow, Python, SQL, Kafka.
● You have experience in operating data lakes on Amazon S3.
● You have experience in operating data warehouses and/or query engines such as Presto.
● You have experience of AWS data and analytics services such as Glue, Kinesis, or QuickSight.
● You have an understanding of data modelling and data governance"
29-Apr-2022 T11:53,Data Engineer,PwC,9 days ago,,Full–time,"Line of Service

Assurance

Industry/Sector

FS X-Sector

Specialism

Risk

Management Level

Senior Associate

Job Description & Summary

We believe that challenges are better solved together. That's why you'll join a diverse, global community of solvers - an unexpected mix of people that come together to build trust in society and solve important problems. With us, you are encouraged to lead with your heart and values, and where your unique skills are developed and put to work in unexpected and exciting ways, superpowered by technology.

Our Risk Assurance Practice provides an invaluable safeguard in today’s complex operating environment with insights and independent assurance. We work with clients to deliver business control to help them to protect and strengthen every aspect of their business from people to performance, systems to strategy, business plans to business resilience. We help clients manage, mitigate and control risks from potential cybersecurity breaches to possible breaks in the supply chain. We assess and prepare businesses by looking into their technology, finance, data analytics, regulatory requirements, data security and privacy, internal audit, and the third parties our clients rely on, to help clients deliver quality results and meet their strategic objectives.

How will you value-add

The Data Trust Services (DTS) team is integrated across different PwC Lines of Services, and works collaboratively with our clients to manage engagements and lead teams of data and analytics resources in all aspects of design and delivery of data and analytics solutions - including Analytics, Data Visualisation, and Data Management.

As a Senior Associate, you'll work as part of a team of problem solvers, and assists clients to leverage data as an asset throughout its business processes. You'll enjoy working in multi-disciplinary teams and gaining valuable experience in various sectors and play a key role in ensuring high quality outputs and service to our clients. PwC Professional skills and responsibilities for this management level include but are not limited to:
• Contribute technically to Data and Analytics projects
• Perform data integration and exploration to assess relevance of data
• Apply data practices in the aspect of data quality, data security
• Communicate effectively with the project manager and team regarding the progress of the project

About you
• Bachelor’s degree and above in Analytics, Information Systems Management, Computer Science or related fields
• 3+ years of experience in data integration strategy, data modeling, designing, building ETLs, data ingestions, and/or transformations
• Experience in working with RDBMS such as DB2, Oracle, Microsoft SQL Server, PostgresSQL, Teradata, etc.
• Experience in data management and integration tool such as Informatica PowerCentre, Oracle Data Integrator, SAP Data Services, Ab Initio, IBM DataStage or Microsoft SSIS
• Process good knowledge and experience in data quality definition, data cleansing and data treatment/profiling process using industry standard tools like Informatica Data Quality, Trillium Software, SAS Data Quality, SAP Data Quality Management, etc. is an advantage
• Good knowledge of data warehouse and data management implementation methodology
• Good knowledge of the Data Management framework, including operating model, data governance, data management, data security, data quality and data architecture
• Experience in Hadoop environment like Cloudera and HortonWorks
• Experience in Cloud technologies including AWS, Azure, Google Cloud
• Experience in API Management software design and build
• Process good knowledge and experience in data visualisation concepts using tools such as Tableau, Microsoft PowerBI, Qlik, etc.
• Experience with Agile Methodology implementation
• Certification in any of database, data integration, data management, visualisation tools and cloud is an added advantage
• Knowledge about the infrastructure paradigms such as OS, network, etc. is an added advantage
• Ability to pick up new tools and able to be independent with minimal guidance from the project leads/managers
• Strong analytical and creative problem-solving capabilities
• Ability to establish personal credibility quickly and demonstrate expertise
• Only shortlisted candidates will be notified due to the high number of applicants for this role . #LI-HJ1

There have been reports of scammers impersonating PwC HR professionals contacting individuals about fraudulent job opportunities using non-PwC domain email addresses and an overseas number. Please note that genuine communications from our HR team will only come from ""@pwc.com"" email address.

Education (if blank, degree and/or field of study not specified)

Degrees/Field of Study required:

Degrees/Field of Study preferred:

Certifications (if blank, certifications not specified)

Required Skills

Optional Skills

Desired Languages (If blank, desired languages not specified)

Travel Requirements

Not Specified

Available for Work Visa Sponsorship?

Yes

Government Clearance Required?

Yes

Job Posting End Date"
29-Apr-2022 T11:53,Data Engineer,Infodrive Solutions Sdn Bhd,Full–time,,,"Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc Technical proficiency on data mining techniques and performance optimization Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL) Experience on SAS will be beneficial but not mandatory Handling of reporting packages (Tableau, QlikView) is nice to have AWS experience is nice to have Passion to learn and master diverse new technologies in the open-source community"
29-Apr-2022 T11:53,Senior Data Engineer,Grab,3 days ago,,Full–time,"Job Description:

Get to know the Role

As a data engineer, you will be working on all aspects of data, from platform and infra build out to pipeline engineering and writing tooling/services for augmenting and fronting the core platform.

You will be responsible for building and maintaining the state-of-the-art data lifecycle management platform, including acquisition, storage, processing and consumption channels.

The team works closely with data scientists, product managers, legal, compliance and business stakeholders across the SEA in understanding and tailoring the offerings to their needs.

As a member of the data organisation, you will be an early adopter and contributor to various open source big data technologies and you are encouraged to think out of the box and have fun exploring the latest patterns and designs in the fields of software and data engineering.

The day-to-day activities
• Build and manage the data asset using some of the most scalable and resilient open source big data technologies like Airflow, Spark, DBT, Kafka,Yarn/Kubernetes, ElasticSearch, Presto/Dremio, Visualization layer and more.
• Design and deliver the next-gen data lifecycle management suite of tools/frameworks, including ingestion and consumption on the top of the data lake to support real-time, API-based and serverless use-cases, along with batch (mini/micro) as relevant
• Build and expose metadata catalog for the Data Lake for easy exploration, profiling as well as lineage requirements
• Enable Data Science teams to test and productionize various ML models, including propensity, risk and fraud models to better understand, serve and protect our customers
• Lead and/or participate in technical discussions across the organization through collaboration, including running RFC and architecture review sessions, tech talks on new technologies as well as retrospectives
• Apply core software engineering and design concepts in creating operational as well as strategic technical roadmaps for business problems that are vague/not fully understood
• Obsess over security by ensuring all the components, from a platform, frameworks to the applications are fully secure and are compliant by the group’s infosec policies.

The must haves
• At least 2+ years of relevant experience in developing scalable, secured, fault tolerant, resilient & mission-critical big data platforms.
• Able to maintain and monitor the ecosystem with high availability
• Must have sound understanding for all Big Data components & Administration Fundamentals. Hands-on in building a complete data platform using various open source technologies.
• Must have good fundamental hands-on knowledge of Linux and building a big data stack on top of AWS/Azure using Kubernetes.
• Strong understanding of big data and related technologies like Spark, Presto, Airflow, HDFS Yarn etc.
• Good knowledge of Complex Event Processing (CEP) systems like Spark Streaming, Kafka, Apache Flink, Beam etc.
• Experience with NoSQL databases – KV/Document/Graph and similar
• Proven ability to contribute to the open source community and up-to-date with the latest trends in the big data space.
• Able to drive best practices like CI/CD, containerization, blue-green deployments, 12-factor apps, secrets management etc in the Data ecosystem.
• Able to develop an agile platform with auto scale capability up & down as well vertically and horizontally.
• Must be in a position to create a monitoring ecosystem for all the components in use in the data ecosystem.
• Proficiency in at least one of the programming languages Java, Scala or Python along with a fair understanding of runtime complexities.
• Must have the knowledge to build Data metadata, lineage and discoverability from scratch"
29-Apr-2022 T11:53,Senior Data Engineer,Igloo,Full–time,,,"Job Description

We want to give people the freedom and confidence to pursue what matters to them in life. Because they know they’re covered.

Because people-first. Igloo is a Singapore-headquartered regional insurtech benefiting millions of people across SEA, in countries that count as the most underinsured.

Build the future of insurance with us by doing what you do best. Join us if you desire to create impact and do good. Work with our teams across Singapore, China, Indonesia, Thailand, Philippines and Vietnam.

What you will do :
• Build data driven systems for risk control, fraud detection, recommendation, customer segmentation, adaptive pricing etc.
• Build, validate, test, and deploy models and algorithms.
• Work with backend engineers to architect data storage and processing pipelines.
• Work with product managers to develop new product features based on insights from data.

What you will need :
• Bachelor’s Degree in Computer Science / Mathematics / Statistics, or fields related to big data preferred.
• Experience in big data processing with Python, R or Scala.
• Min. 2 years experience in building data pipelines using distributed processing frameworks (e.g Spark, Hadoop, Kafka) and MPP databases (e.g BigQuery).
• Operational experience with industrialization, orchestration (e.g Kubernetes), containers in cloud (e.g Docker) is a must.
• Knowledge in supervised / unsupervised learning, classification / clustering algorithms, feature engineering / optimization is a plus
• Outstanding analytical and problem-solving skills.
• Self-motivated, innovative, and proactive. Willing to learn new knowledge and explore unfamiliar domains"
29-Apr-2022 T11:53,Data Engineer,KEYSTONE CABLE (S) PTE LTD,5 days ago,$3.5K–$5.8K a month,Full–time,"Job Scope
• To provide the technical drive on the company’s i4.0 digitalization and automation plans.
• To understand departmental digitalization requirements and use programming or software to reduce digital waste.
• Help to drive digitization / digitalization efforts for data analysis on key operational metrics; to increase operational capacity, efficiency, productivity and manpower utilisation.
• To design and build data solutions that support company’s data and analytics strategy in driving business insights.
• To work closely with ERP, MES and other technology vendors to implement and support the company’s implementations.

Requirements
• Respectful
• Good communication skills
• Growth mindset
• Willing to accept new challenges and learn
• Technical requirements: Industrial control and automation, Power and PLC systems
• Academic qualifications in computer science or related fields
• Min programming skills: Python, SQL database
• Understanding of data governance, data security and data analytics"
29-Apr-2022 T11:53,"Associate/AVP, Data Engineer, Research Data Management 12281",GIC,$6K–$12.4K a month,,Full–time,"Data Strategy Group (DSG) harnesses and leverages on GIC’s most valuable asset – Data, to drive insights, results and solve real business challenges. We ingest, analyse, and combine data with context across all industries and asset classes to enable investment decisions and enhance investment process. Together, we strive to build a high-performance enterprise data infrastructure, create results-driven approach with data products and strive to be a data-driven organization.

We are hiring an Associate/AVP, Data Engineer, Research Data Management, this role will play a key role in expanding and optimizing our data and data pipeline architecture, as well as taking care of the operation and requests from cross functional teams.

Join us if you have a passion for data and be part of a team of forward-looking data professionals in building and developing GIC’s data foundation and infrastructure. You can make a difference in our data journey to shape our world of investments

Responsibilities
• Work closely with data analysts and business end-users to implement and support data platforms using best-of-breed technology and methodology.
• Design robust and scalable solutions to meet business needs and take operational considerations into account. Demonstrate technical expertise in the assigned area.
• Analyse, tackle, and resolve day-to-day operational incidents and advisory to business users.
• Analyse systems operations data (service level agreements (SLA), customer satisfaction, delivery quality and team efficiency) to identify actionable trends for continual improvements.
• Conduct requirement workshop with stakeholders and analyse requirements holistically.
• Work closely with data governance team to build the data quality check framework and ensure the data issues are monitored, tracked, and fixed without breaching SLA.
• Design and implement scalable data pipeline modules with reusability in-mind to support the growing demands from business users.

Requirements
• Bachelor’s degree in Computer Science, Computer Engineering or equivalent.
• At least 4 years’ experience of working as a data engineer or backend developer in a big data field.
• Solid working knowledge of implementing the optimal data structures and algorithms to create efficient and scalable applications in Java or Python.
• Familiar with application integration with RDBMS such as Oracle, MS-SQL or MySQL. (Working knowledge of Oracle and MS-SQL will be a plus).
• Hands-on experience of using Linux (or Unix-like OS) as the development environment and familiar with shell scripts and command line tools in Linux/Unix environment.
• Exposure and knowledge in the following technologies is advantageous:
• Big Data Platforms – Hadoop (Spark, Hive, Impala, HDFS), Snowflake
• Programming and Scripting: Java, Python, Shell Script, REST API, Informatica, Node.js
• AWS
• Docker
• Data Virtualisation – Denodo
• Data Visualisation – Tableau
• Experienced with the Systems Development Life Cycle implementation methodology (SDLC) and/or agile methodologies like Scrum and Kanban.
• Understand and apply the good industry practice of code versioning, testing, CICD workflow and code documentation.
• Good team player, with strong analytical skills and enjoy complex problem solving with innovative ideas
• Strong communication/people skills required to interact with data analysts, business end-users and vendors to design and develop solutions
• Good at working with details and is meticulous for operations"
29-Apr-2022 T11:53,Data Engineer,H2i Pte.Ltd.,8 days ago,,Full–time,"Our Data Science Team is looking for a passionate and enthusiastic Data Engineer.

Your Role:
• Work on building robust and scalable data processing pipelines using Python
• Implement and maintain CI/CD pipelines in Git
• Collaborate with Data Scientists to implement data science models
• Keep the Data platform stable and continuously improve the system.
• Design, develop, and implement reusable components.
• Work closely together with DevOps
• Work closely with our business stakeholders and engineers

It will be a good fit if you have:
• 2 to 5 years of related data engineering experience
• Good knowledge of relational database
• Proficient in Python
• Experience with cloud deployments, cloud tooling
• A Bachelor degree in Computer Science / Computer Engineering or related fields

It will be a bonus if you have:
• Good knowledge of NOSQL
• Affinity with geospatial/physical modeling

Location: Singapore

APPLY HERE"
29-Apr-2022 T11:53,Senior Data Engineer,Encounters Pte. Ltd.,19 days ago,,Full–time,"Are you passionate about building great products Do you want to redefine the way travellers explore the world Are you looking to work with one of the coolest technology companies in the region Keen to be part of this growth journey with a bunch of amazing people Then Pelago is the place for you! We are looking for ambitious and motivated talents who are excited about staying on the cutting edge of Technology and always keen on learning new tools and technologies to build a great end-to-end customer experience. WHO ARE WE Pelago is an Itinerary focused Travel Marketplace that makes every aspect of a Trip - from Discovery & Organising to Booking & Experiencing - easy, fun and personalised! Backed by a market leading brand with strong funding, the company is currently in the early startup stage, and well-positioned to disrupt the $250 Billion travel experiences industry. We are a team of diverse, passionate, empowered, inclusive, authentic and open individuals who share the same values and strive towards a common goal! WHAT CAN WE OFFER YOU - A unique opportunity to take end-to-end ownership of building innovative products that deliver real value to travellers. - Platforms to solve real customer problems concerning travel planning & booking with innovative products/services. - An amazing peer group to work with, and ability to learn from the similarly great minds around you. - An opportunity to influence your fellow engineers, and ability to learn from the similarly great minds around you. - A diverse, fun, and dynamic environment with colleagues from different parts of the world. - Competitive compensation and benefits - including work flexibility, insurance, and more! WHAT WILL YOU DO We are looking for an experienced Data Engineer to create and manage the data warehouse for Pelago. You will be responsible for designing data pipelines to collate and store data from multiple sources. This data will serve as the single source of truth for all analytical and engineering needs - generating analytic reports, decision making, data science etc.You will be a part of the wider Engineering team. - Spearhead development of systems, architectures, and platforms built for scale - Build out scalable and reliable ETL pipelines and processes to ingest data from a large number and variety of data sources - Work closely with data scientists to ensure real-time data availability and hygiene as required - Work with product managers and business to facilitate timely data availability to serve analytic dashboards - Act as the owner of data collection, availability and cleanliness to enable accurate data access - Diagnose and solve issues in our existing data pipelines and envision and build for scale WHAT EXPERTISE YOU NEED TO HAVE - A degree or higher in Computer Science, Electronics or Electrical Engineering, Software Engineering, Information Technology or other related technical disciplines - Strong foundation in data query/manipulation using SQL - Deep understanding of databases and best engineering practices - include handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding how to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipeline - Experience working with Event Streaming platforms like Kafka (preferred), Kinesis etc. - Passionate about data, new data technologies, and discovering new and interesting solutions to the company's data needs - Excellent communication skills to communicate with the product development engineers to coordinate development of data pipelines, and or any new features that can be built on top of the results of data analysis - Experience working with Python If you're as excited as we are in this journey, do apply directly with a copy of your full resume and portfolio (if any). We'll reach out to you as soon as we can"
29-Apr-2022 T11:53,Lead Data Engineer,Nas Academy,1 day ago,,Full–time,"Who we are:

Nas Academy is a remote-first, global team of highly motivated life-long learners and content creators coming from the Nas Daily brand.

We are on a mission to empower content creators through technology and transform education in the process. The great irony is that people don’t enjoy education, but they love learning. We believe that creators can fix online learning with better technology and world-class learning experiences. https://nasacademy.com/about-us

Why work with us:
• We are a remote-first company.
• We are a diverse bunch spread across the globe, ranging from India, Berlin, South Africa to Singapore.
• There’s a high chance your current job is boring and that’s why you are looking for something new. We are not boring.
• You love learning and are excited about the prospect of completely remodeling the way we do education.
• We are completely merit based. You’re good at something or have potential and step up to the task - you do it. That also looks good on LinkedIn!
• We offer a substantial share package. If the company is doing well financially, so will you.
• You choose whatever working equipment you want.

What we are looking for:
• Leaders, we give you autonomy to own and deliver projects/features.
• Collaboration, we want people who help their peers become a better version of themselves.
• Minimum 5 years + of Software Engineering experience with at least 2 years experience working with Data.
• 1-2 years experience leading a team
• Excellent English skills, a high energy level and be a great team player.

Responsibilities
• You will collaborate with all internal teams to help them with their data requirements by understanding their use-cases and developing pragmatic solutions.
• Lead all things data for the whole team: backend data sanity, product analytics, Frontend A/B experimentation, business metrics
• Build BI dashboards with tools such as Tableau and internal BI tools for data insights and business support. We are open to using new tools - expertise on different tools is welcome :)
• Analyze and document business processes, and identify areas for improvement and optimisation
• Develop data processing pipelines for data modeling, analysis, and reporting from large and complex transaction datasets
• Candidates with experience in working with SQL, NoSQL (mongoDB) and Python will be preferred.

How we work:

Tired of sitting through endless sprint plannings, backlog grooming sessions and roadmap meetings? We don’t do any of this. That’s right, no Scrum, no Agile Coaching, no goal setting, no estimations. Instead we meet every Friday to discuss what’s going to be important the next week and then we just work on that the next week. That’s it, simple like that.

What we expect from your application:
• We’re a video company - a nice introduction from your side via video is highly recommended to clear the review round
• Answer all the questions in your application!
• Make sure that you have a proper setup for a remote call in terms of internet, audio quality and lighting

What to expect in your interview process:

Culture fit screening: 20min zoom video call with Recruitment

Hackerrank test: Sometimes, we can ask you to do a technical test before or after the culture fit. Should this be the case, keep a lookout in your Spam for an email from Hackerrank - inviting you to complete a technical test

Technical Interview: If you are successful in the culture fit screening/test you will be invited to a technical interview round, which will last approximately 1-1.30hrs

Final interview: If you pass the technical interview, you will soon meet with our CEO, Nuseir Yassin - for about 15 mins via zoom, where we will dive a bit deeper into your motivations for wanting to join Nas Academy + you will get a chance to ask us any questions too"
29-Apr-2022 T11:53,Data Engineer Big Data,Adecco Personnel Pte Ltd,17 days ago,,Full–time,"Job Description
• On boarding of new projects / tenets on kafka
• Own process / guidelines / technical architecture for the service
• Setup, install and operate kafka cluster for enterprise
• Provide guidance to application team for adopting kafka for their use cases
• Provide guidance to the application team for optimal configuration at client side balancing resiliency / availability / durability.
• Own platform level resiliency / availability
• Provide support to SRE team for critical platform incidents
• Implement platform level best practices, vendor recommendation etc
• Continuously Improve, Review, optimize, automate deployment pipeline / process
• Establish self service capability for the platform
• Good communication and stakeholder management skill and able to speak up and freely interact with other stakeholders

Requirements :
• An undergraduate engineering degree or higher
• 5+ year experience in working on IT / software projects / programs i
• 3+ year experience Integrations tools like Kafka with a verygood understanding of overall administration, management, and operation of Kafka cluster supporting multiple tenants.
• Strong Software Engineering skills
• Expert knowledge of the SDLC, Development methodologies, AGILE
• Present facts and recommendations effectively in oral and written form
• Pro-active, independent, resourceful and able to work in a team
• Ability to operate effectively both independently and within a team environment
• A technical mindset with great attention to detail
• Working knowledge on Software Configuration Management, Quality Management, Version Control Management toolset(Github, bitbucket, jetkins etc)

Saghana Sithara Registration Number : R1550224"
29-Apr-2022 T11:53,Data Engineer (Fresher),TONIK FINANCIAL PTE. LTD.,8 days ago,$3K–$3.8K a month,Full–time,"• Act as a subject matter expert in data engineering and GCP data technologies.
• Closely work with various business teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
• Work with Agile and DevOps techniques and implementation approaches in the delivery.
• Be required to showcase your GCP Data engineering experience when communicating with business team on their requirements, turning these into technical data solutions.
• Be required to build and deliver Data solutions using GCP products and offerings.

Qualifications:
• Liaise and be part of our extensive GCP community, contributing in the knowledge exchange learning programme of the platform.
• Any Bachelor Degree in Computer Science or related fields
• Possess analytical skills, mental resilience and the ability to think systematically under stressful conditions.
• Highly accountable and takes ownership. Outstanding work ethic, high-integrity team player, and a lifelong learner.
• Mentor other engineers define our technical culture and help build a fast-growing team.

Skill:
• E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.
• Regulatory and Compliance work in Data Management.
• E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
• Work on diversified cloud Platforms consisting of Databases and CICD, Logging, and monitoring tools to provide stable, and reliable DevOps service.
• Build CI/CD pipeline; both design and implementation is an added advantage"
29-Apr-2022 T11:53,Senior Data Engineer,Produgie,27 days ago,,Full–time,"What You Will Do
• Co-own data structure used by various engineering teams to design models and schemas of the data to be fed into the platform, making sure they can be processed in a consistent and scalable manner
• Design, Build and Manage data pipeline, cloud-based data lake and warehouse systems
• Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions
• Design, build and support new and existing data infrastructure including data models, data pipeline and data analytics
• Driving a data-driven culture by building democratic, self-service data tools with data privacy and governance as primary design goals
• The work is focused on implementing and maintaining data connectors, external data integration and helping building key showcases
• Investigate and research data quality and integrity from data sources
• Develop on no-code/low code platform to provide bridging features for internal ops while waiting for platform development

What We Are Looking For
• At least 2 years of experience designing and implementing real-time data pipelines and databases
• Professional experience with data management and visualization with relational and non-relational databases is a plus
• Hands-on experience in any modern programming language (Python preferred) and SQL
• Experience in some of the data engineering tools such as BigQuery, Spark, Jupyter, Hadoop, Hive, Flink, Storm, Elasticsearch, Redshift, Pandas, or Airflow
• Strong analytical skills; ability to analyze and manipulate raw data, drawing conclusions from insight acquired.
• Experience in building and operating data applications in cloud environments (AWS, Azure or GCP)
• Experience with Infrastructure as code (IaC) tools such as Terraform is a plus
• Able to write and converse in English
• Possess data modeling and schema design skills
• Experience with data visualization tools like Google Analytics, GoodData and Looker

Job Type: Full-time"
29-Apr-2022 T11:53,Network Data Engineer,Anchor Search Group Pte. Ltd.,17 days ago,,Full–time,"Job Description: Manage customer's network infrastructure such as internet links, traffic shapers, routers, and switches Support in daily operations on incident management, problem(s) / issue(s) remediation, and service(s) restoration Fulfilling of service request(s) following the Change Management procedure. Track and assess all announcements and/or advisories (from device principal, customer internal security team, Government IT Security Incident Response (GITSIR) Team, etc. on patches on vulnerabilities, software bugs and firmware upgrades for network devices. Planning and applying of devices security patches and firmware upgrades in accordance with the severity. Preparation of monthly reports on operational issues, link performance, patch status for all network infrastructure equipment. Create and maintain documentations of network configuration, network diagram, mapping, processes, and service records. Any other tasks assigned by the customer. Job Requirement: Relevant Diploma or bachelor's degree in Computer Engineering (or equivalent) Minimum CCNP certification (routing & switching) Min. 3 years of strong experience supporting a campus network infrastructure, with in-depth hands-on experience on network devices such as Cisco Nexus switches, Catalyst switches, ASR Routers, Networking Monitoring Tools, etc. Knowledge on network compliance is an added advantage Excellent problem-solving skills in a multi-tasking, fast-paced and complex work environment. Good communication skills and written skills in English, positive attitude, team player, resourceful and resolve problems independently. Clarice Lim (R1656152) | Anchor Search Group Pte Ltd (17C8528"
29-Apr-2022 T11:53,Senior Data Engineer,Rakuten Viki,18 days ago,"$5,250–$10,500 a month",Full–time,"Based in Singapore, this Senior Engineer, Data role reports into Engineering Manager and will play a critical role in building the pioneer Data Engineering Team at Viki!

About the Data Engineering Team

Viki is establishing a Data Engineering team from the ground up, for the purpose of addressing the business’s growing data needs. This team is going to be responsible for designing and implementing a data architecture that is able to provide reliable data systems and clean data for various stakeholders across Viki including but not limited to
• Data Analysts who need to spend a lot of time finding insights from the data, build reports to track business performance against OKRs,
• Product Managers who need to understand our customers’ behaviors, their journey on our platform, understand customer funnels,
• Marketing teams to be able to build customer segments for marketing campaigns,
• Content Operations to track the performance of our shows across various markets and customer segments,
• CRM team to understand our customer and manage our relationships with them, and so on

Building this overall data architecture includes designing and building the ingestion systems for different data formats (files, databases, events), designing processing pipelines that can scale with data volume, data management strategies (Data Lake, Data Warehouse) that’s optimal for long term storage, queries for reporting / visualization, building APIs as well as ML models on top of and data sharing with third-party applications for both batch and streaming data. While doing so, set up proper data governance practices and policies for data retention, compliance, PII handling, GDPR/PDPA/CCPA handling, among other things.

In addition to this, in the longer term, the team is expected to build abstractions and data models that can enable future needs with building systems for content recommendations, search recommendations, building as well as operationalizing machine learning models for subtitle translations, recommendations, churn prediction and so on.

Key Responsibilities:
• Translating the pipelines into reusable and scalable data pipelines and frameworks for ingestion, processing, storage and consumption
• Perform root cause analysis on internal and external data systems to answer specific business questions, identifying and calling out data and systems issues, and improvements in a timely manner
• Improving and maintaining the existing application & workflows’ correctness, performance, SLAs and architecture’s integrity
• Upholding adherence to the right data engineering practices while building the data systems and pipelines, such as proper automation testing, CI / CD, logging, monitoring and alerting, while highlighting areas of improvements
• Contribute to POCs in evaluating SaaS or PaaS vendors that can solve specific problems in our architecture
• Identifying patterns in code and refactor them into modules that are easy to extend / reuse
• Performing code reviews of the team’s PRs and ensuring high standards of code quality, in addition to ensuring that development guidelines are followed
• Guiding junior members of the team on technically complex aspects of the system, or wherever necessary

Requirements:
• Bachelors or Masters in Computer Science or a related field, or a strong past work experience in building software systems or products
• 4-8 years of experience in developing production critical software, including 3-4 years working on data related systems.
• Strong knowledge of software concepts, design patterns, refactoring and automated testing
• Good judgment and diligence to know what patterns to use, when and where, and are able to confidently hold constructive conversations on it with the team
• Strong communication skills and are able to explain technical and non-technical concepts to the junior members of the team, as well as the peers and managers
• Good hands-on experience building APIs using: Java, Scala, Golang and /or Python, or willingness to pick one of them / Relational and / or NoSQL DBs (Postgresql or Mysql or MongoDB or equivalent) / Caching technologies like Redis or Memcache
• Very strong SQL knowledge and experience working on query optimization, data modeling
• Good experience working with / using one or more of the following: Data Warehousing technologies such as Redshift, BigQuery, Snowflake or other big data storages like CockroachDB, Cloud Spanner, BigTable, etc / Any Data Processing frameworks and technologies such as Spark, Apache Beam, Dataflow, EMR, AWS Glue / Messaging systems such as Kafka, PubSub and Stream processing / Open File Formats such as Parquet, ORC, etc / Building and operating data applications in cloud environments (AWS or GCP)
• 3rd-party solutions and technologies such as Fivetran, Snowplow, Segment, or the likes of it
• Added advantage, if you have knowledge of data infrastructure management and Infra-as-Code (IaC)

Rakuten is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status. Women, minorities, individuals with disabilities and protected veterans are encouraged"
29-Apr-2022 T11:53,Senior Data Engineer,Newtone consulting,15 hours ago,,Full–time,"Job Description & Requirements

The successful candidate will provide data services for CIB as part of the Cybersecurity datalake application. You will be part of the team responsible for governance, quality, remediation and manages services across data standardisation, analytics, archiving, reporting, dashboards and data management and production support.

Role and Responsibilities:
• Troubleshoot Production issues following a ‘Follow The Sun principle’ [APAC/EMEA/NAR] support model
• Monitoring and proactive support
• Application maintenance and upgrades
• Automation of BAU tasks to improve efficiency
• Strong contribution to Data Analytics, including support in the development of custom add-ins for data collection & analysis
• Suggest improvements and provide guidance and support to the IT Security team to help them improve their monitoring & analytics capabilities

Candidate profile:
• 5 years of experience of IT Production and/or BigData
• Practical knowledge of performance and capacity management from a BigData perspective as well as strong aptitude for automation.
• Strong working knowledge of Linux (RedHat/Ubuntu)
• Strong working knowledge of Elastic stack (Elasticsearch / Logstash / Kibana / Beats) including data ingestion, management, monitoring & analytics
• Experience with Kafka (or similar experience in messaging broker software e.g. Rabbit MQ, ActiveMQ)
• Programming skills (Python or Ruby or Java)
• Experience & skills in automation tools (e.g. Ansible) & DevOps pipelines are appreciated"
29-Apr-2022 T11:53,Data Engineer,Vault Dragon Pte. Ltd.,16 days ago,,Full–time,"We believe there is a better way to deliver healthcare in Asia

Together, our passionate and multicultural team aims to transform the regional digital healthcare space to an integrated, seamless and efficient healthcare ecosystem.

Backed by prominent venture capitalists in the region we hope to achieve our simple vision. We want to build Asia's Google Maps for Healthcare Data.

We want to reshape the digital healthcare space by improving facilitation of data flow between the healthcare sector and biomedical sector, and also shifting the patient journey paradigm from a provider-centric model to a more patient-centered system of delivery.

Already in six markets in Asia and expanding rapidly

Our solutions are trusted by healthcare providers and payers in Singapore, China, Indonesia Thailand, Taiwan and Cambodia.

Based on our success in these five countries, we are confident that our solution will be relevant and in demand in other countries within the region as well.

We are currently finalising expansion plans into Malaysia, Vietnam and Brunei.

Our Product Suite

Our healthcare solutions are trusted by more than 650 healthcare providers in Singapore, China, Indonesia and Thailand, most of them using our platform to manage and run their clinics and health centres.

We have also digitised more than 1,20,000 million unique patient records and are the first vertically integrated medical record company in Asia, offering end-to-end medical record solutions for doctors.

From electronic record archival to case note annotation, to a state-of-the-art multi branch Clinic Management Solution, Vault Dragon offers it all.

Join us as we create Asia's Google maps for healthcare data and are looking for a Data Engineer

What we offer you

Apart from a challenging and fast paced startup environment that fuels and accelerates your ambitions as you help Vault Dragon move to the next level, you will also be well rewarded with a decent compensation package as well as a stake in our growth.

Who are you to be suitable

We are looking for an experienced data engineer to join our team. You will use various methods to transform raw data into useful data systems.

To succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources.

Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods.

If you are detail-oriented, with excellent organizational skills and experience in this field, we'd like to hear from you.

Data Engineer Job Responsibilities :
• Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
• Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
• Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
• Writes unit / integration tests, contributes to engineering wiki, and documents work.
• Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
• Works closely with a team of frontend and backend engineers, product managers, and analysts.
• Designs data integrations and data quality framework.
• Designs and evaluates open source and vendor tools for data lineage.
• Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
• Builds algorithms and prototypes
• Combines raw information from different sources
• Explores ways to enhance data quality and reliability
• Identifies opportunities for data acquisition
• Develops analytical tools and programs

Requirements and skills
• Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
• 5+ years of experience in a Data Engineer role or in a similar role
• Technical expertise with data models, data mining, and segmentation techniques
• Knowledge of programming languages (e.g. Java and Python)
• Hands-on experience with SQL database design
• Great numerical and analytical skills

Experience using the following software / tools :
• Experience with big data tools : Hadoop, Spark, Kafka, etc.
• Experience with relational SQL and NoSQL database, specifically MySQL, MongoDB and CouchDB.
• Experience with data pipeline and workflow management tools : Azkaban, Luigi, Airflow, etc.
• Experience with AWS cloud services : EC2, EMR, RDS, Redshift
• Experience with stream-processing systems : Storm, Spark-Streaming, etc.
• Experience with object-oriented / object function scripting languages : NodeJS, Python
• Experience designing, building, and maintaining data processing systems
• Experience working with either a Map Reduce or an MPP system on any size / scale

Remote Working

Medical Benefits allowance

Laptop allowance

Public Holidays as per the Singapore laws but flexible

Details of the core technology of the company including key components of the tech stack

We work with Node.js (Express.js) in the backend with MongoDB and Redis. Deploy docker containers through Kubernetes to AWS and / or GCP.

Front End

WebPack, Vue.js Bootstrap

Backend

Node.js (Express.js), MongoDB, Redis

Infra

AWS, GCP, Aliyun

Tools / Other technologies

Github, Nginx, Jenkins, Ansible, Vagrant, Docker, Kubernetes

Our software / technology development framework

Our development framework is mainly Javascript Stack - Vue.js / Express.js. We follow the agile process. Our sprint is one week so that we have better projection of progress and productivity.

We use Git for version control and our release / merge strategy is based on git flow workflow. Our versioning is done according to Semantic versioning.

Commits are reviewed through pull requests. Upon approving the pull request, a commit or a group of commits are tagged which then triggers our CI / CD pipeline to deploy into various environment through Kubernetes cluster.
• Methodology : SCRUM
• Versioning : Semantic Versioning
• Git Workflow : Git Flow
• Deployment : As suggested by Git flow we have Develop, Staging, and Master branches.

We have local / remote feature branches that are based from develop branch and is reviewed through pull request for staging release, then in staging QA for production release and lastly production QA"
29-Apr-2022 T11:53,Lead Data Engineer,FINDJOBS PTE. LTD.,8 days ago,$9K–$12K a month,Full–time,"RESPONSIBILITIES:

· You will be responsible for end-to-end development of Data Analytics Platform and AI/ML Ops use cases to take our data engineering and analytics team and system into the future. The role must be hands on. The role is for a self-motivated individual with software engineering skills and expertise with Big Data technologies. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, development, and test of new functionality. Candidate must be agile and flexible with changing priorities based on team’s needs.

· Design & Architecture: You help to design and implement an optimal data pipeline architecture for the Equinix analytics system. Identify, design, and implement improvements to automate manual processes, optimize speed and efficiency of data delivery, architecting and designing for high availability, scaling, and reliability. Architect and design infrastructure to facilitate data extraction, transformation, export and query of data from a variety of data sources, both internal and external, both small and large.

· Massive data: You will source / examine, analyze, engineer data pipelines for gigabytes/terabytes of structured and unstructured data with our platform to create value for customers. You will also be working with Enterprise data.

· Pushing the limits: This role will be on the cutting edge of our Data / Machine Learning platform. As we push to solve more of our customer challenges, you will be prototyping new features, tools, and ideas. Innovate at a very fast pace to maintain our competitive edge.

· Distributed Processing Engine: You will be masterfully working on the data platform which involves processing of both unbounded and bounded data sources, perform im-memory computation & transformation.

· Production deployment: You will be responsible for integration and deployment of the Data Ingestion & machine learning pipelines into production where your ideas can come to life.

· Work with stakeholders including the Data Science teams, Business Systems Analysts, and Architecture teams to assist with data platform technical and organizational issues and support the company’s data and analytics needs.

· Educate, train, and mentor members of the Data Engineering and Analytics teams in the design, implementation, and usage of modern data systems

SKILLS REQUIRED:

· Atleast 12+ years of professional software development experience in multiple programming languages, including modern virtual-machine languages such as Java, as well as common scripting and glue languages such as Python and version control (git), with good analytical & debugging skills.

· Big data: You have extensive experience with data analytics and working knowledge of big data infrastructure such as Google Cloud, Big Query, Data Flow, Hadoop Eco System, HDFS, Apache Storm, Apache Spark. You've routinely built data pipelines with gigabytes/terabytes of data and understand the challenges of manipulating such large datasets.

· Cloud Exposure: Strong experience implementing systems and applications using distributed and cloud infrastructure. GCP preferred, but AWS or Azure are also okay.

· Data Modeling: Flair for data, schema, data model, PL/SQL, Star & snowflake schema, how to bring efficiency in data modeling for efficient querying data for analysis, understands criticality TDD and develops data validation techniques. Working knowledge of distributed relational and tabular data stores, message queues, stream processing facilities, and other scalable big-data platform technologies.

· Experience performing root-cause analysis on bugs and performance problems in distributed systems, including network and source-level debugging.

· Real Time Systems: Understands evolution of databases for in-memory, NoSQL & indexing technologies along with experience on real-time & stream processing systems like Google pub/sub, GCP technologies, Kafka, Storm, Spark Streaming.

· Strong design skills: with a proven track record of success on large/highly complex projects preferably in Enterprise Apps and Integration. Advanced, hands-on knowledge of design, implementation, and optimization of big data architectures, pipelines, and data sets.

· Project management: Professional experience in a modern software development life cycle, including requirements gathering, system design, unit and integration testing, continuous integration, and deployment. You demonstrate excellent project and time management skills, exposure to scrum or other agile practices in JIRA.

· Excellent verbal and written communication skills: Must be able to effectively communicate & work with fellow team members and other functional team members to coordinate & meet deliverables"
29-Apr-2022 T11:53,Senior Data Engineer,Fpt Asia Pacific Pte. Ltd.,8 days ago,,Full–time,"We are looking for Senior Data Engineer to work in finance domain. Required key skill: - Python - SQL - SSAS - AWS Redshift (good to have) Support the Portfolio Execution Group (PEG) Technology Team for designing, building and supporting the evolving ecosystem of critical applications for various trading, financing and treasury functions. Responsibilities: Design, build and support a high-performance, high-availability, real-time multi-asset limit reservation/booking system in the trading technology space. Design and develop API to allow connection of the new system with various upstream/downstream systems for data transfer. Responsible for building and maintaining an automated CI/CD environment. Requirements: Possess a degree in Computer Science or related fields At least 4 years of working experience in data engineering on any relevant database technology - Experience in ETL tools, development in Microsoft SQL Server and Microsoft SQL Server Analysis Services (or any similar SQL/OLAP technology) Good communication skills, able to work independently with minimal supervision Good team player as this role will be part of a bigger team Good understanding of data modeling concepts Experience in Python or Microsoft SQL Server Analysis Services is a plus Strong understanding of data modeling concepts and good ability to design various components of data model and data engineering solution Able to guide junior team members, review solution design and perform code review"
29-Apr-2022 T11:53,Data Engineer,BigPay | Challenge Banking,9 days ago,,Full–time,"We are looking for a talented and independently motivated Data Engineer to join our Data Science team. Working directly with the Head of Data Science, you will be working on our core data pipelines (batched and streaming) and

with our cloud infrastructure.

About The Job
• Maintain and extend our existing ETL infrastructure
• Design schemas, ETL processes to pull data from various system and platforms into BigQuery
• Extend or exiting data pipelines automation framework
• Assist in maintaining and extending our existing Data Visualisation systems (built on Google Data Studio)
• Partner with management and operational teams to deep dive on core issues and use our data to find answers
• Building and maintaining data monitoring and analysis systems
• Maintain and monitor key systems metrics via dashboards
• Assist with other efforts as required

To be successful
• Good language and reasoning skills, in particular in English
• Experience in writing production Python code
• SQL (e.g MySQL, PostGreSQL, SQLServer)
• Linux and linux shell scripting
• Google cloud services (BigQuery, Pub/Sub, Cloud DataFlow)
• Developing ML/AI applications
• Git (or any other version control system)
• DevOps (particular cloud deployments) and System Administration
• Java, Rust development (particularly microservices)
• Terraform"
29-Apr-2022 T11:53,Data Engineer Intern,Fairmart,5 days ago,,Internship,"As a Data Engineer Intern, you will
• Be building and maintaining some of the largest retail-focused datasets in the region.
• Work on data visualization and design of complex data models.
• Work with smart, motivated people in a supportive environment.
• Potentially work on integrating datasets with our backend (mostly NodeJS).
• Influence others and build consensus using your strong written and verbal communication skills.
• Enjoy a strong culture of wanting to be better and making it happen.
• Feel empowered to try things out and make a real difference for our teams and our customers.

About you
• Critical thinker and problem-solving skills.
• Experience with Javascript or desire to learn.
• Team player who enjoys working with a small team in a fast-paced environment.
• Sense of ownership and pride in your performance and its impact on the company’s success.
• Are excited about taking a products from conception to execution and are excited about digitising the retail industry.
• Care about writing clean, well-documented code.

Bonus
• BA in Computer Science or similar relevant field.
• Knowledge of data modelling with large and varied data sets
• Experience in automating repetitive processes & machine learning.
• NodeJS, Dynamodb, Hadoop, Grafana, Elasticsearch experience or desire to learn.
• Experience with TDD, BDD and other forms of automated testing.
• Experience with Agile Methodologies such as Scrum or Kanban.

Perks and Benefits
• Exciting opportunities as we take our platform to the global market.
• Refreshingly flexible but professional environment.
• Competitive salary commensurate with experience.
• The chance to join the ship early on and go all the way path to the recognised leader in the industry.
• Great career and development prospects - being able to push the boundaries of what is possible"
29-Apr-2022 T11:53,Data Engineer,Agensi Pekerjaan Spring Professional (Malaysia) Sdn Bhd,16 days ago,,Full–time,"Job Description : About the Client

About the Client

Our client is a global social mediacompany with a presence in multiple countries. They are hiring for a Data Engineer to join the team.

Responsibilities
• Build data pipelines to portray business status, based on a deep understanding of our fast changing business and data-driven approach
• Extract information and signals from a broad range of data and build hierarchies to accomplish analytical and mining goals for 'Packaged Business Capability' such as user-growth, gaming and searching
• Keep improving the integrity of data pipelines to provide a comprehensive data service.

Preferred Qualification
• Bachelor's degree in Computer Science, Statistic, Data Science or a related field
• Skilled in SQL and additional object-oriented programming language (e.g. Scala, Java, or Python)
• Experience in issue tracking and problem solving on data pipelines
• Fast business understanding and collaborative in teamwork.

Bonus

Industry experience working with user growth.

Interested to Apply

If you are keen to hear more about this role, please send your updated word format resume to Felicia Lim at

Personal Registration No. R22105545

EA License No. 09C5803"
29-Apr-2022 T11:53,Data Engineer,ST ENGINEERING UNMANNED & INTEGRATED SYSTEMS PTE. LTD.,7 days ago,$3.6K–$5.5K a month,Full–time,"We are looking for an experienced AI Engineer to join our multidisciplinary team in transforming our MRO business, using deep learning and neuro-linguistic programming (NLP) to help us improve various business outcomes and drive innovation.

Job Responsibilities:
• Perform research and development (R&D) and processes to meet the needs of our AI strategy
• Work with operation teams to identify and prioritize key areas of the business where AI solutions can drive significant business benefit
• Design and develop AI Solutions for MRO business Optimization.
• Work on functional design, process design (including scenario design, flow mapping), prototyping, testing, training, and defining support procedures, working with Group Engineering Centre and Operation Teams.
• Document and articulate solution architecture and lessons learned for each exploration and accelerated incubation

Job Requirements:
• Degree in Computer science, Electrical Engineering / Mechanical Engineering or equivalent
• 2+ years of experience in applying AI to practical and comprehensive technology solutions
• Proficient in C, C++ and Python programming
• Familiar with Windows and Linux programming environments
• Proven experience with ML, deep learning, Tensorflow, NLP
• Strong interpersonal and communication skills.
• Ability to contribute as a team player or independently.
• Ability to demonstrate a high level of initiative and resourcefulness.
• Location: Ang Mo Kio
• Singaporean only"
29-Apr-2022 T11:53,Data Engineer,KKR SINGAPORE PTE. LTD.,2 days ago,,Full–time,"Position Summary

We are looking for a savvy Data Engineer to be based in Hong Kong / Singapore, to join our growing team of data and analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced cloud-based services, data architecture, data engineering, data pipeline development (ETL), and analytical tools. We are seeking an engineer who enjoys optimizing data systems and building enterprise scale data services. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Skill / Experience Required
• Bachelor’s Degree in Computer Science/Engineering or a related discipline.
• 10+ years Development experience
• Experience in Python and related open source modules
• Experience in Python development including Web application frameworks such as Flask / FAST API
• Experience working with RESTful API Services
• Strong database skills with a thorough understanding of relational database and understanding of Object Oriented databases
• Exposure with the AWS Stack / RDS / is preferred
• Knowledge of open source solutions and trending technologies
• Good communication and written skills
• Ability to be self-sufficient and proactive individual contributor
• Exposure to Private/Public Markets

Desirable
• Understanding of Object Oriented Programming and Design Patterns
• Knowledge of web standards, security, accessibility, browser compatibility
• Knowledge of JavaScript, HTML5 and awareness of frameworks such as React.js/Vue.js
• Experience in a Business Intelligence tool e.g. Tableau and Dremio
• Exposure to ML / AI and python libraries (pandas, NumPy, SciPy) and concepts"
29-Apr-2022 T11:53,Senior Data Engineer,Jones Lang Lasalle Technology Services Pte. Ltd.,23 days ago,,Full–time,"We are looking for a Senior Data Engineer: Willing to specialize in data modelling, ETL (Extract Transform Load) development, DWH/data mart implementation and BI solution delivery With strong analytical skills, back-end solution design knowledge BigQuery and Google Cloud Platform knowledge would be considered a plus If this describes you, do not hesitate to apply. In this role you will be focusing on designing and delivering complex back-end solutions through your technical and business expertise. Responsibilities: Working hands-on on developing Business Intelligence solutions for our clients; Collaborating closely with stakeholders to understand their business needs and concepts; Designing solutions that take advantage of all Google Cloud Platform functionalities Designing and developing scripts for ETL batch scheduling, monitoring & automation Helping clients use their data and find opportunities for improvement, spot trends, as well as recognize potential issues and propose solutions; Providing guidance and mentorship to other junior team members during more challenging projects and assignments. Requirements: Speaking fluent English (preferably C1-C2) - we are an international company, and we interact with folks from around the globe daily; Knowledge of Data Warehouse & Dimensional Modelling and experience in ETL Architecture & Frameworks is a must-have Knowledge of querying languages (SQL, Big Query,etc) is a must-have Exposure to GCP Data services & Tools is preferred, but not required Being a creative, attentive to details person that finds well-balanced solutions to everyday problems. We will make sure that we make the best use of your analytical skills in a creative and challenging way. We are looking for someone who takes initiative and stays curious. If you like working on meaningful, innovative projects and are energized by building something from scratch, we will be a good match. Strong communication skills: as you will be working closely with stakeholders from different backgrounds and with different roles and levels of seniority Creative vision, a passion for learning and willingness to adopt and share innovative ideas. What you can expect from us: Working with us will allow you to develop a skillset that is highly desired on the job market - you can become a highly skilled technical specialist with a strong business perspective and the ability to manage clients and other stakeholders. From early on we enable our teammates to step out of their comfort zone and gain experience working directly with project stakeholders. This allows them to quickly understand how the business works. You're more focused on the technical side of things That's great - you will have plenty of opportunities to work with new technologies. This is not your usual corporate job. Apart from the development possibilities it brings, we also value the relationships that it provides. We are a laid-back team that enjoys a good laugh at the job and understand that all of us have their lives beyond work. Apply today"
29-Apr-2022 T11:53,Data Engineer (Open for Fresh Grads),ST Engineering,$2.1K–$3.8K a month,,Full–time,"We are seeking Data Engineers to join our analytics software product, advisory, and delivery teams within the Group Engineering Centre in ST Engineering.

The candidate must be able to work in a fast paced environment, understand the complexities of different customer data sources and infrastructure setup, have a passion for exploring/cleaning/preparing data sets for analytics modelling. He/She will work with Data Scientists, Data Analysts, Data Architects, DevOps Engineers and other internal stakeholders to assist with data related technical issues and support their data pipeline infrastructure requirements.

The ideal candidate will thrive in a work environment that requires strong problem-solving skills, self-motivation, and an aptitude for team collaboration and open communication.

Requirements
• Bachelor’s or Master’s degree in computer science, computer engineering, information systems or related quantitative field.
• Background understanding of Big Data, Data Warehousing Business Intelligence tech & concepts
• Broad knowledge of various aspects of Big Data and Hadoop based technologies
• Understanding of both relational and NoSQL database technologies such as PostgreSQL, Oracle DB, Cassandra, MongoDB, Neo4J etc.
• Good knowledge in programming languages such as Java, Python or Scala on Linux/Windows platforms.
• Familiarity with or willingness to learn Big Data visualization and reporting software.
• Familiarity with or willingness to learn to design ETL/BI solutions.
• Familiarity with or willingness to learn in DevSecOps, DataOps, MLOps
• Familiar with Linux/UNIX system administration
• Willing to provide operational support in delivering Big Data solutions.
• Effective oral and written communication with strong analytical, problem solving, multitasking and project management skills
• Fresh Grads are welcome to apply"
29-Apr-2022 T11:53,Data Engineering Lead (Remote Possible),Glints,7 days ago,,Full–time,"At Glints, we are building the #1 tech-enabled recruitment and career discovery platform in Southeast Asia that helps people and organisations realise their human potential by joining great organisations, learning the right skills and building great teams.

We’re looking for a Data Engineering Lead to join our Data Engineer team, to

improve Data Governance and promote Data Democratization in the company

What You’ll Be Doing
• Identifying organizational needs for Data Engineering and developing a roadmap within the organization
• To promote ownership of data across the organization
• To improve the quality and integrity of Data in Glints
• To improve discoverability of data and encourage usage of data
• To develop security around data usage
• Conveying organization need to Data Engineering Team for implementation of the solution
• Mentor and groom members of the team to be technical leaders; Hire and expand the team if needed

Why You Should Join Us
• Opportunity to determine Data Engineering Roadmap and realise it with a team of enthusiastic engineers

Who We Are Looking For
• Proficient with Python and Scrum Framework
• Experience with leading Data Engineering Team in improving Data Warehouse
• Managerial or Leadership experience in mentoring and grooming Data Engineering Team
• Able to understand the business equation of the companies and identify levers which the Data Engineering Team can pull

Let’s Realise Human Potential.

We have impacted many lives since we were founded in 2013, but there’s still plenty to be done. If you’re ready to grow and make an impact, you’ve come to the right place.

What is Glints?

Glints is an online talent recruitment and career discovery platform with the enduring purpose of contributing to a world where people and organisations can realize their human potential.

Our tech-enabled approach to recruitment and career discovery connects more than 1.5 million candidates to their next dream opportunities, has supported more than 30,000 companies in finding top talents, doubled recruiter efficiency, and has raised more than US$30M+ from best venture investors in Asia to date.

Our agility and firm hold on our core purpose and values have allowed us to remain resilient and thrive through tumultuous times, and we are proud to be recognised by LinkedIn as one of the Top 10 Startups in Singapore in 2020.

Who We Are

At Glints, personal and professional growth are just as important as business growth. That's why we created the Glints Culture Code: #RIIBCOH. It defines our values, guides our decisions and actions, and is what makes us special.

Relentlessly Resourceful: Whatever it takes, just make it happen (ethically)

Integrity: Have courage, be guided by the truth, don’t be afraid

Impact: Missionaries, not mercenaries

Beginners’ Mindset: Stay humble, don’t be attached to ego

Customer Obsessed: Customers First

Ownership: Care intensely about the mission and take responsibility

High Standards: Dream big and deliver epic outcomes fast

Where We Work

Glints operates in multiple locations across Greater Southeast Asia, including Singapore, Indonesia, Vietnam, Taiwan and Malaysia.

Learn more about Glints and our culture at bit.ly/glintsculture,

Or check out our Careers Page at https://glints.com/careers"
29-Apr-2022 T11:53,Data Engineer - Ref: YC,A-IT SOFTWARE SERVICES PTE LTD,10 days ago,$4.5K–$8K a month,Contractor,"The Data Analyst/Engineer will define the enterprise metrics and deliver the capture of correct data in the automation and data analytics initiatives. The Data Analyst/Engineer is responsible to identify data sources, analyse existing data and ensure capture of new data by collaborating with multiple stakeholders and deliver successful implementation of the respective initiative.

Responsibilities

1. Build and maintain data queries and data pipelines using tools such as SQL

2. Build and maintain ETL/ELT job.

3. Build complex robot to read, validate and analyse data.

4. Build sophisticated dashboards and automated reports for core business metrics and performance trends using visualization tools

5. Conduct data mapping based on business requirements by leveraging on tools

6. Working with highly collaborative teams and to build quality solutions.

7. Develop user guides and technical documentation

Requirements

1. Bachelor’s degree in Computer Science, Mathematics or Statistics a related technical field or combination of equivalent practical experience and education.

2. Minimum 3+ years’ experience data mining/ingestion from various sources as a data engineer.

3. Strong programming experience with frameworks including Python or Java and ETL/ELT is essential

4. Proven skills on any Data visualization tool are essential.

5. Data pipeline, relational database development, SQL query, any database procedure and function development.

6. Prior experience with database and model design and segmentation techniques.

7. Proven analytic skills, including mining, evaluation, analysis, and visualization.

8. Technical writing experience in relevant areas, including queries, reports, and presentations.

9. Independent fast learner with keen sense in automation tools and programming languages.

10. Possess good problem-solving skill and can adapt to changes in business requirements.

11. Effectively prioritise and execute tasks in a high-pressure, fast paced, global environment.

12. Strong organisational skills to manage assignments effectively and working within tight deadlines.

13. Strong communications skills to collaborate with developers, QA, project managers and other stakeholders.

14. Good to have experience working in Agile teams using JIRA tool.

15. Able to work independently and as part of a team.

16. Willing to learn new skills, existing and emerging technologies"
29-Apr-2022 T11:53,"Senior Data Engineer (Platform, Customer Facing)",Connect Energy,2 days ago,,Full–time,"Responsibilities
• Conduct data analytics lifecycle discovery workshops with the largest enterprise customers.
• Execute design and implementation of data analytic projects

Requirements
• Design and implement any third party analytics services in a variety of distributed computing, enterprise environments.
• Lead large-scale data warehousing and analytics projects.
• Develop in Java, Python, R, or other high-level languages.
• Work with distributed scalable Big Data storage, processing, and computation, including Hadoop, Spark, etc.
• Familiar with large-scale real-time streaming service such as Kafka, AWS Kinesis, etc.
• Utilize real-time, large-scale data processing engine, like Apache Spark.
• Develop innovative solutions to complex business and technology problems
• Experience of building and migrating data lake in cloud (AWS, Azure, GCP), moving data applications to the cloud, and developing cloud native data applications
• Knowledge of SQL or Hadoop technology (Hive, Pig Impala, Spark SQL, Presto)
• Understanding of database and analytical technologies in the such as MPP and NoSQL databases, Data Warehouse design, BI reporting and Dashboard development"
29-Apr-2022 T11:53,Data Engineer,Refinitiv,"$8,000–$14,333 a month",,Full–time,"As a part of our growing Labs team of data scientists, engineers and UX/UI designers you will be part of our global network of like-minded colleagues in our global sister lab in London.

Our team offers a great flexible working environment, values curiosity and supports an open and learning culture for all levels of experience & seniority.

You should be a master of methodologies to scale and parallelise infrastructure, applications and data processing pipelines. You will also be comfortable with databases, machine learning techniques and tooling, and able to craft efficient ETL tools with ease.

Required Skills
· 2 years+ experience as a Data Engineer or equivalent.
· 2 years+ experience as a Software Developer or equivalent.
· Diploma / Degree in Information Technology / Computer Science / or equivalent experience.
· Strong skills and experience developing in Python, R, C++, SQL, or other relevant languages.
· Experience with databases and data warehousing solutions (e.g. SQL NoSQL, AWS Redshift).
· Experience with distributed processing frameworks l(e.g. Apache Hadoop, Apache Spark).
· Experience with interfacing and retrieving data from various sources (e.g. API’s, FTP, SQS, S3).
· Ability to utilise and if necessary craft efficient ETL tools and incorporate these in to pipelines.
· Experience with CI/CD pipelines utilising infrastructure as code (e.g. Terraform, Cloud Formation, Ansible).
· Experience with administration of Linux operating systems and are comfortable with the CLI and Shell scripts.
· Familiarity with data science tools and techniques (e.g. regression, clustering, NLP, cross-validation).
· Familiarity with alternative data sets (e.g. Image recognition/analysis, OpenCV, geospatial data).
· Critical thinking, value judgment and common sense over process.
· Able to work independently as well as in a team.
· Positive attitude to learning new skills and technologies.
· Strong interpersonal and communication skills"
29-Apr-2022 T11:53,"Lead, Data Engineering",Mediacorp,Full–time,,,"Description

We are looking for a high-performing data engineer with full stack experience creating web-based data applications. You will be a key contributor to the Data Engineering team, primarily by applying and building tools to aggregate data from disparate sources, processing and loading the transformed data to support internal and external constituencies.

You will be required to maintain and enhance our data infrastructure and proprietary analytical solutions. This role is a hands-on engineering position.

Job Responsibilities
• Translate business requirements to responsive functional web solutions. This includes designing and building APIs, front-end interfaces and scalable tools that ingest and transform real-time data using a variety of open-source and proprietary big data technologies.
• Recommend and implement ways to improve data reliability, efficiency and quality
• Work closely with stakeholders to ensure high standards of data governance during implementation
• Serve as technical subject matter expert on the latest big data technologies

Requirements
• Hands-on proven track record in developing products from front-end interface, middle-tier, to backend infrastructure
• 7+ years of superior experience developing commercial web-based applications
• 4+ years of full-stack web development experience in Javascript (node.js and React framework)
• Proficiency in SQL is mandatory
• Excellent scripting knowledge in Python, Shell etc
• Those with strong production experience in Scala or Spark programming languages will be considered favourably
• Relevant experience in web, video, mobile or adtech domain is a definite plus
• Demonstrated clear and thorough analytical thinking
• Good eye for aesthetics and an attention to detail
• A proven team player and contributor who can multi-task and deliver against timelines
• Minimum degree in Computer Science / Engineering, or equivalent"
29-Apr-2022 T11:53,Data Engineer,EMBRIO CONSULTING,2 days ago,,Full–time,"Job Information
• Salary SGD - / Fixed Monthly Salary
• Shift Normal working hours
• No. of Openings 2 openings
• Job Level : Non-Managerial
• Job Experience : 5 Years or more experience
• Job Qualifications ITE / Nitec

Job Description

Duties and Responsibilities
• Design, implement and oversee maintenance of data flow channels and data processing systems that support the collection, storage, batch and real-time processing, and analysis of information from structured and unstructured sources in a scalable, repeatable, and secure manner
• Develop data management standards and define best practices

Qualifications
• At least 5 years of relevant experience in designing and delivering data management and advanced analytics solutions, especially in areas of data warehousing, Big Data storage and analytics
• Knowledge in one or more programming languages such as e.g. Python, R, Java or C#
• In-depth knowledge of the following:

1.Big Data storage and analytics products, frameworks and methodologies (e.g. Apache HDFS, Kafka, Spark, Hive)

2.Relational databases (e.g. Oracle, MS SQL, PostgreSQL, Teradata etc.)

3.Data modeling and repository design (e.g. operational data stores, dimensional data stores, data marts)

4.Data orchestration and ETL (e.g. Informatica Powercenter)

Interested applicants are open to apply through this job ad with your most updated Resume/CV.

EA Reg No: R1988435

EMBRIO CONSULTING PTE. LTD.

EA License Number 10C4154

Job Types: Full-time, Contract, Permanent"
29-Apr-2022 T11:53,DATA ENGINEER,3I INFOTECH ASIA PACIFIC PTE. LTD.,5 days ago,$3K–$8K a month,Contractor,"JOB DESCRIPTION - DATA ENGINEER

1- Work closely with the project team to develop and validate the data engineeringapplications under the guidance of Product Manager

2 Assist with the design and maintenance of data engineering pipelines as well as the associated authorisation that meet business, technical and security requirements, using the established processes and available tools on the Authority’s Enterprise Data Analytics Platform, within the stipulated timeframe and allocated effort

3 Implement the data engineering pipelines in accordance to the design specifications and work closely with the data engineering team to extract, transform and load the required data from the source systems into the data store.

4 Develop the extract, transform and load tasks in Informatica software in accordance to the design specifications.

5 Ensure optimal design of the data engineering pipeline with due consideration for security control measures and system performance

6 Provide and maintain documentations according to the Authority’s Quality Management System (QMS) guidelines

7 Work closely with relevant technical teams (e.g. Enterprise Data Analytics Platform team, etc.) during development, system integration and user acceptance testing,implementing code changes if required

8 Facilitate the conduct of user acceptance tests and conduct end user training sessions

9 Provide on-site support to troubleshoot any problems

Educational Qualification

Relevant educational credentials in a quantitative discipline (e.g. statistics, computer science, data analytics, operations research, or robotics) or equivalent practical experience

a. Minimum two (2) years of experience in designing and developing applications using Informatica.

b. Minimum one (1) year of data manipulation experience in SQL.

c. Experience in dimensional modelling, data warehousing techniques (e.g. data cleansing, merging and other data manipulation tasks) would be advantageous.

d. Knowledge of methodology and techniques in software testing, and techniques in security vulnerability assessment and testing would be advantageous.

f. Possesses good planning and coordination skills with strong ability to work independently, efficiently and as a team, manage timelines and expectations, and is responsible and conscientious in producing high quality deliverables (which mimimally include documentation, presentations, and prototypes).

g. Possesses strong writing, verbal communication and presentation skills.

At least two (2) years hands-on experience in developing data pipelines, and has completed at least one (1) medium scale analytics project implementation.

Preferably have completed at least (1) project using Scrum or equivalent Agile development framework

3i Infotech Asia Pacific Pte Ltd

Future is of what you make of it today! We are continuously growing and are looking for that persona that wants to grow along. I am looking for a Data Analyst to lead the transformation journey with 3i Infotech Asia Pacific Pte Ltd. If you are that someone and available to join in short notice and looking to bring the change around, pls inbox your profile to

zarina.bevi@3i-infotech.com

Best Regards

Zarina

Hr

Asia Pacific Region

Contact +65 94576290"
29-Apr-2022 T11:53,Data Engineer (EduHub),Government Technology Agency of Singapore,Full–time,,,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an outside-in view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications and facilitate collaboration with citizens and businesses to co-develop technologies. Join us as we support Singapore's vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering. About us - Experimental Systems and Technology Lab (ESTL) The Experimental Systems and Technology Lab of GovTech is an engineering team within the Ministry of Education (MOE). We are made up of engineers, user experience designers and education officers. Our team aims to design and develop software applications that help MOE to transform their systems and services through digitalisation and innovation. About the role As a Data Engineer in the Experimental Systems Technology Lab, you will be working closely with the Lead Data Engineer in architecting, designing and building next-generation data warehouse from scratch to galvanise digitalisation in the Ministry of Education, one of the largest ministries in Government. We are looking for enthusiastic and passionate engineers to join us in this journey to make a difference. What you will be working on: Architecting and scaling data analytics infrastructure on cloud environment; finding opportunities to improve and optimize the workloads, processes to ensure that performance levels can support continuous accurate, reliable, and timely delivery of data products Collaborate with product managers, software engineers, data analysts and data scientists to build scalable and data-driven platforms and tools Manage and run production services that provide analytics capabilities to our various data users across the ministry Development of systems, architectures, and platforms that can scale large volume and variety What we are looking for: Bachelor's Degree in Data Engineering, Computer Science, Information Technology or a related discipline Background or strong interest in data science, analytics or engineering Aptitude and attitude to learn Experience in Tableau will be an advantage We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you so work from home or take a break to exercise if you need to*. We also believe it's important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round. *Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:53,Senior Data Engineer,Quantexa,4 days ago,,Full–time,"Description

Founded in 2016 with only a handful of individuals, Quantexa was built with a purpose that through a greater understanding of context, better decisions can be made. 6 years, 10 locations and 500+ employees later we still believe that today. We connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data.

Due to the continuous success and high demand from our customers, we are looking for a Senior Data Engineer with a proven track record to join the Quantexa family. 🚀

What does a Senior Data Engineer role at Quantexa look like?

In order to be a successful data Engineer at Quantexa, you’ll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.

Being Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. You’ll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service. 🥇

We want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala, Data Fusion and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but don’t worry If that’s not your strongest language or if you haven’t used it before, we make sure that every Quantexan goes through our training academy so they’re comfortable and confident with using our platform.

Requirements
• We’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.
• The desire to learn and code in Scala
• Experience in working in an Agile environment
• Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.
• A strong coding background in either Java, Python or Scala
• Experience of building data processing pipelines for use in production “hands off” batch systems, including either traditional ETL pipelines and/or analytics pipelines.
• Passion and drive to grow within one of the UK’s fastest growing scale-ups.
• Consulting or business facing skills and a desire to work with customers.

Benefits

Why join Quantexa?

We know that just having an excellent glass door rating isn’t enough, so we’ve put together a competitive package as a way of saying thank you for all your hard work and dedication.

We Offer
• Competitive salary 💰
• Company bonus
• Private healthcare with Sigma plus life insurance and critical illness
• Free Calm App Subscription #1 app for meditation, relaxation and sleep 🧘♀️
• Annual leave, national holidays + birthday off! 🌴
• Ongoing personal development
• Great WeWork Office Space & Company wide socials"
29-Apr-2022 T11:53,Data Engineer,Azendian Solutions,30 days ago,"$4,000–$7,333 a month",Full–time,"We are looking for a qualified Data Engineer who will be part of the data engineering team. The ideal candidate will design and develop high quality data products - data warehouse, data marts, data lake data hubs and dashboards; either on cloud or on-premises system environments.

Responsibilities:
• Build and support the data pipeline and all the associated Software Engineering infrastructure tasks.
• Liaise with clients, technical architects, data architects, data scientists and BI analysts to gather the requirements.
• Analyse the data requirements, prepare the functional and non-functional specifications for the data products.
• Develop the data pipeline using either ETL/ELT approach to load or synchronise data in near real time or batch mode.
• Analyse and interpret data into business insights.
• Design and develop the dashboards and visualizations using the BI tools.
• Ability to understand and convert the business logics into SQL queries and validate it against the data.
• Prepare the Test Plans and conduct Unit Testing, System Integration Test, User Acceptance Test and Performance Test.
• Implement the deployment approaches and methods to roll-out the system changes.
• Stay up to date with industry standards and technological advancements that will improve the quality of the data products.

Requirements:
• A keen learner with a minimum bachelor’s degree in Computer Science or related fields.
• At least 1 year of end-to-end data warehouse, data lake and big data implementation experience.
• Proficient with at least one or more of the following technologies: Informatica ETL Tools, SSIS, SQL Skills, SQL Server, Azure Data Factory, Data Warehouse, ETL Framework.
• Preferably possess a good knowledge about: Power BI, Qliksense, MicroStrategy, SAS and Tableau.
• Preferably possess a good knowledge about: DataStage, Attunity, Hadoop Ecosystem, Data APIs, Unstructured Data and Data Modelling.
• Able to work under pressure when there is an escalated demand in the project cycle.
• Understand the solution requirements and progress to develop, build and operationalise the solutions.
• Enjoy problem solving in different domains and industries.
• Love working with a highly energetic and competent team.
• A self-starter with an analytical approach to problem solving.
• A client-centric, outcome driven and quality focused team player"
29-Apr-2022 T11:53,Senior Platforms and Data Engineer,TechBridge Market,10 days ago,,Full–time,"Job Overview

Reporting to the Director of Platforms and Data Engineering, the Senior Platforms and Data Engineer will work closely with Data Scientists, Threat Researchers/Analysts, and Infrastructure. Engineers to develop and manage high-performance analytics solutions. The incumbent will be

accountable for the design, development, deployment, and maintenance of big data platforms as well as their data processing workflows.

Duties and Responsibilities
• Familiarize with the company’s business domain and objectives to develop and deploy big data analytics applications that meet internal business requirements and the needs of partners and customers.
• Lead the design, development, testing, deployment of efficient and reliable big data processing workflows that follow secure SDLC practices.
• Design, develop, manage data warehouse architecture and relational databases.
• Provide monitoring, maintenance, and support for system operations as part of M&S as required in commercial projects.
• Embrace the challenge of dealing with terabytes to petabytes of data on a daily basis.
• Manage different experimentation, development, staging, production environments to provide overall system functionality, health, scalability, resiliency, and security.
• Responsible for implementing and maintaining complex big data projects with a focus on collecting, parsing, managing, and analyzing large sets of data to turn information into insights using multiple platforms.
• Deliver detailed documentation and ensure quality throughout the project lifecycle.

Requirements
• Bachelor’s Degree in Computer Science/Information Systems/Computer Engineering or equivalent.
• Minimum 5 years of experience working on big data (e.g. Hadoop, Apache Spark, MPP DBs).
• Good in-depth knowledge of the Hadoop ecosystem (e.g. HDFS, Impala, Kafka, Spark, NiFi, Elasticsearch, etc.), associated tools, and cloud-based technologies (e.g. EMR, Redshift, S3, etc.).
• Extensive experience in programming (PySpark, Scala,) for data.
• Understanding of modern software engineering tools such as Git, Bitbucket, Jenkins, Maven.
• Highly proficient at reading, profiling, parsing, transforming, cleansing, and integrating data from various sources (structured, semi-structured, and unstructured).

Preferred Skills/Qualities
• Knowledge in Agile and desirable.
• Comfort and experience working in a Linux environment.
• Aptitude for automation and software profiling.
• Experience in the Cybersecurity/Telco industry will be an advantage.
• Proven ability to handle multiple projects concurrently.
• Detail-oriented, solution-focused, and problem solver"
29-Apr-2022 T11:53,Data Engineer,PERCEPT SOLUTIONS PTE. LTD.,22 days ago,$4K–$6.5K a month,Full–time,"We are looking for an IT professional who has gathered some years of working experience in Business Intelligence, Data Analytics, Report Development, SQL and DWH design.

Role Responsibilities:
• Use SQL and/or programming language and tools to perform tasks such as data discovery, data QA, and data preparation based on requirements provided by the business.
• Build data assets in AWS and aligning the data architecture based on business requirements and working towards a common data model.
• Support the Business Analyst in translating business requirements to technical requirements
• Support Data Analytics Projects & Initiatives, like Campaign Automation with AI
• Constantly searching for optimizations, improvements and innovations (e.g. predictive AI)

Skills and Competencies:
• Data Engineering & Data Analytics Expert, DWH Design, SQL, ETL, AWS, Tableau, QliSense, Power BI
• Independent and reliable working style as well as enjoying team collaboration
• Good understanding about Agile software development methods and IT security is a plus
• Business Knowledge of Captive or Financial Services, Banking is a big plus

To apply please click the Apply button or send us your updated profile to recruit@percept-solutions.com

EA Licence No.:18S9405 / EA Reg. No.:R1330864

Percept Solutions is undergoing a growth phase and are on the lookout for talent. Applicants are encouraged to follow Percept Solutions on LinkedIn @ https://www.linkedin.com/company/percept-solutions/ to stay up to date on our upcoming roles and events"
29-Apr-2022 T11:53,Senior Data Engineer,Singtel Group,Full–time,,,"DataSpark was created from a vision to transform Singtel’s rich and unique repository of data into business value and social impact.

Our data products and services provide powerful insights and advanced analytics capabilities to businesses, government agencies, and other telecommunication companies.

We strive for our analytics to be trustworthy and relevant to our clients while adhering to high standards of data privacy.

We are looking for an experienced Data Engineer to join us to build robust data platforms and applications that incorporate data science and machine learning algorithms and models that solve problems in telco network management, transportation, urban planning, real time crowd management, and retail intelligence, to name a few.

This is a great opportunity to apply your established expertise in data science and machines learning.

At Dataspark, you get to work with rich and diverse datasets, cutting edge technology, and you get to see the impact of your results in real business and government decisions, which in turn provide positive social benefit for consumers at a large scale.

As a startup that is part of Singtel, DataSpark provides an enviable work environment with spirited trailblazing and industrial countenance.

Working alongside creative, energetic and passionate teammates from around the world, you get to be a part of our exciting growth journey as we build the company to the next level.

Responsibilities
• design and implement scalable and robust software platform for ingesting and transforming telco network datasets in (near) real-time using a variety of open-source and proprietary Big Data technologies
• recommend and implement ways to improve data reliability, efficiency and quality
• collaborate with product management, sales and marketing, and solution delivery teams to support the objectives that customer requirements are well managed and reflected in product releases
• support the deployment of DataSpark software within clients' IT environment
• working closely with stakeholders to ensure high standards of data governance during implementation
• serve as technical subject matter expert in latest big data technologies

Requirements
• 7+ years of superior software development experience building commercial large-scale software systems and database systems
• Excellence in algorithms, data structure, discrete math, data base and data warehousing
• Expert knowledge in data management technologies and software engineering tools to efficiently process large volume of data
• Demonstrated clear and thorough logical and analytical thinking, as well as problem solving skills
• Experience of data warehouses in excess of 10TB
• Experience of Web UI, middle tier, and data back end development
• Good understanding of Telco data models, knowledge about telco network capabilities a plus
• Expert knowledge in SQL and Relational Database Management System
• Strong experience in big data processing stack on Hadoop, HDFS, HBase, Hive, Spark
• Strong experience in Java / Scala, Python, Amazon AWS
• Superior and proactive communications skills, including verbal, written, and presentation.
• A proven team player and contributor.
• Self-directed, ability to work independently and research innovative solutions to business problems
• Aptitude of working on multiple projects in parallel
• Attention to details and data accuracy
• MS or BS degree in Computer Science / Engineering, Statistics, Mathematics, or equivalent is required for this position"
29-Apr-2022 T11:53,Data Engineer,dentsu international,18 days ago,,Full–time,"The role is responsible for supporting the decision making based on data analysis and the development of business insights from the various data assets. The individual within this role will work closely with the team of the business and will be responsible

iProspect helps our clients achieve transformative change in the digital economy. Gleaning insight from data is fundamental to that change. We are looking for data engineer to help grow our technical specialism.

A balance of business and technical skill, Data engineer guide how we manage data to achieve business needs and objective.

Your technical skills make you an aspiring data engineer, versed in data acquisition, manipulation and analyses. Your commercial interest ensures that you would like to apply those skills to client business challenges and see the results; not be stuck in the background. You have the ability to (re)design a process as well as distill complexity into a simple message.

You will join a distinctly ambitious team. We ask a lot, but in return you will build excellent career foundations within an industry at the heart of the digital revolution.

Detailed Description

The key elements of the role are;
• Influence stakeholders, using your analytical experience and technical specialism
• Assist on multiple projects across a portfolio of clients
• Build and improve existing analytical solutions
• Gain practical experience across a range of technologies, including cloud platforms, Ad/MarTech stacks, customer data platforms, verification technology, visualisation tools and more
• Oversee data management and governance
• Identify weaknesses in processes and reengineer them
• Partner closely with the wider agency; working alongside iProspect and other Dentsu teams to apply data to business decisions
Essential Qualities
• Strong logic & quantitative skills, including analytical abilities and mathematical proficiency
• Proficient coding; experience in SQL, R, Python, Go, JavaScript are all relevant
• Experience with cloud service providers (Google Cloud, AWS, Azure)
• Knowledge of analytical modelling; able to determine why a number is right or wrong.
• Self-motivated & articulate in explaining technical concepts to a less specialist audience
• Great interpersonal skills; ability to build and maintain strong working relationships internally and externally
• Strong Microsoft Office skills, notably;
• Especially strong in Excel – data wrangling, pivot tables, macros etc
• Strong in PowerPoint – data presentation & story telling
• Remain calm when faced with multiple tasks; able to prioritise and deliver on time
• Willingness to ‘pick up and run’ with projects when necessary; own deliverables and see them to completion
• Comfortable working & learning independently, as well as in a team
• Understand how we add value to our clients’ business
• Fluency in English (written and oral)
Additional Skills
• Knowledge of digital/marketing fields e.g. SEO, PPC, programmatic and social
• Experience with digital tracking technologies
• Experience with cloud service providers (AWS, Azure, Google Cloud)
• Experience with container orchestration (Docker, Kubernetes)
• Experience with infrastructure-as-code tools (Terraform, Pulumi)
• Experience with data visualisation tools (Datorama, Qlikview, Tableau, Data Studio)
• Experience with machine learning tools
• Experience with consuming APIs
• Experience in additional coding languages"
29-Apr-2022 T11:53,Data Engineer,PropertyGuru,15 days ago,,Full–time,"Department: Technology

Make A Real Difference At PropertyGuru.

Real Aspirations. Real People. Real impact.

At PropertyGuru Group, we believe that every person – no matter what their circumstance – should have a place to call home. That’s why we’ve been on a mission to transform how people find, finance, and own home across Southeast Asia over the last 13 years.

Voted by property seekers as “Asia’s Most Influential Brand for Online Property Search”, PropertyGuru enables real-world aspirations through digital transformation and constant innovation. Every day, the work that we do has a real and positive impact on thousands of lives.

As an employee, you’ll be empowered by our community work culture, where everyone has the autonomy, support, and resources to do the best work of their careers. As we evolve our journey to help people make confident property decisions, we stay true to our core values to guide the way we work and the decisions we make every step of the way: we own it and deliver it, we have fun and celebrate success, we respect and care for each other, we push beyond good, and we create what’s next.

PropertyGuru is constantly bridging out into new customers’ experience and businesses across all our markets. One of the core roles will be the Data Engineer role for DataSense fully owned subsidiary of PropertyGuru Group

Your work will be to design, develop, support and maintain our existing infrastructure and democratizing data access within and outside the company. Your work will support the following areas:
• Real time streaming infrastructure:
• Enable teams to move quickly and get accurate information to the right people with minimum delay would be the key focus of the data engineering team
• Advance our streaming platform which allows easy development of the streaming applications
• Interactive Data Analytics:
• Query the data and compute the aggregates on various dimensions to support the various decisions made on the data and machine learning products that are built around it
• Infrastructure management:
• Help manage multiple terabyte-scale clusters, easy-to-use systems to handle security and replication are in development
• Data workflow management:
• Use Airflow and Azkaban to schedule data related workflows supporting various analytical and machine learning workloads
• Machine learning infrastructure:
• Develop the end-to-end platform that will allow us to develop and deploy various machine learning models into the PropertyGuru websites and apps with ease
• Data engineers play a very big role in this platform development and has the potential to significantly cut down the development time of the machine learning models

Requirements
• 5+ years of industry experience in working with terabyte scale datasets
• Working knowledge of relational databases and query authoring (SQL)
• Experience with data workflow management tools Azkaban, Airflow
• Ability to write high performance quality code
• Experience in Python is a must. Other equivalent languages like C++, Java, Go, Scala is a plus.
• Experience with open source technologies like Kafka, Presto and Spark would be a plus
• Awareness of various cloud-based solutions such as AWS Redshift, Google Big Query, Qubole is a plus

Qualification:
• Bachelor’s degree in IT or relevant field. Alternatively, lesser qualifications with strong experience in machine learning will also be considered

PropertyGuru Group is an equal opportunity employer committed to fostering an inclusive, innovative and learning environment with the best employees. Therefore, we provide employment opportunities without regard to gender, identity, race, religion, nationality, age, marital status, disability, or any other protected status, per applicable law. If there is anything we can do to help ensure you have a comfortable and positive interview experience, please let us know.

For a full listing of our jobs, visit https://careers.propertygurugroup.com

Advertised: 15 Jul 2021 Singapore Standard Time

Applications close"
29-Apr-2022 T11:53,Data Engineer,Rakuten Asia Pte Ltd,3 days ago,,Full–time,"Rakuten Group, Inc is a global Internet and e-commerce company, with over 1.3 billion registered users worldwide. Our e-commerce platform is the largest of its kind in Japan and among the world's largest by sales. Rakuten has many subsidiaries, including well-known brands like Rakuten VIKI, Rakuten TV, Rakuten Kobo and Rakuten Viber.

About the Team:

The Global Data Supervisory Department (aka GDSD) oversees the development and operation of our data platform in Rakuten group. We provide data products and platforms for Rakuten’s line of businesses, as well as technology solutions.

GDSD services are important to Rakuten as it has big contribution to Rakuten's profit and being one of the key drivers for growing various platform businesses of Rakuten Group including our e-commerce platform business.

We are looking for Data Engineer, who is passionate to deal with complex technical challenges and who are interested to work closely in the fast pacing business environment.

Key Responsibilities:
• Analyze and understand business requirements to envision best fit analytical solution
• Collaborate with stakeholders, business development team, architect, project manager and data engineers to understand business requirement & questions, processes, and related data, and convert the information into measurable technical design & deliverables and achieving tangible business goals
• Develop analytics and visualization solution that meet criteria for business value and reusability
• Good oral and written presentation skills, produce useful documentation, organizing & participate in workshops, and brownbag sessions to promote product adoption & utilization, provide the essential support to the stakeholders for the products user journey, and a good story-telling skills
• Passionate about emerging technologies and early adoption
• Assist in other ad-hoc analytics projects

Essential Competencies :
• Graduate degree educated in computer science or a relevant subject.
• 3+ years of experience in developing & delivering analytics solutions, Data Warehousing, Data Platforms, ETL pipelines (both batch and streaming), and data modeling experience is advantage
• 3+ years of solid experience in any data visualization tool (preferably google data studio)
• 2+ years of experience in developing, monitoring, and optimizing data platform and pipelines using public cloud services (preferably gcp or aws big data services)
• 2+ years of solid experience in Big Data Technologies - Apache spark, Airflow, Apache Beam, Kafka, HDFS, HIVE, and/or big data services in any public cloud (preferably gcp or aws)
• 2+ years of experience in writing and optimizing code using SQL, python(pySpark), java
• Experience in working agile environment and using tools like JIRA (epics, stories, subtasks)
• knowledge of building APIs (REST), Serverless (functions) and NoSQL DBs is advantage
• knowledge of docker and Kubernetes (preferably gcp GKE) is advantage

Rakuten is an equal opportunities employer and welcomes applications regardless of sex, marital status, ethnic origin, sexual orientation, religious belief or age"
29-Apr-2022 T11:53,Data Engineer,TECH MAHINDRA LIMITED (SINGAPORE BRANCH),8 days ago,$6K–$12K a month,Full–time,"• Should have good knowledge and working experience in: Database Teradata(SQL, BTEQ scripting) and Hadoop (Hive, Impala, Kudu).
• Good to have experience in working with No SQL as well as virtualized Database Environment.
• FSLDM and Finance industry experience
• How BI tools integrate with Data Mart and Data Lake (Qlik Sense, Power BI)
• Scripting using (Shell script, awk programming, quick automation to integrating any third party tools), BMC monitoring tools
• Using CI:CD tools (Bitbucket, Github), quick automation to integrating with any third party tools for automated deployment
• Good understanding in
• Hadoop, In memory, No SQL
• Data Modeling using industry standard data model (FSLDM)
• Automated testing using industry standard testing tool
• Mandatory experience in developing banking application using ETL, Hadoop and Teradata
• In-depth knowledge of technology stack at global banks
• Flexibility to stretch and take on challenges, Communication & Interpersonal skills
• Attitude to learn and execute"
29-Apr-2022 T11:55,Data Engineer,ITCONNECTUS PTE. LTD.,16 hours ago,$5K–$13K a month,Full–time,"Immediate joiners

Requirements
• Technical Data Architect who is strong on Data Warehousing, Big Data, Data Analytics & Data Science for solutioning
• Strong Google Cloud Platform Data Components – BigQuery, BigTable, CloudSQL, Dataproc, Data Flow, Data Fusion, Etc
• Demonstrate extensive skills and success in the implementation of technology projects within a professional environment, with a particular focus on data engineering
• Experienced in delivering Medium to Big Enterprise Data Projects
• GCP Experience including and not limited to MPP systems, Database systems, ETL and ELT systems and Data Flow compute Good to have skills
• A high-level understanding of AI and ML technologies and desire to develop your skills in these areas of emerging technology.
• Should be able to advise the best of breed for the client solutions; Skills Needed
• The Data Engineer coaches the junior data engineering personnel position by bringing them up to speed and help them get better understanding of overall Data ecosystem.
• Prior experience developing, building and deploying on GCP
• Working on Solution deck, IP build, client meetings on requirement gathering"
29-Apr-2022 T11:55,Data Engineer,Manpower Singapore,2 days ago,,Full–time,"Data Engineer

Key Role and Responsibilities:
• Partner with the global and in-market data experts to discover and derive value from connecting external and internal data sources.
• Build (ETL) new and evolve data models and pipelines to power algorithmic based Business Intelligent solutions that addresses business problems requiring descriptive, diagnostic, predictive, and/or prescriptive analytics for pricing, promotion, trade spending, assortment, and sales performance management.
• Translates the algorithms and analytic models into data models as business needs evolves on a going basis after they are put into production.
• Develop a roadmap that scales existing and new data models, to support the portfolio of solutions.
• Expand into Business Intelligence solution development focused on automation and scale up of solutions.

Key Skills Required:

Professional Skills:
R/Python programming languages
MS Power BI
MS Excel
Data Visualisation

General Skills:
Client Management
Project Management
• Diploma or B.S. in a relevant technical field (Operations Research, Computer Science, Statistics, Business Analytics, Econometrics, or Mathematics).
• Overall experience of 2-3 years
• Ability to manipulate and high-volume of data from varying sources
• Expert knowledge of an analysis tool such as Microsoft PowerBI
• Proficiency in R/Python
• Basic track record in working independently with minimal guidance

Interested applicants, please submit your resume to : abelene.kang@manpower.com.sg
Abelene Marianne Kang Mrs Abelene Marianne Rozario R2089914
EA License No: 02C3423 Reg No: 199505951H

Kang Abelene Marianne Mrs Rozario Abelene Marianne

EA License No.: 02C3423 | Personnel Reg No.: R2089914

Please note that your response to this advertisement and communications with us pursuant to this advertisement will constitute informed consent to the collection, use and/or disclosure of personal data by ManpowerGroup Singapore for the purpose of carrying out its business, in compliance with the relevant provisions of the Personal Data Protection Act 2012. To learn more about ManpowerGroup's Global Privacy Policy, please visit https://www.manpower.com.sg/privacy-policy"
29-Apr-2022 T11:55,Data Engineer - User Profile,Shopee,Full–time,,,"Collaborate closely with the product team and provide data solutions to business problems and support business growth and data driven decision makingsDesign, build and maintain batch and realtime streaming user data pipelines using big data platform and technologiesResponsible for user data collection, processing, storage and building the user profile data collectionsAdopt and maintain data engine solutions to support online data services for various business use casesImprove efficiency, scalability, and stability of existing systemsEnsure data quality, consistency and timeliness"
29-Apr-2022 T11:55,"Principal Data Engineer, Data Engineering",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Principal Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you are responsible for helping to shape the direction of the data engineering domain in GovTech; working with other domain experts, the delivery teams, vendors, and partners to influence technology roadmaps and the adoption of suitable data solutions across government. In addition, you will teach, mentor, and proactively grow fellow data engineers within the organization.

You will be leading multiple teams of data engineers to drive impact at scale, by building whole-of-government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will architect, design and build next-generation central data infrastructure to galvanise digitalisation in the public sector. Taking a more integrated approach to lay the foundation infrastructure and driving pervasive adoption of advanced data technologies throughout the economy and society, you will architect solutions that enable public agencies and sectoral partners to better integrate, exploit and understand their data to improve operations and decision-making, and deliver high-quality digital services to citizens and businesses in Singapore.

Join us on this journey to make a difference for the nation and our future generations, if you are:
• Experienced leading data pipeline building and data wrangling and enjoys optimizing data systems and building them from the ground up as part of multiple cross-functional teams,
• Have strong project management and organizational skills, self-directed and comfortable supporting the data needs across multiple teams, systems and products,
• Excited by the prospect of optimizing or even re-designing central and agencies’ data architecture to support next generation of government products and data initiatives, and
• Recognized expert with proven technical leadership, experienced working in a agile team to build systems from architecture to deployment for cloud and on-prem infrastructure, and
• Have the desire to serve the public good through the use of technology.

What you will be working on:
• Take the driving seat and lead our data engineers in complex technical projects to:
• Expand and optimize our data and data pipeline architecture, as well as optimize data flow systems for cross functional teams in a secured and scalable manner.
• Assemble large, complex data sets that meet functional / non-functional business requirements, transforming data into formats that are easy to consume and analyse.
• Identify, design, and implement internal process improvements, e.g., automating manual processes, optimizing data delivery, re-designing infra for greater scalability.
• Build the infra required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and industry standard Infra-as-Code tools.
• Build analytics tools that utilize the data pipeline to provide actionable insights into key performance metrics and support our next gen of products and data initiatives.
• Work with stakeholders including the Executive, Product, Data and Design teams to support their data and data pipelining use cases, and ensure optimal and secured data delivery architecture is consistent across projects, products and platforms.
• Drive innovative initiatives that uplift data capability across the government sector:
• Develop a strategic sense of key organisations, software and hardware tools, methodologies, and best practices in the data engineering landscape.
• Be the bridge between engineering and product line management and translate market requirements into product definitions and architecture designs.
• Analyse and solve complex data engineering problems, and translate architecture designs into implementations that satisfy the market requirements.
• Educate the team on new technological advances and work alongside them in proof-of-concept research projects and subsequent agile product delivery.
• Provide expert perspective to help shape data-related strategy and initiatives, and be our technical ambassador by writing internal blogs or publishing white papers.

What we are looking for:
• Candidate with 8+ years of experience in a Lead Data Engineer role, who has attained a Bachelor’s degree in Computer Science or other relevant engineering degree.
• In-depth working knowledge across a variety of relational SQL and NoSQL databases, including efficient query authoring, performance optimisation and troubleshooting.
• Experience building and optimizing data, data pipelines, and architectures, with processes to support transformation, data structures, metadata, dependency and workload management.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for further improvement.
• Working knowledge of message queuing, stream processing, and scalable data stores.
• A successful history of processing and extracting value from large datasets from different sources and systems, with strong analytics skills related to working with unstructured data.
• You should also have experience using the following software/tools:
• Big data and stream-processing tools: Hadoop, Spark, Beam, Flink, Kafka, etc.
• Data pipelining and workflow management tools: Luigi, Airflow, etc
• Cloud computing environments and services: AWS, GCP, Azure
• DevSecOps tools: Git, Kubernetes, Docker, Terraform etc
• Programming languages: Python, Java, C++, Scala, etc

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours”"
29-Apr-2022 T11:55,"Data Engineer, Data Engineering (6 months Contract)",Bank of Singapore,10 days ago,$8K–$12K a month,Contractor,"Bank of Singapore opens doors to new opportunities.

Start your career with Bank of Singapore as a Data Engineer (6 months contract) in our Data Engineering team!

Bank of Singapore opens doors to new opportunities. At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programs that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group’s global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today!

Bank of Singapore has defined an ambitious roadmap for accelerating its data excellence through the implementation of various bank-wide programs. As part of this roadmap, building the foundation of good data governance and management plays a key role in establishing trust in the data that we use.

General Description

The incumbent will be based in the Data engineering team within the Data Hub team, supporting Bank of Singapore’s data sourcing, curation, transformation, and data analytic services to accelerate their data initiatives. He/she will be responsible for expanding and optimizing BOS data pipelines as well as supporting data analytics needs.

Core Activities
• Be responsible for supporting data analytics and dashboarding with respects to the data availability / stability.
• Understanding the data and identifying any potential data issues as well as correcting issues arising from data.
• Define and implement data models suitable for dashboarding
• Power BI dashboarding support
Requirement

General Knowledge & Experience
• Minimum of 6 years of working experience in data related functions
• Understanding of banking / financial services with exposure to data analytics preferred
Programming & Data
• Strong hands-on skills in SQL & PL/SQL
• Solid background in traditional structured database environments such as Teradata / Oracle
• Knowledge on Power BI dashboard functionality
• Strong skills in Power BI related technologies like power query and DAX
• Exposure to programming languages such as python
Communication & Soft Skills
• Curiosity & a real passion for understanding “why?”
• Pragmatic “can do” approach to finding data based solutions to problems
• Good communication skills with ability to engage across business and technical audiences"
29-Apr-2022 T11:55,Data Engineer,Thakral One Pte. Ltd.,3 days ago,,Full–time,"Job responsibilities: Work with Banking team to understand existing SAS code logic written by techno-functional users Develop the SAS logic in Big Data environment, using spark, scala and pyspark utilizing client Hadoop ecosystem of the inhouse Data Lake Perform unit testing and system integration testing Work closely with Business Analysts team to review the test results and obtain sign off Deploy the new code in the client Production environment Prepare necessary design/operations documentation for future usage Perform peers Code quality review and be gatekeeper for quality checks 5-8 years of application development experience in Spark, Spark SQL, Scala is a must Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc Technical proficiency on data mining techniques and performance optimization Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL) Experience on SAS will be beneficial but not mandatory Handling of reporting packages (Tableau, QlikView) is nice to have AWS experience is nice to have Degree in Computer Science or Engineering is a must Good problem diagnosis and creative problem-solving skills Passion to learn and master diverse new technologies in the open-source community Accuracy and attention to detail Team-working, Verbal and Written communication skills"
29-Apr-2022 T11:55,Data Engineer,ASM TECHNOLOGY SINGAPORE PTE LTD,2 days ago,$4K–$5.5K a month,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform. We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets. You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).

Your Responsibilities
• Manage and support on-premise and Cloud-based data lake and warehouse systems
• Design, build, support and optimize new and existing data structure and ETL processes
• Build scalable and efficient data pipelines & services to help analytics teams to process the data
• Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results
• Liaise with third party tool providers to understand and improve data workflow
• Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms
• Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification
• Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines
• At least 2 years’ experience in data engineering, automation and integration is preferred
• Strong programming and scripting skills in Python and other modern programming languages
• Strong data management, schema design and SQL development skills
• Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)
• Self-motivated and proactive, willing to learn new things
• Good communication skills and strong team player

What our preferred candidates have?
• Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions
• Understand and experienced with Cloud platform, eg. Microsoft Azure, AWS, GCP
• Business intelligence and reporting tools, eg. Power BI, Tableau, Qlik, etc
• Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)
• REST/Web API development and management
• Knowledge in Statistical software is an advantage
• Experience In building machine learning models is a plus"
29-Apr-2022 T11:55,Data Engineer,POWER IT SERVICES,13 hours ago,,Full–time,"Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

· Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

· A successful history of manipulating, processing and extracting value from large datasets.

· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science Statistics, Informatics, Information Systems or another quantitative field.

They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:55,Data Engineer,HCL Technologies,2 days ago,"$7,031–$12,608 a month",Full–time,"We are looking for an experienced Data Engineer to join our growing team of analytics. The candidate will be responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Candidate must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

Responsibilities
· Create and maintain optimal data pipeline.
· Assemble large, complex data sets that meet functional / non-functional business requirements.
· Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing Jobs/code for greater scalability, etc.
· Work with stakeholders including the Product owner, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
· Work with data and analytics experts to strive for greater functionality in our data systems.

Requirements
· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.
·Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.
· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
·Strong analytic skills related to working with structured and unstructured datasets.
· Build processes supporting data transformation, data structures, metadata, dependency and workload management.
·A successful history of manipulating, processing and extracting value from large datasets.
·Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
· Experience supporting and working with cross-functional teams in a dynamic environment.
· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:

· Experience with:
o Big data tools: Hadoop, Spark, Kafka, etc.
o Relational SQL and NoSQL databases, including Postgres and Cassandra.
o Data pipeline and workflow management tools: Airflow, etc.
o AWS cloud services or GCP.
o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:55,Head of Data Engineering,TEKsystems (Allegis Group Singapore Pte Ltd),3 days ago,,Full–time,"Head of Data Engineering

We are looking for a high-caliber data expert and leader working within a team that provides world-class and innovative solutions to support the ever-growing demands in the industry.

This is a permanent opportunity.

What's in it for you:
Exposure to understanding and dealing with the data engineering & analytics requirements from various business units, hands-on analysis of the various data systems, and an opportunity to directly influence and impact the company's data infrastructure to enable data democratization.

The Position:

• 10+ years of experience in managing large-scale data initiatives within the tech or start-up space
• Experience in leading teams that design and build highly scalable data pipelines/data infrastructure in fast paced environments
• Experience in managing teams comprised of other Data Engineers, Data Scientists, Data Analysts and Software Engineers
• Strong understanding of building data models for the target data warehouse
• Clear understanding of distributed computing, especially in databases
• Hands-on in SQL with a deep understanding of query optimization
• It would be great to have experience working on any of the Cloud platforms (GCP, AWS, Azure)
• Exposure to Machine Learning / Artificial intelligence is a plus
• Strong communications skills We regret to inform that only shortlisted candidates will be notified / contacted.

For more information you can email Nicole Sichon in our Singapore office on Nicole.Sichon@teksystems.com quoting Job Reference NicoleSichon531489 or alternatively, apply here to register your interest.

http://jobs.en-sg.teksystems.com/N5oQPt/head-of-data-engineering-itcommunications-unknown-singapore-15264683

Job Reference: Nicole Sichon 531489

EA Registration No.: R1873628, Sichon Andreana Nicole Ong

Allegis Group Singapore Pte Ltd, Company Reg No. 200909448N, EA License No. 10C4544"
29-Apr-2022 T11:55,Senior Data Engineer,Grasshopper Pte. Ltd.,16 hours ago,,Full–time,"OVERVIEW :

We seek a Senior Data Engineer to support our high-frequency trading and quantitative research. At Grasshopper we recognize that effective research and trading strategies are built upon a solid foundation in data collection, storage and service.

Grasshopper trades at a number of exchanges globally, and our success depends on our ability to scale by leveraging the potential of data.

You will, in close collaboration with stakeholders, design and develop highly scalable and fault-tolerant data pipelines in a cloud environment focusing on scalability and performance, as well as the next generation of tools that empower users to generate insights on the data they work with.

RESPONSIBILITIES :
• Work with a team of data engineers across locations, managing project schedules.
• Expand the imagination of data stakeholders on what is possible and educate them in leveraging data they already have.
• Work proactively with traders, researchers and various members outside the data team to meet their needs for timely, accurate and complete data sets.
• Build highly scalable data pipelines to process and analyse billions of messages in real time.
• Set strong technical / architectural / cultural foundations, advocate good design in software development as well as data engineering, and lead by example.

SKILLS & KNOWLEDGE REQUIREMENTS :
• Strong technical leadership qualities, good at working with both people and with code.
• Extensive experience with data modelling and designing / supporting both streaming and batch ETL pipelines.
• Extensive experience in SQL and databases.
• Domain knowledge in finance, especially in buy-side trading, is not required but good to have.
• Proficiency in at least two mainstream programming languages (preferably Python, Java, C++).
• Proficiency in a programming language of a non-OOP paradigm (e.g. functional / logic programming).
• Experience with FP libraries like scalaz / cats / ZIO is a plus.
• Demonstrates good judgment in software design balancing design principles with pragmatics, with an emphasis on simplicity.

See Rich Hickey's Simple Made Easy talk : https : / / www.youtube.com / watchv oytL881p-nQ
• Good to have experience in Google BigQuery.
• Experience working in some cloud services platform (e.g. Google Cloud, AWS, Azure).
• Experience with messaging middleware such as Solace or Kafka.
• Experience with machine learning is good to have.

WHO WE ARE LOOKING FOR :

We are looking for team-players who are creative in their approach to problem solving. They take the initiative to explore different ways to resolve an issue, and systematically find the most efficient and effective way to do it.

They are adept at expressing their ideas and solutions into clean and maintainable code.

THE ENVIRONMENT WE OFFER :

As a growing firm with a tightly knit team, we respect and listen to all our employees. You will get the chance to make an impact by having your voice heard by everyone, including the management.

Our employees enjoy a high level of autonomy at work. We focus on substance, not form - as long as you can perform, you will be recognized and rewarded.

We are also dedicated to supporting our staff and ensuring they develop holistically to maximize their potential in the long-term.

We also provide flexible working arrangement as required and a casual and fun work environment to boot"
29-Apr-2022 T11:55,Data Engineer,Vestiaire Collective,16 days ago,,Full–time,"Vestiaire Collective is the leading global platform for desirable pre- loved fashion. We transform the fashion industry for a more sustainable future, by empowering a community to drive change. Encouraging consumers to join the circular economy as the sustainable alternative to throw-away fashion, the platform is unique due to its highly engaged community, its rare desirable inventory and its authenticity and quality control process. Launched in Paris in October 2009, Vestiairecollective.com has close to 15 million members across 80 countries worldwide with offices in cities Paris, New York, Berlin, Hong Kong, Seoul, Singapore, Shanghai, Ho Chi Minh City and Tokyo.

Our values have built our success and made us who we are as a fast-growing company because we think collective: we work with style, with entrepreneurial spirit and with passion. We currently have a diverse global team of 650 employees representing more than 50 nationalities. Our values are community, activism transparency, dedication and greatness. We are proud to be a BCorp.

We are hiring a Data Engineer and you'll create and innovate the Vestiaire Collective Data Platform in collaboration with our Technical Leads, BI Engineers and Architects

About the role

This is a full-time role based out of our Singapore office reporting to the CTO.

What you'll do
• Co-creating and continuously developing the technical architecture of the Vestiaire Collective Data Platform
• Implementing data ingestion strategies for diverse data sources, such as internal databases, third-party sources, user data trackers
• Managing the Data Platform setup on infrastructure side, including managing the Data cloud account, setting up and managing instances, managing computing and storage capacity within budget constraints
• Guiding Data Warehouse developers with their ETL implementations, pointing to optimal technical solutions to data transformation objectives
• Working closely with Data Scientists to ensure smooth and quick implementation of algorithms in production
• Implementing tools for task scheduling, data quality controls, stability monitoring and alerting
• Continuous learning and staying up-to-date with the latest developments in the data technology space in order to keep the team ahead of the curve

Who you are
• 2+ years of working experience, including work with relational databases, Hadoop, NOSQL and/or cloud infrastructure (e.g. AWS)
• Educational background in Computer Science / Electrical Engineering or other engineering fields
• Solid understanding of database concepts and experience with data processing tools (SQL, Hive, Pig, Spark, etc.)
• Hands-on experience with at least one of the following programming languages: Java, Python, Scala - and curiosity to learn others
• Creative approach toward problem solving, passion for exploring new technologies
• Experience with real-time / stream computing technologies is a plus (e.g. Flink, Storm, Spark Streaming)
• You do not need to be micromanaged; to accomplish team and company goals, you can wear multiple hats and pickup new technologies and languages.

What we offer
• A meaningful job with an impact on the way people consume fashion and promote sustainability
• Flexible work arrangements
• The opportunity to create impact in a high growth environment
• The possibility to work as part of a global diverse team with more than 50 nationalities
• 2 days to help Project - reinforcing your activist journey and volunteer for an association
• Investment in your learning and growth
• Competitive compensation and benefits package
Vestiaire Collective is an equal opportunity employer. We strive to develop an inclusive work environment that reflects the diversity of our fashion activist community"
29-Apr-2022 T11:55,"Data Engineer, MBG",Meta,1 day ago,,Full–time,"Meta is looking for exceptionally talented and experienced engineers to join the MBG Technology team. Our team provides analytics and workflow tools for Meta Business Group ( MBG), partnering with sales, marketing, measurement, support and operations teams.In this role, you’ll work with some of the brightest minds in the industry, work with one of the richest data sets in the world, use cutting edge technology, and get an opportunity to solve some of the most challenging business and engineering problems, at a scale that few companies can match. You will do so by partnering with stakeholders/teams and building scalable, reliable solutions that provide business critical insights and metrics, while ensuring the best uptime and responsiveness.

Data Engineer, MBG Responsibilities:
• Manage data warehouse plans for a business vertical or a group of business verticals
• Build data expertise and own data quality for allocated areas of ownership
• Design, build, optimize, launch and support new and existing data models and analytical solutions
• Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions
• Conduct design and code reviews
• Work with data infrastructure to triage infra issues and drive to resolution
• Manage the delivery of high impact dashboards, tools and data visualizations

Minimum Qualifications:
• 2+ years of experience in the data warehouse space, custom ETL design, implementation and maintenance
• 2+ years of experience in SQL or similar languages, and development experience in at least one language (Python, PHP etc.)
• Experience with data architecture, data modeling, schema design and software development
• Experience in leading data driven projects from definition through interpretation and execution
• Experience with large data sets, Hadoop, and data visualization tools
• Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience working in support of diverse communities"
29-Apr-2022 T11:55,"AVP, Data Engineer - BI Tools",United Overseas Bank,1 day ago,,Full–time,"AVP, Data Engineer - BI Tools

AVP, Data Engineer - BI Tools

Posting Date: 25-Apr-2022

Location: Alexandra, Singapore, SG

Company: United Overseas Bank Limited

About UOB
United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices.
Our history spans more than 80 years. Over this time, we have been guided by our values - Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.

About the Department
The Technology and Operations function is comprised of five teams of specialists with distinct capabilities: business partnership, technology, operations, risk governance and planning support and services. We work closely together to harness the power of technology to support our physical and digital banking services and operations. This includes developing, centralising and standardising technology systems as well as banking operations in Singapore and overseas branches.

Job Responsibilities
You will be responsible for the end-to-end software development and support for all work related to projects, quarterly change requests, L3 production fixes. This includes software product implementation and administration, application design, development, implementation, testing and support. You will be expected to work on Finance & Risk Analytics.
You will also be responsible for quality assurance of the team's delivery in conformance with the Bank-defined software delivery methodology and tools. You will partner with other technology functions to help deliver required technology solutions.

Other responsibilities include:

• Create frameworks, technical features which helps in faster operationalisation of Data models, Analytical models(including AI/ML) and user generated contents (dashboards, reports etc.)

• Effectively partner with citizen data scientists in enabling faster adoption of AL/ML model based systems

• Independently install, customise and integrate software packages and programs

• Carry out POCs involving new data technologies

• Design and develop application frameworks for data integration

• Create technical documents such as solution design, program specifications for target solutions

• Perform design and development of applications which may not be limited to: Software Applications, Data Integration, User Interfaces, Automation

• Maintain and recommend software improvements to ensure a platform centric management of software applications

• Performance tuning

• Work with production support team members to conduct root cause analysis of issues, review new and existing code and/or perform unit testing

• Perform tasks as part of a cross functional development team using agile or other methodologies and utilising project management software

Job Requirements
Functional skillsets
Data Lake, EDW, Data Mart, Data Integration & Visualization
Hands-on experience in implementing large scale data warehouse & analytics platforms in financial services industry with good functional knowledge of products & services offered in Retail bank / Wholesale / Global Markets covering some of the following analytics domains:

• Setting up and running BI tools oriented platform
• Design and develop QlikSense & Microsoft Power BI applications
• Design and develop Applications in SAS, Microsoft-R, Python
• Integration of BI tools with data stores (EDW, data marts )
• Experience in Data Modeling, Data mapping for Data Warehouse and Data Marts solutions
• Experience in financial domain - Retail , Wholesale, Compliance, Digital
• Expertise in design of role based fine grained access control
• Designing cloud ready data solutions, Virtualization
Technical skillsets

• Expertise in installing and configuring of BI/Analytical tools like Qlik, Power BI, SAS, Microsoft R
• Expertise in administration (QMC, powershell scripting), infrastructure design of such tools
• Integration with other data systems like EDW, Data mart, CRM, Digital
• Expertise in building BI semantic layer (Qlik - QVD, PowerBI - Direct Query, SSAS, In Memory SSAS modelling, SSRS, Excel Add In)
• Creating application framework specific to BI tools to speed up user adoption
• Good working experience in security features of BI tools including integration with Active Directory, solutions to protect data in motion and at rest
• Good working experience in fine tuning on line BI queries, BI models, analysis of DAX queries
• Reports/Dashboard/Application development expertise using QLIK, PowerBI, SAS, Microsoft R, Python
2 to 3 technical certifications from enclosed list:

• Cloudera Hadoop distribution - Hive, Impala, Spark, Kudo, Kafka, Flume
• Teradata - Bteq, Query Grid, GCFR, MDM, Data Mover, BAR
• Informatica Data Integration - PC, IDR, BDM, MM, IDQ, EDC
• Data modelling tools (Erwin)
• QlikSense
• Microsoft Power BI - Direct Query, SSAS, SSRS, Excel Add In
• Microsoft - R
• Data science workbenches - Cloudera Workbench, Jupyter, DataRobot, H2O.AI, IBM DSX
• Data Virtualization tool - Denodo, Dremio
• AS400
• Language - SQL, Java, Python, Scala, Pyspark
• Automation / scripting - CtrlM, Shell Scripting, Groovy
Experience for an added advantage:

• CI/CD software, Testing Tools - Jenkins, SonarQube
• Version Control Tool - Aldon+LMe, CA Endeavor
• Deployment Tool kit -Jenkins
• Service or Incident Management (IcM) Tools - Remedy
• Source Code Repository Tool - Bitbucket
• Scheduling Tool - Control-M
• Defect Management Tool - JIRA
• Application Testing tool - QuerySurge
• Cloud certification
• Platforms provided by FICO, Experian, SAS for credit and portfolio management

Be a part of UOB Family
UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.

Apply now and make a difference"
29-Apr-2022 T11:55,Data Engineer,Supernova,16 hours ago,,Full–time,"About Supernova

We are the fastest growing lifestyle & beauty omni-channel ecommerce company. We build iconic beauty and lifestyle brands for a new generation of women worldwide. Independently owned with a global marketing reach, we connect with over 15 million women on social media every week. Headquartered in Singapore, we currently have offices in Berlin, London and LA. Our brands include Sand & Sky and Coco & Eve.

About Supernova's Internal Engine

Together we create Superstars! Our three company pillars are guided by this motto: Our product and brands are of the highest quality and within the top five in their category; our customers are the focus of our actions and their lives improve with our products; our talent and employees grow and develop within Supernova.

This means that investing in hiring, developing and retaining the brightest minds in the world is a top priority. Living our core values of #MakeAnImpact, #TalkAboutIt, #SeekNewOpportunities, #DelightAndSurprise and #MoveForwardTogether creates a unique culture across five different time zones and allows us to realise the enormous potential of this exciting industry. If you're an ambitious, smart, natural collaborator who likes taking risks, influencing, and innovating in a challenging hyper-growth environment, we'd love to talk to you about joining our team.

The role

This role provides tremendous opportunity for the right candidate: to capture the energy and rapid pace of a fast growing start up, while being responsible for a sizeable and growing part of our global footprint.

As Supernova's Data Engineer, you are responsible for the design and implementation of modern, scalable data-centric services and solutions.

You have strong data infrastructure and architecture skills, with a proven track record of handling high data volumes to serve batch as well as streaming needs.

This is a remote position, working as part of a strong and growing Data Team.

Responsibilities:
• Define and build data pipelines from a wide variety of data sources that will enable faster, better, data-informed decision-making within the business.
• Develop custom integrations via REST/SOAP/event-driven architecture.
• Work with stakeholders including the Brand, Product, Ops and Marketing teams to assist with data-related issues and support their data infrastructure needs.
• Build tools for effective maintenance and monitoring of the data infrastructure.
• Contribute to an ongoing effort to improve data reliability, efficiency and quality.
• Performance tuning of data pipelines jobs for optimal end-user experience.

Your Profile:
• Minimum Bachelor's degree in a quantitative field is required e.g. Computer Science, Statistics, Mathematics
• 5+ years of experience in data engineering roles using Python / Java.
• Experience in building the solution architecture, provisioning of infrastructure and delivering data-centric services and applications either in GCP or AWS.
• Strong experience in Event Driven Architecture using Kafka or Pub/Sub etc.
• Extensive knowledge in ELT processing and workflow orchestration using Airflow/ Nifi or similar tools.
• Experience with development ecosystems such as Git, Docker, Jenkins and CI/CD.
• Preferably with a background in Startup / Ecommerce.
• Ability to plan your work and commit to deadlines.
• Ability to work with cross-functional teams to deliver quality results.

This position is available immediately. All applications will be treated confidentially.

Please note that we can only contact successful applicants.

Supernova is an equal opportunity employer. Our values define the working culture and environment we strive to create – fair, diverse, respectful and welcoming of different views. We embrace a culture reflecting a variety of perspectives, insights and backgrounds to drive innovation. We build talented and diverse teams to drive business results and encourage our people to develop to their full potential. We promote harmonious working relationships and encourage a learning and development culture. We welcome applications from talented people coming from all cultures, countries, races, genders, sexual orientations, disabilities, beliefs and generations and are committed to providing a working environment free from harassment, discrimination and retaliation.

See our Applicant Data Protection policy for information on how we collect, use and process your personal data: http://www.supernova.xyz/applicant-data-protection-notice"
29-Apr-2022 T11:55,Data Engineer - APAC,Tamr,16 hours ago,,Full–time,"Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE
who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being
customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation
This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.
Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion
Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:55,Data Engineer (Analytics),Mediacorp,Full–time,,,"Description

We are looking for a Data Engineer (Web Analytics) to join our Data Architecture team. The role will involve executing the data tracking requirements for Web, Mobile apps, and Smart TV platforms.

Key Responsibilities:
• Manage the tagging of advanced analytics tools across different platforms within the organization.
• Utilize deep understanding of Adobe Analytics, along with other analytics platforms to assist the various decision making teams.
• Execute A/B tests and work on personalization and recommendation systems
• Design, develop and support reporting and analytics applications leveraging data integration tools.
• Work closely with Data Architects & Data Scientists to enforce tracking requirements across different platforms

Requirements
• Degree holder in a technical discipline (Computer Science, IS/IT or related disciplines)
• Minimum 3 years of experience in working with Tag Management Solutions Primarily on Adobe Launch
• Good to know - Google Tag Manager or Tealium/ Ensighten.
• Strong Experience in Tag migration - Adobe Heartbeat implementation, Mobile app Adobe AEP SDK implementation
• Strong Proficiency in JavaScript, specifically as it relates to digital analytics implementation.
• Experience in A/B testing tools such as Adobe Target and/or Optimizely

Mediacorp is committed to creating an inclusive and diverse workplace where talent thrives. Our hiring decisions are made based on merit and fit-to-role. If you have a disability or special need which requires accommodation to participate in the recruitment process, please inform us when you submit your online application. We will be happy to support as necessary.

Thank you for your interest and application to this role. Please note that only short-listed candidates will be contacted"
29-Apr-2022 T11:55,Data Engineer,Lilith Games Sg Pte. Ltd.,2 days ago,,Full–time,"Big Data Development Engineer (Advertising Direction) Big Data Development Engineer (Advertising Direction) What you will be doing: 1Responsible for the construction of Big Data Cloud Platform and the maintenance of Advertising Data Warehouse. 2Responsible for the calculation, cleaning and layering of Advertising Data based on Flink, Spark, etc., and store it through Hadoop, Clickhouse, etc. 3Responsible for Advertising Data Analysis and Advertising System Report development. Qualifications & Skills 1Bachelor degree or above, major in computer and other related majors. 2Proficient in Python or Golang coding. 3Proficient with Mysql, Memcache, Redis, Message Queue and other common WEB components. 4Having experience in using one or more of HDFS, Hive, HBase, MongoDB, Kafka, Flink and Spark is preferred. 5Experience in operation and maintenance development is preferred. 6Familiar with Alibaba Cloud and other cloud computing resource deployment and optimization is preferred 7Proficient in coding complex SQL Statements, and have the ability and experience of query optimization. 8Positive and optimistic, strong sense of responsibility, with good team communication and cooperation 大数开工程师（广方） 工作责 1. 负责大数平建，广数建与维护 2. 负责基于FlinkSpark等对广数进行计算清洗分等工作并通过HadoopClickhouse等进行存 3. 负责广数分广系统报表开 任 1.大学本(统招)以上学历，计算机通信等相关业 2.熟练掌Python或Golang代编写 3.熟练使MysqlMemcacheRedis消队列等常WEB件 4.有使HDFS, Hive, HBase, MongoDB, Kafka，Flink, Spark中一项或多项验优先 5.有维开验优先 6.熟阿里等计算部署与优化者优先 7.熟练编写sql语，具备查询优化能力验 8.积，责任心强，工作认真细致，具有良好团队沟通与?作能力"
29-Apr-2022 T11:55,Staff Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,16 hours ago,$8K–$13K a month,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 5 years of experience in Data Engineering role and have good knowledge / working experience in:

• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.

• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.

• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.

• Functional programming languages, e.g. Scala.

• Virtualization and container environment such as Docker and Kubernetes.

•Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:55,Data Engineering Lead,Stemly Pte. Ltd.,16 hours ago,,Full–time,"We are looking for a Senior Data Engineer with a track record of building enterprise grade data platform for cloud products .

This is an exciting opportunity for a technologist to build an innovative SaaS product using state-of-the-art and modern cloud technology stack.

As a Data Engineering Lead, you will be working on all aspects of data, from platform and infra build out to pipeline engineering and writing tooling / services for augmenting and fronting the core platform.

The data platform interfaces with large systems to ingest data (batch, micro-batch) to create statistical / machine learning models in a distributed setting.

You will work closely with data scientists, product managers, legal, compliance and business stakeholders.

You will also evaluate new technologies / frameworks and contribute to various open source big data technologies. You are encouraged to think out of the box and have fun exploring the latest patterns and designs in the fields of software and data engineering.

Your responsibilities
• Architecting and implementing a robust data platform for Stemly's products. Design and develop extremely efficient, reliable and observable data pipelines using technologies such as Airflow, Kubernetes, ElasticSearch, Kafka / KubeMQ, Postgres, ClickHouse etc.
• Design and deliver the next-gen data lifecycle management suite of tools / frameworks , including ingestion and consumption on the top of the data lake to support real-time, API-based and serverless use-cases, along with batch (mini / micro)
• Build and expose metadata catalogue for the Data Lake for easy exploration, profiling as well as lineage requirements
• Work on the development of the data platform iteratively by building quick POCs and converting ideas into real products
• Make data discoverable and easy to use for Data Scientists and Analysts. Enable Data Science teams to test and productize various ML models
• Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.

Support your colleagues by reviewing code and designs
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner

About you
• 8+ years of professional experience in software development in Python and Pandas
• Extensive experience with object-oriented design, coding and testing patterns, including experience with engineering software platforms and data infrastructures
• Must have sound understanding for Big Data components & administration fundamentals
• Hands-on in building a complete data platform using various open source technologies. Must have the knowledge to build metadata, lineage, observability and discoverability for data platform
• Experienced in DevOps best practices like CI / CD, containerization, blue-green deployments, secrets management etc. in the Data ecosystem
• Expert in the principles of distributed computing and data modelling
• Expert in building optimized SQL queries
• Conversant with the latest developments in the areas of Machine Learning models and efficiently supporting the data scientists is a plus
• Experience with Airflow, ClickHouse, Postgres, Kubernetes, GraphDB is a big plus
• Familiarity with Google Cloud Platform (e.g. GCS, Cloud Composer, BigQuery) is a plus
• Outstanding communication and interpersonal skills

About us

We are scientists and engineers with a strong passion for cutting-edge technology and decision science. Our mission is to enable enterprises to reduce time to decision with state-of-the-art forecasting and optimization.

Our algorithms are based on automatic machine learning techniques and allow for the discovery of optimal forecasting models.

We unlock sizable business and financial impact in terms of lower lost demand, decrease in inventory, lower working capital and financing costs.

How to apply

You may already know if you're a fit, but perhaps you're worried about some of the requirements We're looking for smart and passionate Engineers;

if you're the right candidate, we're flexible"
29-Apr-2022 T11:56,Data Engineer,ITCONNECTUS PTE. LTD.,16 hours ago,$5K–$13K a month,Full–time,"Immediate joiners

Requirements
• Technical Data Architect who is strong on Data Warehousing, Big Data, Data Analytics & Data Science for solutioning
• Strong Google Cloud Platform Data Components – BigQuery, BigTable, CloudSQL, Dataproc, Data Flow, Data Fusion, Etc
• Demonstrate extensive skills and success in the implementation of technology projects within a professional environment, with a particular focus on data engineering
• Experienced in delivering Medium to Big Enterprise Data Projects
• GCP Experience including and not limited to MPP systems, Database systems, ETL and ELT systems and Data Flow compute Good to have skills
• A high-level understanding of AI and ML technologies and desire to develop your skills in these areas of emerging technology.
• Should be able to advise the best of breed for the client solutions; Skills Needed
• The Data Engineer coaches the junior data engineering personnel position by bringing them up to speed and help them get better understanding of overall Data ecosystem.
• Prior experience developing, building and deploying on GCP
• Working on Solution deck, IP build, client meetings on requirement gathering"
29-Apr-2022 T11:56,Data Engineer,Manpower Singapore,2 days ago,,Full–time,"Data Engineer

Key Role and Responsibilities:
• Partner with the global and in-market data experts to discover and derive value from connecting external and internal data sources.
• Build (ETL) new and evolve data models and pipelines to power algorithmic based Business Intelligent solutions that addresses business problems requiring descriptive, diagnostic, predictive, and/or prescriptive analytics for pricing, promotion, trade spending, assortment, and sales performance management.
• Translates the algorithms and analytic models into data models as business needs evolves on a going basis after they are put into production.
• Develop a roadmap that scales existing and new data models, to support the portfolio of solutions.
• Expand into Business Intelligence solution development focused on automation and scale up of solutions.

Key Skills Required:

Professional Skills:
R/Python programming languages
MS Power BI
MS Excel
Data Visualisation

General Skills:
Client Management
Project Management
• Diploma or B.S. in a relevant technical field (Operations Research, Computer Science, Statistics, Business Analytics, Econometrics, or Mathematics).
• Overall experience of 2-3 years
• Ability to manipulate and high-volume of data from varying sources
• Expert knowledge of an analysis tool such as Microsoft PowerBI
• Proficiency in R/Python
• Basic track record in working independently with minimal guidance

Interested applicants, please submit your resume to : abelene.kang@manpower.com.sg
Abelene Marianne Kang Mrs Abelene Marianne Rozario R2089914
EA License No: 02C3423 Reg No: 199505951H

Kang Abelene Marianne Mrs Rozario Abelene Marianne

EA License No.: 02C3423 | Personnel Reg No.: R2089914

Please note that your response to this advertisement and communications with us pursuant to this advertisement will constitute informed consent to the collection, use and/or disclosure of personal data by ManpowerGroup Singapore for the purpose of carrying out its business, in compliance with the relevant provisions of the Personal Data Protection Act 2012. To learn more about ManpowerGroup's Global Privacy Policy, please visit https://www.manpower.com.sg/privacy-policy"
29-Apr-2022 T11:56,Data Engineer - User Profile,Shopee,Full–time,,,"Collaborate closely with the product team and provide data solutions to business problems and support business growth and data driven decision makingsDesign, build and maintain batch and realtime streaming user data pipelines using big data platform and technologiesResponsible for user data collection, processing, storage and building the user profile data collectionsAdopt and maintain data engine solutions to support online data services for various business use casesImprove efficiency, scalability, and stability of existing systemsEnsure data quality, consistency and timeliness"
29-Apr-2022 T11:56,"Principal Data Engineer, Data Engineering",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Principal Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you are responsible for helping to shape the direction of the data engineering domain in GovTech; working with other domain experts, the delivery teams, vendors, and partners to influence technology roadmaps and the adoption of suitable data solutions across government. In addition, you will teach, mentor, and proactively grow fellow data engineers within the organization.

You will be leading multiple teams of data engineers to drive impact at scale, by building whole-of-government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will architect, design and build next-generation central data infrastructure to galvanise digitalisation in the public sector. Taking a more integrated approach to lay the foundation infrastructure and driving pervasive adoption of advanced data technologies throughout the economy and society, you will architect solutions that enable public agencies and sectoral partners to better integrate, exploit and understand their data to improve operations and decision-making, and deliver high-quality digital services to citizens and businesses in Singapore.

Join us on this journey to make a difference for the nation and our future generations, if you are:
• Experienced leading data pipeline building and data wrangling and enjoys optimizing data systems and building them from the ground up as part of multiple cross-functional teams,
• Have strong project management and organizational skills, self-directed and comfortable supporting the data needs across multiple teams, systems and products,
• Excited by the prospect of optimizing or even re-designing central and agencies’ data architecture to support next generation of government products and data initiatives, and
• Recognized expert with proven technical leadership, experienced working in a agile team to build systems from architecture to deployment for cloud and on-prem infrastructure, and
• Have the desire to serve the public good through the use of technology.

What you will be working on:
• Take the driving seat and lead our data engineers in complex technical projects to:
• Expand and optimize our data and data pipeline architecture, as well as optimize data flow systems for cross functional teams in a secured and scalable manner.
• Assemble large, complex data sets that meet functional / non-functional business requirements, transforming data into formats that are easy to consume and analyse.
• Identify, design, and implement internal process improvements, e.g., automating manual processes, optimizing data delivery, re-designing infra for greater scalability.
• Build the infra required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and industry standard Infra-as-Code tools.
• Build analytics tools that utilize the data pipeline to provide actionable insights into key performance metrics and support our next gen of products and data initiatives.
• Work with stakeholders including the Executive, Product, Data and Design teams to support their data and data pipelining use cases, and ensure optimal and secured data delivery architecture is consistent across projects, products and platforms.
• Drive innovative initiatives that uplift data capability across the government sector:
• Develop a strategic sense of key organisations, software and hardware tools, methodologies, and best practices in the data engineering landscape.
• Be the bridge between engineering and product line management and translate market requirements into product definitions and architecture designs.
• Analyse and solve complex data engineering problems, and translate architecture designs into implementations that satisfy the market requirements.
• Educate the team on new technological advances and work alongside them in proof-of-concept research projects and subsequent agile product delivery.
• Provide expert perspective to help shape data-related strategy and initiatives, and be our technical ambassador by writing internal blogs or publishing white papers.

What we are looking for:
• Candidate with 8+ years of experience in a Lead Data Engineer role, who has attained a Bachelor’s degree in Computer Science or other relevant engineering degree.
• In-depth working knowledge across a variety of relational SQL and NoSQL databases, including efficient query authoring, performance optimisation and troubleshooting.
• Experience building and optimizing data, data pipelines, and architectures, with processes to support transformation, data structures, metadata, dependency and workload management.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for further improvement.
• Working knowledge of message queuing, stream processing, and scalable data stores.
• A successful history of processing and extracting value from large datasets from different sources and systems, with strong analytics skills related to working with unstructured data.
• You should also have experience using the following software/tools:
• Big data and stream-processing tools: Hadoop, Spark, Beam, Flink, Kafka, etc.
• Data pipelining and workflow management tools: Luigi, Airflow, etc
• Cloud computing environments and services: AWS, GCP, Azure
• DevSecOps tools: Git, Kubernetes, Docker, Terraform etc
• Programming languages: Python, Java, C++, Scala, etc

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours”"
29-Apr-2022 T11:56,"Data Engineer, Data Engineering (6 months Contract)",Bank of Singapore,10 days ago,$8K–$12K a month,Contractor,"Bank of Singapore opens doors to new opportunities.

Start your career with Bank of Singapore as a Data Engineer (6 months contract) in our Data Engineering team!

Bank of Singapore opens doors to new opportunities. At Bank of Singapore, we are constantly on the lookout for exceptional individuals to join our team. We promote a culture of openness, teamwork and fairness. Most importantly, we invest in our people through our programs that develop them on both professional and personal levels. Besides attractive remuneration packages, we offer non-financial benefits and opportunities to develop your potential within OCBC Group’s global network of subsidiaries and offices. If you have passion, drive and the will to succeed, rise to the challenge today!

Bank of Singapore has defined an ambitious roadmap for accelerating its data excellence through the implementation of various bank-wide programs. As part of this roadmap, building the foundation of good data governance and management plays a key role in establishing trust in the data that we use.

General Description

The incumbent will be based in the Data engineering team within the Data Hub team, supporting Bank of Singapore’s data sourcing, curation, transformation, and data analytic services to accelerate their data initiatives. He/she will be responsible for expanding and optimizing BOS data pipelines as well as supporting data analytics needs.

Core Activities
• Be responsible for supporting data analytics and dashboarding with respects to the data availability / stability.
• Understanding the data and identifying any potential data issues as well as correcting issues arising from data.
• Define and implement data models suitable for dashboarding
• Power BI dashboarding support
Requirement

General Knowledge & Experience
• Minimum of 6 years of working experience in data related functions
• Understanding of banking / financial services with exposure to data analytics preferred
Programming & Data
• Strong hands-on skills in SQL & PL/SQL
• Solid background in traditional structured database environments such as Teradata / Oracle
• Knowledge on Power BI dashboard functionality
• Strong skills in Power BI related technologies like power query and DAX
• Exposure to programming languages such as python
Communication & Soft Skills
• Curiosity & a real passion for understanding “why?”
• Pragmatic “can do” approach to finding data based solutions to problems
• Good communication skills with ability to engage across business and technical audiences"
29-Apr-2022 T11:56,Data Engineer,ASM TECHNOLOGY SINGAPORE PTE LTD,2 days ago,$4K–$5.5K a month,Full–time,"ASM Data Engineer

As ASMPT is rapidly growing, we embark to become a data-driven organization and are expanding our data platform. We seek an individual with strong passion in Data Engineering who will be comfortable to work in both on-premise and Cloud environments to build the data platform to support structured & unstructured datasets. You will be able to use different tools or create customized tools to transform and load data, administrate databases such as SQL and Hadoop, as well as implementing machine learning models for AIoT (Artificial Intelligence of Things).

Your Responsibilities
• Manage and support on-premise and Cloud-based data lake and warehouse systems
• Design, build, support and optimize new and existing data structure and ETL processes
• Build scalable and efficient data pipelines & services to help analytics teams to process the data
• Design useful dashboards and visualisation tools to display data insights and prediction/forecasting results
• Liaise with third party tool providers to understand and improve data workflow
• Work closely with data scientists and data analysts to deliver analytical solutions with robust underlying data platforms
• Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions

Minimum Qualification
• Bachelor Degree in Computer Science, Software Engineering, Information Technology or any related disciplines
• At least 2 years’ experience in data engineering, automation and integration is preferred
• Strong programming and scripting skills in Python and other modern programming languages
• Strong data management, schema design and SQL development skills
• Deep understanding of databases and best engineering practices – which include logging, scaling up computation, continuous integration and continuous development (CI/CD)
• Self-motivated and proactive, willing to learn new things
• Good communication skills and strong team player

What our preferred candidates have?
• Passionate in dealing with data, learning new data technologies, and discovering innovative and interesting solutions
• Understand and experienced with Cloud platform, eg. Microsoft Azure, AWS, GCP
• Business intelligence and reporting tools, eg. Power BI, Tableau, Qlik, etc
• Experienced in development using Big Data platform (Hadoop/Hive/Hbase/Spark, etc.)
• REST/Web API development and management
• Knowledge in Statistical software is an advantage
• Experience In building machine learning models is a plus"
29-Apr-2022 T11:56,Data Engineer,Thakral One Pte. Ltd.,3 days ago,,Full–time,"Job responsibilities: Work with Banking team to understand existing SAS code logic written by techno-functional users Develop the SAS logic in Big Data environment, using spark, scala and pyspark utilizing client Hadoop ecosystem of the inhouse Data Lake Perform unit testing and system integration testing Work closely with Business Analysts team to review the test results and obtain sign off Deploy the new code in the client Production environment Prepare necessary design/operations documentation for future usage Perform peers Code quality review and be gatekeeper for quality checks 5-8 years of application development experience in Spark, Spark SQL, Scala is a must Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc Technical proficiency on data mining techniques and performance optimization Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL) Experience on SAS will be beneficial but not mandatory Handling of reporting packages (Tableau, QlikView) is nice to have AWS experience is nice to have Degree in Computer Science or Engineering is a must Good problem diagnosis and creative problem-solving skills Passion to learn and master diverse new technologies in the open-source community Accuracy and attention to detail Team-working, Verbal and Written communication skills"
29-Apr-2022 T11:56,"Data Engineer, MBG",Meta,1 day ago,,Full–time,"Meta is looking for exceptionally talented and experienced engineers to join the MBG Technology team. Our team provides analytics and workflow tools for Meta Business Group ( MBG), partnering with sales, marketing, measurement, support and operations teams.In this role, you’ll work with some of the brightest minds in the industry, work with one of the richest data sets in the world, use cutting edge technology, and get an opportunity to solve some of the most challenging business and engineering problems, at a scale that few companies can match. You will do so by partnering with stakeholders/teams and building scalable, reliable solutions that provide business critical insights and metrics, while ensuring the best uptime and responsiveness.

Data Engineer, MBG Responsibilities:
• Manage data warehouse plans for a business vertical or a group of business verticals
• Build data expertise and own data quality for allocated areas of ownership
• Design, build, optimize, launch and support new and existing data models and analytical solutions
• Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions
• Conduct design and code reviews
• Work with data infrastructure to triage infra issues and drive to resolution
• Manage the delivery of high impact dashboards, tools and data visualizations

Minimum Qualifications:
• 2+ years of experience in the data warehouse space, custom ETL design, implementation and maintenance
• 2+ years of experience in SQL or similar languages, and development experience in at least one language (Python, PHP etc.)
• Experience with data architecture, data modeling, schema design and software development
• Experience in leading data driven projects from definition through interpretation and execution
• Experience with large data sets, Hadoop, and data visualization tools
• Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience working in support of diverse communities"
29-Apr-2022 T11:56,Data Engineer,HCL Technologies,2 days ago,"$7,031–$12,608 a month",Full–time,"We are looking for an experienced Data Engineer to join our growing team of analytics. The candidate will be responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Candidate must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

Responsibilities
· Create and maintain optimal data pipeline.
· Assemble large, complex data sets that meet functional / non-functional business requirements.
· Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing Jobs/code for greater scalability, etc.
· Work with stakeholders including the Product owner, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
· Work with data and analytics experts to strive for greater functionality in our data systems.

Requirements
· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.
·Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.
· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
·Strong analytic skills related to working with structured and unstructured datasets.
· Build processes supporting data transformation, data structures, metadata, dependency and workload management.
·A successful history of manipulating, processing and extracting value from large datasets.
·Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
· Experience supporting and working with cross-functional teams in a dynamic environment.
· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:

· Experience with:
o Big data tools: Hadoop, Spark, Kafka, etc.
o Relational SQL and NoSQL databases, including Postgres and Cassandra.
o Data pipeline and workflow management tools: Airflow, etc.
o AWS cloud services or GCP.
o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:56,Head of Data Engineering,TEKsystems (Allegis Group Singapore Pte Ltd),3 days ago,,Full–time,"Head of Data Engineering

We are looking for a high-caliber data expert and leader working within a team that provides world-class and innovative solutions to support the ever-growing demands in the industry.

This is a permanent opportunity.

What's in it for you:
Exposure to understanding and dealing with the data engineering & analytics requirements from various business units, hands-on analysis of the various data systems, and an opportunity to directly influence and impact the company's data infrastructure to enable data democratization.

The Position:

• 10+ years of experience in managing large-scale data initiatives within the tech or start-up space
• Experience in leading teams that design and build highly scalable data pipelines/data infrastructure in fast paced environments
• Experience in managing teams comprised of other Data Engineers, Data Scientists, Data Analysts and Software Engineers
• Strong understanding of building data models for the target data warehouse
• Clear understanding of distributed computing, especially in databases
• Hands-on in SQL with a deep understanding of query optimization
• It would be great to have experience working on any of the Cloud platforms (GCP, AWS, Azure)
• Exposure to Machine Learning / Artificial intelligence is a plus
• Strong communications skills We regret to inform that only shortlisted candidates will be notified / contacted.

For more information you can email Nicole Sichon in our Singapore office on Nicole.Sichon@teksystems.com quoting Job Reference NicoleSichon531489 or alternatively, apply here to register your interest.

http://jobs.en-sg.teksystems.com/N5oQPt/head-of-data-engineering-itcommunications-unknown-singapore-15264683

Job Reference: Nicole Sichon 531489

EA Registration No.: R1873628, Sichon Andreana Nicole Ong

Allegis Group Singapore Pte Ltd, Company Reg No. 200909448N, EA License No. 10C4544"
29-Apr-2022 T11:56,Senior Data Engineer,Grasshopper Pte. Ltd.,16 hours ago,,Full–time,"OVERVIEW :

We seek a Senior Data Engineer to support our high-frequency trading and quantitative research. At Grasshopper we recognize that effective research and trading strategies are built upon a solid foundation in data collection, storage and service.

Grasshopper trades at a number of exchanges globally, and our success depends on our ability to scale by leveraging the potential of data.

You will, in close collaboration with stakeholders, design and develop highly scalable and fault-tolerant data pipelines in a cloud environment focusing on scalability and performance, as well as the next generation of tools that empower users to generate insights on the data they work with.

RESPONSIBILITIES :
• Work with a team of data engineers across locations, managing project schedules.
• Expand the imagination of data stakeholders on what is possible and educate them in leveraging data they already have.
• Work proactively with traders, researchers and various members outside the data team to meet their needs for timely, accurate and complete data sets.
• Build highly scalable data pipelines to process and analyse billions of messages in real time.
• Set strong technical / architectural / cultural foundations, advocate good design in software development as well as data engineering, and lead by example.

SKILLS & KNOWLEDGE REQUIREMENTS :
• Strong technical leadership qualities, good at working with both people and with code.
• Extensive experience with data modelling and designing / supporting both streaming and batch ETL pipelines.
• Extensive experience in SQL and databases.
• Domain knowledge in finance, especially in buy-side trading, is not required but good to have.
• Proficiency in at least two mainstream programming languages (preferably Python, Java, C++).
• Proficiency in a programming language of a non-OOP paradigm (e.g. functional / logic programming).
• Experience with FP libraries like scalaz / cats / ZIO is a plus.
• Demonstrates good judgment in software design balancing design principles with pragmatics, with an emphasis on simplicity.

See Rich Hickey's Simple Made Easy talk : https : / / www.youtube.com / watchv oytL881p-nQ
• Good to have experience in Google BigQuery.
• Experience working in some cloud services platform (e.g. Google Cloud, AWS, Azure).
• Experience with messaging middleware such as Solace or Kafka.
• Experience with machine learning is good to have.

WHO WE ARE LOOKING FOR :

We are looking for team-players who are creative in their approach to problem solving. They take the initiative to explore different ways to resolve an issue, and systematically find the most efficient and effective way to do it.

They are adept at expressing their ideas and solutions into clean and maintainable code.

THE ENVIRONMENT WE OFFER :

As a growing firm with a tightly knit team, we respect and listen to all our employees. You will get the chance to make an impact by having your voice heard by everyone, including the management.

Our employees enjoy a high level of autonomy at work. We focus on substance, not form - as long as you can perform, you will be recognized and rewarded.

We are also dedicated to supporting our staff and ensuring they develop holistically to maximize their potential in the long-term.

We also provide flexible working arrangement as required and a casual and fun work environment to boot"
29-Apr-2022 T11:56,Data Engineer,Vestiaire Collective,16 days ago,,Full–time,"Vestiaire Collective is the leading global platform for desirable pre- loved fashion. We transform the fashion industry for a more sustainable future, by empowering a community to drive change. Encouraging consumers to join the circular economy as the sustainable alternative to throw-away fashion, the platform is unique due to its highly engaged community, its rare desirable inventory and its authenticity and quality control process. Launched in Paris in October 2009, Vestiairecollective.com has close to 15 million members across 80 countries worldwide with offices in cities Paris, New York, Berlin, Hong Kong, Seoul, Singapore, Shanghai, Ho Chi Minh City and Tokyo.

Our values have built our success and made us who we are as a fast-growing company because we think collective: we work with style, with entrepreneurial spirit and with passion. We currently have a diverse global team of 650 employees representing more than 50 nationalities. Our values are community, activism transparency, dedication and greatness. We are proud to be a BCorp.

We are hiring a Data Engineer and you'll create and innovate the Vestiaire Collective Data Platform in collaboration with our Technical Leads, BI Engineers and Architects

About the role

This is a full-time role based out of our Singapore office reporting to the CTO.

What you'll do
• Co-creating and continuously developing the technical architecture of the Vestiaire Collective Data Platform
• Implementing data ingestion strategies for diverse data sources, such as internal databases, third-party sources, user data trackers
• Managing the Data Platform setup on infrastructure side, including managing the Data cloud account, setting up and managing instances, managing computing and storage capacity within budget constraints
• Guiding Data Warehouse developers with their ETL implementations, pointing to optimal technical solutions to data transformation objectives
• Working closely with Data Scientists to ensure smooth and quick implementation of algorithms in production
• Implementing tools for task scheduling, data quality controls, stability monitoring and alerting
• Continuous learning and staying up-to-date with the latest developments in the data technology space in order to keep the team ahead of the curve

Who you are
• 2+ years of working experience, including work with relational databases, Hadoop, NOSQL and/or cloud infrastructure (e.g. AWS)
• Educational background in Computer Science / Electrical Engineering or other engineering fields
• Solid understanding of database concepts and experience with data processing tools (SQL, Hive, Pig, Spark, etc.)
• Hands-on experience with at least one of the following programming languages: Java, Python, Scala - and curiosity to learn others
• Creative approach toward problem solving, passion for exploring new technologies
• Experience with real-time / stream computing technologies is a plus (e.g. Flink, Storm, Spark Streaming)
• You do not need to be micromanaged; to accomplish team and company goals, you can wear multiple hats and pickup new technologies and languages.

What we offer
• A meaningful job with an impact on the way people consume fashion and promote sustainability
• Flexible work arrangements
• The opportunity to create impact in a high growth environment
• The possibility to work as part of a global diverse team with more than 50 nationalities
• 2 days to help Project - reinforcing your activist journey and volunteer for an association
• Investment in your learning and growth
• Competitive compensation and benefits package
Vestiaire Collective is an equal opportunity employer. We strive to develop an inclusive work environment that reflects the diversity of our fashion activist community"
29-Apr-2022 T11:56,Data Engineer,POWER IT SERVICES,13 hours ago,,Full–time,"Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

· Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

· A successful history of manipulating, processing and extracting value from large datasets.

· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science Statistics, Informatics, Information Systems or another quantitative field.

They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:56,"AVP, Data Engineer - BI Tools",United Overseas Bank,1 day ago,,Full–time,"AVP, Data Engineer - BI Tools

AVP, Data Engineer - BI Tools

Posting Date: 25-Apr-2022

Location: Alexandra, Singapore, SG

Company: United Overseas Bank Limited

About UOB
United Overseas Bank Limited (UOB) is a leading bank in Asia with a global network of more than 500 branches and offices in 19 countries and territories in Asia Pacific, Europe and North America. In Asia, we operate through our head office in Singapore and banking subsidiaries in China, Indonesia, Malaysia and Thailand, as well as branches and offices.
Our history spans more than 80 years. Over this time, we have been guided by our values - Honorable, Enterprising, United and Committed. This means we always strive to do what is right, build for the future, work as one team and pursue long-term success. It is how we work, consistently, be it towards the company, our colleagues or our customers.

About the Department
The Technology and Operations function is comprised of five teams of specialists with distinct capabilities: business partnership, technology, operations, risk governance and planning support and services. We work closely together to harness the power of technology to support our physical and digital banking services and operations. This includes developing, centralising and standardising technology systems as well as banking operations in Singapore and overseas branches.

Job Responsibilities
You will be responsible for the end-to-end software development and support for all work related to projects, quarterly change requests, L3 production fixes. This includes software product implementation and administration, application design, development, implementation, testing and support. You will be expected to work on Finance & Risk Analytics.
You will also be responsible for quality assurance of the team's delivery in conformance with the Bank-defined software delivery methodology and tools. You will partner with other technology functions to help deliver required technology solutions.

Other responsibilities include:

• Create frameworks, technical features which helps in faster operationalisation of Data models, Analytical models(including AI/ML) and user generated contents (dashboards, reports etc.)

• Effectively partner with citizen data scientists in enabling faster adoption of AL/ML model based systems

• Independently install, customise and integrate software packages and programs

• Carry out POCs involving new data technologies

• Design and develop application frameworks for data integration

• Create technical documents such as solution design, program specifications for target solutions

• Perform design and development of applications which may not be limited to: Software Applications, Data Integration, User Interfaces, Automation

• Maintain and recommend software improvements to ensure a platform centric management of software applications

• Performance tuning

• Work with production support team members to conduct root cause analysis of issues, review new and existing code and/or perform unit testing

• Perform tasks as part of a cross functional development team using agile or other methodologies and utilising project management software

Job Requirements
Functional skillsets
Data Lake, EDW, Data Mart, Data Integration & Visualization
Hands-on experience in implementing large scale data warehouse & analytics platforms in financial services industry with good functional knowledge of products & services offered in Retail bank / Wholesale / Global Markets covering some of the following analytics domains:

• Setting up and running BI tools oriented platform
• Design and develop QlikSense & Microsoft Power BI applications
• Design and develop Applications in SAS, Microsoft-R, Python
• Integration of BI tools with data stores (EDW, data marts )
• Experience in Data Modeling, Data mapping for Data Warehouse and Data Marts solutions
• Experience in financial domain - Retail , Wholesale, Compliance, Digital
• Expertise in design of role based fine grained access control
• Designing cloud ready data solutions, Virtualization
Technical skillsets

• Expertise in installing and configuring of BI/Analytical tools like Qlik, Power BI, SAS, Microsoft R
• Expertise in administration (QMC, powershell scripting), infrastructure design of such tools
• Integration with other data systems like EDW, Data mart, CRM, Digital
• Expertise in building BI semantic layer (Qlik - QVD, PowerBI - Direct Query, SSAS, In Memory SSAS modelling, SSRS, Excel Add In)
• Creating application framework specific to BI tools to speed up user adoption
• Good working experience in security features of BI tools including integration with Active Directory, solutions to protect data in motion and at rest
• Good working experience in fine tuning on line BI queries, BI models, analysis of DAX queries
• Reports/Dashboard/Application development expertise using QLIK, PowerBI, SAS, Microsoft R, Python
2 to 3 technical certifications from enclosed list:

• Cloudera Hadoop distribution - Hive, Impala, Spark, Kudo, Kafka, Flume
• Teradata - Bteq, Query Grid, GCFR, MDM, Data Mover, BAR
• Informatica Data Integration - PC, IDR, BDM, MM, IDQ, EDC
• Data modelling tools (Erwin)
• QlikSense
• Microsoft Power BI - Direct Query, SSAS, SSRS, Excel Add In
• Microsoft - R
• Data science workbenches - Cloudera Workbench, Jupyter, DataRobot, H2O.AI, IBM DSX
• Data Virtualization tool - Denodo, Dremio
• AS400
• Language - SQL, Java, Python, Scala, Pyspark
• Automation / scripting - CtrlM, Shell Scripting, Groovy
Experience for an added advantage:

• CI/CD software, Testing Tools - Jenkins, SonarQube
• Version Control Tool - Aldon+LMe, CA Endeavor
• Deployment Tool kit -Jenkins
• Service or Incident Management (IcM) Tools - Remedy
• Source Code Repository Tool - Bitbucket
• Scheduling Tool - Control-M
• Defect Management Tool - JIRA
• Application Testing tool - QuerySurge
• Cloud certification
• Platforms provided by FICO, Experian, SAS for credit and portfolio management

Be a part of UOB Family
UOB is an equal opportunity employer. UOB does not discriminate on the basis of a candidate's age, race, gender, color, religion, sexual orientation, physical or mental disability, or other non-merit factors. All employment decisions at UOB are based on business needs, job requirements and qualifications. If you require any assistance or accommodations to be made for the recruitment process, please inform us when you submit your online application.

Apply now and make a difference"
29-Apr-2022 T11:56,Data Engineer,Supernova,16 hours ago,,Full–time,"About Supernova

We are the fastest growing lifestyle & beauty omni-channel ecommerce company. We build iconic beauty and lifestyle brands for a new generation of women worldwide. Independently owned with a global marketing reach, we connect with over 15 million women on social media every week. Headquartered in Singapore, we currently have offices in Berlin, London and LA. Our brands include Sand & Sky and Coco & Eve.

About Supernova's Internal Engine

Together we create Superstars! Our three company pillars are guided by this motto: Our product and brands are of the highest quality and within the top five in their category; our customers are the focus of our actions and their lives improve with our products; our talent and employees grow and develop within Supernova.

This means that investing in hiring, developing and retaining the brightest minds in the world is a top priority. Living our core values of #MakeAnImpact, #TalkAboutIt, #SeekNewOpportunities, #DelightAndSurprise and #MoveForwardTogether creates a unique culture across five different time zones and allows us to realise the enormous potential of this exciting industry. If you're an ambitious, smart, natural collaborator who likes taking risks, influencing, and innovating in a challenging hyper-growth environment, we'd love to talk to you about joining our team.

The role

This role provides tremendous opportunity for the right candidate: to capture the energy and rapid pace of a fast growing start up, while being responsible for a sizeable and growing part of our global footprint.

As Supernova's Data Engineer, you are responsible for the design and implementation of modern, scalable data-centric services and solutions.

You have strong data infrastructure and architecture skills, with a proven track record of handling high data volumes to serve batch as well as streaming needs.

This is a remote position, working as part of a strong and growing Data Team.

Responsibilities:
• Define and build data pipelines from a wide variety of data sources that will enable faster, better, data-informed decision-making within the business.
• Develop custom integrations via REST/SOAP/event-driven architecture.
• Work with stakeholders including the Brand, Product, Ops and Marketing teams to assist with data-related issues and support their data infrastructure needs.
• Build tools for effective maintenance and monitoring of the data infrastructure.
• Contribute to an ongoing effort to improve data reliability, efficiency and quality.
• Performance tuning of data pipelines jobs for optimal end-user experience.

Your Profile:
• Minimum Bachelor's degree in a quantitative field is required e.g. Computer Science, Statistics, Mathematics
• 5+ years of experience in data engineering roles using Python / Java.
• Experience in building the solution architecture, provisioning of infrastructure and delivering data-centric services and applications either in GCP or AWS.
• Strong experience in Event Driven Architecture using Kafka or Pub/Sub etc.
• Extensive knowledge in ELT processing and workflow orchestration using Airflow/ Nifi or similar tools.
• Experience with development ecosystems such as Git, Docker, Jenkins and CI/CD.
• Preferably with a background in Startup / Ecommerce.
• Ability to plan your work and commit to deadlines.
• Ability to work with cross-functional teams to deliver quality results.

This position is available immediately. All applications will be treated confidentially.

Please note that we can only contact successful applicants.

Supernova is an equal opportunity employer. Our values define the working culture and environment we strive to create – fair, diverse, respectful and welcoming of different views. We embrace a culture reflecting a variety of perspectives, insights and backgrounds to drive innovation. We build talented and diverse teams to drive business results and encourage our people to develop to their full potential. We promote harmonious working relationships and encourage a learning and development culture. We welcome applications from talented people coming from all cultures, countries, races, genders, sexual orientations, disabilities, beliefs and generations and are committed to providing a working environment free from harassment, discrimination and retaliation.

See our Applicant Data Protection policy for information on how we collect, use and process your personal data: http://www.supernova.xyz/applicant-data-protection-notice"
29-Apr-2022 T11:56,Data Engineer (Analytics),Mediacorp,Full–time,,,"Description

We are looking for a Data Engineer (Web Analytics) to join our Data Architecture team. The role will involve executing the data tracking requirements for Web, Mobile apps, and Smart TV platforms.

Key Responsibilities:
• Manage the tagging of advanced analytics tools across different platforms within the organization.
• Utilize deep understanding of Adobe Analytics, along with other analytics platforms to assist the various decision making teams.
• Execute A/B tests and work on personalization and recommendation systems
• Design, develop and support reporting and analytics applications leveraging data integration tools.
• Work closely with Data Architects & Data Scientists to enforce tracking requirements across different platforms

Requirements
• Degree holder in a technical discipline (Computer Science, IS/IT or related disciplines)
• Minimum 3 years of experience in working with Tag Management Solutions Primarily on Adobe Launch
• Good to know - Google Tag Manager or Tealium/ Ensighten.
• Strong Experience in Tag migration - Adobe Heartbeat implementation, Mobile app Adobe AEP SDK implementation
• Strong Proficiency in JavaScript, specifically as it relates to digital analytics implementation.
• Experience in A/B testing tools such as Adobe Target and/or Optimizely

Mediacorp is committed to creating an inclusive and diverse workplace where talent thrives. Our hiring decisions are made based on merit and fit-to-role. If you have a disability or special need which requires accommodation to participate in the recruitment process, please inform us when you submit your online application. We will be happy to support as necessary.

Thank you for your interest and application to this role. Please note that only short-listed candidates will be contacted"
29-Apr-2022 T11:56,Data Engineer,REAL ESTATE ANALYTICS PTE. LTD.,4 days ago,$4K–$6.5K a month,Full–time,"Key Responsibilities
• Maintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable and timely delivery of key insights for decision making
• cleaning and normalizing subsets of data of interest as preparatory step before deeper analysis by the data scientists
• Run Modern high performance analytical databases and computation engines
• Design and implement various data health checks to ensure the data quality and consistency across systems
• Design and implement data extraction solution in a distributed system

Skills required
• Experience in handling large data sets and working with structured, unstructured and geographical datasets
• Understanding of databases and best engineering practices - include handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding how to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipeline
• Experience with DevOps and AWS will be an advantage
• Real passion for data, new data technologies, and discovering new and interesting solutions to the company’s data needs
• Excellent communication skills to communicate with the product development engineers to coordinate development of data pipelines, and or any new products features that can be built on top of the results of data analysis"
29-Apr-2022 T11:56,Data Engineer,Lilith Games Sg Pte. Ltd.,2 days ago,,Full–time,"Big Data Development Engineer (Advertising Direction) Big Data Development Engineer (Advertising Direction) What you will be doing: 1Responsible for the construction of Big Data Cloud Platform and the maintenance of Advertising Data Warehouse. 2Responsible for the calculation, cleaning and layering of Advertising Data based on Flink, Spark, etc., and store it through Hadoop, Clickhouse, etc. 3Responsible for Advertising Data Analysis and Advertising System Report development. Qualifications & Skills 1Bachelor degree or above, major in computer and other related majors. 2Proficient in Python or Golang coding. 3Proficient with Mysql, Memcache, Redis, Message Queue and other common WEB components. 4Having experience in using one or more of HDFS, Hive, HBase, MongoDB, Kafka, Flink and Spark is preferred. 5Experience in operation and maintenance development is preferred. 6Familiar with Alibaba Cloud and other cloud computing resource deployment and optimization is preferred 7Proficient in coding complex SQL Statements, and have the ability and experience of query optimization. 8Positive and optimistic, strong sense of responsibility, with good team communication and cooperation 大数开工程师（广方） 工作责 1. 负责大数平建，广数建与维护 2. 负责基于FlinkSpark等对广数进行计算清洗分等工作并通过HadoopClickhouse等进行存 3. 负责广数分广系统报表开 任 1.大学本(统招)以上学历，计算机通信等相关业 2.熟练掌Python或Golang代编写 3.熟练使MysqlMemcacheRedis消队列等常WEB件 4.有使HDFS, Hive, HBase, MongoDB, Kafka，Flink, Spark中一项或多项验优先 5.有维开验优先 6.熟阿里等计算部署与优化者优先 7.熟练编写sql语，具备查询优化能力验 8.积，责任心强，工作认真细致，具有良好团队沟通与?作能力"
29-Apr-2022 T11:56,Data Engineering Lead,Stemly Pte. Ltd.,16 hours ago,,Full–time,"We are looking for a Senior Data Engineer with a track record of building enterprise grade data platform for cloud products .

This is an exciting opportunity for a technologist to build an innovative SaaS product using state-of-the-art and modern cloud technology stack.

As a Data Engineering Lead, you will be working on all aspects of data, from platform and infra build out to pipeline engineering and writing tooling / services for augmenting and fronting the core platform.

The data platform interfaces with large systems to ingest data (batch, micro-batch) to create statistical / machine learning models in a distributed setting.

You will work closely with data scientists, product managers, legal, compliance and business stakeholders.

You will also evaluate new technologies / frameworks and contribute to various open source big data technologies. You are encouraged to think out of the box and have fun exploring the latest patterns and designs in the fields of software and data engineering.

Your responsibilities
• Architecting and implementing a robust data platform for Stemly's products. Design and develop extremely efficient, reliable and observable data pipelines using technologies such as Airflow, Kubernetes, ElasticSearch, Kafka / KubeMQ, Postgres, ClickHouse etc.
• Design and deliver the next-gen data lifecycle management suite of tools / frameworks , including ingestion and consumption on the top of the data lake to support real-time, API-based and serverless use-cases, along with batch (mini / micro)
• Build and expose metadata catalogue for the Data Lake for easy exploration, profiling as well as lineage requirements
• Work on the development of the data platform iteratively by building quick POCs and converting ideas into real products
• Make data discoverable and easy to use for Data Scientists and Analysts. Enable Data Science teams to test and productize various ML models
• Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.

Support your colleagues by reviewing code and designs
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner

About you
• 8+ years of professional experience in software development in Python and Pandas
• Extensive experience with object-oriented design, coding and testing patterns, including experience with engineering software platforms and data infrastructures
• Must have sound understanding for Big Data components & administration fundamentals
• Hands-on in building a complete data platform using various open source technologies. Must have the knowledge to build metadata, lineage, observability and discoverability for data platform
• Experienced in DevOps best practices like CI / CD, containerization, blue-green deployments, secrets management etc. in the Data ecosystem
• Expert in the principles of distributed computing and data modelling
• Expert in building optimized SQL queries
• Conversant with the latest developments in the areas of Machine Learning models and efficiently supporting the data scientists is a plus
• Experience with Airflow, ClickHouse, Postgres, Kubernetes, GraphDB is a big plus
• Familiarity with Google Cloud Platform (e.g. GCS, Cloud Composer, BigQuery) is a plus
• Outstanding communication and interpersonal skills

About us

We are scientists and engineers with a strong passion for cutting-edge technology and decision science. Our mission is to enable enterprises to reduce time to decision with state-of-the-art forecasting and optimization.

Our algorithms are based on automatic machine learning techniques and allow for the discovery of optimal forecasting models.

We unlock sizable business and financial impact in terms of lower lost demand, decrease in inventory, lower working capital and financing costs.

How to apply

You may already know if you're a fit, but perhaps you're worried about some of the requirements We're looking for smart and passionate Engineers;

if you're the right candidate, we're flexible"
29-Apr-2022 T11:56,Data Engineer - APAC,Tamr,16 hours ago,,Full–time,"Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE
who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being
customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation
This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.
Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion
Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:56,Senior Data Engineer,eyos.one,Full–time,,,"We are a dynamic team with great offices in Singapore, London, Bangkok, Jakarta and Sydney, serving our great customers worldwide. We’re expanding fast and are looking for a passionate and driven Senior Data Engineer to join our global tech team in Singapore.

Here's What You Will Be Doing:
• We are looking for a savvy Senior Data Engineer to join our growing team of retail analytics experts and expected to be involved from inception of projects, understand requirements, architect, develop, deploy, and maintain data platform and/or data pipelines, which involves partnering with program and product managers to expand product offering based on business demands.
• Our goal is to drive up retail analytics and adoption of the platform while constantly working towards improving platform performance and scalability, deployment and maintenance require close interaction with various teams.
• Production support for applications is usually required for issues that cannot be resolved by the operations team. Creative and inventive problem-solving skills for reduced turnaround times are highly valued.
• Collaborate with Data Scientists, Data Analysts, and other internal stakeholders to assist with data-related technical issues and support their data pipeline infrastructure and data preparation needs.
• Preparing user documentation to maintain both development and operations continuity is integral to the role.

Here's What You Need To Have/Know:
• BE/BS in Computer Science or equivalent practical experience
• At least 3 - 7 years experience in data analytics environments
• Experience in data warehouses like Snowflake, Google BigQuery and OLTP databases like MySQL, SQL Server, Oracle.
• Hands-on experience in SQL, Python or Scala and Spark
• Hands-on experience with cloud environments (AWS & GCP preferred)
• Experience with dev ops tools like Git, Bitbucket
• Understanding of airflow, dagster or any kind of orchestration tools
• Extensive experience on distributed environment and scalable systems
• Good analytical and problem-solving skills with strong attention to detail
• Excellent communication skills with an ability to work well with others (across APAC and EMEA)
• Knowledge of and experience in automation technologies

Skills that would be a plus
• Basic understanding of containers technologies (like dockers, Kubernetes)
• Exposure to visualization technologies such as Tableau or Sisense
• Prior experience and knowledge on machine learning models deployment processes in AWS or any other cloud systems.
• Exposure to NoSQL platforms

What will you learn on the job?
• FMCG brands with granular market intelligence, targeted marketing automation and data-driven sales optimization, especially in independent trade channel that attracts majority of grocery shopping trips in the Asian markets.
• Data pipeline flow, which converts physical receipts to digital receipts and connect 100% of in-store transactions into any platform they use in real-time.
• Build a platform for data-driven growth in physical retail shops that enables large scale analytics and data science workflows.
• Exposure to massive retail of datasets and deal with some of the most exciting data & analytics challenges"
29-Apr-2022 T11:56,Data Engineer Lead,Robert Walters,6 days ago,,Full–time,"An exciting Data Engineer Lead job opportunity has become available at a leading e-commerce company.

About the Data Engineer Lead Role:
This leading e-commerce organisation has a strong presence in Singapore. They are looking to hire a Data Engineer Lead reporting to the head of department.

Key Responsibilities:
• Creating and maintain optimal data pipeline architectures from various channels
• Ensuring optimal accuracy and timeliness of data on real-time basis while implementing monitoring tools to detect data issues
• Proper processing of semi-structured and unstructured datasets
• Working with internal teams to understand requirements and optimise performance

To succeed in this Data Engineer will need to have around five years of experience.

Key Requirements:
• Bachelor’s degree in computer science or equivalent
• Experience in business intelligence tools such as Tableau and PowerBI
• Experience in Python and R
• Able to implement ETL logic using SQL
• Knowledge in Hadoop of spark for data processing
• ELT pipelines management experience along with background in analytics

This is an excellent opportunity to be part of a company with strong footprint within the region.

If you are driven, determined and want to take the next step in your career, this Data Engineer role is right for you. Excellent career progression opportunities await the right person in this exciting role.

Apply today or contact me at +65 6228 5350 to discuss this new opportunity. Alternatively, send your resume to sachet.sethi@robertwalters.com.sg.

Do note that we will only be in touch if your application is shortlisted.

Robert Walters (Singapore) Pte Ltd

ROC No.: 199706961E | EA Licence No.: 03C5451

EA Registration No.: R1439850 Sachet Sethi"
29-Apr-2022 T11:56,Data Engineer,Morgan McKinley,4 days ago,,Full–time,"Data Engineer

Job Summary
• Singapore
• Permanent
• BBBH811650
• Apr 05, 2022
• Competitive Job Description
Our client is looking for an experienced Data engineer to drive development of information products for data and digital transformation across their group.

Mandatory Skills:
• Master of Business Administrations or master's in quantitative fields (Computer Science, Statistics or similar) with minimum of 5 - 12 years of overall experience.
• Experience programming in Python, Java, SQL, PLSQL.
• Experience with traditional RDBMS based systems and more modern NoSQL technology stacks.
• Expertise building ETL and data pipelines on Databricks.
• Experience working with Big Data technologies such as Hadoop, Cloudera (CDH), Hortonworks (HDP), DataBricks, Spark, Delta, HDFS, HBase, Hive.
• Experience in event streaming with Kafka.
• Experience in ML Model Productionization, Docker.
• Real Time, Batch, Unstructured Data, DW, MDM, Data Marts.
• Proficient in using data visualization tool such as Tableau, Power BI, D3, AmCharts etc.
• Understanding FS industry fundamentals and business problems to find new ways to leverage data.
• Intellectual curiosity to solve data driven problems.
• Independent thoughts and unique ideas on solving business problems.
• Able to collaborate / virtually manage multi-cultural, multi-disciplined, globally dispersed teams.
• Sound understanding of functional areas and analytics problems in areas of Customer, Marketing, Channel, Pricing, Digital, Operations, HR/Corporate analytics.
• String communication skills to be able to explain highly technical problems in simple layman form.
• Ability to articulate the impact of decisions and recommend improvements. Desired Skills:
• Relevant experience in Banking and financial institutions. Job Responsibilities:
• Exceptional data engineering & visualization experience & technical skills. State-of-the-art expertise across, data/information preparation and data insight & visualization using BI (or similar tools).
• Be a data engineering & visualization technical expert. Lead and explain/educate to all levels people in these areas, Coding Databases, Data Integration, Frameworks, Deployment, Architectures and Visualization.
• Contribute to the development of in-house data products. Use your data engineering & visualization expertise plus FS domain & functional knowledge, problem solving skills and independent thinking to create in-house data analytics and data management products.
• Be a team player & an individual contributor. Work with group data office and other business function people as part of larger deliveries, as well as being able to work independently or in small teams to continuously deliver business value.
• Be a trusted partner of our client. Someone that anyone in the company can reach out to for help with creating data engineering & visualization driven business transformation. Those who are keen for the role and would like to discuss the opportunity further, please click ""Apply Now"" or email Alimpan at amukherjee@morganmckinley.com with your updated CV.

Only shortlisted candidates will be responded to, therefore if you do not receive a response within 14 days please accept this as notification that you have not been shortlisted.

Alimpan Mukherjee

Morgan McKinley Pte Ltd

EA Licence No: 11C5502

EA Registration Number: R2198629"
29-Apr-2022 T11:56,Senior Data Engineer,Toptal,4 hours ago,,Full–time,"About The Job

Toptal developers work with speed and efficiency to deliver the highest quality of work. We are looking for someone who is passionate about their client’s business, and ready to work on exciting projects with Fortune 500 companies and Silicon Valley startups, with great rates and zero hassles. If you are looking for a place to advance your career, enhance your skill set, and build connections around the globe, Toptal is right for you.

About Toptal

Toptal is an exclusive network of top freelancers from around the world. Fortune 500 companies and Silicon Valley startups hire Toptal for their most important projects. Toptal is one of the fastest-growing fully remote networks and empowers freelance software developers, designers, finance experts, product managers, and project managers worldwide to grow and excel in their freelance careers.

Toptal clients vary in sizes and industries, from enterprise organizations and big tech companies to Silicon Valley startups and renowned universities. Once you enter the network, our matchers will contact you with project opportunities that fit your expertise and preferences. We have experts in over 120 countries who get to work remotely on projects that meet their career ambitions.

About The Role

As a Data Engineer, your main goal is to be one step ahead of data scientists and analysts. You will support them by providing infrastructure and tools they can use to deliver end-to-end solutions to business problems that can be developed rapidly and maintained easily. This is more than building and maintaining ETL pipelines. We need innovation, creativity, and solutions that will have a significant impact on the client’s velocity.

Requirements
• 3+ years of professional experience in software development
• Working experience with Python and Pandas.
• Familiarity with the basic principles of distributed computing and data modeling.
• Extensive experience with object-oriented design and coding and testing patterns, including experience with engineering software platforms and data infrastructures.
• Working experience with Airflow and Luigi is a big plus.
• Working experience with Scala is a plus.
• Familiarity with Google Cloud Platform (e.g. GCS and BigQuery) is a plus.
• Working experience with Dimensional Modeling and Rails is a plus.
• Outstanding communication and interpersonal skills.
• Full-time availability is a strong advantage

If you’re interested in pursuing an engaging career working on full-time freelance jobs for exclusive clients, take the next step by clicking apply and filling out the short form to get started.

#RemoteJobDataEngineering"
29-Apr-2022 T11:56,Data Engineer,Power It,2 days ago,,Full–time,"· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.
· Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.
· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
· Strong analytic skills related to working with structured and unstructured datasets.
· Build processes supporting data transformation, data structures, metadata, dependency and workload management.
· A successful history of manipulating, processing and extracting value from large datasets.
· Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
· Experience supporting and working with cross-functional teams in a dynamic environment.
· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics Information Systems or another quantitative field.
They should also have experience using the following software/tools:
· Experience with:
o Big data tools: Hadoop, Spark, Kafka, etc.
o Relational SQL and NoSQL databases, including Postgres and Cassandra.
o Data pipeline and workflow management tools: Airflow, etc.
o AWS cloud services or GCP.
o Stream-processing systems: Spark-Streaming, Flink etc.

Job Type: Permanent

Schedule:
• Monday to Friday"
29-Apr-2022 T11:56,Finance WebApp Data Engineer,Apple,Full–time,,,"Summary
Posted: 27 Mar 2022
Weekly Hours: 40
Role Number:200360362

Apple’s Finance Business Process Reengineering team is seeking a Data Engineer to join our organization. In this role you will help architect, maintain and continually improve data analytics and automation capabilities. Sitting inside a business unit, you must adapt to varied and shifting needs. You will be working with business users, Apple IS&T developers, Web Application developers and data analysts on your team to deliver complete, accurate, well-secured data that enables reporting and analytics across all Finance functions. Comfort with and ability to learn and perform a wide variety of development and engineering tasks combined with knowledge of Finance business processes sets you apart in this role.

Key Qualifications

Key Qualifications
• Experience in building backend infrastructure for scalable data processing and analytics
• Strong SQL and Python skills
• Experience tracking and measuring data quality solid knowledge of database technologies including Snowflake, Teradata, MySQL, MariaDB
• Experience building scalable data pipelines
• Experience with applying data encryption and data security standards
• Preferred experience in tools like Dataiku
• Able to quickly learn new technologies

Description

Description
The Finance Business Process Reengineering (FBPR) organization supports Apple's Finance function worldwide. Finance Data and Technology (FDT) team within the FBPR org enables the Finance organization by providing quality data accessibility, analytics, reporting and automation services. We are looking to expand capabilities in the areas of data privacy, high-performance computing, advanced analytics and general business intelligence.

A Finance WebApp Data Engineer is a technical expert and works tightly with WebApp developers, other data engineers and data analysts on the team to create data integrations, ETL, pipelines, transformation and codebase to drive innovative analytics projects from initial experimentation to production level deployment. They work on critical data engineering problems, building bespoke, reliable, accurate, consistent, and architecturally sound solutions that are aligned with business needs.

The Finance Data Engineer architects, maintains and continually improves data analytics and automation capabilities. The role requires working cross-functionally with business users, Apple IS&T developers, and data analysts to deliver complete, accurate, well-secured data that enables reporting and analytics. This role is required to learn and perform a wide variety of development and engineering tasks, on top of growing their knowledge of Finance business processes to efficiently identify data applicable business questions.

Finance Data Engineers work predominately in Apple’s enterprise data warehouse (EDW), identifying and combining data in an efficient, scalable manner to help answer business questions. When the necessary data is not available on an enterprise system or is generated offline by business users or third parties, the Finance Data Engineer must develop methods to reliably source, validate, and integrate the data into EDW.

The Finance Data Engineer must learn and understand a variety of available IS&T solutions, when and how to use them, and when to develop custom solutions. This paired with a proven record of excellent problem solving and a sharp, open mind will be more important than deep expertise in any one area.

Education & Experience

Education & Experience
Master’s degree or equivalent in Computer Science or related field and two years of experience. Alternatively, a Bachelor’s degree in Computer Science or related field and five years of progressive experience.

Additional Requirements

Additional Requirements
• Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.
• We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation"
29-Apr-2022 T11:56,Data Engineer,Glp Pte. Ltd.,16 hours ago,,Full–time,"The Data Engineer will be a key player in the enterprise-wide data transformations projects. You will engage in developing and automating data processing pipelines for data modelling, analysis, and reporting from various data sources system; The primary responsibility of this position is to assist to establish the enterprise data Lake architecture under Microsoft Azure Data factory, Databricks and Synapse and deliver data driven solutions.Job description:· Assist in architecture design, develop, document, and implement end-to-end data pipelines and data driven solutions.· Define roadmap to transform data architecture focusing on scalability, performance and stability for the entire data lifecycle;· Build data flows for data acquisition, aggregation, and modelling, using both batch and streaming paradigms.· Perform data analysis, data profiling, data cleansing, data lineage, data mapping and data transformation.· Develop high-quality code for the core data stack including data integration hub, data warehouse and data pipelines under Azure service.· Execute and deliver best practices in data management and data lifecycle processes, including modular development of data processes, coding and configuration standards, error handling and notification standards, auditing standards, and data archival standards.· Implement security and standards, documenting technical specifications and operating procedures.· Collaborate across developers as part of a SCRUM team ensuring collective team productivity· Provide technical support for any data issues with recommendations, and resolutions.Requirements:· 2 years professional experience as a data engineer, software engineer, data analyst, data scientist, or related role.· Experience with Microsoft Azure Data Integration Stack (Azure Data Lake Gen2, Azure Data Factory, Delta Lake, SSIS, SQL Server, Azure Data Warehouse), Databricks, Spark.· Working experience in Investment or Real Estate industry, preferably with business and functional knowledge.· Expertise building ETL and data pipelines on Databricks using data engineering languages Python and SQL on Azure.· Advanced knowledge and experience working with Python & SQL;· Proven experience with all aspects of the Data Pipeline (Data Sourcing, Transformations, Data Quality, Etc…).· Experience with visual modelling tools including UML· Proficient in using data visualization tool such as Power BI, Workiva and in standard office tools such as Excel.· Familiar with DevOps and Agile methodology"
29-Apr-2022 T11:56,Data Engineer (Trading),Michael Page,10 days ago,,Full–time,"About Our Client

Our client is a fintech company that is a trusted crypto exchange brand. They are looking for a Data Engineer that has experience in the finance industry.

Job Description
• Collaborate with trading applications and backend developers to advise and fulfill data requirements in terms of consistency, latency and scalability
• Responsible for the implementation and maintenance of relevant database technologies and data quality to meet these data needs
• Collaborate with existing data team to design and implement data synchronization
• Work with development team for the maintenance and enhancement of latency-sensitive market data feed components

The Successful Applicant
• 3 years' working experience in data engineering, database administration, AWS data solutions or relevant field
• Proficient in Python, R, Julia (at least one), SQL, Linux and C++, Java or Rust (at least one)
• Proficient in data pipeline development involving cloud data solutions such as Redshift Snowflake or databases such as SQL, kdb+ or other time series databases
• Experience in market data gateway protocols such as ITCH or socket and web socket programming or real-time messaging frameworks such as Apache Kafka, Aeron, Chronicle is highly desirable

What's on Offer
• Permanent role
• Career Advancement Opportunity
• Opportunity to work on cutting edge technologies"
29-Apr-2022 T11:56,Data Engineer (APAC),TEKsystems (Allegis Group Singapore Pte Ltd),6 days ago,,Full–time,"Data Engineer (APAC)

We are looking for high-calibre Data Engineers to be a part of a growing tech firm!

This is a Permanent opportunity.

The Position:

• Experience in designing and building robust and highly scalable data pipelines
• Strong programming proficiency using Python and SQL
• Strong understanding of building data models and data warehouse technologies.
• Experience with Airflow
• Good to have: BI tools like Tableau or Google Data Studio
• Strong communications skills
What's in it for you?

• Exposure to the latest cutting-edge technologies
• Rewarding work: Opportunity to make real and immediate impact on the business and the industry
• You'll work with a great team. They're exceptional at their jobs, and winning market share from their competitors every day.
• Vibrant international team with a fun, friendly and open start-up company culture We regret to inform that only shortlisted candidates will be notified / contacted.

For more information you can email Krystal in our Singapore office on krystal.fernandez@teksystems.com quoting Job Reference KrystalFernandez531475 or alternatively, apply here to register your interest.

http://jobs.en-sg.teksystems.com/fQHPOu/data-engineer-apac-itcommunications-singapore-singapore-15264412

Job Reference: Krystal Fernandez 531475

EA Registration No.: R21103744, Krystal Anna Fernandez

Allegis Group Singapore Pte Ltd, Company Reg No. 200909448N, EA License No. 10C4544"
29-Apr-2022 T11:56,Staff Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,16 hours ago,$8K–$13K a month,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 5 years of experience in Data Engineering role and have good knowledge / working experience in:

• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.

• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.

• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.

• Functional programming languages, e.g. Scala.

• Virtualization and container environment such as Docker and Kubernetes.

•Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:56,Data Engineer,Tech Mahindra Limited,3 days ago,$60K–$120K a year,Full–time,"What to expect:
• Manage & develop data warehouse/data lake and pipeline plans.
• Develop and uphold best practices with respect to documentation & data protocols
• Design, build and launch new data models in production.
• To also develop new data extraction, transformation and loading processes in production.
• Work in a cross-functional team, Interfacing with engineers, product managers and product designers to understand data needs to define and review technical specifications.
• Build machine learning/reasoning models at scale.

How To Succeed:
• Degree in Computer Science, Information Systems, Computer Engineering, Mathematics, or related disciplines.
• At least 3 years of experience in data engineering, with experience in Kotlin/Scala/ Python, and experiences in Amazon Web Services stack, such as ECS, Kinesis, EMR, and DynamoDB.
• Experienced in building production-grade models using machine learning frameworks such as Tensorflow, Scikit-learn, PySpark and/or others is required.
• Experienced in custom ETL & ELT & Streaming big data design, implementation & maintenance and data warehouse/data lake space.
• Experienced/willing to use Infrastructure-as-code for deploying pipelines.
• Hands-on and deep experience with schema design and dimensional data modelling.
• Ability to write efficient SQL statements.
• Ability to analyse data to identify deliverables, gaps and inconsistencies.
• Excellent communication skills including the ability to identify and communicate data-driven insights.
• Experience in using Hadoop, Spark, HBase, Hive and Pig, is a plus.
• Experience in software engineering, with experience in Kotlin or Java, and Spring Boot is a plus.

What to expect: * Manage & develop data warehouse/data lake and pipeline plans. * Develop and uphold best practices with respect to documentation & data protocols * Design, build and launch n

Skills: Excellent Communication Skills, Machine Learning, Pyspark, Big Data, Pipelines, Kotlin, Hadoop, Software Engineering, Etl, Dynamodb, data engineering , Emr, Sql, Python, Java

Experience: 2.00-5.00 Years"
29-Apr-2022 T11:56,Senior Data Engineer,Singtel Group,27 days ago,,Full–time,"Jobscope
• Develop big data solutions for near real-time stream processing, as well as batch processing on the Big Data platform
• Work with business domain experts, data scientists, and solution designers to identify data relevant for analysis and develop Data solutions
• Fine-tuning of new and existing data pipelines
• Schedule and maintain data pipelines
• Drive optimization, testing and tooling to improve data quality
• Assemble large, complex data sets that meet functional / non-functional business requirements
• Develop APIs to support high throughput data processing, feature engineering, and use cases
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc
• Review and approve high level & detailed design to ensure that the solution delivers to the business needs and align to the data & analytics architecture principles and roadmap
• Understand various data security standards and use secure data security tools to apply and adhere to the required data controls for user access
• Support and contribute to development guidelines and standards for data ingestion
• Adapt and learn new technologies surrounding Data Platform

The Ideal Candidate should possess the following:
• Bachelor’s Degree in IT, Computer Science, Software Engineering, Business Analytics or equivalent.
• Minimum of 6 years of experience with highly distributed, scalable, concurrent and low latency systems working with one
• or more of the following technologies:
• Hadoop distributions Spark
• NoSQL data warehouses such as HBase, Cassandra
• Excellent software & data engineering principles and design patterns
• Proficient in creating and maintaining complex data pipelines end-to-end while maintaining high reliability and security
• Excellent hands-on experience in Scala or Python
• Excellent hands-on experience with SQL and Spark
• Experience with Kafka
• Experience with CI/CD tools and environment
• Experience migrating from on-premise data stores to cloud solutions
• Good working experience with one or more major cloud vendors (ie: Azure,AWS, GCP)
• Experience working in Telco Data Warehouse and / or Data Lake advantageous
• Highly organized, selfmotivated, pro-active, and able to plan
• Ability to analyze and understand complex problems
• Ability to explain technical information in business terms
• Ability to communicate clearly and effectively, both verbally and in writing
• Strong in User Requirements Gathering, Maintenance and Support
• Good experience managing users and vendors
• Experience with Agile Methodology

We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated"
29-Apr-2022 T11:56,Data Engineer,Endowus,5 days ago,,Full–time,"About us
Endowus is Asia's leading fee-only digital wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping all investors grow their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.
Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones. Our mission is clear: help people invest better so they can live easier today, and better tomorrow.

The people you will work with
The team has deep domain knowledge in finance and technology, bringing together decades of experience at Goldman Sachs, Morgan Stanley, UBS, Credit Suisse, The Blackstone Group, AQR, Grab, Dropbox, Lyft, Redmart, Carousell, Bytedance, Grab, Kakao, Alibaba, and more See our leadership team here . We practise inclusion and treasure our diversity in background and experience. A diverse team is our biggest asset and we look for people who share our belief in Endowus' clear mission.

Investors, recognition, licensing
Endowus is backed by global leading strategic and venture capital investors including UBS, Samsung Ventures, EDBI, Prosus Ventures, ZVC, Singtel Innov8, Lightspeed Venture Partners, and SoftBank Ventures Asia.

Endowus has been recognised by the industry with the following awards: Singapore's Rising Star and Fintech Innovation (Asia Asset Management's Best of the Best Awards 2021), LinkedIn Top Start-ups 2021, WealthTech of the Year (Asia FinTech Awards 2021), and the top 15 Singapore's Best Workplaces Award (Great Place to Work).
Endowus is licensed by the Monetary Authority of Singapore (MAS).

About this role; responsibilities & ownership
• We are looking for a Data Engineer who thrives in a fast-paced environment and enjoys driving innovation through rapid prototyping and iterative development.
• Being part of Endowus's Technology team, you will build end-to-end product features that you are confident of delighting user experiences.
• You will need to leverage the entire technology stack to realise these goals and will be part of a team that is constantly tackling difficult questions of scale, architecture, and interaction. Requirements & qualifications

• Bachelors' or above in Computer Science, a related field, or equivalent professional experience
• 3-6 years of experience in designing and implementing key components for highly scalable, distributed data collection and analysis systems built for handling large volumes of data in cloud
• Advanced working SQL knowledge and experience working with relational databases or BigQuery, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimising data pipelines and data sets in Apache Flink or Apache Spark to answer specific business questions.
• Experience in at least one of the JVM languages such as Java, Scala.
• Experience Build processes supporting data transformation, data structures, metadata, dependency and workload management. (experience in Airflow is Plus)
• Experience with distributed databases, such as Cassandra, and the key issues affecting their performance and reliability.
• Hands-on development with key technologies including Scala, Apache Flink and other relevant distributed computing languages, frameworks, and libraries
• Independence and self-reliance while being a proactive team player with excellent communication skills.
• Self-driven, has a strong sense of ownership and able to work and learn independently
• Comfortable with numbers and motivated by steep learning curves Nice to haves
• Experience with message broker systems, such as Apache Kafka.
• Developed mission-critical products, e.g. trading platforms, medical applications
• Basic knowledge of finance and trading
• Preferably some exposure to CQRS / Event Sourcing patterns
• Familiarity with Docker, Kubernetes and Terraform Remote Okay
• We are open to hiring remotely in Asia time zones. Benefits & perks
• Stock options in a fast growing company *
• Employee discount for investing at Endowus
• Flexible working hours and locations so you can live life and your best work - you are trusted to be responsible
• Competitive Staff Benefits; Annual Leave, Medical insurance, Dental *
• Productivity Equipment; Omnidesk Standing Desk & Ergonomic Chairs, Dell UltraSharp USB Type-C Monitor, high-end computer peripherals
• Choice of Apple or PC equipment
• Additional S$250 /HKD 1450 budget for your favourite hardware to boost productivity * Note: * is only applicable to Full-Time employees"
29-Apr-2022 T11:56,(Senior) Data Engineer (APAC),foodpanda Singapore,20 mins,,Full–time,"foodpanda is the largest food and grocery delivery platform in Asia, outside of China. Operating in more than 400 cities across 12 markets, foodpanda continues to expand and grow in its core food delivery business as well as in new verticals like grocery deliveries, with a strong tech infrastructure at its core. From our restaurants-partners, cloud kitchens and cloud grocery stores — foodpanda is just one tap away, delivering everything you need quickly and conveniently to your doorstep.

We are looking for a (Senior) Data Engineer to join our growing data team to help Foodpanda make smart, data-driven business decisions. If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!

What's on your plate:
• Defines and builds data pipelines that will enable faster, better, data-informed decision-making within the business
• Architect, build, and deploy new data models that provide intuitive analytics across the business
• Manage and own the entire data processing system (internally, as well as interfacing with other teams or third party services), provide advice on any necessary infrastructure changes
• Experiment with, select, and implement cutting edge Big Data tools and frameworks required to provide requested capabilities
• Perform all needed data transformation to populate a reporting optimised data warehouse
• Mentoring junior team members through code review and enablement training
• Participate in building machine learning models, deliver analyses and insights to support business decision making

What you bring to the table:
• More than 3 years experience in data analytics / engineering
• Ability to write clean, structured, and high performance SQL and Python code
• Strong experience with big data, Data Warehouse technologies
• Strong oral and written communication skills
• Strong business mindset and ability to grasp business requirements from stakeholders
• Knowledge of Machine Learning is a plus
• Experience in machine learning operations, or setting up environments for data scientists/machine learning engineers would be advantageous

What we can offer you:
• A vibrant and international team with multi-cultural and diverse backgrounds.
• Solving challenges with inspiring colleagues in an all hands-on deck environment.
• Management team that recognizes top performers, welcome our newbies, and shares a love for good food.
• Competitive package, incentives, allowances, food perks, insurance, pension and more"
29-Apr-2022 T11:56,Cloud Data Engineer for IT Data Analytics Team,Garranto Pte. Ltd.,2 days ago,,Full–time,"Type : Full Time / Permanent Role

Location : Singapore

Job Description ï'·
• Act as a subject matter expert in data engineering and GCP data technologies. ï'·
• Closely work with various business teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
• Work with Agile and DevOps techniques and implementation approaches in the delivery. ï'·
• Be required to showcase your GCP Data engineering experience when communicating with business team on their requirements, turning these into technical data solutions. ï'·
• Be required to build and deliver Data solutions using GCP products and offerings.

Qualifications
• Liaise and be part of our extensive GCP community, contributing in the knowledge exchange learning programme of the platform.
• Any Bachelor Degree in Computer Science or related fields
• Minimum 5 years of experience as a data engineer in banking environment.
• Possess analytical skills mental resilience and the ability to think systematically under stressful conditions.
• Highly accountable and takes ownership. Outstanding work ethic, highintegrity, team player, and a lifelong learner.
• Mentor other engineers define our technical culture and help build a fast-growing team.

Skill
• Hands on and deep experience working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, AI Building Blocks, Looker, Cloud Data Fusion, Dataprep, etc.).
• Experience in Spark / Scala / Python / Java / Kafka.
• Experience in MDM, Metadata Management, Data Quality and Data Lineage tools.
• E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.
• Regulatory and Compliance work in Data Management.
• E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
• Experience with SQL and NoSQL modern data stores.
• Work on diversified cloud Platforms consisting of Databases and CICD, Logging, and monitoring tools to provide stable, and reliable DevOps service.
• Hands-on experience with terraform is a plus.
• Build CI / CD pipeline; both design and implementation is an added advantage.

If you are interested please send your CV to HIDDEN TEXT"
29-Apr-2022 T11:56,Data Engineer,TreeDots,3 days ago,,Full–time,"Job description

& requirements

Job Description

TreeDots is currently hiring for a Data Engineer with strong experience in building and maintaining data pipelines.

At TreeDots we drive 3 products, respectively related to social commerce, logistics, and b2b e-commerce. Our vision is to ensure a holistic delivery of agrifood from source to merchants and end-consumers. We believe that environmental issues coming from food waste are possible to solve and those are the main goals of our products.

Duties and Responsibilities

You will be responsible for collecting, managing, and converting raw data into usable information for stakeholders to interpret. You will be responsible for creating the data flows between the data source to the data warehouse and finally to use visualization tools to display prepared data-sets.

Mandatory requirements

● 2-4+ years of experience as a Data Engineer
● Experience with building optimized SQL queries
● Experience with Google Cloud Platform, especially BigQuery and Google Data Studio
● Understanding of the SDLC best practices
● Familiar with API queries and at least 1 programming language
● Familiar with git and Github

Required skills

Git Github SQL"
29-Apr-2022 T11:56,"Data Engineer (5 days, Orchard, $3500-4000)",MACHSPEED HUMAN RESOURCES PTE. LTD.,9 hours ago,,Full–time,"Leading System Integrator company providing IT solutions for network, IT infrastructure, analytics, cyber security and managed services.

Job Responsibilities
• Design, engineer, configure and administer BI project based on given functional and technical requirements
• Collaborate with pre-sales, project and relevant internal teams to deliver the optimal technical solution to client's business problems
• Develop real-time and batch data ingesting and processing pipelines to be used for analysis, machine learning, dashboards, alerts and visualizations
• Review and monitor ETL tasks and performance
• Support testing and deployment
• Provide recommendations and implementation changes to optimize in the customer environment.
• Write and develop custom scripts as needed

Requirements
• Minimum Degree/Diploma in Computer Science, Engineering or IT with at least 2 years of relevant experience
• Hands-on experience in scripting/programming
• Possess knowledge in Networking and Servers(Windows and Linux)
• Possess CEH/ECSA/CISSP/ECSA/CompTIA Security+ certification will be an added advantage
• Working hours: 9am - 6pm, Mondays to Fridays
• Salary range: $3500 - 4000

We thank you for your interest in this position, applicants with relevant experience please send Microsoft word resume with expected salary to

apply@machspeed.com.sg

for faster processing, alternatively, please visit https://www.facebook.com/MachspeedHR for more jobs available, thank you.

You may also call +6563362530 (Look for BingCheng) to find out more

Thank you very much.

Agency License No. 12C6200

EA Registration No: R1437671"
29-Apr-2022 T11:56,Senior Data Engineer,Kkr Singapore Pte. Ltd.,3 days ago,,Full–time,"Position Summary We are looking for a savvy Data Engineer to be based in Hong Kong / Singapore, to join our growing team of data and analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced cloud-based services, data architecture, data engineering, data pipeline development (ETL), and analytical tools. We are seeking an engineer who enjoys optimizing data systems and building enterprise scale data services. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives. Skill / Experience Required Bachelor's Degree in Computer Science/Engineering or a related discipline. 10+ years Development experience Experience in Python and related open source modules Experience in Python development including Web application frameworks such as Flask / FAST API Experience working with RESTful API Services Strong database skills with a thorough understanding of relational database and understanding of Object Oriented databases Exposure with the AWS Stack / RDS / is preferred Knowledge of open source solutions and trending technologies Good communication and written skills Ability to be self-sufficient and proactive individual contributor Exposure to Private/Public Markets Desirable Understanding of Object Oriented Programming and Design Patterns Knowledge of web standards, security, accessibility, browser compatibility Knowledge of JavaScript, HTML5 and awareness of frameworks such as React.js/Vue.js Experience in a Business Intelligence tool e.g. Tableau and Dremio Exposure to ML / AI and python libraries (pandas, NumPy, SciPy) and concepts"
29-Apr-2022 T11:56,Senior Data Engineer (Data Engineering),GVT Government Technology Agency (GovTech),2 days ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Senior Data Engineer in the Data Engineering team of GovTech’s Data Science and Artificial Intelligence Division, you will be building Whole-of-Government data infrastructure to power the insights needed for evidence-based decision-making and enhancing agencies’ service-delivery. You will be architecting, designing and building next-generation data infrastructure to galvanise digitalisation in the public sector. You will be given opportunities to lead other engineers to drive impact at scale.

We are looking for enthusiastic and passionate engineers to join us in this journey to make a difference.

What you will be working on:
• Design and build resilient and efficient data pipelines for both batch and real-time streaming data
• Architect and design data infrastructure on cloud using industry standard Infrastructure-as-Code tools
• Execute projects with an Agile mindset
• Build software frameworks to solve data problems at scale
• Collaborate with product managers, software engineers, data analysts and data scientists to build scalable and data-driven platforms and tools
• Be put in the driving seat as an engineering leader

What we are looking for:
• Bachelor’s Degree in Computer Science or have equivalent professional experience
• Have more than 4 years of experience in a technical role
• Experience with data processing tools such as Spark, Beam, Flink
• Experience with the cloud (e.g. AWS, GCP, Azure)
• Experience implementing batch and streaming data pipelines
• Experience writing efficient SQL
• In-depth knowledge of both SQL and NoSQL databases, including performance tuning and troubleshooting
• Familiar with DevOps tools such as Git, Docker, Terraform
• Experience in the public sector is a bonus
• Previous technical leadership experience is a bonus

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round.
• Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:56,Senior Data Engineer,GENPACT CONSULTING (SINGAPORE) PTE. LTD.,2 days ago,$8K–$10K a month,Full–time,"• Work closely with the Product Owners and stake holders to design and build data systems and pipelines to meet the requirements of the proposed solution.

• Play an active role in leading team meetings and workshops with clients.

• Oversee day-to-day Data Engineering team’s operation and performance.

• Interact directly with clients to understand project requirements and deadlines.

• Evaluate business needs and objectives.

• Designing and implementing highly performant data ingestion pipelines from multiple sources using SQL, Python, Apache Spark and Databricks.

• Responsible for deploying codes using Git/Bitbucket as per the CI/CD process.

• Design and Build datasets on Snowflake for faster reporting.

• Design Develop Maintain ETL data pipelines.

• Analyze and organize raw data.

• Research, Diagnose, and Monitor Performance Bottlenecks, etc.

• Ensure standardization of SQL coding practices and adherence to coding standards, change control, and SQL best practices

• Prepare data for prescriptive and predictive modelling.

• Responsible for data integration and data quality

• Enabling data from different sources into ready to consume datasets.

• Performing Data Validation/Exploration tasks.

• Migrating current lake data/ external data into Exasol/Databricks (Lakehouse)

• Helping business stakeholders to visualize and analyze the customized view/data and enable better decisions without any hassle.

• Partnering and coordinating with cross functional stakeholders across timezones"
29-Apr-2022 T11:56,Data Engineering (Internship),Endowus,2 days ago,,Full–time,"About us
Endowus is Asia's leading fee-only digital wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping all investors grow their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.
Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones. Our mission is clear: help people invest better so they can live easier today, and better tomorrow.

The people you will work with
The team has deep domain knowledge in finance and technology, bringing together decades of experience at Goldman Sachs, Morgan Stanley, UBS, Credit Suisse, The Blackstone Group, AQR, Grab, Dropbox, Lyft, Redmart, Carousell, Bytedance, Grab, Kakao, Alibaba, and more See our leadership team here . We practise inclusion and treasure our diversity in background and experience. A diverse team is our biggest asset and we look for people who share our belief in Endowus' clear mission.

Investors, recognition, licensing
Endowus is backed by global leading strategic and venture capital investors including UBS, Samsung Ventures, EDBI, Prosus Ventures, ZVC, Singtel Innov8, Lightspeed Venture Partners, and SoftBank Ventures Asia.

Endowus has been recognised by the industry with the following awards: Singapore's Rising Star and Fintech Innovation (Asia Asset Management's Best of the Best Awards 2021), LinkedIn Top Start-ups 2021, WealthTech of the Year (Asia FinTech Awards 2021), and the top 15 Singapore's Best Workplaces Award (Great Place to Work).
Endowus is licensed by the Monetary Authority of Singapore (MAS).

About Data Engineering Intern Role
• We are looking for a Data Engineering Intern who thrives in a fast-paced environment and enjoys driving innovation through rapid prototyping and iterative development.
• Being part of Endowus's Data Engineering team, you will shadow a senior colleague and support in building end-to-end product features that the team is working on.
• You will need to leverage the entire technology stack to realise these goals and will be part of a team that is constantly tackling difficult questions of scale, architecture, and interaction.
• You will experience real-life day to day operations as a valued member of the team Requirements:
• Pursuing Bachelor Degree in Computer Science, a related field, or equivalent professional experience
• Able to commit to a 10 weeks summer internship.
• Able to work independently while being a proactive team player with excellent communication skills.
• Some fluency with hands-on software development and strong understanding of SDLC activities.
• Ability to analyse data (e.g. using SQL or Excel)
• Knowledge of CI/CD practises is a plus.
• Knowledge in at least one of the JVM languages such as Java, Scala is a plus
• Motivated, positive attitude, responsible and proactive.
• Passionate in learning and working with new technologies. Nice to haves
• Basic knowledge of finance and trading Benefits & perks
• Stock options in a fast growing company *
• Employee discount for investing at Endowus
• Flexible working hours and locations so you can live life and your best work - you are trusted to be responsible
• Competitive Staff Benefits; Annual Leave, Medical insurance, Dental *
• Productivity Equipment; Omnidesk Standing Desk & Ergonomic Chairs, Dell UltraSharp USB Type-C Monitor, high-end computer peripherals
• Choice of Apple or PC equipment
• Additional S$250 /HKD 1450 budget for your favourite hardware to boost productivity * Note: * is only applicable to Full-Time employees"
29-Apr-2022 T11:56,Senior Data Engineer,Selby Jennings,23 days ago,,Full–time,"A global multi-manager platform with pods of internal and partner portfolio managers, are searching for a Senior Data Engineer to join their Singapore office. This firm work across quantitative, fundamental equity and tactical trading strategies with over 30+ years of experience globally. They focus on investing in their own proprietary technologies so are able to be compete to be one of the best in their field.

The Senior Data Engineer will be responsible for...
• Maintaining, enhancing and optimising the data platform
• Creating new data processing pipelines
• Improving data processes including integrating new data formats, analytical tools etc
• Collaborate with Portfolio Managers, Data Scientists and Data Ops for data sets

The Senior Data Engineer should have the following...
• Strong knowledge and experience with python scripting
• Experience with Financial Data and Big Data
• Experience building, deploying and running on cloud (AWS Preferred)
• Mentoring junior engineers

For further information about this position please apply for this Senior Data Engineer in Singapore"
29-Apr-2022 T11:56,Data Engineer – SQL,AiDA Technologies,Full–time,,,"[vc_row css_animation="""" row_type=""row"" use_row_as_full_screen_section=""no"" type=""full_width"" angled_section=""no"" text_align=""left"" background_image_as_pattern=""without_pattern""][vc_column][vc_column_text]
Job Title: Data Engineer

Industry: Computer Software, Financial Services, Information Technology & Services
Employment Type: Full-time
Job Location: Singapore
Seniority Level: Entry level
Job Functions: Information Technology

Job summary

We are looking for a Data Engineer who will work on the data transformation and system deployment. The primary focus will be on understanding data science solutions of a particular problem, implement data transformations required, as web APIs . You will also work with a team of highly motivated members and integrate the solution in client environments.

Responsibilities and duties
• Writing SQL queries
• Linking different tables into one table based on a provided schema
• Implementing data transformation (including ETL) for machine learning modelling
• System integration and testing

Qualifications and skills (requirement)
• Diploma, B.Eng or BS
• Hand-on experience working with SQL-based technologies on traditional RDBMS, such as Oracle, Postgresql or MySQL
• Proficiency in Python programming or Java

Qualifications and skills (good to have)
• Database System Design
• Basic Knowledge on Machine Learning
• Experience with AWS services including S3, Redshift, EMR and RDS.
• DevOps tools such as Docker
• UI development

Benefits and perks
• An exciting learning opportunity at a fast paced startup with open and friendly colleagues
• Competitive salary based on skills & experience with excellent insurance and other benefits.
• Opportunity to work with best data scientists in the world,including Kaggle Grand Master
• Deliver high impact solutions for prestigious clients across ASEAN, India, and Hong Kong.

[/vc_column_text][/vc_column][/vc_row"
29-Apr-2022 T11:56,Data Engineer,Persolkelly Singapore Pte. Ltd.,3 days ago,,Full–time,"Responsibilities
• Design and implement relevant data models in the form of data marts stored in Operational Data Stores, Data Warehouses or Big Data platforms
• Build data pipelines to bring information from source systems, harmonise and cleanse data to support analytics initiatives for core business metrics and performance trends.
• Perform data profiling to understand data quality and advise practical measures to address such data issues through data transformation and data loading
• Dive into company data to identify sources and features that will drive business objectives.
• Work closely with project manager and technical leads to provide regular status reporting and support them to refine issues / problem statements and propose / evaluate relevant analytics solutions

Requirements :
• Prior experience building large scale enterprise data pipelines using commercial and / or open source data management tools from vendors such as Informatica, Talend, Microsoft, IBM or Oracle
• Strong knowledge of data manipulation languages such as SQL necessary to build and maintain complex queries and data pipelines
• Practical appreciation of data quality metrics and remediation strategies
• Data modelling and architecting skills including strong foundation in data warehousing concepts, data normalisation, and dimensional data modelling such as OLAP
• Undergraduate or graduate degree in Computer science or equivalent

To Apply :

Interested candidates, who wish to apply for the above position; please send in your resume to HIDDEN TEXT or click the ' Apply Now ' below and ' ATTN : BVIN '

We regret that only shortlisted applicants would be notified.

B Vidita Nantini REG No : R22105644

PERSOLKELLY SINGAPORE PTE LTD EA License No : 01C4394

By sending us your personal data and curriculum vitae (CV), you are deemed to consent to PERSOLKELLY Singapore Pte Ltd and its affiliates collecting, using and disclosing my personal data for the purposes set out in the Privacy Policy which is available at www.

persolkelly.com.sg I also acknowledge that I have read, understood, and agree to the said Privacy Policy"
29-Apr-2022 T11:56,Data Engineer,Huxley,1 day ago,,Full–time,"Huxley is currently working with a digital assets company that is rapidly expanding in the region. This is an exciting opportunity for highly motivated individuals keen to work in a busy environment and contribute with ideas.

We are currently looking to fill 2 Data Engineer positions focusing on data modelling and risk related projects:

Data Engineer should have the following:
• 3+ year of experience developing machine models
• Proficient in either Python, Java and/or Scala
• Master's Degree with working experience OR Phd
• Understanding of deep learning is a plus

Data Engineer will be responsible for:
• Building Models reliable on machine learning, graph mining and other data driven technologies. Identify potential threats in KYC, Payments, Credit, Reward etc
• Maintain the Models to meet business expectations.
• Work on data that analyses customer feedback and satisfaction analysis by leveraging on machine learning techniques.
• Collaborate effectively with various stakeholders

Please note that this opportunity is open for Singaporeans only.

If you are interested in the above-mentioned Data Engineer role located in Singapore, do contact me and attach your latest CV. I will be glad to provide additional information and guide you through the next steps where necessary.

Consultant Details:

Anastasija Horoscuka

EA License No.: R2093146

Huxley is a trading division of SThree Pte Limited

(Registration Number: 200720126E | SThree Pte Limited Licence Number 16S8216 | Huxley Licence Number 53132076J)

As per Ministry of Manpower (MoM) requirements, if you're suitable for any roles that we will be putting you forward for, we will need to request for your identification number. Please be assured that this will not be disclosed with our client or any other parties other than MoM.

Award winner of:

Recruitment Agency of the Year by Asia Recruitment Awards 2019 | Best Client Services by Asia Recruitment Awards 2017 | Best Overseas Operation by Global Recruiters 2017 | Highly Commended for Best Large Recruitment Business 2017 | Commended for Best In-House Training by Global Recruiters 2017"
29-Apr-2022 T11:56,"Data Engineer, MBG",Meta,2 days ago,,Full–time,"Meta is looking for exceptionally talented and experienced engineers to join the MBG Technology team. Our team provides analytics and workflow tools for Meta Business Group ( MBG), partnering with sales, marketing, measurement, support and operations teams.In this role, you’ll work with some of the brightest minds in the industry, work with one of the richest data sets in the world, use cutting edge technology, and get an opportunity to solve some of the most challenging business and engineering problems, at a scale that few companies can match. You will do so by partnering with stakeholders/teams and building scalable, reliable solutions that provide business critical insights and metrics, while ensuring the best uptime and responsiveness.

Data Engineer, MBG Responsibilities:
• Manage data warehouse plans for a business vertical or a group of business verticals
• Build data expertise and own data quality for allocated areas of ownership
• Design, build, optimize, launch and support new and existing data models and analytical solutions
• Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions
• Conduct design and code reviews
• Work with data infrastructure to triage infra issues and drive to resolution
• Manage the delivery of high impact dashboards, tools and data visualizations

Minimum Qualifications:
• 2+ years of experience in the data warehouse space, custom ETL design, implementation and maintenance
• 2+ years of experience in SQL or similar languages, and development experience in at least one language (Python, PHP etc.)
• Experience with data architecture, data modeling, schema design and software development
• Experience in leading data driven projects from definition through interpretation and execution
• Experience with large data sets, Hadoop, and data visualization tools
• Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience working in support of diverse communities"
29-Apr-2022 T11:56,Data Engineer,KS CONSULTING PTE. LTD.,24 days ago,$5.5K–$8K a month,Full–time,"Our client, a leading financial services company is looking to hire a Data Engineer with excellent communication skills and experience in data modelling and optimizing ETL jobs.

Responsibilities:
• Manage data modeling design, writing, and optimizing ETL jobs
• Participate in building and enhancing enterprise cloud data warehouse
• Deliver and manage in-house and cloud-native data solutions to meet business requirements across firm-wide business units
• Design and implement Data and AI central data platform as well as related tools/systems for advanced business analytics and enterprise data governance
• Assist in creating and monitoring analytics dashboards, for different business functions
• Ensure quality, integrity and accuracy of datasets through tracked, secured and auditable controls

Requirements:
• Hands-on experience with Linux and shell scripting
• Working experience with containerization (Docker/K8S) and task orchestration tools (Airflow/Luigi, etc.)
• Experience with cloud service and tools (AWS/Azure/GCP), as well as cloud data warehouse platforms.
• Experience with modern DevOps practices including version control, TDD, CI/CD, etc., for both code and configuration changes.
• Excellent coding skills with Python and SQL, and solid understanding of object-oriented analysis and design
• Working knowledge of common algorithms and data structures, with strong analytical and problem-solving skills
• Basic understanding and experience with ML/AI concepts (e.g. deep learning, deep reinforcement learning, deep bayesian learning), workflows, and toolsets (Jupyter Notebook, etc.), and libraries (Numpy, Pandas, Scikit Learn, PyTorch, etc.) preferably in both cloud-native and desktop deployments.
• Experience with traditional RDBMS based systems, including Data Lake, Data Warehouses and Marts, and more modern NoSQL and cloud-native big-data

If you or anyone within your network is keen to discuss it further then please share your resue with manisha@kstalentsolutions.com"
29-Apr-2022 T11:56,"Manager, Product Development (Data Engineer)",MasterCard,Full–time,,,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion (https://www.mastercard.us/en-us/vision/who-we-are/diversity-inclusion.html) for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Manager, Product Development (Data Engineer)

Scope

Create and maintain optimal data pipeline architecture

Participate in development of data and analytic infrastructure for product development

Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.

Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.

Keep up to date with latest open source tools for data engineering

Expert knowledge in ML lifecycle

Qualifications

At least 4 years of experience as data engineer, building and deploying data pipeline

High proficiency in using Python/Scala, Spark(tuning jobs), Hadoop platforms to build Big Data products & platforms

Experience with data pipeline and workflow management tools: NIFI, Airflow.

Experienced with Agile frameworks, e.g., Scrum.

Experience with open source tools to build data pipeline, deploying and monitoring the algorithm

Comfortable in developing shell scripts for automation

Comfortable in creating CI/CD pipeline for testing, deploying and monitoring algorithm

Motivation, flexibility, self-direction, and desire to thrive on small project teams

Experience with the visualization tools like tableau, looker

COVID-19 Considerations

We value the safety of each member of our community because we know we’re all in this together. In many locations, which may change over time, we’ve implemented a virtual hiring process and continue to interview candidates by video or phone. In addition, in some locations, only individuals who have been fully vaccinated will be permitted inside Mastercard offices until further notice.

In the US, Mastercard is a government contractor, which may legally require most Mastercard employees to be vaccinated unless a verified approved medical or religious exemption is granted. Further, we are currently making every effort towards having employees return to work in the office 2 days per week, if that makes sense for their team. Everyone must be vaccinated to enter Mastercard offices at this time. Therefore, we expect all candidates to be vaccinated or to be approved for a medical or religious accommodation prior to commencing work at Mastercard.

Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.

Requisition ID: R-155294"
29-Apr-2022 T11:56,Data Engineer -Python ( 1 year extendable contract),Michael Page,$60K–$78K a year,,Contractor,"About Our Client

Our client is a renowned name in the Banking industry and is currently seeking for data engineers to design,develop and implement data applications

Job Description
• Solid hands on experience in writing Python scripts for data processing and analytics use cases.
• Should have experience on building batch and streaming data pipelines.
• Extensive experience in implementing scalable Data pipeline using Python or R/Scala, SQL.
• Exposure to core banking system, good understanding of Private banking product's life cycle
• Participate in reviews and meetings and provide updates on project progress
• Take responsibility for ensuring that risks and issues are identified and managed closely and drive all stakeholders to deliver on time and to the required quality standards
• Ensure production stability and timely communication to stake holders

The Successful Applicant
• Minimum 3 years of experience in Designing and developing Data applications using python and other programming languages.
• Bachelor's degree in information technology or computer science;
• Experience building APIs using python libraries.
• Experience working with structured and unstructured data, SQL scripting, Kafka streaming.
• Good to have an understanding of Kafka messaging system.
• Experience working in a Private Banking environment is preferred.
• Experience working on building data pipelines for Data and analytics aplications or a Data Warehouse system

What's on Offer

You will be part of an organisation who sees value in investing in their employees. Stability in your career is a key for them.The remuneration for this role will be competitive and in line with the market"
29-Apr-2022 T11:56,Big Data Engineer,Salt,22 days ago,$8.5K–$9.5K a month,Contractor,"SALT is hiring Big Data Engineer for a global technology client on a two-year convertable contract role.

You will be responsible for
• Responsible for the monitoring and uptime of all production and Non-Production Big Data Stack.
• Strive to improve the stability, security, efficiency, scalability, and availability of production systems by applying software engineering practices and by implementing monitors and alerts.
• Estimate Bigdata stack capacities; develop methods for monitoring capacity and usage.
• Lead efforts to develop and improve procedures for automated monitoring and proactive intervention, reducing any need downtime.
• Provide and drive architecture and the design of Big Data solutions
• Design and implement high performance systems supporting large volume real time and batched data processes
• Propose and design data architecture solutions for scalability, high availability, fault tolerance, and elasticity for various application needs.
• Develop ETL pipelines with robust monitoring and alarming
• Develop data models that are optimized for business usability and understanding
• Develop and optimize data tables using best practices for partitioning, compression, parallelization, etc.
• Optimize SQL and ETL solutions to solve various reporting requirements.

You should be coming from
• Overall 8+ years of relevant industry experience
• Good knowledge of big data technology landscape and concepts related to distributed storage/computing.
• Experience with big data frameworks (e.g., Hadoop, Spark) and distributions (Cloudera, Hortonworks, MapR).
• Experience with batch & ETL jobs to ingest and process data from multiple data sources.
• Experience with NoSQL databases (e.g., Cassandra, MongoDB, Neo4J, Elasticsearch, InfluxDB)
• Experience with querying tools (e.g., Hive, Spark SQL, Impala).
• Experience in real-time stream processing, using solutions such as Kafka, AWS Kinesis, Flume, and/or Spark Streaming.
• Experience in DevOps and DataOps principles (e.g., Infrastructure as code, automating different parts of the data pipeline).
• Monitor and evaluate Big Data stack performance and adjust configuration as appropriate
• Participate in disaster recovery design, testing and execution

If this sounds like your ideal next move, I want to talk to you! Reach out to me at jvenkataraman@welovesalt.com or apply via the posting.

CEI No: R1659595 / Licence No: 07C3147

Salt is acting as an Employment Business in relation to this vacancy.

Job Information
Job Reference: JO-2204-253832
Salary: Negotiable
Salary per: month
Job Duration:
Job Start Date: 01/06/2022
Job Industries: Technology
Job Locations: Singapore
Job Types: Contract

Apply for this Job

Name *

Please enter your full name.

Email *

Enter a valid email address.

Upload a CV *

Upload your CV to accompany your application for this job.

Please tick this box to consent to us using your data. How we use your data is outlined in our privacy policy *

Fields marked with * are required"
29-Apr-2022 T11:56,Azure Data Engineer,Avanade,18 mins,,Full–time,"As a Data Engineering Consultant at Avanade, you will use modern data engineering techniques to build Azure based data pipelines and platforms to support Avanade's customers in making better business decisions.

In this role, you will be engaged in the Data Engineering role family inside the Analytics Talent Community. You will design and build data pipelines, data platform, dashboarding and reporting solutions, data consumption APIs and a variety of other solutions and tools for end users. You will be a critical point within the data supply chain, while ensuring that all stakeholders have access to and can work with the latest data.

In line with an always increasing market demand, Avanade is looking to expand its Analytics team with experienced Data Engineer (but not only, see other job postings) to work on state-of-the-art data platform on the Azure cloud.

""In addition to the opportunity to work on great projects with the latest technologies, Avanade is distinguished above all by its informal culture and a strong us-feeling. Everyone is willing to help each other, is approachable and frequently there are evenings or days organized for training, knowledge sharing or team bonding. With a good mix of working hard on fun and challenging projects and attending fun events organized for colleagues, Avanade is a Top Employer for a reason."" - Robin - Data Engineer at Avanade.

You make the difference because:
• You like to work with the latest tools & techniques in the field of data to help the largest companies in Belgium (or other countries, if you consider it);
• Microsoft Azure Data & Analytics PaaS & SaaS Services such as Databricks (Spark), HD Insight, Data Factory, Data Lake, CosmosDB, SQL and DevOps have no/little secrets for you, or you have a similar technology stack expertise that can be transposed;
• In addition, you have experience with data modeling/warehousing techniques and data integration;
• You also have advanced knowledge of at least one programming languages (Python, Scala or Java);
• CI/CD, VCS, Parallel Processing (MPP) and NoSQL are not abstract concepts to you;
• You have at least 3 years of relevant work experience, or can justify a similar expertise;
• You have a Bachelor’s or Master’s degree in Computer Science, Mathematics, Statistics or another relevant field;
• You speak English, preferably also Dutch or French.

Why should you choose Avanade?
• At Avanade we continuously invest in the development of your skills. We help you to keep up to speed with the latest in technology and business.
• Besides an exciting international environment, a unique chance to participate in challenging and state-of-the-art projects and interaction with inspiring colleagues who have a professional attitude and attach considerable importance to teamwork; we stand for:
• A competitive salary with additional benefits, such as a company car, phone, insurances.
• A very strong training & development focus, with an internal University and ample opportunity to learn and grow.
• A personal career plan, controlled by you and guided by Avanade.

Do you feel the match?

Do you recognize yourself in this description and would you like to get to know us? Then apply immediately by sending in your CV via our website. Applying by e-mail is not possible. If we see a match, we will call you quickly for an introduction and you will have 2 job interviews. Do you have any questions or want to know why we love working at Avanade? Then contact our recruiter Eline Rubertus"
29-Apr-2022 T11:56,Staff Data Engineer,Twitter,3 days ago,,Full–time,"Who We Are :

Twitter users generate many terabytes of data every day; Twitter engineers run hundreds of experiments; Twitter Data Engineers build data pipelines and data processes that calculate metrics and scale increasingly sophisticated models of users and content.

The Data Science team at Twitter is at the intersection of all this data and strives to make it actionable to all business units around Twitter.

Data Engineers work alongside Data Scientists analyze this data via observational analyses, trend analyses, modeling, and new measurement strategies.

We also implement metrics to track the impact of new product experiments and more generally find ways to make very large scale data approachable to guide our decisions.

What Youll Do :

Twitter has very large and complex datasets. As a Twitter Data Engineer you will build datasets and make them accessible to our partner teams by writing great production code to simplify the complexity.

Your work will enable Product Managers and other decision-makers across the company to bring together insights and inform our product and strategy.

In every decision that you influence, you will see the product improve and be more valuable to Twitter users.

We are trying to improve Twitter. To improve something, we need to be able to measure it. As a Data Engineer you will enable better measurements and ensure measurement accuracy so that we know where we are doing well and where we want to improve.

As such, you will :

Design, develop, and launch extremely efficient and reliable data pipelines to move data and to provide intuitive analytics to our partner teams.

Make Twitter-scale data more discoverable and easy to use for Data Scientists and Analysts across the company.

Collaborate with other engineers and Data Scientists to discover the best solutions.

Support your colleagues by reviewing code and designs.

Diagnose and solve issues in our existing data pipelines and envision and build their successors.

Who You Are :

You want to be part of a community of the most talented, forward-thinking Data Scientists and Engineers in the industry.

You are a strong Scala or Java developer. You demonstrate clear and concise communication and data-driven decision-making.

You are passionate about learning or growing your expertise in some or all of the following :

Qualifications

B.S. and / or M.S. in Computer Science or a related technical field, or equivalent experience

8+ years of experience in either data infrastructure or backend systems

Strong understanding of SQL

Broad knowledge of the data infrastructure ecosystem

Experience with Hadoop or other MapReduce-based architectures

Experience working with large data volumes

Good understanding of one or more of the following : Scala, C++, or Java

Experience with any of the following is a plus :

Scalding

Full Stack Development

Company Description

Twitter serves the public conversation because conversation is a force for good in the world. The opportunity to help the world connect, debate, learn, and solve problems is what draws us to careers at Twitter, and its what keeps us here.

Additional Information

We are committed to an inclusive and diverse Twitter. Twitter is an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, age, disability, veteran, genetic information, marital status or any other legally protected status.

San Francisco applicants : Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records"
29-Apr-2022 T11:56,"Senior Data Engineer, Global Business Intelligence",Apple,16 days ago,,Full–time,"Summary
Posted: Apr 14, 2022

Role Number:200366968

At Apple, new ideas have a way of becoming extraordinary products, services and customer experiences. Bring passion and dedication to your job and there's no telling what you could accomplish!

The people here at Apple don’t just create products — they create the kind of wonder that’s revolutionised entire industries. It’s the diversity of those people and their ideas that inspires the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it!

The Global Business Intelligence team provides data services, analytics, reporting and data science solutions to Apple’s business groups including Retail, AppleCare, Operations, Finance, Sales, Marketing, Apple Media Products and external partners. We lead enterprise data stores with petabytes of data, process billions of rows of data in batch and real-time to provide solutions to our users helping them gain insight and to make informed business decisions.

We are looking for a Senior Data Engineer to join our team. You will engage directly with key business partners to understand the business strategies and solution needs, drive and lead technical design discussions with development teams. You should be passionate about data and its nuances, and enjoy getting hands-on with technical solution design and development work. In Apple, we leverage a diverse technology stack such as Teradata, PostgreSQL, Snowflake, SingleStore, HANA, Spark, Cassandra and beyond. Designing, developing and scaling solutions using these technologies are a core part of our daily job.

Key Qualifications

Key Qualifications
• Experience in designing and building dimensional data models to improve accessibility, efficiency and quality of data.
• Hands on database development experience with Relational or MPP/distributed systems such as Snowflake/ Teradata/ SingleStore/ Hadoop
• Programming experience in building end to end data pipelines with SQL, Python and/or Scala
• Proficient in writing Advanced SQL with expertise in performance tuning
• Experience working with data at scale
• Ability to communicate effectively, both written and verbal, with technical and non- technical teams
• Strong understanding of development processes and agile methodologies
• Experience with data science and machine learning tools as well as cloud technologies is a plus

Description

Description
Design and build data structures on platforms such as Snowflake, and highly scalable data pipelines using technologies such Spark and Kafka to provide efficient reporting and analytics capability,

Build data and reporting solutions (in platforms such as SingleStore) that are highly optimised for fast data access

Translate complex business requirements into scalable technical solutions meeting data warehousing design standards. Solid understanding of analytics needs and proactiveness to build generic solutions to improve overall efficiency.

Strong understanding of analytics needs and a proactive approach, focusing on reusable solutions

Lead/work with many global teams, communicate effectively, both written and verbal, with technical and non-technical multi-functional teams.

Keep up-to-date on the newest technology solutions in market to generate innovative ideas to solve business challenges.

We seek a self starter, forward-thinking person with strong leadership capabilities.

Education & Experience

Education & Experience

Additional Requirements

Additional Requirements
• Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.
• We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation"
29-Apr-2022 T11:56,Data Engineer,Raffles Medical Group,1 day ago,,Full–time,"Data Engineer, Raffles Medical Group

Raffles Medical Group (RMG) is a leading integrated regional healthcare provider, listed on the Singapore Stock Exchange (SGX:BSL). Our regional presence spans 14 cities with medical facilities in Singapore, China, Japan, Vietnam and Cambodia.

At RMG, you’ll be part of a team who is focused on delivering high quality healthcare experiences to our broad spectrum of clients. We strive for an environment where we inspire one another, encourage you to achieve your best, and importantly, share success together. We create opportunities to develop your skills and chart your career with us.

We are looking for a Data Engineer to build data infrastructure to power the insights needed for evidence-based decision-making and enhance service-delivery. You will be architecting, designing and building next-generation data infrastructure to drive digital transformation in the heath sector.

Key Responsibilities/Duties

Design and build resilient and efficient data pipelines for both batch and real-time streaming data

Architect and design data infrastructure on cloud using industry standard Infrastructure-as-Code tools

Execute projects with an Agile mindset

Build software frameworks to solve data problems at scale

Collaborate with product managers, software engineers, data analysts and data scientists to build scalable and data-driven platforms and tools

Obsess over cyber security by identifying threat & risk, ensuring all the components, from a platform, frameworks to the applications are fully secure and are compliant to infosec policies

Develop metrics to measure and monitor client and server API health and utilization

Requirements

Degree in Computer Science or have equivalent professional experience

At least 3 years of relevant experience in developing scalable, secured, fault tolerant, resilient & mission-critical data platforms

Experience with data processing tools such as Spark, Beam, Flink, Airflow, Kafka, dbt

Experience with the cloud (e.g. AWS, GCP, Azure)

Experience implementing batch and streaming data pipelines

Experience writing efficient SQL

Proficiency in at least one of the programming languages Java, Scala, or Python along with a fair understanding of runtime complexities.

In-depth knowledge of both SQL and NoSQL databases, including performance tuning and troubleshooting

In-depth knowledge of data modelling, data warehouse, and data lake

Familiar with DevOps tools such as Git, Docker, Terraform

Familiar with MLOps tools such as MLFlow, Kubeflow

Passionate about working in healthcare and raising the bar on healthcare solutions in Singapore and the region

Thank you for your interest. We regret that only shortlisted applicants will be notified"
29-Apr-2022 T11:56,Data Engineer / Software Engineer - Data,Zoom,4 days ago,,Full–time,"Work Styles at Zoom

In most cases, you will have the opportunity to choose your preferred working location from the following options when you join Zoom: in-person, hybrid or remote. Visit this page for more information about Zoom's Workstyles.

About Us

Zoomies help people stay connected so they can get more done together. We set out to build the best video product for the enterprise, and today help people communicate better with products like Zoom Contact Center, Zoom Phone, Zoom Events, Zoom Apps, Zoom Rooms, and Zoom Webinar.

We’re problem-solvers, working at a fast pace to design solutions with our customers and users in mind. Here, you’ll work across teams to deliver impactful projects that are changing the way people communicate and enjoy opportunities to advance your career in a diverse, inclusive environment.

You will be part of a team whose focus is to solve cutting edge AI / MLproblems and deploy models that constantly advance the state-of-the-art. You will be working on designing the data infrastructure and solve other interesting bottlenecks that are challenging at Zoom's scale.

Responsibilities :

Design and implementation of data infrastructure for Machine Learning (ML) projects for our Research and Development Org

Assembling large, complex data sets and make the data more discoverable and easy to be used by the AI / ML team to support various Machine Learning initiatives. This includes unifying pre-processing work flows for data for different AI tasks.

Build and manage data sets, also by identifying and using suitable big data technologies, such as Apache Hadoop, DVC, Apache Drill,

Write tools to transform raw data sources into easily accessible models by coding across several languages such as Python, and SQL

Deploy, develop and maintain tools for data collection, data crawling and data annotation, e.g. as web applications using Django and Angular or similar technology.

Along with looking at external sources of data, build data expertise and own data quality for the pipelines you create.

Requirements :

Bachelor’s or higher degree in Computer Science or related fields

Experience in building and optimising data pipelines
• Docker
• Experience with AWS cloud services, e.g. S3
• Experience working with large data volumes
• Broad knowledge of the data infrastructure ecosystem
• Knowledge of technologies like DVC or Hadoop
• Good understanding of one or more of the following : Python, JavaScript, C++, Django or Angular

Please note : This job opportunity is based out of Karlsruhe, Germany

We believe that the unique contributions of all Zoomies is the driver of our success. To make sure that our products and culture continue to incorporate everyone's perspectives and experience we never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. Zoom is proud to be an equal opportunity workplace and is an affirmative action employer. All your information will be kept confidential according to EEO guidelines.

We welcome people of different backgrounds, experiences, abilities and perspectives including qualified applicants with arrest and conviction records and any qualified applicants requiring reasonable accommodations in accordance with the law. If you need any assistance or accommodations due to a medical condition, or if you need assistance accessing our website or completing the application process, please let us know by emailing us at careers@zoom.us.

Zoom requires all U.S. employees who will work in person at a Zoom office, attend in-person Zoom meetings or have in-person customer meetings to be fully vaccinated. Zoom will consider requests for reasonable accommodations for religious or medical reasons as required under applicable law.

At Zoom, we care about our employees, their families, and their well-being. As part of our award-winning workplace culture and commitment to delivering happiness, our benefits program offers a variety of perks, benefits, and options to help employees maintain their physical, mental, emotional, and financial health; support work-life balance; and contribute to their community in meaningful ways. To view our benefits, click here.

Explore Zoom:
• Hear from our leadership team
• Browse Awards and Employee Reviews on Comparably
• Visit our Blog
• Zoom with us!
• Find us on social at the links below and on Instagram
• View more jobs, sign up for job alerts and join our talent community. Visit the Zoom careers site.

#LI-Remote"
29-Apr-2022 T11:56,Software/Data Engineer#TeSA #CLT,Aida Technologies Pte. Ltd.,3 days ago,,Full–time,"Training Programme This Company-Led Training (CLT) programme is a 9 to 12 months on the job training with Aida Technologies Pte Ltd to equip fresh professionals with industry skills to become an Artifical Intelligence / Machine learning (AI/ML) Engineer. Each trainee will be trained and mentored in one or more of the following areas: AI and ML applications Data Management process Machine learning, deep learning techniques and tools Delpu AI/ML as a data product and/or data service Minimum Entry Requirements Singapore citizen Diploma/Degree in Computer Science or Engineering. Fresh and Mid-level professionals are welcome Application Process Interested individuals can apply directly with Aida Technologies Pte Ltd. Please email your CV to [Confidential Information]."
29-Apr-2022 T11:56,Data Engineer - HFT - Singapore- Leading Quant Fund | London,Oxford Knight,3 days ago,,Full–time,"Data Engineer - HFT - Singapore

Summary

The positive feel of a start-up with the benefits that come with a more established player, this leading HFT firm is looking for a dynamic software engineer to join one of their most successful quant trading teams. Collaborating extensively with traders and other technologists, you'll design, write and maintain a complex Python infrastructure. This role is as front office as developers come within the firm without being a Quant.

The ideal candidate will be passionate about: data, development, and keen to learn about automated electronic trading. You will be expected to solve difficult technical problems in a fast-paced and energetic environment, with a focus on processes that are robust, scalable and fault-tolerant. Most problems require high-availability, high-throughput and low-latency solutions.

The trading team see technology as a key component of their continued success and candidates will be exposed to cool, cutting-edge technologies.

Financial experience is not essential; just a willingness to learn, and learn quickly.
• **Interviews will be remote; ideally start dates will be in-house, but they're flexible.***

Requirements

• Excellent problem-solving skills, ability to make the right engineering decisions to achieve maintainability, extensibility and debuggability
• Solid knowledge of Python and Linux. (Some C++ preferable)
• Self-directed and able to take ownership of several projects at once
• Knowledge of SQL, Redis, InfluxDB. is a bonus
• Bachelor's degree (or higher) in Computer Science or Computer Engineering (or equivalent)
Benefits

• Competitive base salary + bonus; they will pay leading market rate / are flexible for the right candidate
• They're willing to be flexible with WFH
• Collaborative and rewarding work environment and culture
• Free breakfast , lunch and dinner

Contact
If this sounds like you, or you would like to know more, please get in touch.

Andy Stirling-Martin
andy@oxfordknight.co.uk
07453 28768
linkedin.com/in/andrew-stirling-martin-7664a946"
29-Apr-2022 T11:56,"Director, Data Engineering",Singtel Group,19 days ago,,Full–time,"Singtel, Asia’s leading communications technology group, provides an extensive range of telecommunications and digital services to millions of consumers and businesses across Asia, Australia, Africa and the USA. With over 140 years of innovation behind us, we continue to push boundaries in our networks and services, to enrich lives and transform businesses.

Our core values – Customer Focus, Challenger Spirit, Teamwork, Integrity, and Personal Excellence – shape the way we work. We are passionate about making a difference and have an open and inclusive culture where everyone is empowered to do their best. Our diverse business means you will enjoy unique opportunities and rewarding experiences to learn and grow your career in a dynamic industry.

Join us and experience what it’s like to be with an Employer of Choice*. Together, let’s create a brighter digital future for all. *Awarded at the HR Fest Awards 2020

About the role

As part of our expansion and increasing demands for Big Data projects within Singtel Group, we are looking for a Big Data Director to:
• Define and govern Big Data strategy, architecture and roadmap
• Drive, lead and align new capabilities in the area of Big Data and Data Integration across Singtel Group
• Lead and manage Big Data development and operations teams
• Track and monitor vendor’s SLA performance
• Plan and manage annual budget for both capital expenditures and operating expenses
• Develop multi-year IT investment roadmaps that link to the Group IT strategy
• Define standards and guidelines for development and operations
• Build, maintain and improve strong relationship with business leaders, Group IT domains (departments) and IT service providers to deliver value via data
• Represent Group IT at assigned project and / or program steering committees
• Present and pitch at relevant senior leadership levels and /or executive steering committees
• Drive Group IT and / or domain initiatives to the team
• Promote a DevOps culture through building relationships with development & operations and driving enhancements to the end-to-end release process
• Proactively advise the senior management team on the emerging technologies and digital trends that are most relevant to the company's goals and evolving needs
• Hire, develop, evaluate, reward, and retain a highly-qualified team

Responsibilities
• Develop, define and govern Singtel Big Data and data management architecture, guidelines and roadmap with stakeholders
• Build up new capabilities for Big Data to support business initiatives
• Overall responsible for delivery and operations for Big Data functions
• Manage multiple Big Data delivery / project teams comprise of internal IT professionals and several IT service providers to ensure that projects / enhancements are delivered within the agreed scope, budget and schedule
• Lead the application support teams comprise of internal IT professionals and IT service providers in managing, monitoring and optimizing of Big Data Platforms to ensure these applications meet the agreed SLA agreed with business
• Troubleshoot Big Data platforms and software, including performance-tunes of BI applications
• Budgeting, tracking and forecasting capital & operational expenditure
• Interface with product vendors to keep abreast of new technologies, pricing and customer applicability. Participate in vendor evaluations
• Lead and establish development guidelines, standards and best practices
• Implement the most appropriate and effective IT organizational design to support and engage with the business
• Work with business stakeholders to develop and analyze big data needs
• Manage and drive outsourced vendors to achieve defined delivery and operations objectives
• Manage resource capacity and staffing
• Provide functional leadership to assigned staff and deliver input to staff performance and development
• Identify capability for the team and provide guidance, training, and problem-solving assistance to team members
• Present and engage senior leadership levels and /or executive steering committees on Big Data initiatives, projects and operations
• Drive Group IT initiatives to the team
• Develop and execute an analytics program that will allow company business leaders to make data-based decisions

The ideal candidate should possess
• Bachelor's degree in Business Management, IT, Computer Science or equivalent.
• At least 12 years of experience in the area of big data, data warehousing, BI & reporting and / or data management, including at least 8 years of experience in managing BI or big data development and operations team
• Experience in managing and driving outsourced vendors to delivery and operations objectives
• Hands-on experience in handling incident, problem, configuration, capacity and availability management
• Successfully implemented large-scale data warehouse / data lake
• Expert in building and optimizing ‘big data’ data pipelines
• Experience in data management, data architecture and design
• Strong technical knowledge of data integrations, ETL and data warehouse data modelling
• Strong knowledge of BI reporting tools
• Strong knowledge of SQL
• Good understanding of Telco data models
• Experience to do cost estimation and working with external vendors
• Experience with DevOps tools and environment
• Experience in building an enterprise level data analytics capability.
• Experience in leading complex, major change initiatives; skills in change management
• Customer-service oriented
• Strong background in operational and capital finances, and IT budget development

We believe in the strength of a vibrant, diverse and inclusive workforce where backgrounds, perspectives and life experiences of our people help us innovate and create strong connections with our customers. We strive to ensure all our people practices are non-discriminatory and provide a fair, performance-based work culture that is diverse, inclusive and collaborative"
29-Apr-2022 T11:56,[Internship 2022] Data Engineer Intern (APAC),foodpanda Singapore,20 mins,,Full–time,"This is a full-time internship opportunity starting in May 2022

At foodpanda, we're on a mission to redefine how tech, food, people and culture are connected. Operating in more than 400 cities across 12 locations worldwide, foodpanda continues to expand and grow in its core food delivery business as well as in new verticals like grocery deliveries, with a strong tech infrastructure at its core. From our restaurants-partners, cloud kitchens and cloud grocery stores — foodpanda is just one tap away, delivering everything you need quickly and conveniently to your doorstep.

We are looking for undergraduates with a passion for data and insights to join our APAC Regional teams in Singapore! You will be empowered to find the most effective way of using data to help foodpanda make smart, data-driven business decisions.

This is an amazing opportunity for individuals who would like to learn, thrive and hone their analytical skills. If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!

What's on the menu for you:
• Defines and builds data pipelines that will enable faster, better, data-informed decision-making within the business
• Architect, build, and deploy new data models that provide intuitive analytics across the business
• Manage and own the entire data processing system (internally, as well as interfacing with other teams or third party services), provide advice on any necessary infrastructure changes
• Experiment with, select, and implement cutting edge Big Data tools and frameworks required to provide requested capabilities
• Perform all needed data transformation to populate a reporting optimised data warehouse
• Participate in building machine learning models, deliver analyses and insights to support business decision making

What you bring to the table:
• Penultimate or final year undergraduate students pursuing bachelor’s degree in Computer Science, Engineering, Data Analytics, Mathematics, or related discipline
• Ability to write clean, structured, and high performance SQL code
• Strong oral and written communication skills
• Strong business mindset and ability to grasp business requirements from stakeholders
• Champion of data and visualization with strong presentation and story-telling skills
• Knowledge and experience with BI tools (Tableau, Data Studio), big data, Data Warehouse technologies, Python or R.
• Knowledge of Machine Learning, Big Data, Data Pipelines, or setting up environments for data scientists/machine learning engineers is a plus.

What we Offer:
• A dynamic and challenging work environment.
• A company committed to developing you personally and professionally.
• A great working atmosphere with regular company and team events.
• A vibrant and international team committed to diversity and inclusion.
• Responsibility from day one in a fast growing and global company.
• Other benefits include free food and learning and development opportunities"
29-Apr-2022 T11:56,Data Engineer,Quesscorp Singapore Pte Ltd,11 days ago,,Full–time,"Deliver big data solution based on premise Hadoop or cloud-based systems like AWS.

• Design ingestion layer for structured & unstructured data (text, voice, xml etc) & implement insurance specific data model for business & analytics use.
• Deliver ELT solution including data extraction, transformation, cleansing, data integration and data management.
• Implement batch & near real time data ingestion pipelines based on reference architecture like Lambda.
• Build advanced data sources for self-service analytics systems
• Operationalize analytics models for production usage with big data workflows, proper security & access control.
• Provide development and architecture support for internal analytics tools & optimize execution performance for complex data pipelines.
• Bachelor’s Degree in Computer Science / IT
• 3+ years of experience in Big Data Engineering using tools like Spark, Hadoop, Hive, etc.
• 3+ years of experience with Software Engineering using Python / development using modern Relational and NoSQL databases like MySQL, PostgreSQL, and MongoDB
• Worked on Talend (Big Data) and/or Informatica Data Engineering 10.4 platform using modules DEI, DES, PowerExchange and PowerCenter
• Experience in developing medium to large scale Data Integration projects, including Real-Time and APIs based initiatives.
• Good knowledge on spark coding and shell scripting
• Hand on experience with Kafka for publishing and consuming Data"
29-Apr-2022 T11:56,Data Engineer,SOL-X PTE. LTD.,4 days ago,$6K–$9K a month,Full–time,"This role is for a highly skilled data engineer with deep knowledge in data systems, will be able to work independently with minimal supervision and will have a divide and conquer mindset when attacking data flows. This role is responsible for expanding and optimizing our data and data pipelines architecture as well as optimizing data flows and collections for analytics. This role will support data scientists, products and engineering team on the data initiatives and will ensure optimal data delivery throughout multiple ongoing projects.

Responsibilities:

· Create and maintain optimal data pipeline architecture.

· Assemble complex data sets to meet specific requirements.

· Creating data systems that ingest data from various sources.

· Implement flows with distributed systems and cloud architecture.

· Write efficient, well documented, and highly readable code.

· Schedule/automate data pipelines and monitor their performance.

· Write ad-hoc queries in order to perform analysis

· Interact with the teams to gather requirements and explain his work

Qualifications & Experience:

· Bachelor’s degree in engineering majors / computer science or similar relevant field

· 5+ years of experience in Software Engineering / BI / Data Warehouse design and development using modern

Relational and NoSQL databases like MySQL, PostgreSQL, and MongoDB

· 3+ years of experience in building SQL scripts and automating DWH pipelines.

· Good knowledge and experience with Python for data manipulation

· Experience with Unix and cloud solutions such as Azure Blob, AWS EC2, AWS EMR

· Data Modelling and Data Science experience building predictive models

· Experience with data automation/data orchestration tools such as Prefect is a plus"
29-Apr-2022 T11:56,Senior/Principal Big Data Engineer,ORACLE CORPORATION SINGAPORE PTE LTD,6 days ago,,Full–time,"Applicants are required to read, write, and speak the following languages: English

The Java Platform Group is looking for an experienced, passionate, and highly motivated Big Data Engineer. Our team is responsible for delivering the Java platform that is used by millions of developers. We are looking for an engineer with a thorough working experience of both the Java Platform and the implementation of the cloud native services. We are hiring to further expand our cloud service to more Oracle commercial regions and we are continuously investing to grow the service's capabilities and footprint across the globe.

As a Big Data Engineer with the Java Platform Group, you will be a leading contributor in Big Data Analytics of Oracle’s latest Cloud Services Technologies. You will take an active role in the definition and evolution of standard practices and procedures. Additionally, you will be responsible for scaling our existing infrastructure, incorporating new data sources, building robust data pipelines for production level cloud ********** you have a passion Big Data processing this is the place where you can make a difference.

Responsibilities

Develop ETL pipelines with robust monitoring and alarming

Develop data models that are optimized for business usability and understanding

Develop and optimize data tables using best practices for partitioning, compression, parallelization, etc.

Develop and maintain metadata, data catalog and documentation in regard to the data flow of the system.

Ensure the handling of the data is in compliant to security and privacy requirements of the organization.

Optimize SQL and ETL solutions to solve various reporting requirements.

Maintain and continuously improve Java Management Services operations inline with the service's SLOs

Ensure JMS performance, uptime and scale, maintaining high standards of code quality

Work with agile development methodology, adhering to best practices established by Oracle and project team and pursuing continued learning opportunities

Knowledge sharing and provide mentorship to junior engineers.

Perform in-depth data analysis of incoming JVM telemetry data

Provide technical consulting and mentoring to team members on optimization techniques, new algorithm design,

Implementation and conformance to architecture standards

You will be responsible for architecting and developing data models

Help deploy and productionize Machine Learning models.

Skills we require:

10+ years experience with designing, implementing, testing and operating cloud native in an Agile and DevOps setup

3+ years of industry experience in Data Engineering with experience manipulating, and extracting data from large datasets.

3+ years of Data Warehousing experience preferably with Oracle or related technology displaying strength in SQL, python/pyspark scripting, data modeling and ETL development.

2+ years working in a role as Lead / Principal Engineer or Architect

Broad knowledge of different types of data storage engines - (non)relational, row/column oriented dbs. e.g. Oracle, Postgres, MySQL, QB/redshift, Elastic, Hive, HBase

Advanced query language (SQL) knowledge

Experience with stream processing engines (Kafka, Kinesis)

Experience in using a Java framework in development preferable using Dropwizard

Experience deep working experience with one or more major cloud vendors (OCI, Azure, AWS, GCP)

Strong analytical skills, 2+ years’ experience with Python and an interest in Machine Learning

Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Design, develop, troubleshoot and debug software programs for databases, applications, tools, networks etc.

As a member of the software engineering division, you will take an active role in the definition and evolution of standard practices and procedures. You will be responsible for defining and developing software for tasks associated with the developing, designing and debugging of software applications or operating systems.

Work is non-routine and very complex, involving the application of advanced technical/business skills in area of specialization. Leading contributor individually and as a team member, providing direction and mentoring to others. BS or MS degree or equivalent experience relevant to functional area. 7 years of software engineering or related experience.

: Product Development

: Regular Employee Hire

: Oracle

All over the world, people's lives are better because of Oracle. Want to make a difference? Join our company of change-makers.

From Oracle to culinary school and back again. Bonnie Carlson Kaypaghian uses the skills she learned to create recipes for her daughter’s Type 1 Diabetes and has written a cookbook to share with the world. #LifeatOracle

The Java Platform Group is looking for an experienced, passionate, and highly motivated Big Data Engineer. Our team is responsible for delivering the Java platform that is used by millions of developers. We are looking for an engineer with a thorough working experience of both the Java Platform and the implementation of the cloud native services. We are hiring to further expand our cloud service to more Oracle commercial regions and we are continuously investing to grow the service's capabilities and footprint across the globe.

As a Big Data Engineer with the Java Platform Group, you will be a leading contributor in Big Data Analytics of Oracle’s latest Cloud Services Technologies. You will take an active role in the definition and evolution of standard practices and procedures. Additionally, you will be responsible for scaling our existing infrastructure, incorporating new data sources, building robust data pipelines for production level cloud ********** you have a passion Big Data processing this is the place where you can make a difference.

Responsibilities

Develop ETL pipelines with robust monitoring and alarming

Develop data models that are optimized for business usability and understanding

Develop and optimize data tables using best practices for partitioning, compression, parallelization, etc.

Develop and maintain metadata, data catalog and documentation in regard to the data flow of the system.

Ensure the handling of the data is in compliant to security and privacy requirements of the organization.

Optimize SQL and ETL solutions to solve various reporting requirements.

Maintain and continuously improve Java Management Services operations inline with the service's SLOs

Ensure JMS performance, uptime and scale, maintaining high standards of code quality

Work with agile development methodology, adhering to best practices established by Oracle and project team and pursuing continued learning opportunities

Knowledge sharing and provide mentorship to junior engineers.

Perform in-depth data analysis of incoming JVM telemetry data

Provide technical consulting and mentoring to team members on optimization techniques, new algorithm design,

Implementation and conformance to architecture standards

You will be responsible for architecting and developing data models

Help deploy and productionize Machine Learning models.

Skills we require:

10+ years experience with designing, implementing, testing and operating cloud native in an Agile and DevOps setup

3+ years of industry experience in Data Engineering with experience manipulating, and extracting data from large datasets.

3+ years of Data Warehousing experience preferably with Oracle or related technology displaying strength in SQL, python/pyspark scripting, data modeling and ETL development.

2+ years working in a role as Lead / Principal Engineer or Architect

Broad knowledge of different types of data storage engines - (non)relational, row/column oriented dbs. e.g. Oracle, Postgres, MySQL, QB/redshift, Elastic, Hive, HBase

Advanced query language (SQL) knowledge

Experience with stream processing engines (Kafka, Kinesis)

Experience in using a Java framework in development preferable using Dropwizard

Experience deep working experience with one or more major cloud vendors (OCI, Azure, AWS, GCP)

Strong analytical skills, 2+ years’ experience with Python and an interest in Machine Learning

Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy

Oracle is the world’s leading provider of business software. But you probably already knew that. With a presence in over 175 countries, we are one of the biggest technology companies on the planet. What you might not know is that we are leading a cloud revolution. We’re using emerging technologies like AI, machine learning, and blockchain to solve critical real-world problems. From advancing energy efficiency to reimagining online commerce, the work we do is not only transforming the world of business—it’s helping governments, powering nonprofits, and giving billions of people the tools they need to outpace change and make a difference. Mission: To help people see data in new ways, discover insights, unlock endless possibilities.
Future Creators We are creators, free-thinkers, and basically a bunch of problem-solvers who are eternally asking ‘what if"
29-Apr-2022 T11:56,"Data Engineer, Capability Development (DART)",GVT Government Technology Agency (GovTech),2 days ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Data Engineer in GovTech’s Data Science & Artificial Intelligence Division, you will be involved in architecting, managing and running advanced analytics and data platforms. For this role, you will be part of the Data Analytics Readiness Team (DART) within GovTech, where you will help drive GovTech’s Data Transformation efforts. You will assist to establish and implement effective operational workflows, data pipelines, procedures and best practices in accordance with government infrastructure and security policies to ensure that the platforms are monitored, secure, available and reliable.

We are looking for an independent and motivated engineer who has experience in both small and large-scale analytics platform projects, and in building and maintaining the infrastructure.

What you will be working on:
• Design, Architect, Deploy, and maintain solutions on Microsoft Azure or AWS using different Cloud & Big Data Technologies to provide secure and governed access to data for business users.
• Manage the full life-cycle of a data warehouse/lakehouse solutions from requirement gathering and analysis to platform selection, design of the architecture, and deployment.
• Collaborate with data stewards, data analysts and data scientists to build data pipelines from enterprise systems such as Workday to collect, clean, harmonise, merge and consolidate data sources for data warehouse/lakehouse.

Where we are looking for:
• Diploma/Degree in Computer Science or Information Technology or related disciplines
• Experience with the cloud (e.g. AWS, GCP, Azure)
• Hands-on experience in implementing Data Lake/Data Warehouse with technologies like – Databricks, Azure Synapse Analytics, SQL Database, AWS Lake formation.
• Programming & debugging skills in Python, Scala or R.
• Proficient in SQL.
• Knowledge of both SQL and NoSQL databases.
• Comfortable with DevOps tools like AWS Cloud Formation/Terraform, Docker and Git for CI/CD development.
• Prior experience with data engineering tools and frameworks like Airflow, Kafka, Hadoop, Spark, Kubernetes.
• Familiar in building REST services is good to have.

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:56,Senior Data Engineer,Denodo,3 days ago,,Full–time,"Denodo is looking for a technical, passionate, hands-on data advocate, customer-oriented, and willing to take Denodo learning and adoption to the next level to join our Global Office team, as part of the Customer Success organization.

Your mission: ensure Denodo customers, partners, and any user of the Denodo Platform have available all the material they need to be successful in their Data Management solutions.
In this role you will combine high technical expertise, data ecosystem knowledge and customer-facing skills to interact with data, customers (internal and external), and other stakeholders to get inspired with new ideas, and put them into practice to facilitate Denodo adoption.

Duties and responsibilities:
• Obtain a strong knowledge of the Denodo Platform, be able to deliver a superb technical pitch, including overview of our key and advanced features and benefits, differentiation, and competitive positioning.
• Constantly learn new things and maintain an overview of modern technologies.
• Ensure Denodo users are well-equipped with the resources and training needed to effectively perform with the Denodo Platform in an environment of fast-paced change
• Follow up on the release calendar of updates and versions to identify opportunities to create new material.
• Interact with internal and external stakeholders to provide updates on the roadmap and get feedback.
• Interact with the local managers to coordinate the development and maintenance of technical resources.
• Develop white papers, presentations, training materials or documentation on related topics.
• Define metrics, analyze issues, data and trends to make data-driven decisions that let improve the quality of the technical resources and new ways of working.
• Ensure that upper level management is aware of issues regarding processes, protocols or educational materials

Qualifications

Desired skills and experience
• Self-motivated, proactive and solution oriented. A passion for improving Denodo user experience.
• Excellent organizational skills being able to manage change effectively.
• Excellent verbal and written communication skills to be able to interact with technical and business counterparts
• Strong analytical and problem solving abilities.
• Active listener.
• Lots of curiosity. You never stop learning new things.
• Creativity. We love to be surprised with innovative solutions.
• Be a team worker with positive attitude
• Willingness to occasionally travel.
• BS or higher degree in Computer Science or Information Systems.
• 3+ years of demonstrated experience in a similar role.
• Experience with the data landscape.
• Fluent in English ( min. C1 Level) (only for non English speaking countries)

Employment Practices

Denodo is an equal opportunity employer and prohibits discrimination and harassment of any kind. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by applicable law. Denodo will provide reasonable accommodation to employees who have protected disabilities in accordance with applicable law.

We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee"
29-Apr-2022 T11:56,(Senior) Data Engineer (Information Management Systems),Centre for Strategic Infocomm Technologies,Full–time,,,"As a data engineer in the Information Management Systems team, you will develop critical pipelines to convert and serve large volumes of varied unstructured data into usable datasets for other services and end-users. Together with your driven and multi-talented product team, you have the autonomy to determine the best way to deliver value and the technologies used.

As CSIT is an agency under the Ministry of Defence (Singapore), only Singapore Citizens will be considered."
29-Apr-2022 T11:56,Principal Software Engineer (Data Engineer),COMMSCOPE COMMUNICATIONS SINGAPORE PTE. LTD.,9 hours ago,,Full–time,"You will work with a dynamic and focused team to develop state-of-the-art big data applications in Analytics and Artificial Intelligence (AI). As our Staff Software Engineer, you will be implementing our core software components, and be involved in scalable design for cloud software architecture. This is an exciting opportunity to join our talented team and be involved in the next technology trend: Analytics & AI in Networking.

You Will Excite Us If You Have
• Strong working knowledge in SaaS application development and Agile software development methodologies.
• Good working knowledge using services from any public cloud offerings. Experience in Google Cloud Platform would be a plus.
• Good analytic skills working with both structured & unstructured datasets.
• Good experience building big data data pipelines, ETL architecture.
• Working knowledge of stream processing pipeline for big data would be a plus.
• At least 10-12 years of experience in Data Engineering role and have good knowledge / working experience in:
• Relational (e.g. PostgreSQL) & non relational databases such as Graph and time series.
• Streaming & data pipeline tools, e.g. Kafka, Spark Streaming.
• Big data processing technologies, e.g. Spark, Hadoop, Ignite, Parquet.
• Functional programming languages, e.g. Scala.
• Virtualization and container environment such as Docker and Kubernetes.
• Compiler and scripting languages, e.g. Golang, Ruby, Node.js"
29-Apr-2022 T11:56,Data Engineer - Search,Shopee,Full–time,,,"Design, build and maintain the ingestion system to support various types of data (e.g. User behavior, RDS, NoSQL DB and others) to be ingested to the data warehouse more timely and accuratelyTranslate data requirements into scalable technical data service with low latency and high concurrencyDesign, build and maintain the batch or real-time data pipeline in production using Hadoop big data technologyAnalyse and improve efficiency, scalability, and stability of the systemDefine and manage service level agreement and data quality for all data sets in allocated areas of ownership"
29-Apr-2022 T11:56,"Data Engineer, FinTech",foodpanda,4 days ago,"$5,923–$11,769 a month",Full–time,"We are looking for a Data Engineer, to join our growing team that who will work closely with our fintech squads based in Singapore. You will be part of an international team of highly talented and motivated people.

This role reports directly to our Director of Data & Product Analytics.

What's On Your Plate
• Build and execute SQL queries & database schema as required by the business
• Work with software engineers and architects to design the target data architecture as per business requirements
• Architect, build and deploy data models with adequate documentation and validation
• Implement data processing and pipelining queries that will enable data-informed decision-making within the business
• Enable data consumers such as analytics, data products, machine learning etc. by actively seeking opportunities to automate data pipelines
• Continuously experiment state of the art technologies and proactively seek opportunities to improve the data ecosystem
• Actively document the codes models and data dictionaries that are being developed.
What You Bring To The Table
• 2 - 4 years of relevant experience in data analytics / engineering, preferably on a cloud setup
• Strong knowledge on SQL, data structures and database schemas
• Experience in data pipelining tools such as Airflow
• Good working knowledge of Python / Java or any other programming language
• Knowledgeable in big data, Data Warehouse technologies
• Knowledgeable in agile tool sets such as Jira, Confluence etc.
• Preferred qualifications:
• Experience in Google Cloud Platform
• Working knowledge on NoSQL datastores such as MongoDB, DynamoDB etc.

What We Can Offer You
• A vibrant and international team with multicultural and diverse backgrounds.
• Solving challenges with inspiring colleagues in an all hands-on deck environment.
• Management team that recognizes top performers, welcomes our newbies, and shares a love for good food.
• Competitive package, incentives, allowances, food perks, insurance, pension and more"
29-Apr-2022 T11:56,Data Engineer,ITCONNECTUS PTE. LTD.,4 days ago,$6K–$8K a month,Full–time,"Key responsibilities:
• Single point of contact and liaison of all company teams
• Provide environment/integration support for SCP platforms and tools
• Takeover from Transformation Partner and DigiTech for Operate Support
• Liaise with Business/System Owner, DigiTech and Transformation Partner on requests and issues
• Support and follow-up with Project team on requests and issues

Requirements:
• Must have 5 Years of good experience
• Degree or diploma in IT/Computer Science/Information Systems or equivalent
• 3+ years of experience relevant data engineering preferred.
• Experience with data pipeline and ETL development preferred
• Proficient in Pyspark, Python and SQL
• Experience with Java, C#, JavaScript/TypeScript, open-source, Docker and Terraform technologies.
• Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing and operations
• Fast learner and a team player
• Knowledge in AWS/Azure/GCP"
29-Apr-2022 T11:56,Data Engineer,TOTAL EBIZ SOLUTIONS,Full–time,,,"• Ability to build both batch and streaming data pipeline in a big data environment from the upstream systems in collaboration with technology and other data teams.
• Develop an in-depth understanding of the underlying data, data model, and business domain by investigating, reviewing, and analyzing data from structured & unstructured sources. Leverage this knowledge to design and build common/ master data models.
• Independently, collaborate with cross-functional stakeholders – other department teams and other technology teams to understand their data needs, formulate and complete end-to-end analysis that includes business understanding, data gathering, data discovery, building and designing master/ common data models using cloud-based data pipelines.
• Ability to maintain and optimize the data pipelines. Monitor the health of production pipelines frequently to ensure accuracy and stability.
• Diagnose, analyze and provide solutions to issues in scripts, reports, tools, data, etc.
• Strictly adhere to the controls and standards for meeting the enterprise technology data quality requirements, by using various in-house data quality solutions.
• Identify, design, and implement improvements, e.g. automating manual processes, optimizing data delivery, re-designing architecture for greater scalability, etc.
• Train/ mentor junior data engineers and the Lazada business associates and enable Lazada personnel on building and understanding various data querying and data-discovery skills.

Job Requirements
• Strong education background in Computer Science/Information Systems (preferred).
• Must have Big Data/SQL skills.
• Strong understanding of database & data warehouse design/administration, hands-on experience with MySQL, Hadoop, Hive SQL, Flink / Spark technology, and Python. Knowledge of API programming and NoSQL databases will be an added advantage
• More than 5 years of experience in end-to-end data warehousing, data modeling, and as well as distributed processing systems. Previous experience of working in cloud-based environments, especially Alicloud would be an added advantage.
• Good understanding of data quality frameworks and data governance.
• Previous experience in implementing them at an enterprise level would be an added advantage.
• Experience in enablement and other enterprise-wide training activities.
• Strong analytical/problem-solving skill balance with good written and oral communication skills.
• A good understanding of accounting concepts, finance processes, and financial performance metrics is a plus.
• Tenacity to develop ideas independently and thrive in a fast-paced start-up environment.
• A self-motivated, driven, flexible, quick learning, high achieving, “can do” mentality"
29-Apr-2022 T11:56,Data Engineer (SQL/Python),Scientec Consulting Pte. Ltd.,4 days ago,,Full–time,"Responsibilities
• Responsible for databases, data warehouses and large-scale data processing systems
• Develop, construct, test and maintain analytical data warehouse and deploy and continuously improve ETL / ELT tasks
• Design, develop, review and optimize the core database store procedures in batch job scripts and java scripts.
• PL / SQL and SQL Tuning and optimization

Requirements
• Degree in IT or equivalent
• Experience data architecture, data warehousing, data processing, data modelling and ETL / ELT
• Experience in database development (Oracle SQL / PLSQL)
• Expert in using Python

To apply, please send your updated resume to HIDDEN TEXT

By submitting any application or resume to us, you will be deemed to have agreed & consented to us collecting, using, retaining & disclosing your personal information to prospective employers for their consideration.

If you wish to withdraw your consent or correct any of your personal data, please drop us an email at HIDDEN TEXT to let us know.

Note : Any resumes of job applications sent to this mailbox will not be attended as it is solely for the purpose of personal data protection related matters.)

We will contact you if your skills and experience are suitable for the role, or if there is a similar opportunity that is available presently or in the future.

Resquid Quitaleg Airene (R1656137)

ScienTec Consulting Pte Ltd - 11C5781"
29-Apr-2022 T11:56,Data Engineer,LILITH GAMES SG PTE. LTD.,1 day ago,,Full–time,"Big Data Development Engineer (Data Center)

What You Will Be Doing
• Plan, design, develop and implement core data system in the business field
• Develop and maintain real-time/ offline data processing tasks
• Build industry-leading distributed systems such as storage and computing to provide a reliable infrastructure for massive data and large-scale business systems
• Develop various data services, such as data developing platform, quality monitoring system,

Qualifications & Skills
• BSc in Computer Science or related major.
• At least 3 years of big data platform R&D.
• Familiar with common algorithms and data structures, and be proficient in Java / Python / Scala / shell language (at least two)
• Familiar with deployment and implementation of docker and k8s related containers.
• Experienced working with big data technologies such as Hadoop, Flink, Hive, Spark, etc.
• Enthusiastic about learning new technologies and persistent in pursuit of high concurrency and distributed architecture design.
• Have good team communication and collaboration skills.

大数据开发工程师（数据中台方向）

工作职责：
• 参与业务领域核心数据体系的规划设计以及开发落地
• 参与实时/离线数据加工任务的开发与维护
• 参与大数据基础架构的开发与迭代
• 参与各类数据服务开发，包含但不限于：数据平台开发，数据质量监控，BI等

岗位要求：
• 本科及以上学历，3年以上大数据平台研发经验。
• 具有扎实的编程功底，熟悉常用的算法和数据结构，精通Java/Python/Scala/Shell语言（至少两种）
• 熟悉Hadoop等大数据框架，有Flink、Spark等相关实时流计算框架经验优先
• 熟悉docker、k8s相关容器的部署实施
• 对新技术保持好奇心，对高并发和分布式架构有执着的追求"
29-Apr-2022 T11:56,Data Engineer - Quantitative Trading Firm/Market Maker,Aptitude Asia,3 days ago,,Full–time,"Our client, a leading high frequency trading firm, currently looking for a Data Engineer to join the team in Singapore. The team is to provide high-quality research and reference data for the firm to utilize, accessing financial data and analytical services which support the trading, research, and operational functions.

Responsibilities
• Responsible for building, maintaining and improving the acquisition and monitoring data platform
• Responsible for delivery and the construction of derived sets of data, analysis and cleaning
• Develop and implement tools to provide autonomy around the platforms data life-cycle
• Maintain an inventory of the data estate
• Implement the control workflow for representing data events
• Work with the investment team, quant research, operations to ensure their data-related requirements are captured and delivered
• Liaise with external vendors confidently and assist in vendor audit and compliance reporting

Requirements
• Master of Mathematics, Computer Science, Physics or Engineering and related field; strong candidates with Bachelor of Science degrees will also be considered.
• At least 3 years exepereinces for data analyst role or relevant data experience in either a financial institution or data vendor
• Strong background in financial markets and associated data sets i.e. market data, reference data
• Must have a working understanding of data engineering practices
• Strong numerical and analytical skills and problem-solving ability
• Skilled in the use and understanding of modern database technologies including relational, document, and temporal column data stores.
• Skilled in the use of a programming language such as Python for data engineering

To apply, please send your updated CV to donna@aptitudeasia.com. Thanks"
29-Apr-2022 T11:56,Data Engineer - Data Warehouse,Shopee,Full–time,,,"The Engineering and Technology team is at the core of the Shopee platform development. The team is made up of a group of passionate engineers from all over the world, striving to build the best systems with the most suitable technologies. Our engineers do not merely solve problems at hand; We build foundations for a long-lasting future. We don't limit ourselves on what we can or can't do; we take matters into our own hands even if it means drilling down to the bottom layer of the computing platform. Shopee's hyper-growing business scale has transformed most ""innocent"" problems into huge technical challenges, and there is no better place to experience it first-hand if you love technologies as much as we do. Browse our Engineering and Technology team openings to see how you can make an impact with us.

Job Description:
• Design and manage Data warehouse plan for a product, such as: design data model, define data metric, data governance and so on
• Collaborate with Business Intelligence Business and Product Management teams
• Use data to solve problems, identify needs and opportunities
• Design, build and maintain the batch or real time data pipeline in production using Hadoop big data technology
• Define and manage service level agreement, data quality for all data sets in allocated areas of ownership

Requirements:
• Bachelor's Degree in Computer Science or a related technical field.
• 2~10+ years working experience with programming languages,such as Java,Scala,Python
• Ability to write efficient SQL statements
• Familiar with Hadoop, Spark and Flink data processing, experience of TB data processing experience is a plus
• Familiar with data warehouse modeling, such as dimensional data modeling and schema design
• Understand data mining or machine learning
• Excellent communication skills including the ability to identify and communicate data driven insights
• Strong logical thinking and analysis abilities
• Passionate, self-motivated, and takes ownership"
29-Apr-2022 T11:56,Head of Data Engineering,Apersona Pte. Ltd.,5 days ago,,Full–time,"What you will do
• Place client value and human experience at the centre of everything we do
• Use data engineering to support material impact and drive disruptive transformation across public and private sectors
• Maintain public trust ensuring fairness, ethics, accountability, and transparency
• Build a world-class team with expert capabilities in data engineering
• Create a culture of excellence and lead with confidence, charisma, context, and humility working effectively at all levels
• Lead delivery of data engineering solutions through incubation, proofs-of-concept, to commercialisation and deployment
• Support development of go-to-market plans for both AI & data, understand strategic opportunities, develop trusted partnerships, and deliver social progress
• Educate, enable, and coach teams on data engineering in Temus, clients and in the broader community
• Adopt a cloud first strategy to enhance agility and elasticity partnering with vendors to support specific public sector needs

Suitable candidates
• Bachelor’s Degree in Statistics/Computer Science/Data Analytics or related quantitative field
• Minimum 10 years of relevant experience in consulting/ data science related functions with with at least 3 years of leadership / people management experience
• Data Development: Deep expertise in developing and managing software for data processing: Python, Java, SQL, KSQL, Scala, Spark, Flink, Beam, AWS Glue, Google Dataflow, etc.
• Data Platforms: Deep expertise in building and managing data platforms: HBase, MongoDB, Cassandra, Redis, PostgreSQL, Oracle, MySQL, Kafka, Kinesis, DynamoDB, Redshift, Cloud SQL, Cloud Spanner, Cloud Bigtable, Firestore, BigQuery, Azure SQL, Cosmos DB, Stream Analytics, Synapse Analytics, DeltaLake, Elasticsearch, Snowflake etc.
• Data Management & Governance: Strong skills in data management: accuracy, integrity, latency, classification, security, lineage, metadata, etc.
• Communication: Strong ability to communicate with a broad range of clients, colleagues, and partners across a variety of contexts and formats.
• Partnership: Strong ability to develop and maintain relationships amongst clients, colleagues, and partners
• Leadership: Demonstrated capability to lead, inspire, coach and mentor team members and colleagues.

Job Types: Full-time, Permanent

Benefits:
• Health insurance
• Parental leave
• Professional development

Schedule:
• Monday to Friday

Supplemental Pay:
• Performance bonus"
29-Apr-2022 T11:56,Market Data Engineer - 12 Months Contract,Adecco Personnel Pte. Ltd.,8 days ago,,Full–time,"This role will not only act as the first line of support for Market Data infrastructure across Asia Pacific regions including applications, servers and connectivity, but also be heavily involved in testing and implementation of real-time market data infrastructure and in-house tools. It requires a wide range of skills and expertise which are critical in the delivery of reliable, timely and cost effective market data products, solutions and services that enable our business lines and their electronic trading systems to make highly informed and competitive decisions.

Key Responsibilities
• Support and maintain market data infrastructures/platforms and expeditiously apply troubleshooting/problem-solving skills to minimize business productivity loss, customer impact, and financial/reputational risk.
• Coordinate with and escalate to market data vendors and exchanges in the region for their data content/technical issues and changes.
• Manage vendor and exchange driven changes, technical activities and relationships.
• Plan and implement production changes after trading hours, and conduct testing to verify the changes are successful.
• Act as the SME of market data distributed by regional exchanges and major vendors, perform in-depth analysis over data content and behavior to effectively support low latency electronic trading applications.
• Design, implement and constantly improve monitoring/capacity/performance/data analysis tools to ensure comprehensive coverage over data accuracy and reliability & infrastructure availability and stability.

Key Requirements
• Overview of financial and banking industry and strong data knowledge for Asia Equity markets
• Strong scripting skills for software/system management and automation (Perl, Python, bash scripting)
• Understanding and experience with market data delivery platforms like TREP/BBG/Direct exchange feeds
• Administration and support experience with server Operating Systems like Red Hat Linux and Windows environment
• IT Service Management knowledge and mindset including Change Management, Incident Management, Problem Management.

Strong analytical skills and logical thinking"
29-Apr-2022 T11:56,Sr Data Engineer Data Engineer,Carousell Group,3 days ago,,Full–time,"We are now looking for Data Engineers/ Sr. Data Engineers to join our Data team.You will: Build, maintain and scale efficient data infrastructure, ETL and reporting pipelinesWork with data scientists and machine learning engineers in getting ML and deep learning models to production readyInvestigate and research data quality and integrity from data sourcesDevelop and maintain analytics platform, business intelligence and experimentation toolsDevelop and maintain scalable platform for tracking business intelligence, build for reliability and redundancyManage data collection and organize the models, and also forecast the future needsCoach and mentor junior data engineers to be more effective individual contributors You have: User obsession and empathy.Drive and resourcefulness to persevere and overcome obstacles achieving challenging goals.Focus on impact and results. You work on the right things and get them done.High integrity and ability to positively collaborate with othersFluent in Python or similar programming languageExperience in data engineering tools like Hadoop, Spark, BigQuery, Airflow, etc.Experience in deploying and scaling ML models. Plus point for deploying and scaling deep learning models.Data-driven and passionate about solving problems through dataInquisitive and curious to delve deep into data to investigate trends or anomaliesDetail oriented and be able to work efficiently in a fast-paced team environmentKeen on data technologies and picking up new skills and tools along the wayStrong critical thinking and ability to frame issues in a logical manner P.S. The job title will be given based on working experience and interview performance"
29-Apr-2022 T11:56,Senior Data Engineer – Cyber Security R&D,NCS Group,4 days ago,$7.5K–$15K a month,Full–time,"• Design and build efficient, scalable and resilient ETL pipelines for both batch and real-time streaming data
• Architect and design data infrastructure on cloud using industry standard Infrastructure-as-Code tools
• Administer and maintain big data infrastructure, including performance tuning and troubleshooting
• Collaborate with data scientists and software engineers to build data-driven security platforms and services
• Assemble and process large and structured/unstructured datasets that meet cyber security business requirements.
• Create impactful demonstrations to showcase cyber analytics capabilities
• Design and build services with a focus on business value and usability
• Contribute to cross-functional technical discussion

Requirements:
• Degree in Computer Science, Computer Engineering, Information Technology, or equivalent
• At least 5-7 years of experience in data modelling and designing ETL pipelines
• Proficient in cloud technologies and services provided by AWS Azure, and GCP
• Familiar with DevOps tools, such as Git, Docker, Terraform
• Proficient in machine learning and data visualisation tools
• Proficient in SQL and other scripting languages, such as Phython and Bash
• In-depth knowledge of both SQL and NoSQL databases, including performance tuning and troubleshooting
• Strong sense of responsibility and accountability
• Excellent communication skills, both written and verbal
• Ability to work in a fast-paced, culturally diverse environment"
29-Apr-2022 T11:56,Data Engineer Intern,Centre for Strategic Infocomm Technologies,,,,"Duration: 2-4 Months, > 4 Months
Internship Type: Undergraduate; Pre-University

Raw data flows at high speed and high volume across enterprise systems. The multiplicity of file format and erroneous file content often make integration and maintenance of systems a challenging task. Data Engineers need to find trends in this complex environment and develop algorithms to make raw streaming data useful to the organisation.

In your application, please submit the following documents in a single PDF file for 'Resume/CV':
1. Resume
2. All Education Certificates to-date (from secondary to tertiary education)
- GCE 'O'/'N' Levels, NITEC
- Prelim/GCE 'A' Levels, IB/Diploma
- Latest university transcript from Year 1 (for current undergraduates)"
29-Apr-2022 T11:56,Data Engineer,LILITH GAMES SG PTE. LTD.,1 day ago,,Full–time,"Big Data Development Engineer (Advertising Direction)

Big Data Development Engineer (Advertising Direction)

What You Will Be Doing

1、Responsible for the construction of Big Data Cloud Platform and the maintenance of Advertising Data Warehouse.

2、Responsible for the calculation, cleaning and layering of Advertising Data based on Flink, Spark, etc., and store it through Hadoop, Clickhouse, etc.

3、Responsible for Advertising Data Analysis and Advertising System Report development.

Qualifications & Skills

1、Bachelor degree or above, major in computer and other related majors.

2、Proficient in Python or Golang coding.

3、Proficient with Mysql, Memcache, Redis, Message Queue and other common WEB components.

4、Having experience in using one or more of HDFS, Hive, HBase, MongoDB, Kafka, Flink and Spark is preferred.

5、Experience In Operation And Maintenance Development Is Preferred.

6、Familiar with Alibaba Cloud and other cloud computing resource deployment and optimization is preferred。

7、Proficient in coding complex SQL Statements, and have the ability and experience of query optimization.

8、Positive and optimistic, strong sense of responsibility, with good team communication and cooperation

大数据开发工程师（广告方向）

工作职责
• 负责大数据云平台搭建，广告数据仓库的搭建与维护。
• 负责基于Flink、Spark等对广告数据的进行计算、清洗、分层等工作。并通过Hadoop、Clickhouse等进行存储。
• 负责广告数据分析及广告系统报表的开发。

任职要求
• 大学本科(统招)及以上学历，计算机、通信等相关专业。
• 熟练掌握Python或Golang代码编写。
• 熟练使用Mysql、Memcache、Redis、消息队列等常用WEB组件。
• 有使用HDFS, Hive, HBase, MongoDB, Kafka，Flink, Spark中的一项或多项的经验优先。
• 有运维开发经验优先。
• 熟悉阿里云等云计算资源部署与优化者优先
• 熟练编写复杂的sql语句，具备查询优化的能力及经验。
• 积极乐观，责任心强，工作认真细致，具有良好的团队沟通与协作能力。"
29-Apr-2022 T11:56,"Data Engineer, Digital Innovation with Singapore Tech Giant",PeopleSearch,6 days ago,,Full–time,"Digital services company delivering innovative Products&Solutions is looking for a Data Engineer to join Digital Transformation center. Responsibilities: • Design, implement innovative solutions using statistical models, ML, other data mining techniques • Work closely with Data scientists defining, building enterprise data exchange platforms. • Handle challenges of real-world data analytics • Design, build, support and optimize new and existing data models and ETL processes. • Develop and support the data pipeline to integrate new data from various data sources • Research and experiment with new advances in analytical methods, algorithms and tools to deepen and develop new capabilities:deep learning, optimization.

Key requirements:
• Over 3 years of Python scripting, use of NumPy, Pandas and/ or scikit-learn
• Nice to have experience in Data Science, Analytics
• Data visualization tools expertince with Tableau, Power BI or packages in Python or Javascript
• Experience with RDBMS such as MySQL, PostgreSQL, MongoDB, MS SQL Server

Stack you will be exposed to (Good to have as an advantage) :
• Experience with deployment and maintenance of models in production, e.g. APIs with capped response time, performance monitoring, automatic retraining and tracking data/ model lineage
• Experience with cloud platforms (AWS, GCP, Azure)
• Experience with Docker, Kubernetes and other supporting container platforms/ tools
• Experience in data crawling including popular social APIs and web scraping
• Experience with application development, particularly full stack web development
• Experience with Hadoop"
29-Apr-2022 T11:56,Data Engineer/Scientist,PATSNAP PTE. LTD.,3 days ago,$4.5K–$8K a month,Full–time,"Company Introduction

PatSnap is a software company helping R&D leaders maximise the value of innovation intelligence within their R&D workflow and strategic planning. As the global leaders in connected innovation intelligence, Patsnap use AI-powered and machine learning technology to comb through billions of datasets, and help innovators connect the dots.

Job description

We are looking for a Data Engineer to join the revolution to help us improve various business outcomes and drive innovation. You will join a multidisciplinary team helping to shape our Product development. This is an excellent opportunity to take advantage of emerging trends and technologies to a real-world difference.

Responsibilities
• Study and transform various kind of structured and unstructured data
• Big data analytics
• Create data pipeline for regular data updates
• Analysing unstructured data using basic NLP techniques and extract important fields
• Store data in databases (nosql and graphdb)
• Develop NLP systems according to requirements
• Perform statistical analysis of results and refine models
• Remain updated in the rapidly changing field of machine learning
• Deploy models and create APIs

Requirements
• Proven experience as Data Engineer or similar role
• Experience with big data analytics libraries such as pyspark is must
• Experience with AWS and database technologies is plus
• Strong communication skills
• An analytical mind with problem-solving abilities
• Degree in Computer Science, Mathematics, Computational Linguistics or similar field
• Domain knowledge of Material sciences and/or chemistry is big plus"
29-Apr-2022 T11:56,Staff Data Engineer,Micron,5 days ago,,Full–time,"Our vision is to transform how the world uses information to enrich life for all.

Join an inclusive team passionate about one thing: using their expertise in the relentless pursuit of innovation for customers and partners. The solutions we build help make everything from virtual reality experiences to breakthroughs in neural networks possible. We do it all while committing to integrity, sustainability, and giving back to our communities. Because doing so can fuel the very innovation we are pursuing.

JR21781 Staff Data Engineer

Broad knowledge and experience in:
• Understanding of Big Data Engineering/processing, Business Intelligence and Advanced analytics
• Developing ETL/ELT processes
• Knowledge in databases and Data warehouse modeling
• Knowledge in Cloud based data engineering and Machine Learning Models
• Knowledge in building APIs for application integration
• Experience with various frameworks and processes, such as Agile
• Determine transformation requirements and develop processes to bring structured and unstructured data from the source to a new physical Data Model
• Work with Data Scientists to implement strategies for cleaning and preparing data for analysis, to develop data imputation algorithms, and optimize performance of big data and machine learning systems

Above average skills in:
• Big Data Engineering and Processing using Hadoop stack (Hadoop, Hive, HDFS, Spark and HBase etc.)
• Develop ETL/ELT processing using Apache Ni-Fi
• Strong background on SQL and databases
• Programming Skills in Python or Scala
• Data Analysis and Validation skills

Demonstrated ability to:
• Work in a dynamic, fast-paced, work environment
• Self-motivated with the ability to work under minimal direction
• To adapt to new technologies and learn quickly
• A passion for data and information with strong analytical, problem solving, and organizational skills
• Work in multi-functional groups, with diverse interests and requirements, to a common objective

Communicate very well with distributed teams (written, verbal and presentation

About Micron Technology, Inc.

We are an industry leader in innovative memory and storage solutions transforming how the world uses information to enrich life for all . With a relentless focus on our customers, technology leadership, and manufacturing and operational excellence, Micron delivers a rich portfolio of high-performance DRAM, NAND, and NOR memory and storage products through our Micron® and Crucial® brands. Every day, the innovations that our people create fuel the data economy, enabling advances in artificial intelligence and 5G applications that unleash opportunities — from the data center to the intelligent edge and across the client and mobile user experience.

To learn more, please visit micron.com/careers

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.

Please note that in order to assist in providing a safe and healthy workplace for all Micron team members, new employment offers for jobs based in India, Malaysia, Singapore, and the U.S., are contingent upon the applicant’s provision of a copy of their COVID-19 vaccination document to Micron on a confidential basis prior to their scheduled start date confirming that they have completed the COVID-19 vaccination process, subject to any written request for medical or religious accommodation and to the extent permitted by applicable law.

To request assistance with the application process and/or for reasonable accommodations, please contact hrsupport_in@micron.com

Micron Prohibits the use of child labor and complies with all applicable laws, rules, regulations, and other international and industry labor standards.

Micron does not charge candidates any recruitment fees or unlawfully collect any other payment from candidates as consideration for their employment with Micron"
29-Apr-2022 T11:56,Data Engineer,Klook,6 days ago,,Full–time,"At Klook, we love creating moments of joy. Our platform connects people around the world with experiences that bring a smile to their faces, at a touch of a button. We are a global team of diverse Klookers who push boundaries every day, learn fast with feedback and take ownership to drive the change we want to see. Together, we help each other make the world a more joyful place. Up for the challenge Join us today! About this role Our Data Engineer is pivotal in working on data ETL pipelines and data integrations as well as other big data systems. This role reports into our Data team lead and will be based in our Singapore office. Our ideal candidate should be familiar with and passionate about big data technologies, strong communication skills, quick learner, and has attention to detail. About Technology & Engineering In a fast-growing industry like ours, we can't afford to stand still. At Technology & Engineering, we constantly test and improve our products to create the best experience in the travel and leisure industry. The team hires curious and analytical people who are always to push boundaries and have real impact. What you'll do Design and develop solutions to store and process data on cloud platforms Design and develop ETL processes to ingest data into data warehouse, and ensure data accuracy and stability Participate in building big data platform that processes data at scale Work on data streaming systems and frameworks to ensure timely delivery of accurate data Work on cloud-native data infrastructure, optimize data performance, implement data monitoring and alert tools, and manage various data workloads Create scripts and workflows to automate repeated data processing tasks and simplify complex task flows Assisting in quality control of quantitative and qualitative research projects Participate in peer code reviews and produce high quality documentation What you'll need BS/MS or higher degree in mathematics, computer science or other technical/quantitative discipline Ideally 2 years experience in data development and engineering. Fresh graduates may be considered for the role. Familiarity with Hadoop, Hive, HBase, Flink, Hudi, Airflow, Kafka, etc Exposure to big data solutions from Amazon AWS/ Google Cloud, such as Redshift, EMR, BigQuery, Dataflow, Composer, etc. Proficient in SQL, and familiar with Java and Python A spirit of collaboration and transparent communication High personal code/development standards (peer testing, unit testing, documentation, etc) Fully vaccinated from COVID-19 Klook is proud to be an equal opportunity employer. We hire talented and passionate people of all backgrounds. We believe that a joyful workplace is an inclusive workplace, one where employees from all walks of life have an equal opportunity to thrive. We're dedicated to creating a welcoming and supportive culture where everyone belongs. Klook does not accept unsolicited resumes from any temporary staffing agency, placement service or professional recruiter (Agency). Klook will not be responsible for, and will not pay, any fees, commissions or other payments related to such unsolicited resumes. An Agency must obtain advance written approval from Klook's Talent Acquisition Team to submit resumes, and then only in conjunction with a valid fully-executed agreement for service and in response to a specific job opening for which the Agency has been requested to submit resumes for. Klook will not be responsible for, and will not pay, any fees, commissions or other payments to any Agency that does not have such agreement in place or does not comply with the foregoing"
29-Apr-2022 T11:56,Data Engineering Team Lead,Endowus,3 days ago,,Full–time,"About us
Endowus is Asia's leading fee-only digital wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping all investors grow their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.
Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones. Our mission is clear: help people invest better so they can live easier today, and better tomorrow.

The people you will work with
The team has deep domain knowledge in finance and technology, bringing together decades of experience at Goldman Sachs, Morgan Stanley, UBS, Credit Suisse, The Blackstone Group, AQR, Grab, Dropbox, Lyft, Redmart, Carousell, Bytedance, Grab, Kakao, Alibaba, and more See our leadership team here . We practise inclusion and treasure our diversity in background and experience. A diverse team is our biggest asset and we look for people who share our belief in Endowus' clear mission.

Investors, recognition, licensing
Endowus is backed by global leading strategic and venture capital investors including UBS, Samsung Ventures, EDBI, Prosus Ventures, ZVC, Singtel Innov8, Lightspeed Venture Partners, and SoftBank Ventures Asia.

Endowus has been recognised by the industry with the following awards: Singapore's Rising Star and Fintech Innovation (Asia Asset Management's Best of the Best Awards 2021), LinkedIn Top Start-ups 2021, WealthTech of the Year (Asia FinTech Awards 2021), and the top 15 Singapore's Best Workplaces Award (Great Place to Work).
Endowus is licensed by the Monetary Authority of Singapore (MAS).

About this team
The Data Engineering Team builds and operates the scalable data platform that powers data analytics and business intelligence for better decision making at Endowus. Working with colleagues in Data Analytics, Growth, Marketing, and Operations teams, the data engineering team creates data solutions that provide them with performant, near real time access to internal & third party data and insights.
We are a full stack team that builds our systems using cloud native patterns and operates them with high standards of engineering & operational excellence.
About this role; responsibilities & ownership
• Own the technical design, delivery, reliability & security of our core data platform.
• Lead, mentor and grow a small talented team of data engineers.
• Work closely with the Product team, other Engineering teams, and stakeholders in Data Analytics, Growth, Marketing, Operations, Compliance & IT Risk to define and prioritise our data platform roadmap.
• Empower the Data Engineering team to achieve high levels of technical quality & reliability. Requirements & qualifications
• Bachelors' or above in Computer Science, a related field, or equivalent professional experience.
• At least 6 years experience in designing and implementing highly scalable, distributed data collection, aggregation, and analysis systems built for handling large volumes of data in the cloud.
• At least 2 years experience as a tech lead facing business users directly while directly or indirectly managing the performance & delivery of the team
• Strong team and project leadership with a history of leading technical initiatives
• Significant hands-on experience building and optimising data pipelines for data collection, transformation, aggregation in Apache Flink or Apache Spark, using dependency and workflow management tools such as Airflow, operating in a public cloud environment like AWS, GCP or Azure.
• Advanced SQL knowledge and strong experience working with relational and non-relational databases.
• Experience integrating BI tools such as Tableau, Mode, Looker, etc.
• Experience integrating data sources using REST and streaming protocols, especially using Kafka.
• Strong grasp of at least one of the JVM languages such as Java, Scala.
• Experience with building systems & processes to handle data quality, data privacy, and data sovereignty requirements.
• Experience with agile processes, testing, CI/CD, and production error/metrics monitoring.
• Self-driven with a strong sense of ownership.
• Comfortable with numbers and motivated by steep learning curves.
• Has a strong product sense and is empathetic to customers' experiences of using the product. Nice to haves
• Domain experience in a B2C context is a strong plus.
• Knowledge of finance, wealth, and trading domain.
• Some exposure to CQRS / Event Sourcing patterns.
• Experience with AWS or GCP, Cassandra, Kafka, Kubernetes, Terraform. Benefits & perks
• Stock options in a fast growing company *
• Employee discount for investing at Endowus
• Flexible working hours and locations so you can live life and your best work - you are trusted to be responsible
• Competitive Staff Benefits; Annual Leave, Medical insurance, Dental *
• Productivity Equipment; Omnidesk Standing Desk & Ergonomic Chairs, Dell UltraSharp USB Type-C Monitor, high-end computer peripherals
• Choice of Apple or PC equipment
• Additional S$250 /HKD 1450 budget for your favourite hardware to boost productivity * Note: * is only applicable to Full-Time employees"
29-Apr-2022 T11:56,Data Engineer,PSA Singapore,4 days ago,,Full–time,"You will work in the Data Engineering team in Infocomm Technology & Data Division to:-

• Design, Develop, Test, Deploy and Maintain data pipelines (ETL) on the Enterprise Data Warehouse and Big Data Platform
• Design and Develop the API /Web Services framework for curation of new datasets whether internal or external (Internet), and to interface with other systems (both internal and external)
• Explore and source new data sets to address emerging business use case needs
• Maintain the data quality and keep improving the data efficiency

Requirements:

• Possess a good Bachelor’s degree in Computer Science or Computer Engineering; a Specialization in Software Engineering will be advantageous.
• Those with 2-3 years of related work experience will be preferred.
• Good grasp of Software Engineering principles such as Requirements Gathering (both functional and non-functional), Modular & Re- usable Design.
• Proficient in ETL using programming language /tools such as Python and/or SSIS and/or Informatica Power Centre
• Able to develop data applications including integration with ICT systems, build APIs and web applications via .Java and/or Python
• Familiarity with MS SQL, PostgreSQL or Oracle is preferred.
• There will an added advantage for any of the following:-
o Proficient in Data Modelling and Data Mining.
o Experience in designing and building scalable database schema for applications.
o Understanding of Object-Oriented Design.
o Knowledge of or prior work experience on Big Data platforms such as Hadoop or using Spark.
o Experience in the cloud environment setup using Microsoft Azure"
29-Apr-2022 T11:56,Data Engineering Tech Lead,Rakuten Asia,3 days ago,$9K–$14K a month,Full–time,"The Global Data Supervisory Department (aka GDSD) oversees the development and operation of our data platform in Rakuten group. We provide data products and platforms for Rakuten’s line of businesses, as well as technology solutions.

GDSD services are important to Rakuten as it has big contribution to Rakuten's profit and being one of the key drivers for growing various platform businesses of Rakuten Group including our e-commerce platform business.

We are looking for Technical Lead – Data Engineering, who is passionate to deal with complex technical challenges and who are interested to work closely in the fast pacing business environment.

Key Responsibilities:
• Collaborate with stakeholders, business development team, architect, project manager and data engineers to understand business requirement & questions, processes, and related data, and convert the information into measurable technical design & deliverables and achieving tangible business goals
• Work with the leadership to set the standards for data engineering practices within team and support across other disciplines
• Lead decision-making process for selection of architecture design & solutions, use the right analytical libraries, tools & technologies, frameworks and google cloud services in delivering sustainable and scalable ETL pipelines and data platforms
• Help and mentor Data Engineering team produce high-quality code following agile methodology, advocating code quality, enhancing automation testing coverage, and deployment approaches
• Good oral and written presentation skills, produce useful documentation, organizing workshops, and brownbag sessions to promote product adoption & utilization, provide the essential support to the stakeholders for the products user journey, and a good story-telling skills
• Develop, ship, and monitor data platforms & cloud infrastructure using best CI/CD (DevOps), DataOps and monitoring tools & optimization best practices
• Passionate about emerging technologies and early adoption
• Help us to shape the next generation of our products

Essential Competencies :
• Graduate degree educated in computer science or a relevant subject.
• 8+ years of experience in developing & delivering Advance Analytics solutions, Data Warehousing, Big Data Platforms, ETL /ELT data pipelines (both batch and streaming), and data modeling
• 5+ years of solid experience in architecting, developing, monitoring, and optimizing data platform and pipelines using public cloud services (preferably google cloud platform or aws big data services)
• 5+ years of solid experience in Big Data Technologies: Apache spark, Apache Airflow, Apache Beam, Kafka, HDFS, HIVE, and/or big data services in any public cloud (preferably gcp BigQuery, Dataflow, PubSub, DataProc, gcs, Cloud Composer, Cloud Catalog)
• 5+ years of solid experience in architecting, monitoring, and optimizing cloud infrastructure usage for data platforms in public cloud environment and strong experience & hands-on knowledge in cloud IAM, infrastructure security, monitoring, IAAS (infrastructure As A Code, Terraform preferred), networking, cloud cost reduction techniques (preferably google cloud platform or aws)
• 8+ years of solid experience in writing, reviewing, and optimizing code using SQL, python, java
• 2+ years of experience in any data visualization tool (preferably google data studio)
• 2+ years of experience in docker, Kubernetes, and Container registry & CI/CD for containers (preferably google cloud services, GKE, Cloud registry and Apache-spark on Kubernetes setup)
• Hands-on experience in using JSON, YAML, parquet & snappy, git, github-actions, Terraform, JIRA ( Epic, Stories, Reports etc.)
• Experience in architecting and building APIs (REST preferably), Serverless (functions), NoSQL DB and event-driven platforms is advantage

Rakuten is an equal opportunities employer and welcomes applications regardless of sex, marital status, ethnic origin, sexual orientation, religious belief or age"
29-Apr-2022 T11:56,"Consultant, Appl Architect",Singtel Group,18 days ago,,Full–time,"Data Engineer (Database Administrator)

The Data Engineer (Database Administrator) is responsible to perform database Administration activities such as database setup, defining/maintaining database objects, backup and recovery. He/She is also heavily involved in database design and review, application SQL tuning and maintaining documentation meeting customer’s expectation. As the experience in this position grows, there will be opportunities to expand to database consultancies areas such as performance tuning, data governance and data policies.

Key Responsibilities :

- Understand the basic concepts of database management

- Understand and follow user work processes

- Understand the requirements from the application team for defining objects

- Perform simple database tasks such as defining objects, conduct backup and recovery.

- Perform SQL tuning and provide suggestions to application teams

- Maintain documentation related to database environment

- Work with application team and setup database environment based on customer policy and application requirements

- Trouble-shoot technical problems faced by the project team

- Assist and respond promptly to incident, investigate & provide temporary &/or permanent resolution of incidents escalated. Provide timely status updates to relevant parties.

- Adhere to strict security practices for compliance

- Manage customer expectation in terms of schedule

- Recommend changes to existing procedures and policies

- Writing scripts/programs for service monitoring and health check of all systems

- Day-to-day monitoring, backup, deployment and maintenance of all databases

- Depending on project requirement, there is a need to support 24x7 requirement. DBA is expected to come back to office and work.

The successful candidate shouls possess:

- Minimum 3 years in software development industry or server infrastructure Project

- Application development experience with relational databases such as SQL server Databases.

- Should have participated in development projects. Participated in data modelling and review meeting.

- Hands on experience in trouble-shoot technical problems faced by the project team.

- Certification in SQL server or other database technologies is value add.

- Experience in setting DB replication and Replication management environment is value add.

- Involved Application performance Tuning and database Performance management is value add.

- Experience with Cloud DB Administration is a plus

- Additionally, working knowledge of NOSQL DB such as Mongo, MariaDB, MySQL is value add.

We are committed to a safe and healthy environment for our employees & customers and will require all prospective employees to be fully vaccinated"
29-Apr-2022 T11:56,Software Engineer - Data Platform,Twitter,4 days ago,,Full–time,"Who We Are

As engineers on the Data Platform team, our mission is to build the fastest, most reliable, and largest-scale data processing technologies in the world - able to cope with ever-increasing volumes of data in real time - and then apply them to the companys most critical and fundamental data problems.

As a member of the team, you will be the source of truth for Twitters most fundamental data - such as Tweet content, engagement data, and user relationships - along with core metrics such as daily and monthly active users.

You will surface these datasets in real time to mission-critical products and business applications throughout the company.

You will empower dozens of engineering teams, hundreds of co-workers, and millions of users to dream of new insights and new possibilities.

What You'll Do

If this sounds like a team you want to be a part of, fantastic! We are looking for experienced engineers who love writing code, data engineering, understanding our customers, and collaborating with teammates to ship useful software.

Sample projects weve built :
• Real-time aggregations of interactions on tweets at 5M / sec scale;
• Unify our batch processing pipelines that count and validate user activity;
• Use hidden Markov modelling to categorize users tweeting states.

Who You Are

You take satisfaction in building resilient, performant, and thoroughly tested distributed systems that can power the most business-critical applications.

You get stuff done and thrive in a small group environment. You have a strong sense of ownership and a curiosity to understand how things work, even if they take you outside your area of expertise.

You welcome feedback on are constantly looking for ways to improve yourself.

Qualifications

On our team, we need people who :
• Have 1+ years of relevant professional experience;
• Have backend development experience with a solid foundation in data pipelines, distributed systems, large-scale data processing;
• Have proficiency with Scala, Java, C / C++ or Python;
• Show deep understanding in at least one data processing framework including Hadoop, Spark, Flink, KafkaStreams, or Dataflow;
• Enjoy working with our internal customers and having empathy for their problems;

Embrace a growth mindset and want to improve ourselves, the team, our processes, and the products we work on.

Additionally it would be nice if you had :
• Success in developing in a hybrid-cloud environment;
• Experience with Lambda architectures and different ways of implementing them;
• Working knowledge of ETL and a query language;
• Experience with on-call responsibilities.

Company Description

Twitter is whats happening and what people are talking about right now. For us, life's not about a job, it's about purpose.

We believe real change starts with conversation. Here, your voice matters. Come as you are and together we'll do what's right (not what's easy) to serve the public conversation.

Additional Information

All your information will be kept confidential according to EEO guidelines.

Twitter is committed to inclusion.

We are committed to an inclusive and diverse Twitter. Twitter is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status.

San Francisco applicants : Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records"
29-Apr-2022 T11:56,Senior Data Engineer,Lynx Analytics,24 mins,,Full–time,"COMPANY OVERVIEW

Founded in 2010, Lynx Analytics is a predictive analytics company run by world-class quantitative marketing scientists and industry-experienced data scientists. Our focus is to become a leading analytics solution provider in our chosen fields of expertise (telecom, retail, life sciences, and financial services) while advancing graph analytics technology.

Lynx is headquartered in Singapore with operations in Hong Kong, Germany, USA, Hungary, South Africa, Indonesia, and several other Southeast Asian countries. We work with some of the world’s largest companies and are constantly looking to expand our knowledge base and geographical footprint. Lynx Analytics’ technology is deployed with various Clients across Asia and has significant growth potential.

We have a diverse and inclusive global team comprising Professors, PhDs, MSc’s, and MBAs from Ivy Leagues, INSEAD and NUS with a broad spectrum of experience in start-ups and blue-chip companies (Google, SAP, Vodafone GE, Morgan Stanley, Barclays, HSBC to name but a few). It is the combination of our industry insight and experience, scalable proprietary technology, and highly qualified people that drives our compelling value proposition.

We are looking for ambitious, innovative, empathetic and relentless team players to explore the career opportunities that we offer as we continue to scale our operations.

As our Senior Data Engineer, you will work on automating and productizing advanced big data transformation and analytics pipelines. You would be working with standard big data technologies (Hadoop, Spark, etc) as well as our proprietary big graph analysis framework.

KEY RESPONSIBILITIES

A Senior Data Engineer’s responsibility is to implement and deploy data analysis pipelines at various clients of Lynx Analytics. This includes participating in and supervising the activities below:
• Understand deeply the business problem that we are trying to solve by our analytical solution
• Through continuous consultations with employees of our client, discover the client's existing data sources that are relevant to the problem we try to solve. This includes discussions with client IT, data owners, future business users, etc.
• Working together with the IT teams of the client, define the technical architecture for the analytical solution that we are to deploy for the client.
• Implement the data ingestion subsystem: this is the system responsible for moving all the necessary data sources to a single location where the actual analysis will happen.
• Implement the data analysis pipelines.
• Integrate the results into business UIs developed by Lynx or pre-existing client software systems

REQUIREMENTS
• Relevant tertiary qualification, preferably at Masters level or above, in Engineering or another relevant discipline with strong academic results
• Strong programming skills
• Solid knowledge of Python and Java (or better yet, Scala)
• Good understanding of the Linux OS including basic sysadmin and shell scripting abilities
• SQL
• Experience in project delivery in a B2B setting
• Good problem-solving skills
• Fluency in English
• Willingness to travel

DESIRABLE
• Experience in Big Data
• A minimum of 6 years of experience in Data Science or Analytics
• Industry experience in working for a big enterprise (like our clients)

WHAT WE OFFER
• Opportunity to work on creating innovative, leading-edge data science pipelines using our state of the art, in-house built big graph tool
• Work closely with the developers of the (big graph) tool you will be building upon
• Be a member of a very strong team with mathematicians, ex-Googlers, Ivy League professors, MBA alumni and telecommunications industry experts
• Startup atmosphere
• Competitive salary
• Equity incentives for employees
• Opportunity to travel (Southeast Asia, US, and Europe)
• Flexible working hours, family-friendly workplace"
29-Apr-2022 T11:56,Data Engineer - APAC,Tamr,22 hours ago,,Full–time,"Company Description

Tamr is the enterprise data mastering company trusted by large enterprises like Blackstone, the US Air Force, Toyota, and GSK. The company’s patented software platform uses machine learning supplemented with human feedback to master and prepare data across myriad silos to deliver previously unavailable business-changing insights. With a co-founding team led by Andy Palmer (founding CEO of Vertica) and Mike Stonebraker (Turing Award winner) and backed by top-tier investors such as NEA and GV, Tamr is transforming how companies get value from their data.

Job Description

Tamr DataOps Engineers are highly technical data scientists and engineers who understand all aspects of the business. We know how to pitch the product to anyone from software architects to business executives. Often we’re working on the most challenging problems in the enterprise and we have to dig deep to understand the business value and deliver technical solutions. As the company’s outward-facing, technical resource, the work of the whole group is varied. We build prototypes, direct product development, work with sales on accounts, implement pilot solutions, and deploy full production rollouts. DataOps Engineers are expected to be highly efficient and resourceful when leading these projects.

We are a US Based company but rapidly expanding in APAC, and we are looking for a DOE

who thrives working with our existing local partners and customers, as well selling, onboarding and working hand in hand with the next wave of Tamr prospects.

We are looking for someone who loves to write code, and has a genuine interest in being

customer-facing. We are building and selling a human-in-the-loop machine learning platform for the enterprise to help companies automatically unify and categorize their data to drive new analytic and operational insight. We are looking for engineers interested in creating front line code to solve real customer problems, ranging from back-end data processing and machine learning, to front end presentation and dashboarding.

Challenges that make this job interesting:
• The problem we’re solving is hard - enterprise data is messy and there is a lot of it. It’s our job to derive value from this data in a flexible and scalable way
• Every customer is different - while there are similar use cases that we see repeatedly, every account presents new challenges and we need to be able to adapt quickly to each new situation

This job might be a good fit for you if:
• You have strong data science and/or software engineering experience
• You are excited about working for a startup and being a key contributor in a new office
• You enjoy working with customers and have excellent interpersonal skills
• You enjoy educating prospects on the the data landscape, and the Tamr solution by means of tailored product demonstrations - target audiences include end users, IT staff, executive sponsors, industry analysts
• Defining and scoping the deliverables of proof of concept projects with clients, and then leading these exercises
• Enabling partners and accelerating the local market by multiplying your impact through others
• You have machine learning knowledge/experience
• You understand the value of data and how it can truly transform an organisation
• You’ve dealt with lots of data - messy, siloed and disparate data sources - and understand how to develop pipelines to clean and get this data ready for analysis
• You’re a great story teller - you can take large amounts of an organisation’s data and develop visualisations and presentations using that data to highlight insights and specific actions the organisation can take to improve. This is to both technical and business audiences.

Qualifications:
• BS, MS or PhD degree in Computer Science / Software Engineering, Physics, Mathematics or similarly quantitative/technical field
• Polyglot programmer, with experience using technologies such as Python, Java, R, SQL
• Willingness and ability to travel to client locations on occasion

Other Preferred Qualifications / Nice to Have:
• Deep understanding of data integration and transformation patterns such as messaging, ETL.
• Hands-on experience with enterprise and corporate systems from a data architecture and development perspective
• Hands-on experience with traditional data warehouse technologies and BI or visualization tools
• Machine learning knowledge/experience
• Front end software development expertise. JS in particular
• Experience building enterprise applications, including integration with COTS systems
• Experience with any of the following technologies: Hadoop, Spark, ElasticSearch, Java, Cloud Partners
• DevOps and/or cloud solutions deployment experience a plus
• Advanced quantitative technical degree (MS or PhD) preferred

Additional Information

This position is available for candidates in the APAC region and will be remote.

Tamr provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws"
29-Apr-2022 T11:56,Data Engineer,MANN+HUMMEL VENTURES PTE. LTD.,4 days ago,$8K–$9K a month,Full–time,"Your challenge
• Working cross-functionally with business managers/product managers/engineers and data scientists to gather requirements and to understand their business processes
• Design, develop, deploy, manage scalable cloud infrastructures that solve business problems
• Designing, implementing, and managing optimal data analytics pipelines from end-to-end
• Making strategic data architecture recommendations
• Implementing data science frameworks to enable organization-wide experiments, enable data science research and development and enable deployment of production solutions
• Implementing systems that enable delivery of insights to business units and customers
• Apply dev-ops processes to deliver and maintain production level systems
• Apply quality processes to ensure the requirements are met
• Implement and manage security in accordance with industry standards
• Document all aspects of design, implement, test and release

Your profile

As a successful applicant, you would have a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field with the ability to manage stakeholders and communicate well. You will have strong experience in cloud technologies (AWS/Azure or similar) with at least 5 years of experience in a Data Engineer role.
• A history of working with large scale reliable data systems
• Proficient with cloud technologies and application of native services
• Experience with big data tools and delivery of big data solutions
• Experience in working with different types of data stores including SQL, NoSQL, warehouses, lakes, etc
• Experience working in Hadoop ecosystem and Spark is a plus
• Experience with data ingestion and data integration tools and frameworks, data pipeline and workflow management, common data science tools such as R, python, Big data technologies, Tableau
• Proven ability to deliver high profile activities to tight timescales
• Proven ability to apply analytical and creative thought
• Ability and desire to learn and pick up new tools and technologies for problem solving, enhancing analysis results and accuracy, and optimizing workflow efficiency
• Independent and possess creative problem solving skills to address business problems from different perspectives
• Ability to distil and communicate results to all organisational levels
• Practice a lean agile scrum process to continuously deliver value to customers
• Proven success in contributing to a team-oriented environment
• Ability to interact with global teams and manage the related cultural challenges
• Be able to interact with development teams to determine project requirements

Please indicate in your resume, the position you are applying.

Are you full of ideas? Are you keen to take on responsibility and really achieve something? Then our doors are open to you. This is a company that lives out its values, gives people the freedom to use their own initiative, offers many development opportunities and many exciting projects – all of which awaits you here"
29-Apr-2022 T11:56,Data Engineer (C# .Net Engineer) -Only Local,Capgemini Singapore Pte Ltd,8 days ago,,Full–time,"Hi Greeting!! .Net Developer Experience:5+ Years Location: Singapore Roles: 4+ years of experience as a C#/.NET Developer Backend Developer in C# or .NET (Engineer background is not a must and client is not interested in front end experience such as ASP.NET/Web API) Good understanding and have experience in entity framework (perhaps also Linq) Expecting the candidate can present / articulate the related technical concepts and how-to implement to show the capabilities to perform hands-on job Can work independently on the tasks assigned with limited support Excellent knowledge and hands-on skills in C#. NET WinForms, Multithreading, TPL, LINQ etc Experience of working on third party libraries like Sync fusion, Infragistics or DevExpress UI libraries and Log4Net Unit testing, integration testing, behaviour testing, end to end testing with JUnit, Mockito, Cucumber, etc. Experience using relational databases and SQL (such as PostgreSQL or Oracle) Nice to have Rest API, Microservice knowledge of at least one specific asset class / line of financial instruments would be valued"
29-Apr-2022 T11:56,"Financial Services Advisory, Data Engineer",KPMG,3 days ago,,Full–time,"As KPMG embarks on a rapid growth strategy to become the preferred data driven strategy execution firm in South East Asia, we established the propositions in data driven transformation, and cloud & data center-of-excellence for all the different line of businesses to serve our clients. The ideal candidate should have experience in digital and data driven transformation while working for cloud providers, consulting firm or financial services organisations.

This role involves:
• Work closely with clients and stakeholders to solve enterprise problems like Database Migrations, Cloud Native and Serverless Applications, Big Data and Analytics Solutions
• Translate business requirements & end to end designs into technical implementations and responsible for building batch and real-time data warehouse
• Build robust batch and streaming data pipelines for production-grade data products /platforms, ensuring scalability and reliability
• Create web services or APIs to connect and integrate analytical stacks to application layers
• Build and maintain both cloud and on-premise data infrastructure
• Manage data modelling design, writing, and optimizing ETL jobs
• Data mining analysis and processing ability, participate in data access, inspection, optimisation and other processing processes
⠀
The ideal candidate should possess:
• Bachelors or Masters in Computer Science, Computer Engineering, Information Systems, or similar IT related courses or Exceptional candidates with Diploma / Specialist Diploma in Informatics / Analytics / Machine Learning / Data Analytics
• 1 – 6 years of experience in ETL, data pipeline building, and data warehousing
• Proficient in designing efficient and robust data integration workflows using at least one of Informatica, Datastage, SSIS, Ab Initio, Talend, etc.
• Exposure to Database technologies such as Oracle, DB2, Ms SQL, MySQL, Postgress, Informix, etc.
• Solid knowledge in data processing, like Hadoop, Mapreduce, Hive, Storm, Spark, Kylin, Scribe, Kafka, HBase, Canal, Sqoop etc
• Proficient in SQL, R and Python
• Advanced understanding of database principles, security, and administration
• Familiar with SQL, dimensional modelling, data warehousing, and data integration
• Familiar with system architecture design and analysis
• Competent in software engineering and agile development
• Able to implement and maintain components of big data technologies for both exploratory and production data science platforms
• Strong client and stakeholder management abilities coupled with excellent communication, written, analytical, organisational and problem-solving skills"
29-Apr-2022 T11:56,Data Engineer 3 - AWS & Python (Contractual),The Economist,1 day ago,,Full–time,"The Economist Intelligence Unit (EIU) is a world leader in global business intelligence. We help businesses, the financial sector and governments to understand how the world is changing and how that creates opportunities to be seized and risks to be managed.

At our heart is a 50 year forward look, a global forecast of the majority of the world’s economies, we seek to analyse the future and deliver that insight through multiple channels and insights, allowing our clients to take better trading, investment and policy decisions.

We’re changing, embedding alternate data sources such as GPS and satellite data into our forecasting, products will increasingly be tailored to individual clients, driven by some of the most innovative data in the market. A highly collaborative team of Product Managers, Customer Experience and Product Engineering is being created with a focus on creating business and customer value driven by real time analytics alongside our traditional products.

What will you experience

At Economist Intelligence Unit (EIU) we believe having the right work-life balance is super important; striking balance between your personal and professional life is critical to wellbeing and happiness. We offer flexible working and have recently shifted to a 'remote first' working policy with a minimum expectation of coming to the office two days a month, however you can come in more often if you wish to.

How you will contribute:
• Build data pipelines: Architecting, creating and maintaining data pipelines and ETL processes in AWS via Python, Glue and Lambda
• Support and Transition: Support and optimise our current desktop data tool set and Excel analysis pipeline to a transformative Cloud scale Big Data Architecture environment.
• Work in an agile environment: within a collaborative agile product team using Kanban
• Collaborate across departments: Work in close relationship with data science teams and with business (economists/data) analysts in refining their data requirements for various initiatives and data consumption requirements.
• Educate and train: Required to train colleagues such as data scientists, analysts, and stakeholders in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases.
• Participate in ensuring compliance and governance during data use: To ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives.
• Become a data and analytics evangelist: This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals.

To succeed in this role it would be an advantage if you possess:
• Experience with programing in Python, and Lambda functions
• Knowledge of building bespoke ETL solutions, and extracting data using Data APIs
• MS SQL Server (data modelling, T-SQL, and SSIS) for managing business data and reporting
• Prior experience in design and developing microservice architecture
• Ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management.
• A combination of IT skills, data governance skills, analytics skills and economics knowledge
• An advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (postgraduation diploma or related) or a related quantitative field or equivalent work experience.
• Experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms"
29-Apr-2022 T11:56,Big Data Engineer,PERSOLKELLY SINGAPORE PTE. LTD.,4 days ago,$7K–$13K a month,Full–time,"Responsibilities
• Develop data processing pipelines for ingestion, modelling, analysis, mining and reporting with Enterprise Big Data Lake
• Responsible for the code writing of the core module of the system
• Develop POC and build data pipeline architecture using of the overall technical framework of the software
• Work closely with teams ensure timely delivery of assignments

Requirements:
• Experience building large scale enterprise data pipelines using commercial and/or open source Big Data platforms from vendors such as Hortonworks/Cloudera, MapR, for Hadoop based platforms or NoSQL platforms such as Cassandra, HBase, DataStax, Couchbase, Elastic Search, Neo4j etc
• Hands on experience in Spark, Scala, Impala, Hive SQL, Apache Nifi necessary to build and maintain complex queries, streaming and real-time data pipelines
• Data modelling and architecting skills including strong foundation in data warehousing concepts, data normalisation, and dimensional data modelling such as OLAP

To Apply:

Interested candidates, who wish to apply for the above position; please send in your resume to TOS3@persolkelly.com or click the ""Apply Now"" below and ""ATTN: BVIN""

We regret that only shortlisted applicants would be notified.

B Vidita Nantini | REG No : R22105644

PERSOLKELLY SINGAPORE PTE LTD | EA License No : 01C4394

By sending us your personal data and curriculum vitae (CV), you are deemed to consent to PERSOLKELLY Singapore Pte Ltd and its affiliates collecting, using and disclosing my personal data for the purposes set out in the Privacy Policy which is available at www.persolkelly.com.sg I also acknowledge that I have read, understood, and agree to the said Privacy Policy"
29-Apr-2022 T11:56,Algorithm Engineer - Customer Lifetime Value (CLV),Shopee,Full–time,,,"Responsible for user CLV prediction, establishing real-time prediction model, and responsible for basic algorithm and strategy research of CLV system in multiple scenarios.Responsible for the feature engineering of algorithms, continue to expand business tags, mine user portraits, and continuously improve prediction accuracy.Build and optimise the algorithm architecture, improve forecasting efficiency, and develop data products and decision-making tools based on business scenarios."
29-Apr-2022 T11:56,"Associate Data Engineer, Data & Analytics - Technology Consulting",EY,6 days ago,,Full–time,"At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all.

The opportunity

EY DnA is the data and advanced analytics capability within EY Asia-Pacific, with over 500 specialist employees working across multiple industry sectors.

We implement information-driven strategies, data platforms and advanced data analytics solution systems that help grow, optimize and protect client organizations. We go beyond strategy and provide end to end design, build and implementation of real life data environments and have some of the best architects, project managers, business analysts, data scientists, big data engineers, developers and consultants in the region.

We are looking for a Data Engineer within the DnA team in our Singapore office. This role is offered on a flexible full time basis.

Your key responsibilities
• Strong analytical and problem-solving skills
• Strong drive to excel professionally, and to guide and motivate others
• Advanced written and verbal communication skills
• Dedicated, innovative, resourceful, analytical and able to work under pressure
• Foster an efficient, innovative and team-oriented work environment

Skills and attributes for success
• Experience in ETL, Data Engineering, Scripting.
• Knowledge and experience in end-to-end project delivery, either traditional SDLC or agile delivery methodologies (or hybrid approaches)
• Experience in a delivery role on Business Intelligence, Data Warehousing, Big Data or analytics projects
• Exceptional communication, documentation and presentation skills and stakeholder management experiences
• Experience in business intelligence, data warehousing/platform, and data strategy projects

To qualify for the role, you must have
• At least 2 years’ experience as a Data Engineer with experience in development and maintenance support
• Minimum 2yrs experience in dealing large data sets in Greenplum / Hadoop / Oracle / DB2
• Experience in shell /batch scripting
• Data Migration experience in AWS, Azure, Hadoop, GCP environments
• Experience in development and maintenance of Data processing pipelines
• Experience developing machine learning workflows
• Work closely with business analysts to create data components
• Application packaging and deployment experience across DEV to PROD environments

Ideally, you’ll also have
• Dashboarding experience with Tableau / Power BI
• Experience in engaging with both technical and non-technical stakeholders
• Consulting experience and background, including engaging directly with clients
• Degree in Computer Science or IT or Business Analytics

What we look for
• Highly motivated individuals with excellent problem-solving skills and the ability to prioritize shifting workloads in a rapidly changing industry.
• An effective communicator, you’ll be a confident leader equipped with strong people management skills and a genuine passion to make things happen in a dynamic organization.

What we offer

EY offers a competitive remuneration package commensurate with your work experience where you’ll be rewarded for your individual and team performance. We are committed to being an inclusive employer and are happy to consider flexible working arrangements, where this may be needed, guided by our FWA Policy.

Plus, we offer:
• Continuous learning: You’ll develop the mindset and skills to navigate whatever comes next.
• Success as defined by you: We’ll provide the tools and flexibility, so you can make a meaningful impact, your way.
• Transformative leadership: We’ll give you the insights, coaching and confidence to be the leader the world needs.
• Diverse and inclusive culture: You’ll be embraced for who you are and empowered to use your voice to help others find theirs.

If you can demonstrate that you meet the criteria above, please contact us as soon as possible.

The exceptional EY experience. It’s yours to build.

Apply now"
29-Apr-2022 T11:56,Senior Data Engineer,Aon,23 days ago,,Full–time,"We're hiring! Aon's Center for Innovation & Analytics (ACIA) is currently recruiting a Senior Data Engineer to join our Health Analytics team in Singapore. About ACIA Aon's Centers for Innovation and Analytics are at the heart of delivering Aon's Data & Analytic Services team's mission to: Accelerate the rate of innovation through digital solutions to help better respond to clients evolving needs Provide foundational data and analytics capabilities in one place for 50,000 Aon colleagues and our global clients who use our risk and people solutions Established in 2012, there are over 100 colleagues in Singapore's Centre today including actuaries, software developers, data scientists, financial analysts and accountants. We are expanding rapidly and looking for dedicated individuals who can leverage emerging technologies and collaborate across Aon's solution lines to help clients and colleagues make better, data-driven decisions today and tomorrow. Responsibilities: Be part of a dynamic team focused on healthcare data and innovation initiatives Collaborate with a team of experienced actuaries, data scientists, developers and business experts to deliver on Aon's data, analytics and reporting capabilities Lead the design, build and maintenance of a scalable data pipeline architecture from ingestion to transformation to storage to consumption across various applications Focus on an efficient approach to design, allowing for agile and robust development to meet ever-changing business needs Contribute to the design of our data lake and the development of all data assets and downstream warehouses/marts Partner with data scientists and analysts to ensure efficient and accurate flow of data into downstream analytics tools and reporting solutions Implement automation wherever possible to streamline the process, minimizing the need for manual intervention Assist in database performance tuning and data lifecycle management Develop and integrate an automated data validation and quality control mechanism throughout the pipeline Problem solve complex issues and work to create elegant, simplified solutions Maintain up-to-date technical and operational documentation Requirements: BS or MS degree in Computer Science, Information Technology or equivalent 5+ years in-depth experience in working with a relational database system (eg. MSSQL, PostgreSQL) Highly knowledgeable in ETL/ELT processes, both design and implementation Some experience in a programming language (eg.Python ,C#) Experience building data pipelines that ingests various types of data sources into a database or data lake and populating a structured warehouse or data mart Fluent in both complex SQL query performance tuning and database performance tuning Experience in Apache Spark/Databricks, Python/Pandas, R/Tidyverse and large file processing and analytics data pipelines Knowledge of data security and privacy practices and regulations, including data access controls and de-identification and protection of sensitive data Proven ability to convey technical information successfully to a wide variety of stakeholders Quick and eager student of new technologies as and when they are needed Knowledge of Agile Scrum and Continuous Delivery practices is desirable Experience in BI tools (e.g. Tableau, Power BI) is an advantage Experience working with medical and prescription drug claims data is an advantage How to Apply Your opportunity to empower results could start right here. Make your mark and apply online today with a brief covering letter and your resume, sharing relevant achievements for this position. Please upload your resume in PDF format. We Offer You A competitive total rewards package, continuing education & training, and tremendous potential with a growing worldwide organization. Our Colleague Experience Every day, our colleagues make a difference, work with the best, own their potential, and value one another. Together, we share this one purpose: to empower economic and human possibility around the world. This unifying goal is at the heart of our identity, and it lives in everything we do. To learn more about our colleague experience, visit Aon Colleague Experience. Aon is an equal opportunities employer. We are committed to creating a winning and inclusive culture where everyone feels valued and has opportunities for growth and development. 2504705"
29-Apr-2022 T11:56,Lead Data Engineer,Evolution Recruitment Solutions Pte. Ltd.,7 days ago,,Full–time,"We're Achilles Systems. Be ready to Innovate.

Achilles Systems is a pioneering Business-to-Government (B2G) and Financial Software-as-a-Solution (SaaS) holding company.

We build technology solutions and applications to promote the growth of Southeast Asia's economy.

Since 2015 OnlinePajak, an Achilles Systems company, has created a new technology category by revolutionizing the way companies and individuals process taxes, invoices and payrolls in Indonesia.

With more than 10% of the Indonesian economy using the OnlinePajak platform, the company benefits from a highly valuable partner ecosystem extending across multiple industries and sectors.

Backed by Sequoia Capital, Tencent, Warburg Pincus, VISA, Altos Ventures, Alpha JWC Ventures, Endeavor Catalyst and Global Innovation fund, OnlinePajak is headquartered in Indonesia with presence in Sydney and Singapore.

Work with innovative teams across multiple functions and topics

We are looking for an experienced Lead Data Engineer to join our team. The successful candidate will have strong technical skills and mentor our team of data engineers to grow to be best in class.

You will use various methods to transform and improve our data engineering systems.

To succeed in this position you should have strong programming skills and the ability to combine data from different sources.

Data engineer skills also include familiarity with data engineering related devops and infrastructure, and some knowledge of analytical methods or machine learning methods.

Responsibilities :
• Lead and mentor our data engineers to grow as a high performing team
• Evaluate business needs and objectives
• Work closely with our data science and insights teams
• Build out and advance our data systems and ETL pipeline
• Build algorithms and prototypes
• Maintain the health and growth of our Data infrastructure
• Combine raw information from different sources
• Explore ways to enhance data quality and reliability
• Identify opportunities for data acquisition
• Develop analytical tools and programs
• Collaborate with data scientists and architects on several projects

Qualifications :
• Bachelor's degree in computer science or management information systems and / or 5+ years equivalent work experience.
• Previous experience as a data engineering lead or in a similar role
• Strong programming abilities in one or two languages (such as Scala, Java,Kotlin, Python, Rust, Go, C#, F#)
• You have proven experience with ETL tools and their design.
• Strong SQL ability.
• A strong understanding of functional and distributed systems.
• Experience with no-SQL or graph databases is valued.
• Interested in, or even better, familiar with machine learning is valued but not required.
• A degree in computer science, data science or statistics is valued but not required"
29-Apr-2022 T11:56,"Data Engineer, Capability Development (DART)",GovTech Singapore,2 hours ago,,Full–time,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies.

Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering.

As a Data Engineer in GovTech’s Data Science & Artificial Intelligence Division, you will be involved in architecting, managing and running advanced analytics and data platforms. For this role, you will be part of the Data Analytics Readiness Team (DART) within GovTech, where you will help drive GovTech’s Data Transformation efforts. You will assist to establish and implement effective operational workflows, data pipelines, procedures and best practices in accordance with government infrastructure and security policies to ensure that the platforms are monitored, secure, available and reliable.

We are looking for an independent and motivated engineer who has experience in both small and large-scale analytics platform projects, and in building and maintaining the infrastructure.

What you will be working on:
• Design, Architect, Deploy, and maintain solutions on Microsoft Azure or AWS using different Cloud & Big Data Technologies to provide secure and governed access to data for business users.
• Manage the full life-cycle of a data warehouse/lakehouse solutions from requirement gathering and analysis to platform selection, design of the architecture, and deployment.
• Collaborate with data stewards, data analysts and data scientists to build data pipelines from enterprise systems such as Workday to collect, clean, harmonise, merge and consolidate data sources for data warehouse/lakehouse.

Where we are looking for:
• Diploma/Degree in Computer Science or Information Technology or related disciplines
• Experience with the cloud (e.g. AWS, GCP, Azure)
• Hands-on experience in implementing Data Lake/Data Warehouse with technologies like – Databricks, Azure Synapse Analytics, SQL Database, AWS Lake formation.
• Programming & debugging skills in Python, Scala or R.
• Proficient in SQL.
• Knowledge of both SQL and NoSQL databases.
• Comfortable with DevOps tools like AWS Cloud Formation/Terraform, Docker and Git for CI/CD development.
• Prior experience with data engineering tools and frameworks like Airflow, Kafka, Hadoop, Spark, Kubernetes.
• Familiar in building REST services is good to have.

We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you – so work from home or take a break to exercise if you need to*. We also believe it’s important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round
• Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:56,Lead Data Engineer,Cake DeFi,7 days ago,"$8,667–$15,111 a month",Full–time,"Cake DeFi’s Business Intelligence and Data Engineering team is on a mission to build a robust, scalable, and resilient Data Platform by leveraging efficient tools and technologies, for data producers and consumers to identify everyday opportunities through data-driven innovation.

The team is entrusted to engineer a next generation data platform, which is a once in a lifetime opportunity to develop a modern data platform from scratch. If building world class tools and technologies which can scale to handle large data volumes excites you, then we’d love for you to be a part of our exciting journey. This team will never stop learning, innovating, and expanding so that we can build the latest and best tools and technologies for Cake’s continued success.

About the role

Data Engineers in Cake get to work in a challenging, fast paced and ever-changing environment that will push you to grow and learn. As a Data Engineer, you'll work very closely with the Product/Business and Engineering team to build efficient tools and technologies to innovate on how data is effectively used at Cake. You would empower users to solve/address complex problems using data in a truly self-serve mode.

What you’ll do:
• Design and define data architecture framework, standards and principles, including modelling, metadata and security
• Recommend solutions to improve new and existing data platforms including migration
• Build, deploy and manage big data tools with solid devops functions. Be able to manage CI/CD pipelines
• Streamline data access and security to enable BI analysts and engineers to easily access data whenever they need to
• Developing automation framework using programming languages such as python/scala and automate the data workflows such as ingestion, aggregation, ETL processing etc
• Maintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable and timely delivery of key insights for decision making
• Run Modern high performance analytical databases, with Solid understanding of distributed computing, and be able to build out scalable and reliable ETL pipelines and processes to ingest data from a variety of data sources with high performance analytical databases and computation engines like Spark, Presto and others

What you’ll need:
• A degree or higher in Computer Science, Software Engineering, Information Technology or other related technical disciplines
• 8+ years of experience working in related areas with deep technical abilities in data management products/technologies
• Proficiency in SQL and deep understanding of DWH architecture and data/table formats such as parquet/json/csv in distributed data processing and storage systems
• Good experience in handling large data sets and working with structured, unstructured data, prior experience with Redshift or other MPP systems is a must
• Good experience in building nifty data pipelines using tools such as Glue, AWS Data pipeline or similar
• Knowledgeable on cloud systems like AWS, Azure
• Good experience with programming languages like Python, Scala, Java and scripting languages such as bash
• Excellent communication skills, to act as an effective liaison between platform engineering, DevOps and BI teams

Good to have:
• Deep understanding on databases and best engineering practices - include handling and logging errors, monitoring the system, building fault-tolerant pipelines, understanding how to scale, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipeline

Why work with Cake:
• Do something with purpose; Be a part of the future that will shape on how people are dealing with their finances in Crypto and Blockchain
• Fast moving, challenging and unique business problems
• International, diverse work environment and flat hierarchy
• Great career development opportunities in a growing company
• Competitive salary
• Flexible working hours, unlimited discretionary leave, casual work attire"
29-Apr-2022 T11:56,DATA ENGINEER (2),3I INFOTECH ASIA PACIFIC PTE. LTD.,5 days ago,$3.5K–$8K a month,Contractor,"DATA ENGINEER

Job Description

A ) Understand data management priorities, data assets, data policies and guidelines.

b) Collaborate with a team to design, architect and manage enterprise data architecture, data pipelines and data products.

c) Integrate and collate data sources with data systems, with compliance to data security and organisational governance standards.

d) Build ingestion pipelines to collect, clean, harmonise, merge and consolidate data sources.

e) Support the design and development of APIs to expose data to systems via secure means.

f) Work with data steward, product managers, software engineers, data analysts and data scientists to build scalable data repositories that facilitate insights discovery by officers.

Candidate Specifications

A )Experience in Data Warehouse operations, with good understanding of data warehousing concepts with good understanding of data warehousing concepts

like (Data Models, SCDs, ETL / ELT, and RBACs). Experience with building data pipelines and writing ETL / ELT scripts for data processing

Technical Skillset and Experience

b) Proficiency in SQL scripting for databases like AWS Aurora, Snowflake, and MSSQL.

c) Familiarity in Python programming.

d) Proficiency with data pipelining and scheduling tools like AWS Glue and Microsoft SSIS.

e) Familiarity with DevOPs and CICD tools such as Git, Docker, Terraform is a plus.

f) Able to administrate tools like Tableau Server.

g) Experience with Amazon Cloud services.

3i Infotech Asia Pacific Pte Ltd

Future is of what you make of it today! We are continuously growing and are looking for that persona that wants to grow along. I am looking for a Data Analyst to lead the transformation journey with 3i Infotech Asia Pacific Pte Ltd. If you are that someone and available to join in short notice and looking to bring the change around, pls inbox your profile to

zarina.bevi@3i-infotech.com

Best Regards

Zarina

Hr

Asia Pacific Region

Contact +65 94576290"
29-Apr-2022 T11:57,Data Engineer,HCL Technologies,1 day ago,,Full–time,"We are looking for an experienced Data Engineer to join our growing team of analytics. The candidate will be responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Candidate must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

Responsibilities

· Create and maintain optimal data pipeline.

· Assemble large, complex data sets that meet functional / non-functional business requirements.

· Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing Jobs/code for greater scalability, etc.

· Work with stakeholders including the Product owner, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

· Work with data and analytics experts to strive for greater functionality in our data systems.

Requirements

· Advanced working SQL knowledge and experience working with RDBMS, Hadoop and NoSQL DB.

·Experience building and optimizing ‘big data’ data pipelines, Jobs and data sets.

· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

·Strong analytic skills related to working with structured and unstructured datasets.

· Build processes supporting data transformation, data structures, metadata, dependency and workload management.

·A successful history of manipulating, processing and extracting value from large datasets.

·Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:

· Experience with:

o Big data tools: Hadoop, Spark, Kafka, etc.

o Relational SQL and NoSQL databases, including Postgres and Cassandra.

o Data pipeline and workflow management tools: Airflow, etc.

o AWS cloud services or GCP.

o Stream-processing systems: Spark-Streaming, Flink etc"
29-Apr-2022 T11:57,Data Engineer,Endowus,5 days ago,,Full–time,"Data Engineering Team | Singapore / Remote

About us

Endowus is Asia’s leading fee-only digital wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping all investors grow their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.

Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones. Our mission is clear: help people invest better so they can live easier today, and better tomorrow.

The people you will work with

The team has deep domain knowledge in finance and technology, bringing together decades of experience at Goldman Sachs, Morgan Stanley, UBS, Credit Suisse, The Blackstone Group, AQR, Grab, Dropbox, Lyft, Redmart Carousell, Bytedance, Grab, Kakao, Alibaba, and more. See our leadership team here. We practise inclusion and treasure our diversity in background and experience. A diverse team is our biggest asset and we look for people who share our belief in Endowus' clear mission.‍

Investors, recognition, licensing

Endowus is backed by global leading strategic and venture capital investors including UBS, Samsung Ventures, EDBI, Prosus Ventures, ZVC, Singtel Innov8, Lightspeed Venture Partners, and SoftBank Ventures Asia.

Endowus has been recognised by the industry with the following awards: Singapore’s Rising Star and Fintech Innovation (Asia Asset Management’s Best of the Best Awards 2021), LinkedIn Top Start-ups 2021, WealthTech of the Year (Asia FinTech Awards 2021), and the top 15 Singapore’s Best Workplaces Award (Great Place to Work).

Endowus is licensed by the Monetary Authority of Singapore (MAS).

About this role; responsibilities & ownership
• We are looking for a Data Engineer who thrives in a fast-paced environment and enjoys driving innovation through rapid prototyping and iterative development.
• Being part of Endowus's Technology team, you will build end-to-end product features that you are confident of delighting user experiences.
• You will need to leverage the entire technology stack to realise these goals and will be part of a team that is constantly tackling difficult questions of scale, architecture, and interaction.

Requirements & qualifications
• Bachelors' or above in Computer Science, a related field, or equivalent professional experience
• 3-6 years of experience in designing and implementing key components for highly scalable, distributed data collection and analysis systems built for handling large volumes of data in cloud
• Advanced working SQL knowledge and experience working with relational databases or BigQuery, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimising data pipelines and data sets in Apache Flink or Apache Spark to answer specific business questions.
• Experience in at least one of the JVM languages such as Java, Scala.
• Experience Build processes supporting data transformation, data structures, metadata, dependency and workload management. (experience in Airflow is Plus)
• Experience with distributed databases, such as Cassandra, and the key issues affecting their performance and reliability.
• Hands-on development with key technologies including Scala, Apache Flink and other relevant distributed computing languages, frameworks, and libraries
• Independence and self-reliance while being a proactive team player with excellent communication skills.
• Self-driven, has a strong sense of ownership and able to work and learn independently
• Comfortable with numbers and motivated by steep learning curves

Nice to haves
• Experience with message broker systems, such as Apache Kafka.
• Developed mission-critical products, e.g. trading platforms, medical applications
• Basic knowledge of finance and trading
• Preferably some exposure to CQRS / Event Sourcing patterns
• Familiarity with Docker, Kubernetes and Terraform

Remote Okay
• We are open to hiring remotely in Asia time zones.

Benefits & perks
• Stock options in a fast growing company *
• Employee discount for investing at Endowus
• Flexible working hours and locations so you can live life and your best work - you are trusted to be responsible
• Competitive Staff Benefits; Annual Leave, Medical insurance, Dental *
• Productivity Equipment; Omnidesk Standing Desk & Ergonomic Chairs, Dell UltraSharp USB Type-C Monitor, high-end computer peripherals
• Choice of Apple or PC equipment
• Additional S$250 /HKD 1450 budget for your favourite hardware to boost productivity *

Note: * is only applicable to Full-Time employees

‍

How to apply

Everyone can be an Endowus client. If you are not yet a client, please create an account to understand our services and technology, and can see if our mission is something that aligns with your personal beliefs. We value your feedback on our product and will ask you for your insights throughout the interview process.

To apply email to

Engineering.Careers@Endowus.com
with the following details:
• Email subject: [Role you are applying for], [Your name]
• Attach your CV
• Content portfolio — Original content creation (English and Traditional Chinese), translation (English to Traditional Chinese), any marketing/social media samples
• Cover letter in body of your email
• Desired salary/package
• Notice period (if any)
• Current location
• Visa assistance for role location? (Y/N"
29-Apr-2022 T11:57,Data Engineer (Python/Snowflake),Keyrus Singapore Pte. Ltd.,19 days ago,,Full–time,"Keyrus is an international consulting firm, specializing in the integration of data intelligence and Digital solutions. With over 3000 employees spread across 20 countries, Keyrus continues to deliver on such projects to a wide range of clients from various industries including but not limited to Banking/Finance, Healthcare/pharmaceuticals, FMCG, Oil & Gas, and more. As part of Keyrus solution delivery, we are also in a position to recruit and place technical consultants to complement on existing client projects with their expertise. As such, we seek innovative and agile people to support ambitious and forthcoming technological challenges. The team in Singapore is currently looking for a Data Engineer with strong expertise in handling building pipelines, in order to support our client activities. The high level job scope and skills required are: Responsibilities - Work closely with our client Data Engineering teams to transition their data warehouse onto Snowflake, including leading and conducting a comprehensive UAT for each market - Collaborate with global Engineering team to integrate new data sources or introduce new features or procedures - Develop, maintain, and optimize data pipelines and assemble datasets in the data lake (Snowflake) that enable analysts to build comprehensive analyses easily and effectively - Develop, maintain, and optimize ETL flows and calculated fields in BI platform (Domo) to ensure the right balance of flexibility and performance based on the dashboard use case - Manage data, scripts, and documentation in line with business requirements and compliance rules - Create and maintain alerts that notify the relevant teams of any data issues, as well as to business users if the data is not ready for consumption - Perform root cause analysis on data and processes to identify opportunities for improvement - Build dashboards, perform analyses, and present findings to solve business questions raised by different teams across the organization - Analyze business trends and proactively suggest new ways to better understand the different markets - Adhere to technology standards and consistency Requirements - Bachelor's degree or higher in any field; computer science, analytics, engineering, or mathematics degrees are a plus - Advanced professional expertise in SQL and Python programming - Minimum 4 years work experience in building data pipelines, transforming, and cleaning data - Hands-on experience managing relational databases; experience with Snowflake, AWS, and Informatica is a huge plus - Strong project management and organizational skills - Strong analytical and problem-solving skills - Clear and effective verbal and written communication skills - Strong business acumen and ability to grasp business needs - Strong attention to detail - Self-starter, curious, and eager to try or learn new skills"
29-Apr-2022 T11:57,Staff Data Engineer (Data Product),TECHKNOWLEDGEY PTE. LTD.,5 days ago,$8K–$12K a month,Full–time,"• Development and programming functions to ensure that projects are delivered on time and within budget with good code quality.
• Work with architects, systems analysts, project managers, QA, and other developers to successfully implement business requirements while applying the latest available tools and technology.
• Responsible for the architecture, design, development, implementation of data-based software applications. This includes working with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations.
• Interact with business units to define requirements/modifications and use case to utilize in designing appropriate solutions.
• Provide recommendation on scope and scale of effort required to develop solution.
• Design, develop, document, and implement new programs and subprograms, as well as
• enhancements, modifications, and corrections to existing software - Develop testing and debugging routines.
• Create documentation and procedures for installation and maintenance.
• Build and maintain relationship with global and virtual teams and third parties on software development or support issues.
• Identify opportunities for further enhancements and refinements to standards, best practices, and development methodologies.
• Work directly with Architects, System Analysts, Dev leads, and QA team leads to manage the technical aspects of a development pipeline.

Requirements
• Extensive experience in architecting and developing real-time applications that are fault tolerant, scalable and can handle high volumes.
• Experience in best practices for API development and design patterns.
• Experience in all phases of software development life cycle including project management, functional requirements definition, technical design, development, testing, quality assurance, system certification, systems implementation, and system validation.
• Consistently able to assess and evaluate problems in a production environment and manage risk to the service when recommending change.
• Strong secure coding practices.
• Good Knowledge on Hadoop framework and related Big Data Technologies (HDFS, Map
• Reduce, Spark, HBase, Kafka).
• Strong knowledge in Java or Scala or Python.
• Strong knowledge of database concepts, systems architecture, and data structures is a must.
• Java/J2EE/Angular, Spring Cloud, Microservices and strong knowledge on API development is desired.
• Experience working in an Agile and Test-Driven Development environment.
• Process oriented with strong analytical and problem-solving skills.
• Work independently and mentor others in the team and with minimal supervision.
• Ability to juggle multiple projects and change direction mid-course based on business drivers.
• Ability to work independently in a high throughput environment.
• Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style"
29-Apr-2022 T11:57,Data Engineer,SPH Media,"$6,000–$8,250 a month",,Full–time,"● Work with a mix of cloud infrastructure and open-source technologies to build, scale, and maintain first-party data workloads.
● Implement and manage both streaming and batch data pipelines.
● Drive audience and behavioural analytics, as well as key business metrics with associated reporting.
● Collaborate with both product engineering and data science teams.
● Ensure sound data governance from collection to storage and activation of a broad range of data products.
● Support the business’s goal of democratisation of data.

You may be a good fit if:

● You have at least 3 years of experience in cloud-based data engineering.
● You have built cloud-based scalable and reliable data pipelines at significant scale.
● You have experience in data-related languages and frameworks such as Spark, AirFlow, Python, SQL, Kafka.
● You have experience in operating data lakes on Amazon S3.
● You have experience in operating data warehouses and/or query engines such as Presto.
● You have experience of AWS data and analytics services such as Glue, Kinesis, or QuickSight.
● You have an understanding of data modelling and data governance"
29-Apr-2022 T11:57,Data Engineer,PwC,9 days ago,,Full–time,"Line of Service

Assurance

Industry/Sector

FS X-Sector

Specialism

Risk

Management Level

Senior Associate

Job Description & Summary

We believe that challenges are better solved together. That's why you'll join a diverse, global community of solvers - an unexpected mix of people that come together to build trust in society and solve important problems. With us, you are encouraged to lead with your heart and values, and where your unique skills are developed and put to work in unexpected and exciting ways, superpowered by technology.

Our Risk Assurance Practice provides an invaluable safeguard in today’s complex operating environment with insights and independent assurance. We work with clients to deliver business control to help them to protect and strengthen every aspect of their business from people to performance, systems to strategy, business plans to business resilience. We help clients manage, mitigate and control risks from potential cybersecurity breaches to possible breaks in the supply chain. We assess and prepare businesses by looking into their technology, finance, data analytics, regulatory requirements, data security and privacy, internal audit, and the third parties our clients rely on, to help clients deliver quality results and meet their strategic objectives.

How will you value-add

The Data Trust Services (DTS) team is integrated across different PwC Lines of Services, and works collaboratively with our clients to manage engagements and lead teams of data and analytics resources in all aspects of design and delivery of data and analytics solutions - including Analytics, Data Visualisation, and Data Management.

As a Senior Associate, you'll work as part of a team of problem solvers, and assists clients to leverage data as an asset throughout its business processes. You'll enjoy working in multi-disciplinary teams and gaining valuable experience in various sectors and play a key role in ensuring high quality outputs and service to our clients. PwC Professional skills and responsibilities for this management level include but are not limited to:
• Contribute technically to Data and Analytics projects
• Perform data integration and exploration to assess relevance of data
• Apply data practices in the aspect of data quality, data security
• Communicate effectively with the project manager and team regarding the progress of the project

About you
• Bachelor’s degree and above in Analytics, Information Systems Management, Computer Science or related fields
• 3+ years of experience in data integration strategy, data modeling, designing, building ETLs, data ingestions, and/or transformations
• Experience in working with RDBMS such as DB2, Oracle, Microsoft SQL Server, PostgresSQL, Teradata, etc.
• Experience in data management and integration tool such as Informatica PowerCentre, Oracle Data Integrator, SAP Data Services, Ab Initio, IBM DataStage or Microsoft SSIS
• Process good knowledge and experience in data quality definition, data cleansing and data treatment/profiling process using industry standard tools like Informatica Data Quality, Trillium Software, SAS Data Quality, SAP Data Quality Management, etc. is an advantage
• Good knowledge of data warehouse and data management implementation methodology
• Good knowledge of the Data Management framework, including operating model, data governance, data management, data security, data quality and data architecture
• Experience in Hadoop environment like Cloudera and HortonWorks
• Experience in Cloud technologies including AWS, Azure, Google Cloud
• Experience in API Management software design and build
• Process good knowledge and experience in data visualisation concepts using tools such as Tableau, Microsoft PowerBI, Qlik, etc.
• Experience with Agile Methodology implementation
• Certification in any of database, data integration, data management, visualisation tools and cloud is an added advantage
• Knowledge about the infrastructure paradigms such as OS, network, etc. is an added advantage
• Ability to pick up new tools and able to be independent with minimal guidance from the project leads/managers
• Strong analytical and creative problem-solving capabilities
• Ability to establish personal credibility quickly and demonstrate expertise
• Only shortlisted candidates will be notified due to the high number of applicants for this role . #LI-HJ1

There have been reports of scammers impersonating PwC HR professionals contacting individuals about fraudulent job opportunities using non-PwC domain email addresses and an overseas number. Please note that genuine communications from our HR team will only come from ""@pwc.com"" email address.

Education (if blank, degree and/or field of study not specified)

Degrees/Field of Study required:

Degrees/Field of Study preferred:

Certifications (if blank, certifications not specified)

Required Skills

Optional Skills

Desired Languages (If blank, desired languages not specified)

Travel Requirements

Not Specified

Available for Work Visa Sponsorship?

Yes

Government Clearance Required?

Yes

Job Posting End Date"
29-Apr-2022 T11:57,Data Engineer,Infodrive Solutions Sdn Bhd,Full–time,,,"Strong understanding and hands on experience of Big Data technologies like CDH, Hadoop/HDFS, S3, Colibra, Claudera Workbench, etc Technical proficiency on data mining techniques and performance optimization Adequate knowledge of database systems (RDBMS, MariaDB, SQL, NOSQL) Experience on SAS will be beneficial but not mandatory Handling of reporting packages (Tableau, QlikView) is nice to have AWS experience is nice to have Passion to learn and master diverse new technologies in the open-source community"
29-Apr-2022 T11:57,Senior Data Engineer,Grab,3 days ago,,Full–time,"Job Description:

Get to know the Role

As a data engineer, you will be working on all aspects of data, from platform and infra build out to pipeline engineering and writing tooling/services for augmenting and fronting the core platform.

You will be responsible for building and maintaining the state-of-the-art data lifecycle management platform, including acquisition, storage, processing and consumption channels.

The team works closely with data scientists, product managers, legal, compliance and business stakeholders across the SEA in understanding and tailoring the offerings to their needs.

As a member of the data organisation, you will be an early adopter and contributor to various open source big data technologies and you are encouraged to think out of the box and have fun exploring the latest patterns and designs in the fields of software and data engineering.

The day-to-day activities
• Build and manage the data asset using some of the most scalable and resilient open source big data technologies like Airflow, Spark, DBT, Kafka,Yarn/Kubernetes, ElasticSearch, Presto/Dremio, Visualization layer and more.
• Design and deliver the next-gen data lifecycle management suite of tools/frameworks, including ingestion and consumption on the top of the data lake to support real-time, API-based and serverless use-cases, along with batch (mini/micro) as relevant
• Build and expose metadata catalog for the Data Lake for easy exploration, profiling as well as lineage requirements
• Enable Data Science teams to test and productionize various ML models, including propensity, risk and fraud models to better understand, serve and protect our customers
• Lead and/or participate in technical discussions across the organization through collaboration, including running RFC and architecture review sessions, tech talks on new technologies as well as retrospectives
• Apply core software engineering and design concepts in creating operational as well as strategic technical roadmaps for business problems that are vague/not fully understood
• Obsess over security by ensuring all the components, from a platform, frameworks to the applications are fully secure and are compliant by the group’s infosec policies.

The must haves
• At least 2+ years of relevant experience in developing scalable, secured, fault tolerant, resilient & mission-critical big data platforms.
• Able to maintain and monitor the ecosystem with high availability
• Must have sound understanding for all Big Data components & Administration Fundamentals. Hands-on in building a complete data platform using various open source technologies.
• Must have good fundamental hands-on knowledge of Linux and building a big data stack on top of AWS/Azure using Kubernetes.
• Strong understanding of big data and related technologies like Spark, Presto, Airflow, HDFS Yarn etc.
• Good knowledge of Complex Event Processing (CEP) systems like Spark Streaming, Kafka, Apache Flink, Beam etc.
• Experience with NoSQL databases – KV/Document/Graph and similar
• Proven ability to contribute to the open source community and up-to-date with the latest trends in the big data space.
• Able to drive best practices like CI/CD, containerization, blue-green deployments, 12-factor apps, secrets management etc in the Data ecosystem.
• Able to develop an agile platform with auto scale capability up & down as well vertically and horizontally.
• Must be in a position to create a monitoring ecosystem for all the components in use in the data ecosystem.
• Proficiency in at least one of the programming languages Java, Scala or Python along with a fair understanding of runtime complexities.
• Must have the knowledge to build Data metadata, lineage and discoverability from scratch"
29-Apr-2022 T11:57,Senior Data Engineer,Igloo,Full–time,,,"Job Description

We want to give people the freedom and confidence to pursue what matters to them in life. Because they know they’re covered.

Because people-first. Igloo is a Singapore-headquartered regional insurtech benefiting millions of people across SEA, in countries that count as the most underinsured.

Build the future of insurance with us by doing what you do best. Join us if you desire to create impact and do good. Work with our teams across Singapore, China, Indonesia, Thailand, Philippines and Vietnam.

What you will do :
• Build data driven systems for risk control, fraud detection, recommendation, customer segmentation, adaptive pricing etc.
• Build, validate, test, and deploy models and algorithms.
• Work with backend engineers to architect data storage and processing pipelines.
• Work with product managers to develop new product features based on insights from data.

What you will need :
• Bachelor’s Degree in Computer Science / Mathematics / Statistics, or fields related to big data preferred.
• Experience in big data processing with Python, R or Scala.
• Min. 2 years experience in building data pipelines using distributed processing frameworks (e.g Spark, Hadoop, Kafka) and MPP databases (e.g BigQuery).
• Operational experience with industrialization, orchestration (e.g Kubernetes), containers in cloud (e.g Docker) is a must.
• Knowledge in supervised / unsupervised learning, classification / clustering algorithms, feature engineering / optimization is a plus
• Outstanding analytical and problem-solving skills.
• Self-motivated, innovative, and proactive. Willing to learn new knowledge and explore unfamiliar domains"
29-Apr-2022 T11:57,Data Engineer,KEYSTONE CABLE (S) PTE LTD,5 days ago,$3.5K–$5.8K a month,Full–time,"Job Scope
• To provide the technical drive on the company’s i4.0 digitalization and automation plans.
• To understand departmental digitalization requirements and use programming or software to reduce digital waste.
• Help to drive digitization / digitalization efforts for data analysis on key operational metrics; to increase operational capacity, efficiency, productivity and manpower utilisation.
• To design and build data solutions that support company’s data and analytics strategy in driving business insights.
• To work closely with ERP, MES and other technology vendors to implement and support the company’s implementations.

Requirements
• Respectful
• Good communication skills
• Growth mindset
• Willing to accept new challenges and learn
• Technical requirements: Industrial control and automation, Power and PLC systems
• Academic qualifications in computer science or related fields
• Min programming skills: Python, SQL database
• Understanding of data governance, data security and data analytics"
29-Apr-2022 T11:57,"Associate/AVP, Data Engineer, Research Data Management 12281",GIC,$6K–$12.4K a month,,Full–time,"Data Strategy Group (DSG) harnesses and leverages on GIC’s most valuable asset – Data, to drive insights, results and solve real business challenges. We ingest, analyse, and combine data with context across all industries and asset classes to enable investment decisions and enhance investment process. Together, we strive to build a high-performance enterprise data infrastructure, create results-driven approach with data products and strive to be a data-driven organization.

We are hiring an Associate/AVP, Data Engineer, Research Data Management, this role will play a key role in expanding and optimizing our data and data pipeline architecture, as well as taking care of the operation and requests from cross functional teams.

Join us if you have a passion for data and be part of a team of forward-looking data professionals in building and developing GIC’s data foundation and infrastructure. You can make a difference in our data journey to shape our world of investments

Responsibilities
• Work closely with data analysts and business end-users to implement and support data platforms using best-of-breed technology and methodology.
• Design robust and scalable solutions to meet business needs and take operational considerations into account. Demonstrate technical expertise in the assigned area.
• Analyse, tackle, and resolve day-to-day operational incidents and advisory to business users.
• Analyse systems operations data (service level agreements (SLA), customer satisfaction, delivery quality and team efficiency) to identify actionable trends for continual improvements.
• Conduct requirement workshop with stakeholders and analyse requirements holistically.
• Work closely with data governance team to build the data quality check framework and ensure the data issues are monitored, tracked, and fixed without breaching SLA.
• Design and implement scalable data pipeline modules with reusability in-mind to support the growing demands from business users.

Requirements
• Bachelor’s degree in Computer Science, Computer Engineering or equivalent.
• At least 4 years’ experience of working as a data engineer or backend developer in a big data field.
• Solid working knowledge of implementing the optimal data structures and algorithms to create efficient and scalable applications in Java or Python.
• Familiar with application integration with RDBMS such as Oracle, MS-SQL or MySQL. (Working knowledge of Oracle and MS-SQL will be a plus).
• Hands-on experience of using Linux (or Unix-like OS) as the development environment and familiar with shell scripts and command line tools in Linux/Unix environment.
• Exposure and knowledge in the following technologies is advantageous:
• Big Data Platforms – Hadoop (Spark, Hive, Impala, HDFS), Snowflake
• Programming and Scripting: Java, Python, Shell Script, REST API, Informatica, Node.js
• AWS
• Docker
• Data Virtualisation – Denodo
• Data Visualisation – Tableau
• Experienced with the Systems Development Life Cycle implementation methodology (SDLC) and/or agile methodologies like Scrum and Kanban.
• Understand and apply the good industry practice of code versioning, testing, CICD workflow and code documentation.
• Good team player, with strong analytical skills and enjoy complex problem solving with innovative ideas
• Strong communication/people skills required to interact with data analysts, business end-users and vendors to design and develop solutions
• Good at working with details and is meticulous for operations"
29-Apr-2022 T11:57,Data Engineer,H2i Pte.Ltd.,8 days ago,,Full–time,"Our Data Science Team is looking for a passionate and enthusiastic Data Engineer.

Your Role:
• Work on building robust and scalable data processing pipelines using Python
• Implement and maintain CI/CD pipelines in Git
• Collaborate with Data Scientists to implement data science models
• Keep the Data platform stable and continuously improve the system.
• Design, develop, and implement reusable components.
• Work closely together with DevOps
• Work closely with our business stakeholders and engineers

It will be a good fit if you have:
• 2 to 5 years of related data engineering experience
• Good knowledge of relational database
• Proficient in Python
• Experience with cloud deployments, cloud tooling
• A Bachelor degree in Computer Science / Computer Engineering or related fields

It will be a bonus if you have:
• Good knowledge of NOSQL
• Affinity with geospatial/physical modeling

Location: Singapore

APPLY HERE"
29-Apr-2022 T11:57,Senior Data Engineer,Encounters Pte. Ltd.,19 days ago,,Full–time,"Are you passionate about building great products Do you want to redefine the way travellers explore the world Are you looking to work with one of the coolest technology companies in the region Keen to be part of this growth journey with a bunch of amazing people Then Pelago is the place for you! We are looking for ambitious and motivated talents who are excited about staying on the cutting edge of Technology and always keen on learning new tools and technologies to build a great end-to-end customer experience. WHO ARE WE Pelago is an Itinerary focused Travel Marketplace that makes every aspect of a Trip - from Discovery & Organising to Booking & Experiencing - easy, fun and personalised! Backed by a market leading brand with strong funding, the company is currently in the early startup stage, and well-positioned to disrupt the $250 Billion travel experiences industry. We are a team of diverse, passionate, empowered, inclusive, authentic and open individuals who share the same values and strive towards a common goal! WHAT CAN WE OFFER YOU - A unique opportunity to take end-to-end ownership of building innovative products that deliver real value to travellers. - Platforms to solve real customer problems concerning travel planning & booking with innovative products/services. - An amazing peer group to work with, and ability to learn from the similarly great minds around you. - An opportunity to influence your fellow engineers, and ability to learn from the similarly great minds around you. - A diverse, fun, and dynamic environment with colleagues from different parts of the world. - Competitive compensation and benefits - including work flexibility, insurance, and more! WHAT WILL YOU DO We are looking for an experienced Data Engineer to create and manage the data warehouse for Pelago. You will be responsible for designing data pipelines to collate and store data from multiple sources. This data will serve as the single source of truth for all analytical and engineering needs - generating analytic reports, decision making, data science etc.You will be a part of the wider Engineering team. - Spearhead development of systems, architectures, and platforms built for scale - Build out scalable and reliable ETL pipelines and processes to ingest data from a large number and variety of data sources - Work closely with data scientists to ensure real-time data availability and hygiene as required - Work with product managers and business to facilitate timely data availability to serve analytic dashboards - Act as the owner of data collection, availability and cleanliness to enable accurate data access - Diagnose and solve issues in our existing data pipelines and envision and build for scale WHAT EXPERTISE YOU NEED TO HAVE - A degree or higher in Computer Science, Electronics or Electrical Engineering, Software Engineering, Information Technology or other related technical disciplines - Strong foundation in data query/manipulation using SQL - Deep understanding of databases and best engineering practices - include handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding how to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipeline - Experience working with Event Streaming platforms like Kafka (preferred), Kinesis etc. - Passionate about data, new data technologies, and discovering new and interesting solutions to the company's data needs - Excellent communication skills to communicate with the product development engineers to coordinate development of data pipelines, and or any new features that can be built on top of the results of data analysis - Experience working with Python If you're as excited as we are in this journey, do apply directly with a copy of your full resume and portfolio (if any). We'll reach out to you as soon as we can"
29-Apr-2022 T11:57,Lead Data Engineer,Nas Academy,1 day ago,,Full–time,"Who we are:

Nas Academy is a remote-first, global team of highly motivated life-long learners and content creators coming from the Nas Daily brand.

We are on a mission to empower content creators through technology and transform education in the process. The great irony is that people don’t enjoy education, but they love learning. We believe that creators can fix online learning with better technology and world-class learning experiences. https://nasacademy.com/about-us

Why work with us:
• We are a remote-first company.
• We are a diverse bunch spread across the globe, ranging from India, Berlin, South Africa to Singapore.
• There’s a high chance your current job is boring and that’s why you are looking for something new. We are not boring.
• You love learning and are excited about the prospect of completely remodeling the way we do education.
• We are completely merit based. You’re good at something or have potential and step up to the task - you do it. That also looks good on LinkedIn!
• We offer a substantial share package. If the company is doing well financially, so will you.
• You choose whatever working equipment you want.

What we are looking for:
• Leaders, we give you autonomy to own and deliver projects/features.
• Collaboration, we want people who help their peers become a better version of themselves.
• Minimum 5 years + of Software Engineering experience with at least 2 years experience working with Data.
• 1-2 years experience leading a team
• Excellent English skills, a high energy level and be a great team player.

Responsibilities
• You will collaborate with all internal teams to help them with their data requirements by understanding their use-cases and developing pragmatic solutions.
• Lead all things data for the whole team: backend data sanity, product analytics, Frontend A/B experimentation, business metrics
• Build BI dashboards with tools such as Tableau and internal BI tools for data insights and business support. We are open to using new tools - expertise on different tools is welcome :)
• Analyze and document business processes, and identify areas for improvement and optimisation
• Develop data processing pipelines for data modeling, analysis, and reporting from large and complex transaction datasets
• Candidates with experience in working with SQL, NoSQL (mongoDB) and Python will be preferred.

How we work:

Tired of sitting through endless sprint plannings, backlog grooming sessions and roadmap meetings? We don’t do any of this. That’s right, no Scrum, no Agile Coaching, no goal setting, no estimations. Instead we meet every Friday to discuss what’s going to be important the next week and then we just work on that the next week. That’s it, simple like that.

What we expect from your application:
• We’re a video company - a nice introduction from your side via video is highly recommended to clear the review round
• Answer all the questions in your application!
• Make sure that you have a proper setup for a remote call in terms of internet, audio quality and lighting

What to expect in your interview process:

Culture fit screening: 20min zoom video call with Recruitment

Hackerrank test: Sometimes, we can ask you to do a technical test before or after the culture fit. Should this be the case, keep a lookout in your Spam for an email from Hackerrank - inviting you to complete a technical test

Technical Interview: If you are successful in the culture fit screening/test you will be invited to a technical interview round, which will last approximately 1-1.30hrs

Final interview: If you pass the technical interview, you will soon meet with our CEO, Nuseir Yassin - for about 15 mins via zoom, where we will dive a bit deeper into your motivations for wanting to join Nas Academy + you will get a chance to ask us any questions too"
29-Apr-2022 T11:57,Data Engineer Big Data,Adecco Personnel Pte Ltd,17 days ago,,Full–time,"Job Description
• On boarding of new projects / tenets on kafka
• Own process / guidelines / technical architecture for the service
• Setup, install and operate kafka cluster for enterprise
• Provide guidance to application team for adopting kafka for their use cases
• Provide guidance to the application team for optimal configuration at client side balancing resiliency / availability / durability.
• Own platform level resiliency / availability
• Provide support to SRE team for critical platform incidents
• Implement platform level best practices, vendor recommendation etc
• Continuously Improve, Review, optimize, automate deployment pipeline / process
• Establish self service capability for the platform
• Good communication and stakeholder management skill and able to speak up and freely interact with other stakeholders

Requirements :
• An undergraduate engineering degree or higher
• 5+ year experience in working on IT / software projects / programs i
• 3+ year experience Integrations tools like Kafka with a verygood understanding of overall administration, management, and operation of Kafka cluster supporting multiple tenants.
• Strong Software Engineering skills
• Expert knowledge of the SDLC, Development methodologies, AGILE
• Present facts and recommendations effectively in oral and written form
• Pro-active, independent, resourceful and able to work in a team
• Ability to operate effectively both independently and within a team environment
• A technical mindset with great attention to detail
• Working knowledge on Software Configuration Management, Quality Management, Version Control Management toolset(Github, bitbucket, jetkins etc)

Saghana Sithara Registration Number : R1550224"
29-Apr-2022 T11:57,Data Engineer (Fresher),TONIK FINANCIAL PTE. LTD.,8 days ago,$3K–$3.8K a month,Full–time,"• Act as a subject matter expert in data engineering and GCP data technologies.
• Closely work with various business teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
• Work with Agile and DevOps techniques and implementation approaches in the delivery.
• Be required to showcase your GCP Data engineering experience when communicating with business team on their requirements, turning these into technical data solutions.
• Be required to build and deliver Data solutions using GCP products and offerings.

Qualifications:
• Liaise and be part of our extensive GCP community, contributing in the knowledge exchange learning programme of the platform.
• Any Bachelor Degree in Computer Science or related fields
• Possess analytical skills, mental resilience and the ability to think systematically under stressful conditions.
• Highly accountable and takes ownership. Outstanding work ethic, high-integrity team player, and a lifelong learner.
• Mentor other engineers define our technical culture and help build a fast-growing team.

Skill:
• E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.
• Regulatory and Compliance work in Data Management.
• E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
• Work on diversified cloud Platforms consisting of Databases and CICD, Logging, and monitoring tools to provide stable, and reliable DevOps service.
• Build CI/CD pipeline; both design and implementation is an added advantage"
29-Apr-2022 T11:57,Senior Data Engineer,Produgie,27 days ago,,Full–time,"What You Will Do
• Co-own data structure used by various engineering teams to design models and schemas of the data to be fed into the platform, making sure they can be processed in a consistent and scalable manner
• Design, Build and Manage data pipeline, cloud-based data lake and warehouse systems
• Work with business team and data analysts to understand business requirements and build efficient and scalable data solutions
• Design, build and support new and existing data infrastructure including data models, data pipeline and data analytics
• Driving a data-driven culture by building democratic, self-service data tools with data privacy and governance as primary design goals
• The work is focused on implementing and maintaining data connectors, external data integration and helping building key showcases
• Investigate and research data quality and integrity from data sources
• Develop on no-code/low code platform to provide bridging features for internal ops while waiting for platform development

What We Are Looking For
• At least 2 years of experience designing and implementing real-time data pipelines and databases
• Professional experience with data management and visualization with relational and non-relational databases is a plus
• Hands-on experience in any modern programming language (Python preferred) and SQL
• Experience in some of the data engineering tools such as BigQuery, Spark, Jupyter, Hadoop, Hive, Flink, Storm, Elasticsearch, Redshift, Pandas, or Airflow
• Strong analytical skills; ability to analyze and manipulate raw data, drawing conclusions from insight acquired.
• Experience in building and operating data applications in cloud environments (AWS, Azure or GCP)
• Experience with Infrastructure as code (IaC) tools such as Terraform is a plus
• Able to write and converse in English
• Possess data modeling and schema design skills
• Experience with data visualization tools like Google Analytics, GoodData and Looker

Job Type: Full-time"
29-Apr-2022 T11:57,Network Data Engineer,Anchor Search Group Pte. Ltd.,17 days ago,,Full–time,"Job Description: Manage customer's network infrastructure such as internet links, traffic shapers, routers, and switches Support in daily operations on incident management, problem(s) / issue(s) remediation, and service(s) restoration Fulfilling of service request(s) following the Change Management procedure. Track and assess all announcements and/or advisories (from device principal, customer internal security team, Government IT Security Incident Response (GITSIR) Team, etc. on patches on vulnerabilities, software bugs and firmware upgrades for network devices. Planning and applying of devices security patches and firmware upgrades in accordance with the severity. Preparation of monthly reports on operational issues, link performance, patch status for all network infrastructure equipment. Create and maintain documentations of network configuration, network diagram, mapping, processes, and service records. Any other tasks assigned by the customer. Job Requirement: Relevant Diploma or bachelor's degree in Computer Engineering (or equivalent) Minimum CCNP certification (routing & switching) Min. 3 years of strong experience supporting a campus network infrastructure, with in-depth hands-on experience on network devices such as Cisco Nexus switches, Catalyst switches, ASR Routers, Networking Monitoring Tools, etc. Knowledge on network compliance is an added advantage Excellent problem-solving skills in a multi-tasking, fast-paced and complex work environment. Good communication skills and written skills in English, positive attitude, team player, resourceful and resolve problems independently. Clarice Lim (R1656152) | Anchor Search Group Pte Ltd (17C8528"
29-Apr-2022 T11:57,Senior Data Engineer,Rakuten Viki,18 days ago,"$5,250–$10,500 a month",Full–time,"Based in Singapore, this Senior Engineer, Data role reports into Engineering Manager and will play a critical role in building the pioneer Data Engineering Team at Viki!

About the Data Engineering Team

Viki is establishing a Data Engineering team from the ground up, for the purpose of addressing the business’s growing data needs. This team is going to be responsible for designing and implementing a data architecture that is able to provide reliable data systems and clean data for various stakeholders across Viki including but not limited to
• Data Analysts who need to spend a lot of time finding insights from the data, build reports to track business performance against OKRs,
• Product Managers who need to understand our customers’ behaviors, their journey on our platform, understand customer funnels,
• Marketing teams to be able to build customer segments for marketing campaigns,
• Content Operations to track the performance of our shows across various markets and customer segments,
• CRM team to understand our customer and manage our relationships with them, and so on

Building this overall data architecture includes designing and building the ingestion systems for different data formats (files, databases, events), designing processing pipelines that can scale with data volume, data management strategies (Data Lake, Data Warehouse) that’s optimal for long term storage, queries for reporting / visualization, building APIs as well as ML models on top of and data sharing with third-party applications for both batch and streaming data. While doing so, set up proper data governance practices and policies for data retention, compliance, PII handling, GDPR/PDPA/CCPA handling, among other things.

In addition to this, in the longer term, the team is expected to build abstractions and data models that can enable future needs with building systems for content recommendations, search recommendations, building as well as operationalizing machine learning models for subtitle translations, recommendations, churn prediction and so on.

Key Responsibilities:
• Translating the pipelines into reusable and scalable data pipelines and frameworks for ingestion, processing, storage and consumption
• Perform root cause analysis on internal and external data systems to answer specific business questions, identifying and calling out data and systems issues, and improvements in a timely manner
• Improving and maintaining the existing application & workflows’ correctness, performance, SLAs and architecture’s integrity
• Upholding adherence to the right data engineering practices while building the data systems and pipelines, such as proper automation testing, CI / CD, logging, monitoring and alerting, while highlighting areas of improvements
• Contribute to POCs in evaluating SaaS or PaaS vendors that can solve specific problems in our architecture
• Identifying patterns in code and refactor them into modules that are easy to extend / reuse
• Performing code reviews of the team’s PRs and ensuring high standards of code quality, in addition to ensuring that development guidelines are followed
• Guiding junior members of the team on technically complex aspects of the system, or wherever necessary

Requirements:
• Bachelors or Masters in Computer Science or a related field, or a strong past work experience in building software systems or products
• 4-8 years of experience in developing production critical software, including 3-4 years working on data related systems.
• Strong knowledge of software concepts, design patterns, refactoring and automated testing
• Good judgment and diligence to know what patterns to use, when and where, and are able to confidently hold constructive conversations on it with the team
• Strong communication skills and are able to explain technical and non-technical concepts to the junior members of the team, as well as the peers and managers
• Good hands-on experience building APIs using: Java, Scala, Golang and /or Python, or willingness to pick one of them / Relational and / or NoSQL DBs (Postgresql or Mysql or MongoDB or equivalent) / Caching technologies like Redis or Memcache
• Very strong SQL knowledge and experience working on query optimization, data modeling
• Good experience working with / using one or more of the following: Data Warehousing technologies such as Redshift, BigQuery, Snowflake or other big data storages like CockroachDB, Cloud Spanner, BigTable, etc / Any Data Processing frameworks and technologies such as Spark, Apache Beam, Dataflow, EMR, AWS Glue / Messaging systems such as Kafka, PubSub and Stream processing / Open File Formats such as Parquet, ORC, etc / Building and operating data applications in cloud environments (AWS or GCP)
• 3rd-party solutions and technologies such as Fivetran, Snowplow, Segment, or the likes of it
• Added advantage, if you have knowledge of data infrastructure management and Infra-as-Code (IaC)

Rakuten is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status. Women, minorities, individuals with disabilities and protected veterans are encouraged"
29-Apr-2022 T11:57,Senior Data Engineer,Newtone consulting,15 hours ago,,Full–time,"Job Description & Requirements

The successful candidate will provide data services for CIB as part of the Cybersecurity datalake application. You will be part of the team responsible for governance, quality, remediation and manages services across data standardisation, analytics, archiving, reporting, dashboards and data management and production support.

Role and Responsibilities:
• Troubleshoot Production issues following a ‘Follow The Sun principle’ [APAC/EMEA/NAR] support model
• Monitoring and proactive support
• Application maintenance and upgrades
• Automation of BAU tasks to improve efficiency
• Strong contribution to Data Analytics, including support in the development of custom add-ins for data collection & analysis
• Suggest improvements and provide guidance and support to the IT Security team to help them improve their monitoring & analytics capabilities

Candidate profile:
• 5 years of experience of IT Production and/or BigData
• Practical knowledge of performance and capacity management from a BigData perspective as well as strong aptitude for automation.
• Strong working knowledge of Linux (RedHat/Ubuntu)
• Strong working knowledge of Elastic stack (Elasticsearch / Logstash / Kibana / Beats) including data ingestion, management, monitoring & analytics
• Experience with Kafka (or similar experience in messaging broker software e.g. Rabbit MQ, ActiveMQ)
• Programming skills (Python or Ruby or Java)
• Experience & skills in automation tools (e.g. Ansible) & DevOps pipelines are appreciated"
29-Apr-2022 T11:57,Data Engineer,Vault Dragon Pte. Ltd.,16 days ago,,Full–time,"We believe there is a better way to deliver healthcare in Asia

Together, our passionate and multicultural team aims to transform the regional digital healthcare space to an integrated, seamless and efficient healthcare ecosystem.

Backed by prominent venture capitalists in the region we hope to achieve our simple vision. We want to build Asia's Google Maps for Healthcare Data.

We want to reshape the digital healthcare space by improving facilitation of data flow between the healthcare sector and biomedical sector, and also shifting the patient journey paradigm from a provider-centric model to a more patient-centered system of delivery.

Already in six markets in Asia and expanding rapidly

Our solutions are trusted by healthcare providers and payers in Singapore, China, Indonesia Thailand, Taiwan and Cambodia.

Based on our success in these five countries, we are confident that our solution will be relevant and in demand in other countries within the region as well.

We are currently finalising expansion plans into Malaysia, Vietnam and Brunei.

Our Product Suite

Our healthcare solutions are trusted by more than 650 healthcare providers in Singapore, China, Indonesia and Thailand, most of them using our platform to manage and run their clinics and health centres.

We have also digitised more than 1,20,000 million unique patient records and are the first vertically integrated medical record company in Asia, offering end-to-end medical record solutions for doctors.

From electronic record archival to case note annotation, to a state-of-the-art multi branch Clinic Management Solution, Vault Dragon offers it all.

Join us as we create Asia's Google maps for healthcare data and are looking for a Data Engineer

What we offer you

Apart from a challenging and fast paced startup environment that fuels and accelerates your ambitions as you help Vault Dragon move to the next level, you will also be well rewarded with a decent compensation package as well as a stake in our growth.

Who are you to be suitable

We are looking for an experienced data engineer to join our team. You will use various methods to transform raw data into useful data systems.

To succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources.

Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods.

If you are detail-oriented, with excellent organizational skills and experience in this field, we'd like to hear from you.

Data Engineer Job Responsibilities :
• Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
• Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
• Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
• Writes unit / integration tests, contributes to engineering wiki, and documents work.
• Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
• Works closely with a team of frontend and backend engineers, product managers, and analysts.
• Designs data integrations and data quality framework.
• Designs and evaluates open source and vendor tools for data lineage.
• Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
• Builds algorithms and prototypes
• Combines raw information from different sources
• Explores ways to enhance data quality and reliability
• Identifies opportunities for data acquisition
• Develops analytical tools and programs

Requirements and skills
• Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
• 5+ years of experience in a Data Engineer role or in a similar role
• Technical expertise with data models, data mining, and segmentation techniques
• Knowledge of programming languages (e.g. Java and Python)
• Hands-on experience with SQL database design
• Great numerical and analytical skills

Experience using the following software / tools :
• Experience with big data tools : Hadoop, Spark, Kafka, etc.
• Experience with relational SQL and NoSQL database, specifically MySQL, MongoDB and CouchDB.
• Experience with data pipeline and workflow management tools : Azkaban, Luigi, Airflow, etc.
• Experience with AWS cloud services : EC2, EMR, RDS, Redshift
• Experience with stream-processing systems : Storm, Spark-Streaming, etc.
• Experience with object-oriented / object function scripting languages : NodeJS, Python
• Experience designing, building, and maintaining data processing systems
• Experience working with either a Map Reduce or an MPP system on any size / scale

Remote Working

Medical Benefits allowance

Laptop allowance

Public Holidays as per the Singapore laws but flexible

Details of the core technology of the company including key components of the tech stack

We work with Node.js (Express.js) in the backend with MongoDB and Redis. Deploy docker containers through Kubernetes to AWS and / or GCP.

Front End

WebPack, Vue.js Bootstrap

Backend

Node.js (Express.js), MongoDB, Redis

Infra

AWS, GCP, Aliyun

Tools / Other technologies

Github, Nginx, Jenkins, Ansible, Vagrant, Docker, Kubernetes

Our software / technology development framework

Our development framework is mainly Javascript Stack - Vue.js / Express.js. We follow the agile process. Our sprint is one week so that we have better projection of progress and productivity.

We use Git for version control and our release / merge strategy is based on git flow workflow. Our versioning is done according to Semantic versioning.

Commits are reviewed through pull requests. Upon approving the pull request, a commit or a group of commits are tagged which then triggers our CI / CD pipeline to deploy into various environment through Kubernetes cluster.
• Methodology : SCRUM
• Versioning : Semantic Versioning
• Git Workflow : Git Flow
• Deployment : As suggested by Git flow we have Develop, Staging, and Master branches.

We have local / remote feature branches that are based from develop branch and is reviewed through pull request for staging release, then in staging QA for production release and lastly production QA"
29-Apr-2022 T11:57,Lead Data Engineer,FINDJOBS PTE. LTD.,8 days ago,$9K–$12K a month,Full–time,"RESPONSIBILITIES:

· You will be responsible for end-to-end development of Data Analytics Platform and AI/ML Ops use cases to take our data engineering and analytics team and system into the future. The role must be hands on. The role is for a self-motivated individual with software engineering skills and expertise with Big Data technologies. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, development, and test of new functionality. Candidate must be agile and flexible with changing priorities based on team’s needs.

· Design & Architecture: You help to design and implement an optimal data pipeline architecture for the Equinix analytics system. Identify, design, and implement improvements to automate manual processes, optimize speed and efficiency of data delivery, architecting and designing for high availability, scaling, and reliability. Architect and design infrastructure to facilitate data extraction, transformation, export and query of data from a variety of data sources, both internal and external, both small and large.

· Massive data: You will source / examine, analyze, engineer data pipelines for gigabytes/terabytes of structured and unstructured data with our platform to create value for customers. You will also be working with Enterprise data.

· Pushing the limits: This role will be on the cutting edge of our Data / Machine Learning platform. As we push to solve more of our customer challenges, you will be prototyping new features, tools, and ideas. Innovate at a very fast pace to maintain our competitive edge.

· Distributed Processing Engine: You will be masterfully working on the data platform which involves processing of both unbounded and bounded data sources, perform im-memory computation & transformation.

· Production deployment: You will be responsible for integration and deployment of the Data Ingestion & machine learning pipelines into production where your ideas can come to life.

· Work with stakeholders including the Data Science teams, Business Systems Analysts, and Architecture teams to assist with data platform technical and organizational issues and support the company’s data and analytics needs.

· Educate, train, and mentor members of the Data Engineering and Analytics teams in the design, implementation, and usage of modern data systems

SKILLS REQUIRED:

· Atleast 12+ years of professional software development experience in multiple programming languages, including modern virtual-machine languages such as Java, as well as common scripting and glue languages such as Python and version control (git), with good analytical & debugging skills.

· Big data: You have extensive experience with data analytics and working knowledge of big data infrastructure such as Google Cloud, Big Query, Data Flow, Hadoop Eco System, HDFS, Apache Storm, Apache Spark. You've routinely built data pipelines with gigabytes/terabytes of data and understand the challenges of manipulating such large datasets.

· Cloud Exposure: Strong experience implementing systems and applications using distributed and cloud infrastructure. GCP preferred, but AWS or Azure are also okay.

· Data Modeling: Flair for data, schema, data model, PL/SQL, Star & snowflake schema, how to bring efficiency in data modeling for efficient querying data for analysis, understands criticality TDD and develops data validation techniques. Working knowledge of distributed relational and tabular data stores, message queues, stream processing facilities, and other scalable big-data platform technologies.

· Experience performing root-cause analysis on bugs and performance problems in distributed systems, including network and source-level debugging.

· Real Time Systems: Understands evolution of databases for in-memory, NoSQL & indexing technologies along with experience on real-time & stream processing systems like Google pub/sub, GCP technologies, Kafka, Storm, Spark Streaming.

· Strong design skills: with a proven track record of success on large/highly complex projects preferably in Enterprise Apps and Integration. Advanced, hands-on knowledge of design, implementation, and optimization of big data architectures, pipelines, and data sets.

· Project management: Professional experience in a modern software development life cycle, including requirements gathering, system design, unit and integration testing, continuous integration, and deployment. You demonstrate excellent project and time management skills, exposure to scrum or other agile practices in JIRA.

· Excellent verbal and written communication skills: Must be able to effectively communicate & work with fellow team members and other functional team members to coordinate & meet deliverables"
29-Apr-2022 T11:57,Senior Data Engineer,Fpt Asia Pacific Pte. Ltd.,8 days ago,,Full–time,"We are looking for Senior Data Engineer to work in finance domain. Required key skill: - Python - SQL - SSAS - AWS Redshift (good to have) Support the Portfolio Execution Group (PEG) Technology Team for designing, building and supporting the evolving ecosystem of critical applications for various trading, financing and treasury functions. Responsibilities: Design, build and support a high-performance, high-availability, real-time multi-asset limit reservation/booking system in the trading technology space. Design and develop API to allow connection of the new system with various upstream/downstream systems for data transfer. Responsible for building and maintaining an automated CI/CD environment. Requirements: Possess a degree in Computer Science or related fields At least 4 years of working experience in data engineering on any relevant database technology - Experience in ETL tools, development in Microsoft SQL Server and Microsoft SQL Server Analysis Services (or any similar SQL/OLAP technology) Good communication skills, able to work independently with minimal supervision Good team player as this role will be part of a bigger team Good understanding of data modeling concepts Experience in Python or Microsoft SQL Server Analysis Services is a plus Strong understanding of data modeling concepts and good ability to design various components of data model and data engineering solution Able to guide junior team members, review solution design and perform code review"
29-Apr-2022 T11:57,Data Engineer,BigPay | Challenge Banking,9 days ago,,Full–time,"We are looking for a talented and independently motivated Data Engineer to join our Data Science team. Working directly with the Head of Data Science, you will be working on our core data pipelines (batched and streaming) and

with our cloud infrastructure.

About The Job
• Maintain and extend our existing ETL infrastructure
• Design schemas, ETL processes to pull data from various system and platforms into BigQuery
• Extend or exiting data pipelines automation framework
• Assist in maintaining and extending our existing Data Visualisation systems (built on Google Data Studio)
• Partner with management and operational teams to deep dive on core issues and use our data to find answers
• Building and maintaining data monitoring and analysis systems
• Maintain and monitor key systems metrics via dashboards
• Assist with other efforts as required

To be successful
• Good language and reasoning skills, in particular in English
• Experience in writing production Python code
• SQL (e.g MySQL, PostGreSQL, SQLServer)
• Linux and linux shell scripting
• Google cloud services (BigQuery, Pub/Sub, Cloud DataFlow)
• Developing ML/AI applications
• Git (or any other version control system)
• DevOps (particular cloud deployments) and System Administration
• Java, Rust development (particularly microservices)
• Terraform"
29-Apr-2022 T11:57,Data Engineer Intern,Fairmart,5 days ago,,Internship,"As a Data Engineer Intern, you will
• Be building and maintaining some of the largest retail-focused datasets in the region.
• Work on data visualization and design of complex data models.
• Work with smart, motivated people in a supportive environment.
• Potentially work on integrating datasets with our backend (mostly NodeJS).
• Influence others and build consensus using your strong written and verbal communication skills.
• Enjoy a strong culture of wanting to be better and making it happen.
• Feel empowered to try things out and make a real difference for our teams and our customers.

About you
• Critical thinker and problem-solving skills.
• Experience with Javascript or desire to learn.
• Team player who enjoys working with a small team in a fast-paced environment.
• Sense of ownership and pride in your performance and its impact on the company’s success.
• Are excited about taking a products from conception to execution and are excited about digitising the retail industry.
• Care about writing clean, well-documented code.

Bonus
• BA in Computer Science or similar relevant field.
• Knowledge of data modelling with large and varied data sets
• Experience in automating repetitive processes & machine learning.
• NodeJS, Dynamodb, Hadoop, Grafana, Elasticsearch experience or desire to learn.
• Experience with TDD, BDD and other forms of automated testing.
• Experience with Agile Methodologies such as Scrum or Kanban.

Perks and Benefits
• Exciting opportunities as we take our platform to the global market.
• Refreshingly flexible but professional environment.
• Competitive salary commensurate with experience.
• The chance to join the ship early on and go all the way path to the recognised leader in the industry.
• Great career and development prospects - being able to push the boundaries of what is possible"
29-Apr-2022 T11:57,Data Engineer,Agensi Pekerjaan Spring Professional (Malaysia) Sdn Bhd,16 days ago,,Full–time,"Job Description : About the Client

About the Client

Our client is a global social mediacompany with a presence in multiple countries. They are hiring for a Data Engineer to join the team.

Responsibilities
• Build data pipelines to portray business status, based on a deep understanding of our fast changing business and data-driven approach
• Extract information and signals from a broad range of data and build hierarchies to accomplish analytical and mining goals for 'Packaged Business Capability' such as user-growth, gaming and searching
• Keep improving the integrity of data pipelines to provide a comprehensive data service.

Preferred Qualification
• Bachelor's degree in Computer Science, Statistic, Data Science or a related field
• Skilled in SQL and additional object-oriented programming language (e.g. Scala, Java, or Python)
• Experience in issue tracking and problem solving on data pipelines
• Fast business understanding and collaborative in teamwork.

Bonus

Industry experience working with user growth.

Interested to Apply

If you are keen to hear more about this role, please send your updated word format resume to Felicia Lim at

Personal Registration No. R22105545

EA License No. 09C5803"
29-Apr-2022 T11:57,Data Engineer,ST ENGINEERING UNMANNED & INTEGRATED SYSTEMS PTE. LTD.,7 days ago,$3.6K–$5.5K a month,Full–time,"We are looking for an experienced AI Engineer to join our multidisciplinary team in transforming our MRO business, using deep learning and neuro-linguistic programming (NLP) to help us improve various business outcomes and drive innovation.

Job Responsibilities:
• Perform research and development (R&D) and processes to meet the needs of our AI strategy
• Work with operation teams to identify and prioritize key areas of the business where AI solutions can drive significant business benefit
• Design and develop AI Solutions for MRO business Optimization.
• Work on functional design, process design (including scenario design, flow mapping), prototyping, testing, training, and defining support procedures, working with Group Engineering Centre and Operation Teams.
• Document and articulate solution architecture and lessons learned for each exploration and accelerated incubation

Job Requirements:
• Degree in Computer science, Electrical Engineering / Mechanical Engineering or equivalent
• 2+ years of experience in applying AI to practical and comprehensive technology solutions
• Proficient in C, C++ and Python programming
• Familiar with Windows and Linux programming environments
• Proven experience with ML, deep learning, Tensorflow, NLP
• Strong interpersonal and communication skills.
• Ability to contribute as a team player or independently.
• Ability to demonstrate a high level of initiative and resourcefulness.
• Location: Ang Mo Kio
• Singaporean only"
29-Apr-2022 T11:57,Data Engineer,KKR SINGAPORE PTE. LTD.,2 days ago,,Full–time,"Position Summary

We are looking for a savvy Data Engineer to be based in Hong Kong / Singapore, to join our growing team of data and analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced cloud-based services, data architecture, data engineering, data pipeline development (ETL), and analytical tools. We are seeking an engineer who enjoys optimizing data systems and building enterprise scale data services. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Skill / Experience Required
• Bachelor’s Degree in Computer Science/Engineering or a related discipline.
• 10+ years Development experience
• Experience in Python and related open source modules
• Experience in Python development including Web application frameworks such as Flask / FAST API
• Experience working with RESTful API Services
• Strong database skills with a thorough understanding of relational database and understanding of Object Oriented databases
• Exposure with the AWS Stack / RDS / is preferred
• Knowledge of open source solutions and trending technologies
• Good communication and written skills
• Ability to be self-sufficient and proactive individual contributor
• Exposure to Private/Public Markets

Desirable
• Understanding of Object Oriented Programming and Design Patterns
• Knowledge of web standards, security, accessibility, browser compatibility
• Knowledge of JavaScript, HTML5 and awareness of frameworks such as React.js/Vue.js
• Experience in a Business Intelligence tool e.g. Tableau and Dremio
• Exposure to ML / AI and python libraries (pandas, NumPy, SciPy) and concepts"
29-Apr-2022 T11:57,Senior Data Engineer,Jones Lang Lasalle Technology Services Pte. Ltd.,23 days ago,,Full–time,"We are looking for a Senior Data Engineer: Willing to specialize in data modelling, ETL (Extract Transform Load) development, DWH/data mart implementation and BI solution delivery With strong analytical skills, back-end solution design knowledge BigQuery and Google Cloud Platform knowledge would be considered a plus If this describes you, do not hesitate to apply. In this role you will be focusing on designing and delivering complex back-end solutions through your technical and business expertise. Responsibilities: Working hands-on on developing Business Intelligence solutions for our clients; Collaborating closely with stakeholders to understand their business needs and concepts; Designing solutions that take advantage of all Google Cloud Platform functionalities Designing and developing scripts for ETL batch scheduling, monitoring & automation Helping clients use their data and find opportunities for improvement, spot trends, as well as recognize potential issues and propose solutions; Providing guidance and mentorship to other junior team members during more challenging projects and assignments. Requirements: Speaking fluent English (preferably C1-C2) - we are an international company, and we interact with folks from around the globe daily; Knowledge of Data Warehouse & Dimensional Modelling and experience in ETL Architecture & Frameworks is a must-have Knowledge of querying languages (SQL, Big Query,etc) is a must-have Exposure to GCP Data services & Tools is preferred, but not required Being a creative, attentive to details person that finds well-balanced solutions to everyday problems. We will make sure that we make the best use of your analytical skills in a creative and challenging way. We are looking for someone who takes initiative and stays curious. If you like working on meaningful, innovative projects and are energized by building something from scratch, we will be a good match. Strong communication skills: as you will be working closely with stakeholders from different backgrounds and with different roles and levels of seniority Creative vision, a passion for learning and willingness to adopt and share innovative ideas. What you can expect from us: Working with us will allow you to develop a skillset that is highly desired on the job market - you can become a highly skilled technical specialist with a strong business perspective and the ability to manage clients and other stakeholders. From early on we enable our teammates to step out of their comfort zone and gain experience working directly with project stakeholders. This allows them to quickly understand how the business works. You're more focused on the technical side of things That's great - you will have plenty of opportunities to work with new technologies. This is not your usual corporate job. Apart from the development possibilities it brings, we also value the relationships that it provides. We are a laid-back team that enjoys a good laugh at the job and understand that all of us have their lives beyond work. Apply today"
29-Apr-2022 T11:57,Data Engineer (Open for Fresh Grads),ST Engineering,$2.1K–$3.8K a month,,Full–time,"We are seeking Data Engineers to join our analytics software product, advisory, and delivery teams within the Group Engineering Centre in ST Engineering.

The candidate must be able to work in a fast paced environment, understand the complexities of different customer data sources and infrastructure setup, have a passion for exploring/cleaning/preparing data sets for analytics modelling. He/She will work with Data Scientists, Data Analysts, Data Architects, DevOps Engineers and other internal stakeholders to assist with data related technical issues and support their data pipeline infrastructure requirements.

The ideal candidate will thrive in a work environment that requires strong problem-solving skills, self-motivation, and an aptitude for team collaboration and open communication.

Requirements
• Bachelor’s or Master’s degree in computer science, computer engineering, information systems or related quantitative field.
• Background understanding of Big Data, Data Warehousing Business Intelligence tech & concepts
• Broad knowledge of various aspects of Big Data and Hadoop based technologies
• Understanding of both relational and NoSQL database technologies such as PostgreSQL, Oracle DB, Cassandra, MongoDB, Neo4J etc.
• Good knowledge in programming languages such as Java, Python or Scala on Linux/Windows platforms.
• Familiarity with or willingness to learn Big Data visualization and reporting software.
• Familiarity with or willingness to learn to design ETL/BI solutions.
• Familiarity with or willingness to learn in DevSecOps, DataOps, MLOps
• Familiar with Linux/UNIX system administration
• Willing to provide operational support in delivering Big Data solutions.
• Effective oral and written communication with strong analytical, problem solving, multitasking and project management skills
• Fresh Grads are welcome to apply"
29-Apr-2022 T11:57,Data Engineering Lead (Remote Possible),Glints,7 days ago,,Full–time,"At Glints, we are building the #1 tech-enabled recruitment and career discovery platform in Southeast Asia that helps people and organisations realise their human potential by joining great organisations, learning the right skills and building great teams.

We’re looking for a Data Engineering Lead to join our Data Engineer team, to

improve Data Governance and promote Data Democratization in the company

What You’ll Be Doing
• Identifying organizational needs for Data Engineering and developing a roadmap within the organization
• To promote ownership of data across the organization
• To improve the quality and integrity of Data in Glints
• To improve discoverability of data and encourage usage of data
• To develop security around data usage
• Conveying organization need to Data Engineering Team for implementation of the solution
• Mentor and groom members of the team to be technical leaders; Hire and expand the team if needed

Why You Should Join Us
• Opportunity to determine Data Engineering Roadmap and realise it with a team of enthusiastic engineers

Who We Are Looking For
• Proficient with Python and Scrum Framework
• Experience with leading Data Engineering Team in improving Data Warehouse
• Managerial or Leadership experience in mentoring and grooming Data Engineering Team
• Able to understand the business equation of the companies and identify levers which the Data Engineering Team can pull

Let’s Realise Human Potential.

We have impacted many lives since we were founded in 2013, but there’s still plenty to be done. If you’re ready to grow and make an impact, you’ve come to the right place.

What is Glints?

Glints is an online talent recruitment and career discovery platform with the enduring purpose of contributing to a world where people and organisations can realize their human potential.

Our tech-enabled approach to recruitment and career discovery connects more than 1.5 million candidates to their next dream opportunities, has supported more than 30,000 companies in finding top talents, doubled recruiter efficiency, and has raised more than US$30M+ from best venture investors in Asia to date.

Our agility and firm hold on our core purpose and values have allowed us to remain resilient and thrive through tumultuous times, and we are proud to be recognised by LinkedIn as one of the Top 10 Startups in Singapore in 2020.

Who We Are

At Glints, personal and professional growth are just as important as business growth. That's why we created the Glints Culture Code: #RIIBCOH. It defines our values, guides our decisions and actions, and is what makes us special.

Relentlessly Resourceful: Whatever it takes, just make it happen (ethically)

Integrity: Have courage, be guided by the truth, don’t be afraid

Impact: Missionaries, not mercenaries

Beginners’ Mindset: Stay humble, don’t be attached to ego

Customer Obsessed: Customers First

Ownership: Care intensely about the mission and take responsibility

High Standards: Dream big and deliver epic outcomes fast

Where We Work

Glints operates in multiple locations across Greater Southeast Asia, including Singapore, Indonesia, Vietnam, Taiwan and Malaysia.

Learn more about Glints and our culture at bit.ly/glintsculture,

Or check out our Careers Page at https://glints.com/careers"
29-Apr-2022 T11:57,Data Engineer - Ref: YC,A-IT SOFTWARE SERVICES PTE LTD,10 days ago,$4.5K–$8K a month,Contractor,"The Data Analyst/Engineer will define the enterprise metrics and deliver the capture of correct data in the automation and data analytics initiatives. The Data Analyst/Engineer is responsible to identify data sources, analyse existing data and ensure capture of new data by collaborating with multiple stakeholders and deliver successful implementation of the respective initiative.

Responsibilities

1. Build and maintain data queries and data pipelines using tools such as SQL

2. Build and maintain ETL/ELT job.

3. Build complex robot to read, validate and analyse data.

4. Build sophisticated dashboards and automated reports for core business metrics and performance trends using visualization tools

5. Conduct data mapping based on business requirements by leveraging on tools

6. Working with highly collaborative teams and to build quality solutions.

7. Develop user guides and technical documentation

Requirements

1. Bachelor’s degree in Computer Science, Mathematics or Statistics a related technical field or combination of equivalent practical experience and education.

2. Minimum 3+ years’ experience data mining/ingestion from various sources as a data engineer.

3. Strong programming experience with frameworks including Python or Java and ETL/ELT is essential

4. Proven skills on any Data visualization tool are essential.

5. Data pipeline, relational database development, SQL query, any database procedure and function development.

6. Prior experience with database and model design and segmentation techniques.

7. Proven analytic skills, including mining, evaluation, analysis, and visualization.

8. Technical writing experience in relevant areas, including queries, reports, and presentations.

9. Independent fast learner with keen sense in automation tools and programming languages.

10. Possess good problem-solving skill and can adapt to changes in business requirements.

11. Effectively prioritise and execute tasks in a high-pressure, fast paced, global environment.

12. Strong organisational skills to manage assignments effectively and working within tight deadlines.

13. Strong communications skills to collaborate with developers, QA, project managers and other stakeholders.

14. Good to have experience working in Agile teams using JIRA tool.

15. Able to work independently and as part of a team.

16. Willing to learn new skills, existing and emerging technologies"
29-Apr-2022 T11:57,"Senior Data Engineer (Platform, Customer Facing)",Connect Energy,2 days ago,,Full–time,"Responsibilities
• Conduct data analytics lifecycle discovery workshops with the largest enterprise customers.
• Execute design and implementation of data analytic projects

Requirements
• Design and implement any third party analytics services in a variety of distributed computing, enterprise environments.
• Lead large-scale data warehousing and analytics projects.
• Develop in Java, Python, R, or other high-level languages.
• Work with distributed scalable Big Data storage, processing, and computation, including Hadoop, Spark, etc.
• Familiar with large-scale real-time streaming service such as Kafka, AWS Kinesis, etc.
• Utilize real-time, large-scale data processing engine, like Apache Spark.
• Develop innovative solutions to complex business and technology problems
• Experience of building and migrating data lake in cloud (AWS, Azure, GCP), moving data applications to the cloud, and developing cloud native data applications
• Knowledge of SQL or Hadoop technology (Hive, Pig Impala, Spark SQL, Presto)
• Understanding of database and analytical technologies in the such as MPP and NoSQL databases, Data Warehouse design, BI reporting and Dashboard development"
29-Apr-2022 T11:57,Data Engineer,Refinitiv,"$8,000–$14,333 a month",,Full–time,"As a part of our growing Labs team of data scientists, engineers and UX/UI designers you will be part of our global network of like-minded colleagues in our global sister lab in London.

Our team offers a great flexible working environment, values curiosity and supports an open and learning culture for all levels of experience & seniority.

You should be a master of methodologies to scale and parallelise infrastructure, applications and data processing pipelines. You will also be comfortable with databases, machine learning techniques and tooling, and able to craft efficient ETL tools with ease.

Required Skills
· 2 years+ experience as a Data Engineer or equivalent.
· 2 years+ experience as a Software Developer or equivalent.
· Diploma / Degree in Information Technology / Computer Science / or equivalent experience.
· Strong skills and experience developing in Python, R, C++, SQL, or other relevant languages.
· Experience with databases and data warehousing solutions (e.g. SQL NoSQL, AWS Redshift).
· Experience with distributed processing frameworks l(e.g. Apache Hadoop, Apache Spark).
· Experience with interfacing and retrieving data from various sources (e.g. API’s, FTP, SQS, S3).
· Ability to utilise and if necessary craft efficient ETL tools and incorporate these in to pipelines.
· Experience with CI/CD pipelines utilising infrastructure as code (e.g. Terraform, Cloud Formation, Ansible).
· Experience with administration of Linux operating systems and are comfortable with the CLI and Shell scripts.
· Familiarity with data science tools and techniques (e.g. regression, clustering, NLP, cross-validation).
· Familiarity with alternative data sets (e.g. Image recognition/analysis, OpenCV, geospatial data).
· Critical thinking, value judgment and common sense over process.
· Able to work independently as well as in a team.
· Positive attitude to learning new skills and technologies.
· Strong interpersonal and communication skills"
29-Apr-2022 T11:57,"Lead, Data Engineering",Mediacorp,Full–time,,,"Description

We are looking for a high-performing data engineer with full stack experience creating web-based data applications. You will be a key contributor to the Data Engineering team, primarily by applying and building tools to aggregate data from disparate sources, processing and loading the transformed data to support internal and external constituencies.

You will be required to maintain and enhance our data infrastructure and proprietary analytical solutions. This role is a hands-on engineering position.

Job Responsibilities
• Translate business requirements to responsive functional web solutions. This includes designing and building APIs, front-end interfaces and scalable tools that ingest and transform real-time data using a variety of open-source and proprietary big data technologies.
• Recommend and implement ways to improve data reliability, efficiency and quality
• Work closely with stakeholders to ensure high standards of data governance during implementation
• Serve as technical subject matter expert on the latest big data technologies

Requirements
• Hands-on proven track record in developing products from front-end interface, middle-tier, to backend infrastructure
• 7+ years of superior experience developing commercial web-based applications
• 4+ years of full-stack web development experience in Javascript (node.js and React framework)
• Proficiency in SQL is mandatory
• Excellent scripting knowledge in Python, Shell etc
• Those with strong production experience in Scala or Spark programming languages will be considered favourably
• Relevant experience in web, video, mobile or adtech domain is a definite plus
• Demonstrated clear and thorough analytical thinking
• Good eye for aesthetics and an attention to detail
• A proven team player and contributor who can multi-task and deliver against timelines
• Minimum degree in Computer Science / Engineering, or equivalent"
29-Apr-2022 T11:57,Data Engineer,EMBRIO CONSULTING,2 days ago,,Full–time,"Job Information
• Salary SGD - / Fixed Monthly Salary
• Shift Normal working hours
• No. of Openings 2 openings
• Job Level : Non-Managerial
• Job Experience : 5 Years or more experience
• Job Qualifications ITE / Nitec

Job Description

Duties and Responsibilities
• Design, implement and oversee maintenance of data flow channels and data processing systems that support the collection, storage, batch and real-time processing, and analysis of information from structured and unstructured sources in a scalable, repeatable, and secure manner
• Develop data management standards and define best practices

Qualifications
• At least 5 years of relevant experience in designing and delivering data management and advanced analytics solutions, especially in areas of data warehousing, Big Data storage and analytics
• Knowledge in one or more programming languages such as e.g. Python, R, Java or C#
• In-depth knowledge of the following:

1.Big Data storage and analytics products, frameworks and methodologies (e.g. Apache HDFS, Kafka, Spark, Hive)

2.Relational databases (e.g. Oracle, MS SQL, PostgreSQL, Teradata etc.)

3.Data modeling and repository design (e.g. operational data stores, dimensional data stores, data marts)

4.Data orchestration and ETL (e.g. Informatica Powercenter)

Interested applicants are open to apply through this job ad with your most updated Resume/CV.

EA Reg No: R1988435

EMBRIO CONSULTING PTE. LTD.

EA License Number 10C4154

Job Types: Full-time, Contract, Permanent"
29-Apr-2022 T11:57,DATA ENGINEER,3I INFOTECH ASIA PACIFIC PTE. LTD.,5 days ago,$3K–$8K a month,Contractor,"JOB DESCRIPTION - DATA ENGINEER

1- Work closely with the project team to develop and validate the data engineeringapplications under the guidance of Product Manager

2 Assist with the design and maintenance of data engineering pipelines as well as the associated authorisation that meet business, technical and security requirements, using the established processes and available tools on the Authority’s Enterprise Data Analytics Platform, within the stipulated timeframe and allocated effort

3 Implement the data engineering pipelines in accordance to the design specifications and work closely with the data engineering team to extract, transform and load the required data from the source systems into the data store.

4 Develop the extract, transform and load tasks in Informatica software in accordance to the design specifications.

5 Ensure optimal design of the data engineering pipeline with due consideration for security control measures and system performance

6 Provide and maintain documentations according to the Authority’s Quality Management System (QMS) guidelines

7 Work closely with relevant technical teams (e.g. Enterprise Data Analytics Platform team, etc.) during development, system integration and user acceptance testing,implementing code changes if required

8 Facilitate the conduct of user acceptance tests and conduct end user training sessions

9 Provide on-site support to troubleshoot any problems

Educational Qualification

Relevant educational credentials in a quantitative discipline (e.g. statistics, computer science, data analytics, operations research, or robotics) or equivalent practical experience

a. Minimum two (2) years of experience in designing and developing applications using Informatica.

b. Minimum one (1) year of data manipulation experience in SQL.

c. Experience in dimensional modelling, data warehousing techniques (e.g. data cleansing, merging and other data manipulation tasks) would be advantageous.

d. Knowledge of methodology and techniques in software testing, and techniques in security vulnerability assessment and testing would be advantageous.

f. Possesses good planning and coordination skills with strong ability to work independently, efficiently and as a team, manage timelines and expectations, and is responsible and conscientious in producing high quality deliverables (which mimimally include documentation, presentations, and prototypes).

g. Possesses strong writing, verbal communication and presentation skills.

At least two (2) years hands-on experience in developing data pipelines, and has completed at least one (1) medium scale analytics project implementation.

Preferably have completed at least (1) project using Scrum or equivalent Agile development framework

3i Infotech Asia Pacific Pte Ltd

Future is of what you make of it today! We are continuously growing and are looking for that persona that wants to grow along. I am looking for a Data Analyst to lead the transformation journey with 3i Infotech Asia Pacific Pte Ltd. If you are that someone and available to join in short notice and looking to bring the change around, pls inbox your profile to

zarina.bevi@3i-infotech.com

Best Regards

Zarina

Hr

Asia Pacific Region

Contact +65 94576290"
29-Apr-2022 T11:57,Data Engineer (EduHub),Government Technology Agency of Singapore,Full–time,,,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an outside-in view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications and facilitate collaboration with citizens and businesses to co-develop technologies. Join us as we support Singapore's vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering. About us - Experimental Systems and Technology Lab (ESTL) The Experimental Systems and Technology Lab of GovTech is an engineering team within the Ministry of Education (MOE). We are made up of engineers, user experience designers and education officers. Our team aims to design and develop software applications that help MOE to transform their systems and services through digitalisation and innovation. About the role As a Data Engineer in the Experimental Systems Technology Lab, you will be working closely with the Lead Data Engineer in architecting, designing and building next-generation data warehouse from scratch to galvanise digitalisation in the Ministry of Education, one of the largest ministries in Government. We are looking for enthusiastic and passionate engineers to join us in this journey to make a difference. What you will be working on: Architecting and scaling data analytics infrastructure on cloud environment; finding opportunities to improve and optimize the workloads, processes to ensure that performance levels can support continuous accurate, reliable, and timely delivery of data products Collaborate with product managers, software engineers, data analysts and data scientists to build scalable and data-driven platforms and tools Manage and run production services that provide analytics capabilities to our various data users across the ministry Development of systems, architectures, and platforms that can scale large volume and variety What we are looking for: Bachelor's Degree in Data Engineering, Computer Science, Information Technology or a related discipline Background or strong interest in data science, analytics or engineering Aptitude and attitude to learn Experience in Tableau will be an advantage We are an equal opportunity employer and value diversity at our company as we believe that diversity is meaningful to innovation. Our employee benefits are based on a total rewards approach, offering a holistic and market-competitive suite of perks. This includes generous leave benefits to meet your work-life needs. We trust that you will get the job done wherever you are, and whatever works best for you so work from home or take a break to exercise if you need to*. We also believe it's important for you to keep honing your craft in the constantly-evolving tech landscape, so we provide and support a plethora of in-house and external learning and development opportunities all year round. *Subject to the nature of your job role that might require you to be onsite during fixed hours"
29-Apr-2022 T11:57,Senior Data Engineer,Quantexa,4 days ago,,Full–time,"Description

Founded in 2016 with only a handful of individuals, Quantexa was built with a purpose that through a greater understanding of context, better decisions can be made. 6 years, 10 locations and 500+ employees later we still believe that today. We connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data.

Due to the continuous success and high demand from our customers, we are looking for a Senior Data Engineer with a proven track record to join the Quantexa family. 🚀

What does a Senior Data Engineer role at Quantexa look like?

In order to be a successful data Engineer at Quantexa, you’ll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.

Being Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. You’ll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service. 🥇

We want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala, Data Fusion and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but don’t worry If that’s not your strongest language or if you haven’t used it before, we make sure that every Quantexan goes through our training academy so they’re comfortable and confident with using our platform.

Requirements
• We’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.
• The desire to learn and code in Scala
• Experience in working in an Agile environment
• Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.
• A strong coding background in either Java, Python or Scala
• Experience of building data processing pipelines for use in production “hands off” batch systems, including either traditional ETL pipelines and/or analytics pipelines.
• Passion and drive to grow within one of the UK’s fastest growing scale-ups.
• Consulting or business facing skills and a desire to work with customers.

Benefits

Why join Quantexa?

We know that just having an excellent glass door rating isn’t enough, so we’ve put together a competitive package as a way of saying thank you for all your hard work and dedication.

We Offer
• Competitive salary 💰
• Company bonus
• Private healthcare with Sigma plus life insurance and critical illness
• Free Calm App Subscription #1 app for meditation, relaxation and sleep 🧘♀️
• Annual leave, national holidays + birthday off! 🌴
• Ongoing personal development
• Great WeWork Office Space & Company wide socials"
29-Apr-2022 T11:57,Data Engineer,Azendian Solutions,30 days ago,"$4,000–$7,333 a month",Full–time,"We are looking for a qualified Data Engineer who will be part of the data engineering team. The ideal candidate will design and develop high quality data products - data warehouse, data marts, data lake data hubs and dashboards; either on cloud or on-premises system environments.

Responsibilities:
• Build and support the data pipeline and all the associated Software Engineering infrastructure tasks.
• Liaise with clients, technical architects, data architects, data scientists and BI analysts to gather the requirements.
• Analyse the data requirements, prepare the functional and non-functional specifications for the data products.
• Develop the data pipeline using either ETL/ELT approach to load or synchronise data in near real time or batch mode.
• Analyse and interpret data into business insights.
• Design and develop the dashboards and visualizations using the BI tools.
• Ability to understand and convert the business logics into SQL queries and validate it against the data.
• Prepare the Test Plans and conduct Unit Testing, System Integration Test, User Acceptance Test and Performance Test.
• Implement the deployment approaches and methods to roll-out the system changes.
• Stay up to date with industry standards and technological advancements that will improve the quality of the data products.

Requirements:
• A keen learner with a minimum bachelor’s degree in Computer Science or related fields.
• At least 1 year of end-to-end data warehouse, data lake and big data implementation experience.
• Proficient with at least one or more of the following technologies: Informatica ETL Tools, SSIS, SQL Skills, SQL Server, Azure Data Factory, Data Warehouse, ETL Framework.
• Preferably possess a good knowledge about: Power BI, Qliksense, MicroStrategy, SAS and Tableau.
• Preferably possess a good knowledge about: DataStage, Attunity, Hadoop Ecosystem, Data APIs, Unstructured Data and Data Modelling.
• Able to work under pressure when there is an escalated demand in the project cycle.
• Understand the solution requirements and progress to develop, build and operationalise the solutions.
• Enjoy problem solving in different domains and industries.
• Love working with a highly energetic and competent team.
• A self-starter with an analytical approach to problem solving.
• A client-centric, outcome driven and quality focused team player"
29-Apr-2022 T11:57,Senior Platforms and Data Engineer,TechBridge Market,10 days ago,,Full–time,"Job Overview

Reporting to the Director of Platforms and Data Engineering, the Senior Platforms and Data Engineer will work closely with Data Scientists, Threat Researchers/Analysts, and Infrastructure. Engineers to develop and manage high-performance analytics solutions. The incumbent will be

accountable for the design, development, deployment, and maintenance of big data platforms as well as their data processing workflows.

Duties and Responsibilities
• Familiarize with the company’s business domain and objectives to develop and deploy big data analytics applications that meet internal business requirements and the needs of partners and customers.
• Lead the design, development, testing, deployment of efficient and reliable big data processing workflows that follow secure SDLC practices.
• Design, develop, manage data warehouse architecture and relational databases.
• Provide monitoring, maintenance, and support for system operations as part of M&S as required in commercial projects.
• Embrace the challenge of dealing with terabytes to petabytes of data on a daily basis.
• Manage different experimentation, development, staging, production environments to provide overall system functionality, health, scalability, resiliency, and security.
• Responsible for implementing and maintaining complex big data projects with a focus on collecting, parsing, managing, and analyzing large sets of data to turn information into insights using multiple platforms.
• Deliver detailed documentation and ensure quality throughout the project lifecycle.

Requirements
• Bachelor’s Degree in Computer Science/Information Systems/Computer Engineering or equivalent.
• Minimum 5 years of experience working on big data (e.g. Hadoop, Apache Spark, MPP DBs).
• Good in-depth knowledge of the Hadoop ecosystem (e.g. HDFS, Impala, Kafka, Spark, NiFi, Elasticsearch, etc.), associated tools, and cloud-based technologies (e.g. EMR, Redshift, S3, etc.).
• Extensive experience in programming (PySpark, Scala,) for data.
• Understanding of modern software engineering tools such as Git, Bitbucket, Jenkins, Maven.
• Highly proficient at reading, profiling, parsing, transforming, cleansing, and integrating data from various sources (structured, semi-structured, and unstructured).

Preferred Skills/Qualities
• Knowledge in Agile and desirable.
• Comfort and experience working in a Linux environment.
• Aptitude for automation and software profiling.
• Experience in the Cybersecurity/Telco industry will be an advantage.
• Proven ability to handle multiple projects concurrently.
• Detail-oriented, solution-focused, and problem solver"
29-Apr-2022 T11:57,Data Engineer,PERCEPT SOLUTIONS PTE. LTD.,22 days ago,$4K–$6.5K a month,Full–time,"We are looking for an IT professional who has gathered some years of working experience in Business Intelligence, Data Analytics, Report Development, SQL and DWH design.

Role Responsibilities:
• Use SQL and/or programming language and tools to perform tasks such as data discovery, data QA, and data preparation based on requirements provided by the business.
• Build data assets in AWS and aligning the data architecture based on business requirements and working towards a common data model.
• Support the Business Analyst in translating business requirements to technical requirements
• Support Data Analytics Projects & Initiatives, like Campaign Automation with AI
• Constantly searching for optimizations, improvements and innovations (e.g. predictive AI)

Skills and Competencies:
• Data Engineering & Data Analytics Expert, DWH Design, SQL, ETL, AWS, Tableau, QliSense, Power BI
• Independent and reliable working style as well as enjoying team collaboration
• Good understanding about Agile software development methods and IT security is a plus
• Business Knowledge of Captive or Financial Services, Banking is a big plus

To apply please click the Apply button or send us your updated profile to recruit@percept-solutions.com

EA Licence No.:18S9405 / EA Reg. No.:R1330864

Percept Solutions is undergoing a growth phase and are on the lookout for talent. Applicants are encouraged to follow Percept Solutions on LinkedIn @ https://www.linkedin.com/company/percept-solutions/ to stay up to date on our upcoming roles and events"
29-Apr-2022 T11:57,Senior Data Engineer,Singtel Group,Full–time,,,"DataSpark was created from a vision to transform Singtel’s rich and unique repository of data into business value and social impact.

Our data products and services provide powerful insights and advanced analytics capabilities to businesses, government agencies, and other telecommunication companies.

We strive for our analytics to be trustworthy and relevant to our clients while adhering to high standards of data privacy.

We are looking for an experienced Data Engineer to join us to build robust data platforms and applications that incorporate data science and machine learning algorithms and models that solve problems in telco network management, transportation, urban planning, real time crowd management, and retail intelligence, to name a few.

This is a great opportunity to apply your established expertise in data science and machines learning.

At Dataspark, you get to work with rich and diverse datasets, cutting edge technology, and you get to see the impact of your results in real business and government decisions, which in turn provide positive social benefit for consumers at a large scale.

As a startup that is part of Singtel, DataSpark provides an enviable work environment with spirited trailblazing and industrial countenance.

Working alongside creative, energetic and passionate teammates from around the world, you get to be a part of our exciting growth journey as we build the company to the next level.

Responsibilities
• design and implement scalable and robust software platform for ingesting and transforming telco network datasets in (near) real-time using a variety of open-source and proprietary Big Data technologies
• recommend and implement ways to improve data reliability, efficiency and quality
• collaborate with product management, sales and marketing, and solution delivery teams to support the objectives that customer requirements are well managed and reflected in product releases
• support the deployment of DataSpark software within clients' IT environment
• working closely with stakeholders to ensure high standards of data governance during implementation
• serve as technical subject matter expert in latest big data technologies

Requirements
• 7+ years of superior software development experience building commercial large-scale software systems and database systems
• Excellence in algorithms, data structure, discrete math, data base and data warehousing
• Expert knowledge in data management technologies and software engineering tools to efficiently process large volume of data
• Demonstrated clear and thorough logical and analytical thinking, as well as problem solving skills
• Experience of data warehouses in excess of 10TB
• Experience of Web UI, middle tier, and data back end development
• Good understanding of Telco data models, knowledge about telco network capabilities a plus
• Expert knowledge in SQL and Relational Database Management System
• Strong experience in big data processing stack on Hadoop, HDFS, HBase, Hive, Spark
• Strong experience in Java / Scala, Python, Amazon AWS
• Superior and proactive communications skills, including verbal, written, and presentation.
• A proven team player and contributor.
• Self-directed, ability to work independently and research innovative solutions to business problems
• Aptitude of working on multiple projects in parallel
• Attention to details and data accuracy
• MS or BS degree in Computer Science / Engineering, Statistics, Mathematics, or equivalent is required for this position"
29-Apr-2022 T11:57,Data Engineer,dentsu international,18 days ago,,Full–time,"The role is responsible for supporting the decision making based on data analysis and the development of business insights from the various data assets. The individual within this role will work closely with the team of the business and will be responsible

iProspect helps our clients achieve transformative change in the digital economy. Gleaning insight from data is fundamental to that change. We are looking for data engineer to help grow our technical specialism.

A balance of business and technical skill, Data engineer guide how we manage data to achieve business needs and objective.

Your technical skills make you an aspiring data engineer, versed in data acquisition, manipulation and analyses. Your commercial interest ensures that you would like to apply those skills to client business challenges and see the results; not be stuck in the background. You have the ability to (re)design a process as well as distill complexity into a simple message.

You will join a distinctly ambitious team. We ask a lot, but in return you will build excellent career foundations within an industry at the heart of the digital revolution.

Detailed Description

The key elements of the role are;
• Influence stakeholders, using your analytical experience and technical specialism
• Assist on multiple projects across a portfolio of clients
• Build and improve existing analytical solutions
• Gain practical experience across a range of technologies, including cloud platforms, Ad/MarTech stacks, customer data platforms, verification technology, visualisation tools and more
• Oversee data management and governance
• Identify weaknesses in processes and reengineer them
• Partner closely with the wider agency; working alongside iProspect and other Dentsu teams to apply data to business decisions
Essential Qualities
• Strong logic & quantitative skills, including analytical abilities and mathematical proficiency
• Proficient coding; experience in SQL, R, Python, Go, JavaScript are all relevant
• Experience with cloud service providers (Google Cloud, AWS, Azure)
• Knowledge of analytical modelling; able to determine why a number is right or wrong.
• Self-motivated & articulate in explaining technical concepts to a less specialist audience
• Great interpersonal skills; ability to build and maintain strong working relationships internally and externally
• Strong Microsoft Office skills, notably;
• Especially strong in Excel – data wrangling, pivot tables, macros etc
• Strong in PowerPoint – data presentation & story telling
• Remain calm when faced with multiple tasks; able to prioritise and deliver on time
• Willingness to ‘pick up and run’ with projects when necessary; own deliverables and see them to completion
• Comfortable working & learning independently, as well as in a team
• Understand how we add value to our clients’ business
• Fluency in English (written and oral)
Additional Skills
• Knowledge of digital/marketing fields e.g. SEO, PPC, programmatic and social
• Experience with digital tracking technologies
• Experience with cloud service providers (AWS, Azure, Google Cloud)
• Experience with container orchestration (Docker, Kubernetes)
• Experience with infrastructure-as-code tools (Terraform, Pulumi)
• Experience with data visualisation tools (Datorama, Qlikview, Tableau, Data Studio)
• Experience with machine learning tools
• Experience with consuming APIs
• Experience in additional coding languages"
29-Apr-2022 T11:57,Data Engineer,PropertyGuru,15 days ago,,Full–time,"Department: Technology

Make A Real Difference At PropertyGuru.

Real Aspirations. Real People. Real impact.

At PropertyGuru Group, we believe that every person – no matter what their circumstance – should have a place to call home. That’s why we’ve been on a mission to transform how people find, finance, and own home across Southeast Asia over the last 13 years.

Voted by property seekers as “Asia’s Most Influential Brand for Online Property Search”, PropertyGuru enables real-world aspirations through digital transformation and constant innovation. Every day, the work that we do has a real and positive impact on thousands of lives.

As an employee, you’ll be empowered by our community work culture, where everyone has the autonomy, support, and resources to do the best work of their careers. As we evolve our journey to help people make confident property decisions, we stay true to our core values to guide the way we work and the decisions we make every step of the way: we own it and deliver it, we have fun and celebrate success, we respect and care for each other, we push beyond good, and we create what’s next.

PropertyGuru is constantly bridging out into new customers’ experience and businesses across all our markets. One of the core roles will be the Data Engineer role for DataSense fully owned subsidiary of PropertyGuru Group

Your work will be to design, develop, support and maintain our existing infrastructure and democratizing data access within and outside the company. Your work will support the following areas:
• Real time streaming infrastructure:
• Enable teams to move quickly and get accurate information to the right people with minimum delay would be the key focus of the data engineering team
• Advance our streaming platform which allows easy development of the streaming applications
• Interactive Data Analytics:
• Query the data and compute the aggregates on various dimensions to support the various decisions made on the data and machine learning products that are built around it
• Infrastructure management:
• Help manage multiple terabyte-scale clusters, easy-to-use systems to handle security and replication are in development
• Data workflow management:
• Use Airflow and Azkaban to schedule data related workflows supporting various analytical and machine learning workloads
• Machine learning infrastructure:
• Develop the end-to-end platform that will allow us to develop and deploy various machine learning models into the PropertyGuru websites and apps with ease
• Data engineers play a very big role in this platform development and has the potential to significantly cut down the development time of the machine learning models

Requirements
• 5+ years of industry experience in working with terabyte scale datasets
• Working knowledge of relational databases and query authoring (SQL)
• Experience with data workflow management tools Azkaban, Airflow
• Ability to write high performance quality code
• Experience in Python is a must. Other equivalent languages like C++, Java, Go, Scala is a plus.
• Experience with open source technologies like Kafka, Presto and Spark would be a plus
• Awareness of various cloud-based solutions such as AWS Redshift, Google Big Query, Qubole is a plus

Qualification:
• Bachelor’s degree in IT or relevant field. Alternatively, lesser qualifications with strong experience in machine learning will also be considered

PropertyGuru Group is an equal opportunity employer committed to fostering an inclusive, innovative and learning environment with the best employees. Therefore, we provide employment opportunities without regard to gender, identity, race, religion, nationality, age, marital status, disability, or any other protected status, per applicable law. If there is anything we can do to help ensure you have a comfortable and positive interview experience, please let us know.

For a full listing of our jobs, visit https://careers.propertygurugroup.com

Advertised: 15 Jul 2021 Singapore Standard Time

Applications close"
29-Apr-2022 T11:57,Data Engineer,Rakuten Asia Pte Ltd,3 days ago,,Full–time,"Rakuten Group, Inc is a global Internet and e-commerce company, with over 1.3 billion registered users worldwide. Our e-commerce platform is the largest of its kind in Japan and among the world's largest by sales. Rakuten has many subsidiaries, including well-known brands like Rakuten VIKI, Rakuten TV, Rakuten Kobo and Rakuten Viber.

About the Team:

The Global Data Supervisory Department (aka GDSD) oversees the development and operation of our data platform in Rakuten group. We provide data products and platforms for Rakuten’s line of businesses, as well as technology solutions.

GDSD services are important to Rakuten as it has big contribution to Rakuten's profit and being one of the key drivers for growing various platform businesses of Rakuten Group including our e-commerce platform business.

We are looking for Data Engineer, who is passionate to deal with complex technical challenges and who are interested to work closely in the fast pacing business environment.

Key Responsibilities:
• Analyze and understand business requirements to envision best fit analytical solution
• Collaborate with stakeholders, business development team, architect, project manager and data engineers to understand business requirement & questions, processes, and related data, and convert the information into measurable technical design & deliverables and achieving tangible business goals
• Develop analytics and visualization solution that meet criteria for business value and reusability
• Good oral and written presentation skills, produce useful documentation, organizing & participate in workshops, and brownbag sessions to promote product adoption & utilization, provide the essential support to the stakeholders for the products user journey, and a good story-telling skills
• Passionate about emerging technologies and early adoption
• Assist in other ad-hoc analytics projects

Essential Competencies :
• Graduate degree educated in computer science or a relevant subject.
• 3+ years of experience in developing & delivering analytics solutions, Data Warehousing, Data Platforms, ETL pipelines (both batch and streaming), and data modeling experience is advantage
• 3+ years of solid experience in any data visualization tool (preferably google data studio)
• 2+ years of experience in developing, monitoring, and optimizing data platform and pipelines using public cloud services (preferably gcp or aws big data services)
• 2+ years of solid experience in Big Data Technologies - Apache spark, Airflow, Apache Beam, Kafka, HDFS, HIVE, and/or big data services in any public cloud (preferably gcp or aws)
• 2+ years of experience in writing and optimizing code using SQL, python(pySpark), java
• Experience in working agile environment and using tools like JIRA (epics, stories, subtasks)
• knowledge of building APIs (REST), Serverless (functions) and NoSQL DBs is advantage
• knowledge of docker and Kubernetes (preferably gcp GKE) is advantage

Rakuten is an equal opportunities employer and welcomes applications regardless of sex, marital status, ethnic origin, sexual orientation, religious belief or age"
29-Apr-2022 T11:57,Data Engineer,TECH MAHINDRA LIMITED (SINGAPORE BRANCH),8 days ago,$6K–$12K a month,Full–time,"• Should have good knowledge and working experience in: Database Teradata(SQL, BTEQ scripting) and Hadoop (Hive, Impala, Kudu).
• Good to have experience in working with No SQL as well as virtualized Database Environment.
• FSLDM and Finance industry experience
• How BI tools integrate with Data Mart and Data Lake (Qlik Sense, Power BI)
• Scripting using (Shell script, awk programming, quick automation to integrating any third party tools), BMC monitoring tools
• Using CI:CD tools (Bitbucket, Github), quick automation to integrating with any third party tools for automated deployment
• Good understanding in
• Hadoop, In memory, No SQL
• Data Modeling using industry standard data model (FSLDM)
• Automated testing using industry standard testing tool
• Mandatory experience in developing banking application using ETL, Hadoop and Teradata
• In-depth knowledge of technology stack at global banks
• Flexibility to stretch and take on challenges, Communication & Interpersonal skills
• Attitude to learn and execute"
